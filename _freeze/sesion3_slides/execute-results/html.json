{
  "hash": "43ba3698f831f48e7440402c150f043e",
  "result": {
    "markdown": "---\ninstitute: Mag铆ster en Data Science - Universidad del Desarrollo\nsubtitle: 'Curso: An谩lisis de datos'\ntitle: 'Sesi贸n 3: Introducci贸n al an谩lisis de regresi贸n'\nauthor: 'Phd (c) Melanie Oyarz煤n - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'\nformat:\n  revealjs:\n    logo: logo_udd.png\n    footer: Curso An谩lisis de Datos - Sesi贸n 3\n    transition: fade\n    background-transition: fade\n    fontsize: 1.8em\n    theme:\n      - simple\n      - custom.scss\n    chalkboard:\n      theme: whiteboard\n      boardmarker-width: 5\n      buttons: true\n    progress: true\n    incremental: true\n    scrollable: true\ncode-link: true\neditor:\n  markdown:\n    wrap: 100\necho: true\noutput-location: fragment\nexecute:\n  freeze: auto\ntoc-depth: 2\ndf-print: paged\n---\n\n<style>\n  table {\n    font-size: 0.7em; /* Reducir el tama帽o de fuente en las tablas al 70% del tama帽o base */\n  }\n</style>\n\n\n## El an谩lisis de regresi贸n\n\n- En las aplicaciones de la ciencia de datos, es muy com煤n estar interesado en la relaci贸n entre dos o m谩s variables.\n  \n- El an谩lisis de regresi贸n es una t茅cnica en la cual buscamos encontrar una funci贸n que pueda describir la relaci贸n observada en los datos entre dos o mas variables.\n- Por ejemplo, podr铆amos querer relacionar los pesos de los individuos con sus alturas...\n  -  驴son los m谩s altos, m谩s pesados?\n  -  y... 驴cu谩nto m谩s pesados?\n\n## El an谩lisis de regresi贸n: Regresi贸n Simple\n\n- Caso m谩s sencillo: univariada o **regresi贸n lineal simple**. \n  - Una variable que deseamos explicar o predecir (Y) como funci贸n de otra (X).\n  - Buscamos la pendiente e intercepto de una funci贸nla recta de la forma:\n\n. . .\n\n  $$Y = \\alpha + \\beta X$$\n  \n  donde:\n\n  - Y es la variale dependiente o que deseamos entender\n  - X es la variable independiente\n  - $\\beta$ es la pendiente de la recta\n  - $\\alpha$ es la constante o intersecci贸n (el valor de y cuando x=0)\n\n## El an谩lisis de regresi贸n\n\nBuscamos los **coeficientes** de la funci贸n entre Y y X: **constante** y **pendiente**\n\n\n```{=html}\n<img src=\"./img/img_sesion3/gif_regresion2.gif\"  style=\"display: block; margin: 0 auto;\">\n```\n\n\n## El an谩lisis de regresi贸n\n\nPara esto, pensamos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: \n\n- una que es **sistem谩tica** o que se puede explicar directamente con una o m谩s variables independientes (Xs o regresores) \n\n- y otra que es **no sistem谩tica** o error ($\\mu$ o $epsilon$) , que es aquella parte que no se puede explicar  y representa a la aleatoriedad del fen贸meno.\n\n. . .\n\n\n```{=html}\n<img src=\"./img/img_sesion3/gif_regresion1.gif\" style=\"display: block; margin: 0 auto;\">\n\n```\n\n## El an谩lisis de regresi贸n\n\n\n```{=html}\n<img src=\"./img/img_sesion3/gif_regresion1.gif\" width=\"600\" style=\"display: block; margin: 0 auto;\">\n\n```\n\n. . . \n\nLa parte sistem谩tica entonces la describimos con una **forma funcional**, que depende de otras variables o regresores. \n\n. . .\n\nEsta forma funcional puede:\n\n- ser lineal univariada,\n-  lineal m煤ltiple o \n-  no lineal. \n\n. . .\nEl tipo de forma funcional, definir谩 el tipo de regresi贸n de la que estemos hablando.\n\n## El an谩lisis de regresi贸n\n\nVentajas del an谩lisis de regersi贸n: es facil decsribir cuantitaivamente una rlaci贸n.\n\nEsquem谩ticamente, los elementos son:\n\n![](img/img_sesion3/regresion_esquema.png)\n\n## 驴Para qu茅 hacer regresiones?\n\nPodemos pensar en tres uso, al menos, del an谩lisis d eregresi贸n:\n\n- Describir cuantitativamente una relaci贸n emp铆rica\n- Probar hip贸tesis sobre ciertas teor铆as\n- Realizar predicciones \n\n\n## Regresi贸n simple y scatterplot\n\n- Por ejemplo, pensemos en la relaci贸n entre los a帽os de educaci贸n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom铆a.\n\n- Usemos un subconjunto de datos de la encuesta CASEN 2022.\n\n. . . \n\n::: {#b6b25673 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a帽os\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_vivienda</th>\n      <th>folio</th>\n      <th>id_persona</th>\n      <th>region</th>\n      <th>area</th>\n      <th>nse</th>\n      <th>expr</th>\n      <th>tot_per_h</th>\n      <th>edad</th>\n      <th>sexo</th>\n      <th>pco1_a</th>\n      <th>e3</th>\n      <th>o6</th>\n      <th>o8</th>\n      <th>y1</th>\n      <th>ytrabajocor</th>\n      <th>esc</th>\n      <th>desercion</th>\n      <th>educ</th>\n      <th>contrato</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>1</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>43</td>\n      <td>3</td>\n      <td>72</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>B谩sica incompleta</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>2</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>43</td>\n      <td>3</td>\n      <td>67</td>\n      <td>1. Hombre</td>\n      <td>S铆</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>B谩sica incompleta</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>3</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>44</td>\n      <td>3</td>\n      <td>40</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No sabe</td>\n      <td>411242.0</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>T茅cnico nivel superior completo</td>\n      <td>No sabe</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000902</td>\n      <td>100090201</td>\n      <td>1</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>51</td>\n      <td>4</td>\n      <td>56</td>\n      <td>1. Hombre</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No sabe</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000902</td>\n      <td>100090201</td>\n      <td>2</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>51</td>\n      <td>4</td>\n      <td>25</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.0</td>\n      <td>Deserci贸n</td>\n      <td>Media humanista completa</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Regresi贸n simple y scatterplot\n\nY lo agruparemos por regi贸n, para facilitar el ejemplo:\n\n. . .\n\n::: {#bee0149b .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregaci贸n\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por regi贸n\n\ncasen_2022_region.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region</th>\n      <th>ytrabajocor</th>\n      <th>esc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Regi贸n de Tarapac谩</td>\n      <td>658026.6250</td>\n      <td>11.679582</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Regi贸n de Antofagasta</td>\n      <td>791351.8125</td>\n      <td>11.833934</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Regi贸n de Atacama</td>\n      <td>666128.3125</td>\n      <td>11.126735</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Regi贸n de Coquimbo</td>\n      <td>656137.8750</td>\n      <td>10.973584</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Regi贸n de Valpara铆so</td>\n      <td>611298.1250</td>\n      <td>11.559877</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Regresi贸n simple y scatterplot\n\nRealicemos un scatter sencillo:\n\n::: panel-tabset\n\n## matplotlib\n\n::: {#ea17dca4 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por regi贸n)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-4-output-1.png){width=845 height=448}\n:::\n:::\n\n\n## seaborn\n\n::: {#1e18fe65 .cell execution_count=4}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-5-output-1.png){width=845 height=448}\n:::\n:::\n\n\n## seaborn + linea de regresion\n\n::: {#edb2564c .cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L铆nea de Regresi贸n y Intervalo de Confianza')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-6-output-1.png){width=845 height=448}\n:::\n:::\n\n\n## Con codigos de region\n\n::: {#14be71c6 .cell execution_count=6}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de regi贸n a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la 煤ltima palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L铆nea de Regresi贸n y Etiquetas de Regi贸n (ltima Palabra)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-7-output-1.png){width=853 height=451}\n:::\n:::\n\n\n:::\n\n\nPodemos ver que se aprecia una relaci贸n positiva: a mayor escolaridad promedio, mayor salario promedio por regi贸n.\n\n\n## Especificaci贸n\n\nLlamamos especifiaci贸n al precisar la relaci贸n entre las variables que deseamos estimar.\n. . . \n\nEn nuestro caso, la funci贸n base que queremos entender es entre salario y educaci贸n:\n. . . \n\n$$ \\text{Salario} = f(Educacion))$$\n\n. . . \n\nEste es una relaci贸n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales:\n\n- agregar el error aleatorio\n- especificar una forma funcional\n- definir una forma de medir las variables en los datos\n\n. . .\n\nEn nuestro caso, entonces el modelo especificado ser铆a:\n\n. . . \n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a帽os educaci贸n}_i + \\mu_i$$\n\n## Interpretaci贸n\n\nCon nuestro modelo especificado:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a帽os educaci贸n}_i + \\mu_i$$\n\nPodemos interpretar $\\beta$ y $alpha$:\n\n- $\\beta = \\frac{\\partial ingr}{\\partial educ}$: un a帽o adici贸nal de educaci贸n, en cuanto incrementa el salario (si nada m谩s cambia) \n\n- $\\alpha$ valor esperado de y, si x=0...\n  \n## Modelo poblaci贸nal y estimaci贸n\n\nEste modelo especificado esta definido en la poblaci贸n:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a帽os educaci贸n}_i + \\mu_i$$\n\npero necesitamos calcularlo con la muestra.... por lo cual tenemos estimadores para los coeficientes poblacionales!\n\n$$\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{a帽os educaci贸n}_i $$\n\n\n## Modelo poblaci贸nal y estimaci贸n\n\nEl m茅todo m谩s comun de estimaci贸n es el de los **m铆nimos cuadrados ordinarios**. Veremos detalles sobre la estimaci贸n, supuestos, propiedades estad铆sticas la proxima sesi贸n.\n\nPor ahora, pensaremos que es el m茅todo que busca la l铆nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi贸n).\n\n$$ \\hat{\\mu}_i= y_i-\\hat{y}_i$$\n\n::: {#756cc0f4 .cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuraci贸n del estilo del gr谩fico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gr谩fico de dispersi贸n con la l铆nea de regresi贸n\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresi贸n lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t茅rmino constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar l铆neas que conecten cada punto a la l铆nea de regresi贸n\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # L铆nea que conecta el punto a la l铆nea de regresi贸n\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresi贸n y residuos')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-8-output-1.png){width=845 height=448}\n:::\n:::\n\n\nEs decir, minimiza $$\\sum_{i}^{n} \\hat{\\mu}_i $$\n\n## Modelo estimado\n\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\n::: panel-tabset\n\n## Agrupados por regi贸n\n\n::: {#6e896a94 .cell execution_count=8}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t茅rmino constante\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:26:49   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n## Todos los datos\n\n::: {#a6664d09 .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t茅rmino constante\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n:::\n\nPodemos ver que un a帽o adicional de educaci贸n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n\n驴y la constante, como la podemos interpretar?\n\n## Modelos simples y m煤ltiples\n\nMuchas veces una sola variable no es suficiente para describir bien un fen贸meno. Necesitamos incluir m谩s variables.\n\nEsto puede ser:\n\n- Una nueva variable\n- Una forma funcional no lineal de la variable ya incluida\n\nNuestra interpretaci贸n del modelo no cambia, solo que ahora efectivamente estamos **controlando** por otros factores.\n\n## Modelos simples y m煤ltiples\n\nProbemos, agregar edad al modelo:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta_1 \\text{a帽os educaci贸n}_i + \\beta_2 \\text{edad}_i + \\mu_i$$\n\n::: {#9948dc16 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t茅rmino constante\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n## Modelos simples y m煤ltiples\n\nEs muy usual, agregar edad al cuadrado.... para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer...\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta_1 \\text{a帽os educaci贸n}_i + \\beta_2 \\text{edad}_i  + \\beta_3 \\text{edad}^2_i + \\mu_i$$\n\n::: {#ea665be2 .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n## Ya no es lineal el modelo?\n\nOjo! La linealidad es en los par谩metros, no en las variables. \n\nLa siguiente ecuaci贸n muestra un modelo lineal en el que el predictor  1  no es lineal respecto a y:\n\n$y = \\beta_0 + \\beta_1x_1 + \\beta_2log(x_1) + \\epsilon$\n\n\n<img src=\"./img/img_sesion3/im1.png\" width=\"400\">\n\nEn contraposici贸n, el siguiente no es un modelo lineal:\n\n$y = \\beta_0 + \\beta_1x_1^{\\beta_2} + \\epsilon$\n\n## Ya no es lineal el modelo?\n\nEn ocasiones, algunas relaciones no-lineales pueden transformarse de forma que se pueden expresar de manera lineal:\n\n- Modelo no-lineal a estimar: $y = \\beta_0x_1^{\\beta_1}\\epsilon$\n\n- Solucion: pasamos todo a logaritmos:\n\n$$log(y)=log(\\beta_0) + \\beta_1log(x_1) + log(\\epsilon)$$\n        \n$$y^{'}=\\beta_0^{'}+\\beta_1x_1^{'} + \\epsilon^{'}$$\n\n- Estimar el modelo y extraer los coeficientes.\n\n- Volvera a la forma funcional incial exponenciando los logaritmos.\n    - $\\beta_1$ es explicito.\n    - $\\beta_0^{'}=log(\\beta_0)=> exp(log(\\beta_0))$\n\n## Un poco m谩s sobre interpretaci贸n\n\nElementos clave en la interpretaci贸n de un modelo de regresi贸n lineal:\n\n- $\\beta_0$: Ordenada en el origen, valor esperado de $y$ cuando todos los predictores son cero.\n- $\\beta_j$: Coeficientes de regresi贸n parcial de cada predictor, representan el cambio promedio esperado en $y$ al aumentar en una unidad $x_j$, manteniendo otros predictores constantes (\"ceteris paribus\").\n\n## Un poco m谩s sobre interpretaci贸n: Magnitud\n\nLos coeficientes est谩n medidos en las unidades que se est谩 trabajando.\n\n![](img/img_sesion3/unidad_medida.png)\n\n**Importancia de coeficientes parciales estandarizados:**\n- Se obtienen al estandarizar las variables predictoras antes del ajuste del modelo.\n- $\\beta_0$ refleja el valor esperado de $y$ cuando los predictores est谩n en su promedio.\n- $\\beta_j$ indica el cambio promedio esperado en $y$ al aumentar en una desviaci贸n est谩ndar $x_j$, manteniendo otros predictores constantes.\n\n\n## Causalidad, regresi贸n y correlaci贸n\n\n**Importante tener en cuenta:**\n\n- Antes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero **si existe o no una relaci贸n entre las variables** de inter茅s.\n- Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes m谩s altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociaci贸n significativa entre las dos variables.\n- Un diagrama de dispersi贸n puede ser una herramienta 煤til para determinar la fuerza de la relaci贸n entre dos variables.\n  \n## Causalidad, regresi贸n y correlaci贸n\n\n**Importante tener en cuenta:**\n\n- Si parece no haber asociaci贸n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi贸n no indica ninguna tendencia creciente o decreciente),\n\n  - entonces ajustar un modelo de regresi贸n lineal a los datos probablemente no proporcionar谩 un modelo 煤til.\n  - Una valiosa medida num茅rica de asociaci贸n entre dos variables es el coeficiente de correlaci贸n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci贸n de los datos observados para las dos variables.\n\n\n## Una perspectiva hist贸rica:\n\n- El origen de la t茅cnica, podemos remontarlo a la gen茅tica.\n\n- Francis Galton estudi贸 la variaci贸n y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolect贸 y estudi贸 datos de altura de familias para tratar de entender la herencia. **Mientras hac铆a esto, desarroll贸 los conceptos de correlaci贸n y regresi贸n.**\n- Pregunta: **驴qu茅 tan bien podemos predecir la estatura de un ni帽o basado en la estatura de los padres?**\n- La t茅cnica que desarroll贸 para responder a esta pregunta, la regresi贸n, tambi茅n puede aplicarse en muchas otras circunstancias.\n\n## Una perspectiva hist贸rica:\n\n::: columns\n:::column\n<img src=\"./img/img_sesion3/galton.png\" width=\"400\">\n:::\n\n:::column\n**Nota hist贸rica:**\n\n- Galton hizo importantes contribuciones a la estad铆stica y la gen茅tica\n- pero tambi茅n fue uno de los primeros defensores de la **eugenesia**\n- un movimiento filos贸fico cient铆ficamente defectuoso favorecido por muchos bi贸logos de la 茅poca de Galton, pero con terribles consecuencias hist贸ricas.\n:::\n:::\n\n## Estudio de caso: 驴es hereditaria la altura?\n\n- Tenemos acceso a los datos de altura de familias recolectado por Galton, a trav茅s del paquete `HistData`.\n\n- Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n. . .\n\n:::{.panel-tabset}\n## Cargar datos\n\n::: {#d422c8a0 .cell execution_count=12}\n``` {.python .cell-code}\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\nprint(galton_data.head(4))\n```\n:::\n\n\n## Tabla de datos\n\n::: {#cfc4ba3f .cell execution_count=13}\n\n::: {.cell-output .cell-output-display execution_count=53}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family</th>\n      <th>father</th>\n      <th>mother</th>\n      <th>midparentHeight</th>\n      <th>children</th>\n      <th>childNum</th>\n      <th>gender</th>\n      <th>childHeight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>1</td>\n      <td>male</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>2</td>\n      <td>female</td>\n      <td>69.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>3</td>\n      <td>female</td>\n      <td>69.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>4</td>\n      <td>female</td>\n      <td>69.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## An谩lisis de caso: 驴es hereditaria la altura?\n\nPara imitar el an谩lisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n. . .\n\n:::{.panel-tabset}\n## Cargar datos\n\n::: {#8443b477 .cell execution_count=14}\n``` {.python .cell-code}\n# Filtrar por g茅nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head()\n```\n:::\n\n\n## Tabla de datos\n\n::: {#0f80e767 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=55}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>father</th>\n      <th>son</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>78.5</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>75.5</td>\n      <td>73.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>75.0</td>\n      <td>71.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75.0</td>\n      <td>68.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>75.0</td>\n      <td>68.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n\n## Estudio de caso: 驴es hereditaria la altura?\n\n- Supongamos que se nos pidiera que resumi茅ramos (describieramos) los datos de padres e hijos.\n\n- Como ambas distribuciones est谩n aproximadas por la distribuci贸n normal, podr铆amos usar los dos promedios y dos desviaciones est谩ndar como res煤menes:\n\n. . .\n\n::: {#0d38cc6b .cell execution_count=16}\n``` {.python .cell-code}\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nprint(resumen_estadistico)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   promedio_padre  sd_padre  promedio_hijo   sd_hijo\n0       69.098883  2.546555      69.248045  2.680733\n```\n:::\n:::\n\n\nSin embargo, este resumen no describe una caracter铆stica importante de los datos: \n\n**la tendencia de que cuanto m谩s alto es el padre, m谩s alto es el hijo.**\n\n## Estudio de caso: 驴es hereditaria la altura?\n\n:::{.panel-tabset}\n## Code\n\n::: {#3cd2637f .cell execution_count=17}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tama帽o de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g茅nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gr谩fico de dispersi贸n con l铆nea de regresi贸n\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relaci贸n entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gr谩fico\nplt.show()\n```\n:::\n\n\n## Plot\n\n::: {#18134db5 .cell execution_count=18}\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-19-output-1.png){width=824 height=520}\n:::\n:::\n\n\n:::\n\n## 驴Regresi贸n? 驴Y la correlaci贸n?\n\n::: columns\n::: column\n- Ambos est谩n muy relacionados.\n- Aprenderemos que el coeficiente de correlaci贸n es un resumen informativo de c贸mo dos variables se mueven juntas\n- y luego veremos c贸mo esto puede ser usado para predecir una variable usando la otra y modelado en una regresi贸n\n:::\n\n::: column\n\n::: {#a2635078 .cell execution_count=19}\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-20-output-1.png){width=812 height=520}\n:::\n:::\n\n\n:::\n\n:::\n\n## Taller de aplicaci贸n 2: \n### Caso aplicaci贸n: Cursos de Verano\n\n::: callout-tip\n## **Taller de aplicaci贸n 2: Pregunta 1**\n\n- Considere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que quer铆amos responder:\n\n- **Asistir a cursos de verano mejora los resultados acad茅micos?**\n\n1.  Plantee un modelo de regresi贸n con los datos disponibles  que deseamos estimar.\n2.   Grafique la dispersi贸n y la recta de regresi贸n estimada.\n\n:::\n\n## El coeficiente de correlaci贸n\n\nEl coeficiente de correlaci贸n se define para una lista de pares $(x_1,y_1),...(x_n,y_n)$  como la media de los productos de los valores normalizados:\n\n$$\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n$$\n\nD贸nde $\\mu$ son promedios y $\\sigma$ son desviaciones est谩ndar. La letra griega para r, $\\rho$ se utiliza com煤nmente en los libros de estad铆stica para denotar la correlaci贸n, porque es la primera letra de regresi贸n. Pronto aprenderemos sobre la conexi贸n entre correlaci贸n y regresi贸n. \n\nPodemos representar la f贸rmula anterior con el c贸digo R usando:\n\n`rho <- mean(scale(x) * scale(y))`\n\nLa correlaci贸n entre las alturas del padre y del hijo es de aproximadamente $0,4$:\n\n## El coeficiente de correlaci贸n\n\n\nPodemos representar la f贸rmula anterior con el siguiente c贸digo usando:\n\n::: {#85409e20 .cell execution_count=20}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aqu铆\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aqu铆\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9999999999999998\n```\n:::\n:::\n\n\n## El coeficiente de correlaci贸n\n\nLa correlaci贸n entre las alturas del padre y del hijo es de aproximadamente $0,4$.\n\n::: {#6878a980 .cell execution_count=21}\n``` {.python .cell-code}\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlaci贸n:\", correlation_coefficient)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoeficiente de Correlaci贸n: 0.4501189204142688\n```\n:::\n:::\n\n\n::: {#f6854ba9 .cell execution_count=22}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g茅nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviaci贸n est谩ndar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviaci贸n Est谩ndar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviaci贸n Est谩ndar de la Altura del Padre:\", sd_father)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviaci贸n Est谩ndar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviaci贸n Est谩ndar de la Altura del Padre: 2.546555038637639\n```\n:::\n:::\n\n\n## El coeficiente de correlaci贸n\n\n::: {#a513a3f8 .cell execution_count=23}\n``` {.python .cell-code}\n# Calcular la correlaci贸n entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5104684885147245\n```\n:::\n:::\n\n\nPara ver c贸mo se ven los datos para los diferentes valores de $\\rho$ aqu铆 hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\n![image](img/img_sesion3/g1.png)\n\n## La correlaci贸n es variable aleatoria\n\nAntes de continuar conectando la correlaci贸n con la regresi贸n, recordemos la variabilidad aleatoria.\n\nEn la mayor铆a de las aplicaciones de la ciencia de datos, observamos datos que incluyen **variaci贸n aleatoria**.\n\n\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra poblaci贸n. Un genetista menos afortunado s贸lo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlaci贸n de la muestra se puede calcular con:\n\n::: {#7245d1bf .cell execution_count=24}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tama帽o 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlaci贸n entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlaci贸n en la Muestra:\", correlation_coefficient)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoeficiente de Correlaci贸n en la Muestra: 0.5072834085504967\n```\n:::\n:::\n\n\n## \n\n\nR es una variable aleatoria. Podemos ejecutar una simulaci贸n de Monte Carlo para ver su distribuci贸n:\n\n* Nota: el objetivo principal de la simulaci贸n de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir c贸mo van a evolucionar.\n\n::: {#4ad9e16d .cell execution_count=25}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlaci贸n\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlaci贸n\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlaci贸n\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-26-output-1.png){width=820 height=448}\n:::\n:::\n\n\n## \n\n\nVemos que el valor esperado de R es la correlaci贸n de la poblaci贸n:\n\n::: {#53b21fed .cell execution_count=26}\n``` {.python .cell-code}\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlaci贸n:\", mean_R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedia de Coeficientes de Correlaci贸n: 0.45842882440885\n```\n:::\n:::\n\n\ny que tiene un error est谩ndar relativamente alto en relaci贸n con el rango de valores que puede tomar R:\n\n::: {#095e96d7 .cell execution_count=27}\n``` {.python .cell-code}\nsd_R = np.std(R)\nprint(\"Desviaci贸n Est谩ndar de Coeficientes de Correlaci贸n:\", sd_R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDesviaci贸n Est谩ndar de Coeficientes de Correlaci贸n: 0.047460810230179874\n```\n:::\n:::\n\n\n## \n\n- Por lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\n\n- Adem谩s, tenga en cuenta que debido a que la correlaci贸n de la muestra es un promedio de extracciones independientes, el teorema del l铆mite central realmente funciona. \n- Por lo tanto, para $N$ lo suficientemente grande la distribuci贸n de $R$ es aproximadamente normal con el valor esperado $\\rho$. \n- La desviaci贸n est谩ndar, que es algo compleja de derivar, es: $\\sqrt{\\frac{1-r^2}{N-2}}$.\n\n##\n\n- En nuestro ejemplo, $N=25$ no parece ser lo suficientemente grande para que la aproximaci贸n sea buena\n\n\n-Si N aumenta ver谩s que la distribuci贸n converge a una normal.\n\n* Nota: El gr谩fico Q-Q, o gr谩fico cuantitativo, es una herramienta gr谩fica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribuci贸n te贸rica como una Normal o exponencial.\n\n::: {#5e4437cc .cell execution_count=28}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlaci贸n\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tama帽o de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gr谩fico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Te贸ricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gr谩fico QQ-plot para los Coeficientes de Correlaci贸n\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # L铆nea de referencia\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-29-output-1.png){width=527 height=520}\n:::\n:::\n\n\n## La correlaci贸n no siempre es un resumen 煤til\n\nLa correlaci贸n no siempre es un buen resumen de la relaci贸n entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlaci贸n de 0,82:\n\n![image](img/img_sesion3/g2.png)\n\nLa correlaci贸n s贸lo tiene sentido en un contexto particular. Para ayudarnos a entender cu谩ndo es que la correlaci贸n es significativa como estad铆stica de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudar谩 a motivar y definir la regresi贸n lineal. Comenzamos demostrando c贸mo la correlaci贸n puede ser 煤til para la predicci贸n.\n\n# Correlaci贸n no es causalidad\n\nLa asociaci贸n no es causalidad es quiz谩s la lecci贸n m谩s importante que se aprende en una clase de estad铆stica. Hay muchas razones por las que una variable $X$ puede correlacionarse con una variable $Y$ sin tener ning煤n efecto directo sobre $Y$. Aqu铆 examinamos tres maneras comunes que pueden llevar a una mala interpretaci贸n de los datos.\n\n## Correlaci贸n espuria\n\nVemos una fuerte correlaci贸n entre las tasas de divorcio y el consumo de margarina.\n\n\n![image](img/img_sesion3/notcausa.png)\n\n\n(Ac谩 pueden encontrar m谩s http://tylervigen.com/old-version.html)\n\n- 驴Significa esto que la margarina causa divorcios?  \n- 驴O los divorcios hacen que la gente coma m谩s margarina? \n\n## La paradoja de Simpson\n\n- Se llama paradoja porque vemos el signo de la correlaci贸n cambiar cuando comparamos toda la data y estratos espec铆ficos. \n\n- Como ejemplo ilustrativo, supongamos que tiene tres variables aleatorias $X$, $Y$ y $Z$ y que observamos realizaciones de estas. \n- Aqu铆 est谩 el gr谩fico de observaciones simuladas para $X$ y $Y$ a lo largo de la correlaci贸n de la muestra:\n\n. . .\n\n<img src=\"./img/img_sesion3/simp1.png\" width=\"600\">\n\n## La paradoja de Simpson\n\n\n- Puedes ver que $X$ e $Y$ est谩n negativamente correlacionados. \n- Sin embargo, una vez que estratificamos por $Z$ (mostrado en diferentes colores abajo) emerge otro patr贸n:\n\n. . . \n\n<img src=\"./img/img_sesion3/simp2.png\" width=\"600\">\n\n## La paradoja de Simpson\n\n<img src=\"./img/img_sesion3/simp2.png\" width=\"600\">\n\n- Es realmente $Z$ que est谩 negativamente correlacionado con $X$. \n- Si estratificamos por $Z$ las variables $X$ e $Y$ est谩n en realidad correlacionados positivamente como se ha visto en el gr谩fico anterior.\n\n## Expectativas condicionales\n\n- Supongamos que nos piden que adivinemos la altura de un hijo seleccionado al azar y no sabemos la altura de su padre.\n\n- Debido a que la distribuci贸n de las alturas de los hijos es aproximadamente normal, sabemos que la altura media, $69.2$, es el valor con la mayor proporci贸n y ser铆a la predicci贸n con mayores posibilidades de minimizar el error.\n\n- Pero, 驴y si nos dicen que el padre es m谩s alto que el promedio, digamos que mide 72 pulgadas de alto, todav铆a esperar铆amos que la altura m谩s probable del hijo sea 69.2 pulgadas?\n\n## expectativas condicionales\n\n- Resulta que si pudi茅ramos recolectar datos de un gran n煤mero de padres que miden 72 pulgadas...\n  - la distribuci贸n de las alturas de sus hijos ser铆a normalmente distribuida. \n  - Esto implica que el promedio de la distribuci贸n calculada en este subconjunto ser铆a nuestra mejor predicci贸n.\n\n## expectativas condicionales\n\n- En general, llamamos a este enfoque condicional. \n- La idea general es que estratificamos una poblaci贸n en grupos y calculamos res煤menes en cada grupo. \n- Por lo tanto, el condicionamiento est谩 relacionado con el concepto de estratificaci贸n descrito. \n\n- Porque la expectativa condicional $E(Y|X=x)$ es el mejor predictor para la variable aleatoria $Y$ para un individuo en los estratos definidos por  $X=x$ muchos de los desaf铆os de la ciencia de datos se reducen a la estimaci贸n de esta cantidad.\n\n## Expectativas condicionales\n\n\n- En el ejemplo que hemos estado considerando, estamos interesados en calcular la altura promedio del hijo condicionada a que el padre tenga 72 pulgadas de altura. \n\n- Queremos estimar $E(Y|X=72)$ usando la muestra recolectada por Galton. \n\n- 驴Cuantos padres miden 72?\n\n. . . \n\n::: {#425fd0f3 .cell execution_count=29}\n``` {.python .cell-code}\ncount_72 = (galton_heights['father'] == 72).sum()\nprint(\"Cantidad de registros con valor 72 en la columna 'father':\", count_72)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de registros con valor 72 en la columna 'father': 8\n```\n:::\n:::\n\n\n- Si cambiamos el n煤mero a 72.5, obtenemos a煤n menos puntos de datos:\n\n. . .\n\n::: {#533e140b .cell execution_count=30}\n``` {.python .cell-code}\ncount_725 = (galton_heights['father'] == 72.5).sum()\nprint(\"Cantidad de registros con valor 72.5 en la columna 'father':\", count_725)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de registros con valor 72.5 en la columna 'father': 1\n```\n:::\n:::\n\n\n## Expectativas condicionales\n\n- Una forma pr谩ctica de mejorar estas estimaciones de las expectativas condicionales, es definir estratos con valores similares de $x$.\n- En nuestro ejemplo, podemos redondear las alturas paternas a la pulgada m谩s cercana y asumir que todas son de 72 pulgadas. \n- Si hacemos esto, terminamos con la siguiente predicci贸n para el hijo de un padre que mide 72 pulgadas de alto:\n\n::: {#772507c7 .cell execution_count=31}\n``` {.python .cell-code}\nconditional_avg = galton_heights[galton_heights['father'].round() == 72]['son'].mean()\nprint(\"Promedio condicional para father == 72:\", conditional_avg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPromedio condicional para father == 72: 70.44285714285715\n```\n:::\n:::\n\n\n## Expectativas condicionales\n\n- Note que un padre de 72 pulgadas es m谩s alto que el promedio -- espec铆ficamente, 72 - 69.1/2.5 = 1.1 desviaciones est谩ndar m谩s alto que el padre promedio. \n- Nuestra predicci贸n, $70.5$, es tambi茅n m谩s alta que el promedio, pero s贸lo $0.49$ desviaciones est谩ndar m谩s grandes que el hijo promedio. \n- Los hijos de padres de 72 pulgadas han regresado algunos a la estatura promedio.\n-  Observamos que la reducci贸n en el n煤mero de SD m谩s altas es de alrededor de $0.5$, lo que resulta ser la correlaci贸n. \n-  Como veremos en una secci贸n posterior, esto no es una coincidencia.\n\n## Expectativas condicionales\n\n- Si queremos hacer una predicci贸n de cualquier altura, no s贸lo de 72, podr铆amos aplicar el mismo enfoque a cada estrato. \n- La estratificaci贸n seguida de los boxplots nos permite ver la distribuci贸n de cada grupo:\n\n::: {#7500d793 .cell execution_count=32}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Crear una nueva columna 'father_strata' con los valores redondeados de 'father'\ngalton_heights['father_strata'] = galton_heights['father'].round().astype(int)\n\n# Crear el gr谩fico de boxplots\nplt.figure(figsize=(10, 6))  # Tama帽o del gr谩fico\nsns.boxplot(data=galton_heights, x='father_strata', y='son')\n\n# Agregar puntos para mostrar las medias condicionadas\nsns.swarmplot(data=galton_heights, x='father_strata', y='son', color='black', size=4)\n\nplt.xlabel('father_strata')\nplt.ylabel('son')\nplt.title('Boxplots de son condicionado por father_strata con Medias Condicionadas')\nplt.xticks(rotation=45)  # Rotar etiquetas del eje x si es necesario\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-33-output-1.png){width=812 height=528}\n:::\n:::\n\n\n## Expectativas condicionales\n\nNo es de extra帽ar que los centros de los grupos aumenten con la altura.\n\n. . .\n\n::: {#716c2fb3 .cell execution_count=33}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n# Crear un gr谩fico de puntos para mostrar el promedio condicional por \"father\"\nplt.figure(figsize=(10, 6))\nplt.scatter(conditional_avg_by_father['father'], conditional_avg_by_father['son'], color='blue')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres\")\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-34-output-1.png){width=812 height=520}\n:::\n:::\n\n\n- Adem谩s, estos centros parecen seguir una relaci贸n lineal. \n\n## Expectativas condicionales\n\n- A continuaci贸n se presentan los promedios de cada grupo. \n- Si tenemos en cuenta que estos promedios son variables aleatorias con errores est谩ndar, los datos son consistentes con estos puntos siguiendo una l铆nea recta:\n\n::: {#0caa355f .cell execution_count=34}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n\nconditional_avg_by_father.head()\n\n\n# Crear un gr谩fico de puntos con ajuste de regresi贸n lineal\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='father', y='son', data=conditional_avg_by_father, color='blue')\nsns.regplot(x='father', y='son', data=conditional_avg_by_father, scatter=False, color='orange')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres con Regresi贸n Lineal\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-35-output-1.png){width=812 height=520}\n:::\n:::\n\n\n## Expectativas condicionales\n\n- El hecho de que estos promedios condicionales sigan una l铆nea **no es una coincidencia**. \n- En la siguiente secci贸n, explicamos que la l铆nea que siguen estos promedios es lo que llamamos la l铆nea de regresi贸n, que mejora la precisi贸n de nuestras estimaciones. \n- Sin embargo, no siempre es apropiado estimar las expectativas condicionales con la l铆nea de regresi贸n, por lo que tambi茅n describimos la justificaci贸n te贸rica de Galton para usar la l铆nea de regresi贸n.\n\n\n\n## La l铆nea/recta de regresi贸n\n\n- Si estamos prediciendo una variable aleatoria $Y$ conociendo el valor de otra variable $X=x$ usando una l铆nea de regresi贸n, entonces predecimos que **para cada desviaci贸n est谩ndar, $\\sigma_x$ que $x$ aumenta por encima de la media $\\mu_x$, $Y$ incrementa $\\rho$ veces la desviaci贸n est谩ndar $\\sigma_Y$ sobre el promedio $\\mu_Y$**, con $\\rho$ la correlaci贸n entre $X$ e $Y$. Por lo tanto, la formula de la regresi贸n es:\n\n$$\n\\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right)=\\rho \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\n$$\n\nLo que podemos reescribir como:\n\n$$\nY=\\mu_Y + \\rho \\big(\\frac{x-\\mu_X}{\\sigma_X}\\big) \\sigma_Y\n$$\n\n## La l铆nea/recta de regresi贸n\n\n- Si existe una correlaci贸n perfecta, la l铆nea de regresi贸n predice un aumento que corresponde al mismo n煤mero de desviacones est谩ndar. \n\n- Si hay correlaci贸n 0, entonces no usamos $x$ en absoluto en la predicci贸n y simplemente predecimos el promedio $\\mu_Y$. \n- Para valores entre 0 y 1, la predicci贸n se encuentra en un punto intermedio. \n- Si la correlaci贸n es negativa, predecimos una reducci贸n en lugar de un aumento.\n\n## Regresi贸n a la media\n\n- N贸tese que si la correlaci贸n es positiva e inferior a 1, nuestra predicci贸n est谩 m谩s cerca (en unidades est谩ndar) de la altura media que de lo que el valor utilizado para predecir, $x$, est谩 del promedio de los $x$. \n- Por eso lo llamamos regresi贸n: el hijo regresa a la estatura media.\n-  De hecho, el t铆tulo del art铆culo de Galton era: Regresi贸n a la mediocridad en la estatura hereditaria (Regression toward mediocrity in hereditary stature.). \n\n## La l铆nea/recta de regresi贸n\n\n- Para a帽adir l铆neas de regresi贸n a los gr谩ficos, necesitaremos la f贸rmula anterior en la forma: $y=b+mx$, con pendiente $m=\\rho \\sigma_y / \\sigma_x$ e intercepto $b=\\mu_y - m \\mu_x$\n\n- Aqu铆 agregamos la l铆nea de regresi贸n a la data original.\n\n::: {#473b2797 .cell execution_count=35}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# C谩lculo de las medias y desviaciones est谩ndar\nmu_x = galton_heights['father'].mean()\nmu_y = galton_heights['son'].mean()\ns_x = galton_heights['father'].std()\ns_y = galton_heights['son'].std()\n\n# C谩lculo del coeficiente de correlaci贸n\nr = galton_heights['father'].corr(galton_heights['son'])\n\n# C谩lculo de la pendiente y el intercepto para la l铆nea de regresi贸n\nm = r * s_y / s_x\nb = mu_y - m * mu_x\n\n# Configuraci贸n del tama帽o de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gr谩fico de dispersi贸n con l铆nea de regresi贸n\nsns.scatterplot(x='father', y='son', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father', y='son', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Son Height\")\nplt.title(\"Relaci贸n entre Altura de Padres e Hijos con L铆nea de Regresi贸n\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-36-output-1.png){width=812 height=520}\n:::\n:::\n\n\n## La l铆nea/recta de regresi贸n\n\n- La f贸rmula de regresi贸n implica que si primero estandarizamos las variables, es decir, restamos el promedio y dividimos por la desviaci贸n est谩ndar, entonces la l铆nea de regresi贸n tiene intercepto 0 y pendiente igual a la correlaci贸n $\\rho$. \n- Aqu铆 est谩 la misma gr谩fica, pero usando unidades est谩ndar:\n\n. . .\n\n::: {#ed03d6e7 .cell execution_count=36}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Estandarizar las variables 'father' y 'son'\ngalton_heights['father_standardized'] = (galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std()\ngalton_heights['son_standardized'] = (galton_heights['son'] - galton_heights['son'].mean()) / galton_heights['son'].std()\n\n# Calcular la correlaci贸n de las variables estandarizadas\nr = galton_heights['father_standardized'].corr(galton_heights['son_standardized'])\n\n# Configuraci贸n del tama帽o de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gr谩fico de dispersi贸n con l铆nea de regresi贸n\nsns.scatterplot(x='father_standardized', y='son_standardized', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father_standardized', y='son_standardized', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height (Standardized)\")\nplt.ylabel(\"Son Height (Standardized)\")\nplt.title(\"Relaci贸n Estandarizada entre Altura de Padres e Hijos con L铆nea de Regresi贸n (Intercepto = 0, Pendiente = Correlaci贸n)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-37-output-1.png){width=874 height=520}\n:::\n:::\n\n\n# Regresi贸n: Definici贸n matem谩tica\n\n\nEl modelo de regresi贸n lineal (Legendre, Gauss, Galton y Pearson) considera que, dado un conjunto de observaciones $\\{y_i, x_{i1},...,x_{np}\\}^{n}_{i=1}$ , la media  $$  de la variable respuesta  $$  se relaciona de forma lineal con la o las variables regresoras  $_1$ ... $x_p$  acorde a la ecuaci贸n:\n\n$$\\mu_y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + ... + \\beta_p x_{p}$$\n \nEl resultado de esta ecuaci贸n se conoce como la l铆nea de regresi贸n poblacional, y recoge la relaci贸n entre los predictores y la media de la variable respuesta.\n\n# Regresi贸n: Definici贸n matem谩tica\n\n\n- Otra definici贸n que se encuentra con frecuencia en los libros de estad铆stica es:\n\n$$y_i= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} +\\epsilon_i$$\n \n- En este caso, se est谩 haciendo referencia al valor de    para una observaci贸n    concreta. El valor de una observaci贸n puntual nunca va a ser exactamente igual al promedio, de ah铆 que se a帽ada el t茅rmino de error  $\\epsilon$.\n\n## Interpretaci贸n:\n\n\nEn ambos casos, la interpretaci贸n de los elementos del modelo es la misma:\n\n- $\\beta_0$: es la ordenada en el origen, se corresponde con el valor promedio de la variable respuesta  $y$  cuando todos los predictores son cero.\n\n- $\\beta_j$: es el efecto promedio que tiene sobre la variable respuesta el incremento en una unidad de la variable predictora  $x_j$, manteni茅ndose constantes el resto de variables. Se conocen como coeficientes de regresi贸n.\n\n- $\\epsilon$: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo. Recoge el efecto de todas aquellas variables que influyen en $y$ pero que no se incluyen en el modelo como predictores.\n\n## Interpretaci贸n:\n\n- En la gran mayor铆a de casos, los valores $\\beta_0$ y $\\beta_j$ poblacionales se desconocen, por lo que, a partir de una muestra, se obtienen sus estimaciones  $\\hat{\\beta_0}$ y $\\hat{\\beta_j}$. \n- **Ajustar el modelo consiste en estimar, a partir de los datos disponibles, los valores de los coeficientes de regresi贸n que maximizan la verosimilitud (likelihood), es decir, los que dan lugar al modelo que con mayor probabilidad puede haber generado los datos observados.**\n\n- El m茅todo empleado con m谩s frecuencia es el **ajuste por m铆nimos cuadrados ordinarios (OLS)**\n  - que identifica como mejor modelo la recta (o plano si es regresi贸n m煤ltiple) \n  - que minimiza la suma de las desviaciones verticales entre cada dato de entrenamiento y la recta, elevadas al cuadrado.\n\n## Magnitud y significancia\n\n- La magnitud de cada coeficiente parcial de regresi贸n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est谩 asociada con la importancia de cada predictor.\n- Una buena practica es estandarizar\n\n\n## Cuidado: hay dos l铆neas de regresi贸n\n\nCalculamos una l铆nea de regresi贸n para predecir la altura del hijo desde la altura del padre. \n\nUsamos estos c谩lculos:\n\n::: {#a2ace4f6 .cell execution_count=37}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviaci贸n est谩ndar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviaci贸n est谩ndar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlaci贸n entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.4658308243792417\n2.56441709039222\n2.6076529186746025\n69.08938547486034\n69.263687150838\n```\n:::\n:::\n\n\n::: {#661a543e .cell execution_count=38}\n``` {.python .cell-code}\n# Calcular la pendiente de la primera l铆nea de regresi贸n\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera l铆nea de regresi贸n\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npendiente 0.47368468778038647\nconstante 36.53710316324001\n```\n:::\n:::\n\n\n## Cuidado: hay dos l铆neas de regresi贸n\n\n- 驴Y si queremos predecir la estatura del padre bas谩ndonos en la del hijo? \n\n- Es importante saber que esto no se determina calculando la funci贸n inversa!.\n\n- Necesitamos computar $E(XY=y)$. Dado que los datos son aproximadamente normales bivariados, la teor铆a descrita anteriormente nos dice que esta expectativa condicional seguir谩 una l铆nea con pendiente e intercepto:\n\n::: {#040e5bca .cell execution_count=39}\n``` {.python .cell-code}\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npendiente 0.4581071808731349\nconstante 37.35919301731116\n```\n:::\n:::\n\n\n## Cuidado: hay dos l铆neas de regresi贸n\n\n- Aqu铆 hay un gr谩fico que muestra las dos l铆neas de regresi贸n:\n\n- con azul para la predicci贸n de las alturas del hijo con las alturas del padre y rojo para la predicci贸n de las alturas del padre con las alturas del hijo.\n\n\n::: panel-tabset\n\n## Codigo\n\n::: {#3d525a74 .cell execution_count=40}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gr谩fico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()\n```\n:::\n\n\n## plot\n\n::: {#9f5f84d2 .cell execution_count=41}\n\n::: {.cell-output .cell-output-display}\n![](sesion3_slides_files/figure-revealjs/cell-42-output-1.png){width=812 height=520}\n:::\n:::\n\n\n:::\n\n\n## Taller aplicaci贸n 2: ALtura de madres, padres, hijos e hijas\n::: callout-tip\n\n## Taller aplicacci贸n 2: Altura de madres, padres, hijos e hijas\n\n\n1) Cargue los datos de `GaltonFamilies` desde el HistData. Los ni帽os de cada familia est谩n ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado `galton_heights` seleccionando ni帽os y ni帽as al azar. (HINT: use `sample`).\n\n2) Haga una gr谩fica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\n\n3) Calcular la correlaci贸n para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\n\n4) Plotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\n\n5) Obtener el modelo de regresi贸n e interpretar los coeficientes.\n:::\n\n",
    "supporting": [
      "sesion3_slides_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}