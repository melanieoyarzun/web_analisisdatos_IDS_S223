{
  "hash": "5812256b681ddcd3cc32eb1b54ffb314",
  "result": {
    "markdown": "---\ninstitute: Magíster en Data Science - Universidad del Desarrollo\nsubtitle: 'Curso: Análisis de datos'\ntitle: 'Sesión 2: Preparando los datos'\nauthor: 'Phd (c) Melanie Oyarzún - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'\nformat:\n  revealjs:\n    logo: logo_udd.png\n    footer: Curso Análisis de Datos - Sesión 3\n    transition: fade\n    background-transition: fade\n    fontsize: 1.8em\n    theme:\n      - simple\n      - custom.scss\n    chalkboard:\n      theme: whiteboard\n      boardmarker-width: 5\n      buttons: true\n    progress: true\n    incremental: true\n    scrollable: true\ncode-link: true\neditor:\n  markdown:\n    wrap: 100\necho: true\noutput-location: fragment\nexecute:\n  freeze: auto\ntoc-depth: 2\ndf-print: paged\n---\n\n<style>\n  table {\n    font-size: 0.7em; /* Reducir el tamaño de fuente en las tablas al 70% del tamaño base */\n  }\n</style>\n\n## La preparación de los datos\n\n- La preparación de datos es una **fase esencial** en el proceso de análisis de datos que involucra una serie de actividades destinadas a garantizar que los datos estén en condiciones óptimas para su posterior análisis.\n\n  - Extraccion de los datos\n  - Limpieza\n  - Transformación\n  - Organización\n  \n# **Trash in , trash out** {.bigger}\n\n::: columns\n::: {.column width=\"50%\"}\nLa preparación de datos es crucial porque **afecta directamente la calidad** y confiabilidad **de los resultados** obtenidos en cualquier análisis posterior.\n:::\n\n::: {.column width=\"50%\"}\n![](img/trash.png)\n:::\n:::\n\n## Tipos de datos\n\n::: notes\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\n:::\n\nPodemos clasificar estos datos que recopilamos de muchas maneras:\n\n-   Estructura\n-   Tamaño\n-   Operacionalización\n-   Temporalidad y unidad de análisis.\n\n\n## Estructura de los datos {.scrollable}\n\n- La estructura de un dato se refiere a su **formato y codificación**.\n\n. . .\n\n![](img/img_sesion2/structuredvsunestructures.png)\n\n\n## **Datos estructurados**\n\n- Conjuntos de tablas forman bases de datos estructuradas.\n- Tablas son formas organizadas y ordenadas de datos.\n- Las entradas en tablas pueden ser texto o números.\n- Los datos estructurados son recopilados y organizados intencionalmente.\n\n\n## **Datos No estructurados**\n\n- Los datos no estructurados son comunes en la sociedad moderna.\n- Pueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\n- No siguen un formato de base de datos definido.\n\n\n## Algunos ejemplos de datos no estructurados son:\n\n-   **Medios enriquecidos:** Datos de medios y entretenimiento, datos de vigilancia, datos\n    geoespaciales, audio, datos meteorológicos\n\n-   **Colecciones de documentos digitalizados:** Facturas, registros, correos electrónicos,\n    aplicaciones de productividad\n\n-   **Internet de las cosas:** Datos de sensores, datos de teletipos.\n\n## Estructurados vs no estructuraods\n\n- Datos estructurados están en sistemas transaccionales y bases de datos.\n- No hay preferencia entre estructurados y no estructurados.\n- Ambos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n##  Datos según tamaño\n\n- **Big Data:** \n  - Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. \n  - Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\n\n##  Datos según tamaño\n\n- **Small Data:** \n  - Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. \n  - Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. \n  - A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n##   Datos según tipo y su operacionalización\n\n- Las variables en análisis de datos pueden ser **cuantitativas o cualitativas**. \n  - Las cuantitativas representan números medibles, como medidas o cantidades. \n  - Las cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden). \n    - Para operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n- Continuas y discretas\n  \n##   Datos según temporalidad y unidad de análisis.\n\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\n- Corte transversal \n- Serie de tiempo\n- Panel - datos longitudinales\n\n## Corte transversal\n\n![](img/corte_transversal.png)\n\n## Serie temporal\n![](img/serie_temporal.png)\n\n## Panel\n![](img/panel.png)\n\n\n# Cargando los datos\n\n## Leyendo datos en Pandas\n\nEn `Pandas` podemos cargar una gran variedad de datos, a través de la familia de funciones\n`read_tipo`\n\n![](img/pandas_read.png)\n\n## Leyendo datos en Pandas\n\nEn `Pandas` podemos cargar una gran variedad de datos, a través de la familia de funciones\n`read_tipo`\n\n::: callout-tip\n## Algunos tipos de argumentos para funciones *read*:\n\n-   **Indexing:** escoger una columna para indexar los datos, obtener nombres de columnas del\n    archivo o usuario\n-   **Type inference y data conversion:** automático o definido por el usuario\n-   **Datetime parsing:** puede combinar información de múltiples columnas\n-   **Iterating:** trabajar con archivos muy grandes\n-   **Unclean Data:** saltarse filas (por ejemplo, comentarios) o trabajar con números con formato\n    (por ejemplo, 1,000,345)\n-   **Memory:** `low_memory` indica que usemos pocos recursos (o no) del sistema.\n:::\n\n## Formato CSV (valores separados por comas) {.smaller}\n\n-   La coma es un separador de campos, newline denota registros\n    -   `a,b,c,d,message`\n    -   `1,2,3,4,hello`\n    -   `5,6,7,8,world`\n    -   `9,10,11,12,foo`\n\n-   Puede tener un encabezado (`a,b,c,d,message`), pero no es requisito\n\n-   Sin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante,\n    etc.)\n\n    -   Default: simplemente mantener todo como string\n    -   inferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que\n        parece ser\n\n-   ¿Y qué pasa con las comas en un valor?: doble comillas\n\n-   Se puede utilizar otros delimitadores (\\|, , )\n\n## CSV en pandas {.smaller}\n\n-   Lectura:\n    -   Básica: `df = pd.read_csv(fname)`\n    -   Utilizar diferente delimitador: `df = pd.read_csv(fname, sep='\\t\\')`\n    -   Saltarse las primeras columnas: `df = pd.read_csv(fname, skiprows=3)`\n\n\n-   Escritura:\n    -   Básica `df.to_csv()`\n    -   Cambiar el delimitador con sep kwarg: `df.to_csv('example.dsv', sep='\\|')`\n    -   Cambiar la representación de missing value `df.to_csv('example.dsv', na_rep='NULL')`\n\n## JSON (Java Script Object Notation) {.smaller}\n\n-   Un formato para datos web\n\n-   Aspecto muy similar a a diccionarios y listas python\n\n-   Ejemplo:\n    `{   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }`\n\n-   No hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos,\n    diccionarios, números, booleanos, o nulos\n\n    -   Keys de diccionario deben ser strings\n    -   Signos de pregunta ayudan a diferenciar string o valores numéricos\n\n## Lectura y escritura de JSON con pandas\n\n-   `pd.read_json(<filename>, orient=<orientation>)`\n-   `df.to_json(<filename>, orient=<orientation>)`\n\n## Orientaciones posibles de JSON\n\n\n-   *split*: tipo diccionario\n        `{index -> [index],  columns -> [columns],data -> [values]}`\n-   *records*: tipo lista\\\n        `[{column -> value}, ... , {column -> value}]`\n-   *index*: tipo diccionario `{index -> {column -> value}}`\n-   *columns:* tipo diccionario `{column -> {index -> value}}`\n-   *values:* solo los valores del arreglo\n\n## eXtensible Markup Language (XML)\n\n-   Formato más antiguo y auto descriptivo, con estructura jerárquica anidada.\n-   Cada campo tiene tags\n-   Tiene un elemento inicial llamado root\n-   No tiene un método incorporado en Python.\n-   Se puede usar la librería `lxml` (también ElementTree)\n\n\n## Ejemplo XML\n\n![](img/xml.png)\n\n## Formatos binarios\n\n-   CSV, JSON y XML son todos formatos de texto\n\n-   ¿Qué es un formato binario?\n\n-   Pickle: Python's built-in serialization\n\n. . .\n\n::: {#335b7ffd .cell execution_count=1}\n``` {.python .cell-code}\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n```\n:::\n:::\n\n\n## Formatos binarios\n\n-   HDF5: Librería para almacenar gran cantidad de datos científicos\n\n    -   Formato jerárquico de datos\n\n    -   Interfaces in C, Java, MATLAB, etc.\n\n    -   Aguanta compresión\n\n    -   Usar pd.HDFStore para acceder\n\n    -   Comandos: read_hdf/to_hdf, se necesita especificar objeto\n\n    -   Excel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\n\n    -   pd.ExcelFile or pd.read_excel\n\n## Bases de datos relacionales\n\n![](img/datos_relacionales.png)\n\n. . .\n\n## Bases de datos relacionales {.smaller}\n\n-   Las bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\n    -   vínculos entre tablas via *foreign keys*\n\n    -   *SQL* para crear, almacenar y *query* datos \n\n-   *sqlite3* es una base de datos simple con *built-in support in python*\n\n-   Python tiene una *database API* que te permite acceder a la mayoría de los sistemas de bases de datos a través de un *API* común\n\n-   Sintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (*MySQL, Microsoft SQL Server, Oracle*, etc.) \n\n-   *SQLAlchemy*: Paquete de python que se abstrae de las diferencias entre distintos sistemas de\n    bases de datos\n\n-   *SQLAlchemy* apoya la lectura de *queries* a *data frame*:\n\n. . .\n\n::: {#61c4373c .cell execution_count=2}\n``` {.python .cell-code}\nimport sqlalchemy as sqla\ndb = sqla.create_engine('sqlite:///mydata.sqlite')\npd.read_sql('select \\* from test', db)\n```\n:::\n\n\n# Dirty Data \n... pero, ¿y si los datos no son o están correctos...\n...confiables\n..en formato correcto?\n\n\n## Dirty data: punto de vista estadístico\n\n-   Los datos son generados desde algún **proceso**\n-   Se quiere modelar el proceso, pero se tienen muestras no ideales:\n    -   **Distorsión:** algunas muestras se corrompieron por un proceso\n    -   **Sesgo de selección:** probabilidad de que una muestra dependa de su valor\n    -   **Censura** de izquierda y derecha: hay valores que no observamos\n    -   **Dependiencia**: las muestras no son independientes (por ejemplo, redes sociales)\n-   Puedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\n-   Trade-off entre precisión y simplicidad\n\n## Dirty data: punto de vista experto en base de datos\n\n-   Se obtuvo un set de datos\n-   Algunos valores están missing, corrompidos, erróneos, duplicados\n-   Los resultados son absolutos (modelo relacional)\n-   Mejores respuestas provienen de mejorar la calidad de los valores en el set de datos\n\n## Dirty Data: Punto de vista del experto en un área\n\n-   Algo se ve mal en los datos\n-   Algo se ve mal en la respuesta\n-   ¿Qué ocurrió?\n    -   Los expertos en un área llevan un modelo implícito de los datos que están testeando\n-   No siempre necesitas ser un experto en un área para hacer esto\n    -   ¿Puede una persona correr 500 km por hora?\n    -   ¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\n    -   Utilizar sentido común\n\n## Dirty Data: Punto de vista del Data Scientist\n\n-   Combinación de los tres puntos de vista anteriores\n-   Todos los puntos de vista presentan problemas con los datos La meta puede determinar las\n    soluciones:\n    -   Valor de la mediana: no preocuparse mucho de los outliers muy improbables\n    -   Generalmente, agregación es menos susceptible a los errores numéricos\n    -   Ser cuidadoso, puede que los datos estén bien...\n\n## ¿Dónde se origina el dirty data?\n\n-   La fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\n-   Las transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de\n    forma incorrecta debido a un software bug\n-   La integración de diferentes sets de datos causa problemas\n-   Propagación del error: se magnifica un error\n\n## Problemas Dirty comunes:\n\n-   Problemas de separadores: por ejemplo, CSV sin respetar comillas dobles\n-   Convenciones de nombres o denominaciones: NYC vs. New York\n-   Pérdida de campos requeridos, por ejemplo, key\n-   Representaciones diferentes: 2 vs. dos\n-   Datos truncados: \"Janice Keihanaikukauakahihuliheekahaunaele\" se vuelve \"Janice -\n    Keihanaikukauakahihuliheek\" en la licencia de Hawaii\n-   Registros redundantes: pueden ser exactamente el mismo o tener alguna superposición\n-   Problemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017\n\n\n## Preparando los datos\n\nMuchas veces los datos con los que queremos trabajar no están en el formato adecuado para los\nanálisis que querenos realizar.\n\n## Data Wrangling\n\n-   Data wrangling: transformar datos en bruto a un formato más significativo que pueda ser\n    analizado mejor\n-   Data cleaning: deshacerse de datos imprecisos\n-   Data transformations: cambiar los datos de una representación a otra\n-   Data reshaping: reorganizar los datos\n-   Data merging: combinar dos sets de datos\n\n## Tidy Data\n\nEn este sentido, generalmente para análisis de *corte transversal* nuestro ideal es trabajar con *Tidy Data*\n\n\n**\"Tidy Data** is a standar way of mapping the **meaning** of a dataset to its structure¨\n\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por [Hadley Wickham\n(2014)](https://vita.had.co.nz/papers/tidy-data.pdf), se ha vuelto un estándar deseable para\nanalizar datos.\n\n## Tidy data\n\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\n-   Cada variable (feature) forma una columna.\n\n-   Cada observación (registro) forma una fila.\n\n-   Cada dato (valor) está en una celda de la tabla.\n\n## Tidy data\n![](img/tidy_data.png)\n\n## Tidy data\n\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:\n\n. . .\n\n![](img/tidydata_2.jpg)\n\n## Tidy & workflow\n\nMuchas veces un flujo de trabajo **workflow** empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n. . .\n\n![](img/flujo_tidy.png)\n\n\n## ¿Porqué datos Tidy?\n\n-   Estandarizanción\n\n. . .\n\n![Fuente: https://allisonhorst.com/other-r-fun](img/tidydata_3.jpg)\n\n\n##   Facilitar la colaboración\n\n![Fuente: https://allisonhorst.com/other-r-fun](img/tidydata_4.jpg)\n\n## Simplifica la reproducibilidad\n\n\n![Fuente: https://allisonhorst.com/other-r-fun ](img/tidydata_5.jpg)\n\n\n## ¿Siempre Tidy?\n\n-   NO!\n\n-   Existen muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o \"desordenadas\".\n\n-   Es importante tener en cuenta que siempre existen múltiples formas de representar la misma información.\n\n## Otras estructuras:\n\n**Existen dos principales motivos para utilizar otras estructuras de datos:**\n\n-   Representaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\n-   Algunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales.\n\n\n#  Data Cleaning\n\n## Algunas tareas comunes\n\n-   Descartar e imputar missing data\n-   Remover duplicados.\n-   Modificar datos\n    -   mapear strings, expresiones aritméticas. Ejemplos:\n        -   Convertir strings de mayúsculas a minúsculas (upper/lower case)\n        -   Convertir T en Fahrenheit a Celsius\n        -   Crear una nueva columna basada en la columna anterior.\n\n## Algunas tareas comunes\n-   Reemplazar valores\n    -   (e.g. -999 → NaN). Usar método df\\['column'\\].replace()\n-   Restringir valores:\n    -   valores por encima o por debajo de los umbrales especificados se establecen en un valor\n        máximo/mínimo.\n \n## Datos perdidos (o missing, NA, NAN)\n\n2 enfoques para lidiar con ellos:\n\n1.  Filtrar\n\n    -   se pueden escoger filas o columnas\n\n2.  Llenar/ reemplazar :\n\n    -   con un valor por default\n    -   con un valor interpolados, otros\n\n## Datos perdidos\n\n![Missing en Pandas](img/pandas_missing.png)\n\n## Ejemplo missing\n\n::: {#95a3f5a9 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', np.nan, 'Carlos'],\n        'Edad': [25, 30, np.nan, 30, 28]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar filas con valores faltantes\ndf = df.dropna()\n\n# Llenar valores faltantes con un valor específico\ndf['Edad'].fillna(0, inplace=True)\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Nombre  Edad\n0    Juan  25.0\n1     Ana  30.0\n4  Carlos  28.0\n```\n:::\n:::\n\n\n## Fitrando y limpiando\n\n-   Encontrar duplicados\n    -   `duplicated` : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n-   Remover duplicados:\n    -   `drop_duplicates`: saca todas las filas donde duplicated es True\n    -   `keep`: Cuál de los valores es el que quiero mantener (first or last)\n-   Puede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key.\n\n## Ejemplo Fitrando y limpiando\n\n::: {#2cfa4659 .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28\n```\n:::\n:::\n\n\n## Ejemplo: Datos de educación\n\n\n\n::: callout-important\n## Taller de aplicación 1: Pregunta 4\n\n\n:::\n\n## Errores de registro y textos\n\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\n-   Una de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\n\n-   `split(<delimiter>)` \"quiebra\" o separa un string en partes: `s = \"12,13,14\"`\n    `slist = s.split(',') # [\"12\", \"13\", \" 14\"]`\n\n-   `<delimiter>.join([<str>])`: Une varios strings por un delimitador\n    `\":\".join(slist) # \"12:13: 14”`\n\n## Errores de registro y textos\n-   strip(): remueve los espacios en blanco iniciales y finales\n    `[p.strip() for p in slist] # [\"12\", \"13\", \"14\"]`\n\n-   `replace(<from>,<to>)`: Cambia un substring por otro `s.replace(',', ':') # \"12:13: 14”`\n\n-   upper()/lower(): casing `\"AbCd\".upper () # \"ABCD”` `\"AbCd\".lower() # \"abcd\"`\n\n## Errores de formato\n\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\n::: {#428db91a .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n```\n:::\n:::\n\n\n## Transformando strings\n\nPodemos encontrar donde estan los problemas facilmente:\n\n`index()`encuentra donde ocurre primero un substring(error si no lo encuentra): \n\n. . .\n\n::: {#6a5dd67c .cell execution_count=6}\n``` {.python .cell-code}\ns = \"12,13, 14\"\ns.index(',') # 2\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n2\n```\n:::\n:::\n\n\n. . .\n\n::: {#3d460be9 .cell execution_count=7}\n``` {.python .cell-code}\n#s.index(':') # ValueError raised\n```\n:::\n\n\n## Encontrar problemas\n\n`find(<str>)`: Lo mismo que index pero -1 si no es encontrado \n. . .\n\n::: {#988b48e6 .cell execution_count=8}\n``` {.python .cell-code}\ns = \"12,13, 14\"\ns.find(',') # 2\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n2\n```\n:::\n:::\n\n\n. . .\n\n::: {#16f09b26 .cell execution_count=9}\n``` {.python .cell-code}\ns.find(':') # -1\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n-1\n```\n:::\n:::\n\n\n## Encontrar problemas\n`startswith()` or `endswith()`: chequeo booleano para la ocurrencia de un string\n. . .\n\n::: {#02fed6fd .cell execution_count=10}\n``` {.python .cell-code}\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nFalse\n```\n:::\n:::\n\n\n## Métodos para strings\n\n![](img/metodos_strings.png)\n\n\n\n## Métodos para strings\n- Cualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\n\n- Está vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido) \n\n- Se debe usar .str.<method_name> (es importante el .str).\n\n## Ejemplo:\n\n:::{.panel-tabset}\n### gmail\n\n::: {#210d6e85 .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n```\n:::\n:::\n\n\n### split\n\n```{ython }\n    \nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n```\n\n\n### 3 char\n\n::: {#39934437 .cell execution_count=12}\n``` {.python .cell-code}\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object\n```\n:::\n:::\n\n\n:::\n\n## Expresiones regulares para strings\n\n![](img/regular_expressions.png)\n\n\n## Transformaciones del formato de los datos\n\n1.  Reshapes\n2.  Eliminar/elegir  columnas \n3.  Re codificar: variables dummies\n4.  Unir  datasets\n\n## Eliminar/ elegir columnas\n\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis.\n. . .\n\n::: {.panel-tabset}\n### Code\n\n::: {#63292774 .cell execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\ndf.head(6)\n```\n:::\n\n\n### tabla original\n\n::: {#f0400d04 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Nombre</th>\n      <th>Edad</th>\n      <th>Peso (kg)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Juan</td>\n      <td>25</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ana</td>\n      <td>30</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Luis</td>\n      <td>35</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Carlos</td>\n      <td>28</td>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### tabla sin peso\n\n::: {#8a914d04 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Nombre</th>\n      <th>Edad</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Juan</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ana</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Luis</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Carlos</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Reshapes/re formato\n\n- El \"reshape\" o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico. \n- Esto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos. \n- El \"reshape\" es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo.\n\n## Reshapes/re formato\n\n![](img/reshape.png)\n\n## Reshapes/re formato\n\n::: {.panel-tabset}\n### Code\n\n::: {#b3383f71 .cell execution_count=16}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n```\n:::\n\n\n### DF ancho\n\n::: {#364eb759 .cell execution_count=17}\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Ciudad</th>\n      <th>A</th>\n      <th>B</th>\n    </tr>\n    <tr>\n      <th>Fecha</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-08-01</th>\n      <td>25</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2023-08-02</th>\n      <td>26</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### DF largo\n\n::: {#2f527f40 .cell execution_count=18}\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fecha</th>\n      <th>Ciudad</th>\n      <th>Temperatura</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-08-01</td>\n      <td>A</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-08-01</td>\n      <td>B</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-08-02</td>\n      <td>A</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-08-02</td>\n      <td>B</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n\n## Crear variables dummies / dicotómicas\n\n-   Muy útil para representar información cualitativa.\n-   Muy usando en análisis de regresión y machine learning.\n-   Queremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\n-   Se pueden generar mediante `pd.get_dummies(df[‘key’])`\n\n## One hot encoding: Razas de perros\n![](img/onehot_dogs.png)\n\n## Ejemplo : Razas de perros\n\n::: {.panel-tabset}\n### Code\n\n::: {#b5a51833 .cell execution_count=19}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n```\n:::\n\n\n### tabla\n\n::: {#5b6bad1f .cell execution_count=20}\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Labrador</th>\n      <th>Poodle</th>\n      <th>Bulldog</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Unir datasets\n\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\n1. Apliar/concatenar/append:\n    Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\n\n2. Uniones Merge / Join:\n    Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como \"inner\", \"outer\", \"left\" y \"right\"\n\n\n\n## 1. Apilar dataframes\n\nCreemos los dataframes:\n. . .\n\n::: {#8c65ffc2 .cell execution_count=21}\n``` {.python .cell-code}\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   P  Q\n0  2  3\n1  4  5\n```\n:::\n:::\n\n\n::: {#3eaf0e55 .cell execution_count=22}\n``` {.python .cell-code}\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   P  Q\n0  6  7\n1  8  9\n```\n:::\n:::\n\n\n## Apilar\n\nHagamos el apilar. Atención a los index\n\n. . .\n\n::: {#2ed5d139 .cell execution_count=23}\n``` {.python .cell-code}\ndf.append(df2)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P</th>\n      <th>Q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Apilar\n![](img/append1.png)\n\n## Ojo al apilar con los index\n\nUsemos ignorar index\n. . .\n\n::: {#13893298 .cell execution_count=24}\n``` {.python .cell-code}\ndf.append(df2, ignore_index=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P</th>\n      <th>Q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Ojo al apilar con los index\n\n![](img/append2.png)\n\n\n## 2. Unir dataframes\n\n![](img/joins.png)\n\n## Unir datasets\n\nUtilizando la función `pd.merge()`\n\n. . . \n\n::: {.panel-tabset}\n### code\n\n::: {#d0ae848e .cell execution_count=25}\n``` {.python .cell-code}\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n```\n:::\n\n\n### inner join\n\n::: {#219f154c .cell execution_count=26}\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>Value_x</th>\n      <th>Value_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>C</td>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### outer join\n\n::: {#94fc44af .cell execution_count=27}\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Key</th>\n      <th>Value_x</th>\n      <th>Value_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>2.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>3.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D</td>\n      <td>NaN</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Transformaciones estadísticas:\n### 1.  Outliers\n- La identificación de valores atípicos es importante por varias razones:\n\n1. Calidad de los datos\n2. Impacto en estadísticas y modelos\n3. Anomalías y problemas reales\n4. Toma de decisiones informada\n\n## Ejemplo outliers\n\nDetección\n\n. . .\n\n::: {#024602d5 .cell execution_count=28}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] < lower_limit) | (df['Valor'] > upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores atípicos:\n   Valor\n6    200\n```\n:::\n:::\n\n\n## 2. Estandarización de Datos\n\n- La estandarización es un proceso importante en el análisis de datos y el aprendizaje automático. \n\n- Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno. \n  \n- Esta transformación se logra mediante la fórmula:\n\n. . .\n\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n\n## Estandarización {.scrollable}\n\n::: {.panel-tabset}\n### Pandas\n\n::: {#25144335 .cell execution_count=29}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n```\n:::\n:::\n\n\n### Numpy\n\n::: {#97ac90b3 .cell execution_count=30}\n``` {.python .cell-code}\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]\n```\n:::\n:::\n\n\n:::\n\n\n## Normalización de Datos\n\n- La normalización es un proceso esencial en el análisis de datos y el aprendizaje automático. \n- Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1. \n  \n- La fórmula matemática utilizada para la normalización es:\n\n. . .\n\n $$x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} $$\n\n## Normalización de Datos {.scrollable}\n\n::: {.panel-tabset}\n### Pandas\n\n::: {#8b376f9b .cell execution_count=31}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n```\n:::\n:::\n\n\n### Numpy\n\n::: {#c1ac9786 .cell execution_count=32}\n``` {.python .cell-code}\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]\n```\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "sesion2_slides_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}