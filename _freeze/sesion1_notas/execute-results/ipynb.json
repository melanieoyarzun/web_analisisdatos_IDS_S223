{
  "hash": "83df70f215003da3a5fdf217724eb9df",
  "result": {
    "markdown": "---\ninstitute: Magíster en Data Science - Universidad del Desarrollo\nsubtitle: 'Curso: Análisis de datos'\ntitle: |-\n  Sesión 1: Planteando y respondiendo preguntas con datos. \n   (y un -breve- repaso a pruebas de hipótesis)\nauthor: 'Phd (c) Melanie Oyarzún - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'\nformat:\n  html:\n    toc: true\n    html-math-method: mathml\n    embed-resources: true\n  ipynb: default\necho: true\neditor:\n  markdown:\n    wrap: 72\nexecute:\n  keep-ipynb: true\n  freeze: auto\ncode-link: true\n---\n\n## Detalles {.unnumbered .unlisted visibility=\"hidden\"}\n\nNotas detalladas de la sesión 1, curso análisis de datos, magíster en\nData Science Universidad del Desarrollo.\n\nFecha: 19 agosto 2023. Versión 1\n\n## Objetivos de aprendizaje de la sesión\n\n1.  Comprender el papel del proceso de adquisición y almacenamiento en\n    un proyecto de análisis de datos, junto a buenas prácticas que\n    promuevan la transparencia y replicabilidad.\n2.  Aprender a formular preguntas y plantear hipótesis que puedan ser\n    abordadas mediante el análisis de datos.\n3.  Desarrollar la habilidad de realizar pruebas de hipótesis y\n    comprender la interpretación de sus resultados.\n\n## Contenidos:\n\n#### El proceso de análisis de datos {.unnumbered .unlisted}\n\n1.  **El proceso de análisis de datos**\n    -   Una visión general a las metodologías de análisis que veremos en\n        el curso\n    -   Adquision y almacenmiento de los datos\n    -   Preparación de los datos\n2.  **Preguntando a los datos**\n    -   Asbtrayendo la realidad, variables aleatorias y probabilidades.\n    -   Planteamiento de preguntas.\n    -   Preguntas y respuestas: el rol de las hipótesis.\n\n#### Respondiendo desde los datos: Pruebas de hipótesis {.unnumbered .unlisted}\n\n1.  **Conceptos Básicos de Pruebas de Hipótesis:**\n    -   Definición de hipótesis nula y alternativa.\n    -   Niveles de significancia y p-values.\n    -   Errores tipo I y tipo II.\n2.  **Tipos de Pruebas de Hipótesis:**\n    -   Pruebas t para comparación de medias.\n    -   Pruebas chi-cuadrado para variables categóricas.\n    -   Pruebas ANOVA para comparación de múltiples grupos.\n3.  **Interpretación de Resultados:**\n    -   Evaluación de p-values y toma de decisiones.\n    -   Significación estadística vs. significación práctica.\n    -   Comunicación de los resultados de las pruebas de hipótesis.\n\n#### Buenas prácticas en análisis de datos {.unnumbered .unlisted}\n\n1.  **Desafíos y Consideraciones:**\n    -   Privacidad y seguridad de los datos.\n    -   Limpieza y transformación durante la preparación de datos.\n2.  **Reproducibilidad y Control de Versiones (GIT):**\n    -   Importancia de mantener un registro de los cambios en los datos.\n    -   Uso de sistemas de control de versiones como GIT para rastrear\n        cambios.\n    -   Aplicación de control de versiones en proyectos de preparación\n        de datos.\n\n## El proceso de análisis de datos\n\nEn el mundo actual, la generación y recopilación de datos se ha vuelto\nmás accesible y significativa que nunca antes. Esta abundancia de\ninformación ofrece la oportunidad de extraer conocimientos valiosos que\npueden influir en la toma de decisiones y el desarrollo de soluciones\neficientes.\n\nSin embargo, el proceso de transformar estos datos crudos en información\nútil y significativa requiere una serie de pasos fundamentales que\nforman parte integral de la disciplina conocida como Ciencia de Datos.\n\n![](img/objetivo_ds.png)\n\nEn esta primera parte, daremos un vistazo general a las metodologías y\nenfoques clave que exploraremos a lo largo del curso, con énfasis en la\nimportancia de la preparación de los datos.\n\nEl proceso de análisis de datos se puede dividir en varias etapas\ninterconectadas, cada una con su propio conjunto de desafíos y\nconsideraciones.\n\n![](img/proceso_datascience.png)\n\nBajo esta mirada, tenemos varias fases clave que están interconectadas.\nEn este curso nos enfocaremos en la preparación de los datos y en su\nanálisis mediante modelos de regresión. Esto con el objetivo de\nresponder preguntas desde los datos, que provean información valiosa.\n\n### Adquisición de datos:\n\nEl primer paso en el proceso de análisis de datos implica la adquisición\ny el almacenamiento de los datos. Esto se refiere a la recolección de\nlos datos necesarios para abordar una pregunta o problema en particular.\n\nPuede implicar la recopilación de datos de fuentes diversas, como bases\nde datos, archivos CSV, páginas web o incluso sensores en tiempo real.\n\nEs crucial comprender cómo recopilar y almacenar estos datos de manera\nadecuada, garantizando su calidad, integridad y seguridad.\n\nExisten tantas fuentes de datos, como podríamos imaginar. ALgunas de las\nmás comunes son las siguientes:\n\n1.  **Encuestas y Cuestionarios:**\n    -   Diseño y administración de encuestas para recopilar datos\n        directamente de los participantes.\n    -   Permite obtener información específica y detallada según las\n        preguntas planteadas.\n2.  **Experimentos Controlados:**\n    -   Diseño de experimentos para recopilar datos bajo condiciones\n        controladas.\n    -   Útil para establecer relaciones causales y evaluar efectos de\n        cambios controlados.\n3.  **Observación y Sensores:**\n    -   Uso de sensores y dispositivos para capturar datos en tiempo\n        real.\n    -   Ampliamente utilizado en aplicaciones IoT (Internet of Things)\n        para monitorizar y recopilar información ambiental.\n    -   Utilización de sensores en dispositivos móviles y wearables para\n        recopilar datos de ubicación, salud y actividad.\n4.  **Recopilación de Datos Existentes:**\n    -   Utilización de datos ya recopilados y disponibles en bases de\n        datos o fuentes públicas.\n    -   Reduce el tiempo y costo de recopilación, pero puede tener\n        limitaciones en términos de calidad y relevancia.\n5.  **Web Scraping (Web Scrapping):**\n    -   Extracción de datos de sitios web utilizando herramientas y\n        técnicas automatizadas.\n    -   Permite recopilar información no estructurada de manera\n        eficiente, pero requiere atención a la ética y términos de uso.\n6.  **Acceso a APIs (Application Programming Interfaces):**\n    -   Interacción programática con sistemas y servicios para obtener\n        datos en tiempo real.\n    -   Común en la obtención de datos de redes sociales, información\n        climática, finanzas, entre otros.\n7.  **Colaboración y Participación Comunitaria:**\n    -   Colaboración con comunidades y grupos para recopilar datos de\n        manera colectiva.\n    -   Puede ser útil para proyectos de mapeo colaborativo, ciencia\n        ciudadana y recopilación de información local.\n8.  **Data Lakes y Almacenamiento en la Nube:**\n    -   Almacenamiento de grandes volúmenes de datos sin estructura\n        definida en sistemas de almacenamiento en la nube.\n    -   Facilita la recopilación y posterior análisis de datos\n        heterogéneos.\n    -   Usualmente se accede a través de querys SQL\n\n::: callout-tip\n#### Datos disponibles para el proyecto\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\n1.  Datos públicos sobre educación chilena\n2.  Datos públicos sobre adjudicaciones municipales\n3.  Datos publicos sobre individuos en comunas chilenas (encuesta Casen)\n4.  Datos sobre crecimiento de paises y complejidad económica\n\nVeamos como acceder algunos de estos datos.\n:::\n\n**Ejemplo: Datos públicos sobre individuos en comunas chilenas (encuesta\nCasen)**\n\nLa Encuesta de Caracterización Socioeconómica Nacional (CASEN) es una\ninvestigación realizada en Chile que tiene como objetivo principal\nrecopilar información detallada sobre la situación socioeconómica de los\nhogares y las personas en el país. Esta encuesta se lleva a cabo de\nmanera periódica y abarca una amplia variedad de temas, como ingresos,\neducación, empleo, salud, vivienda y otros aspectos relevantes para\ncomprender la realidad socioeconómica de la población chilena. La\ninformación recopilada en la Encuesta CASEN se utiliza para informar\npolíticas públicas, tomar decisiones informadas y analizar la evolución\nde indicadores sociales a lo largo del tiempo.\n\n[Sitio Web\noficial](https://observatorio.ministeriodesarrollosocial.gob.cl/encuesta-casen)\n\nSi tenemos los datos alojados en una dependencia, simplemente los\ncargamos. El formato mas comun es .csv, pero a veces estan en formatos\nextraños. Por ejemplo, .dta de STATA.\n\n::: {#d6af64da .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>folio</th>\n      <th>o</th>\n      <th>id_persona</th>\n      <th>region</th>\n      <th>comuna</th>\n      <th>zona</th>\n      <th>expr</th>\n      <th>edad</th>\n      <th>sexo</th>\n      <th>tot_per</th>\n      <th>...</th>\n      <th>esc2</th>\n      <th>educ</th>\n      <th>o1</th>\n      <th>yaut</th>\n      <th>yauth</th>\n      <th>yautcor</th>\n      <th>yautcorh</th>\n      <th>ytrabajocor</th>\n      <th>ytrabajocorh</th>\n      <th>yae</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.101100e+11</td>\n      <td>1</td>\n      <td>5</td>\n      <td>Región de Tarapacá</td>\n      <td>Iquique</td>\n      <td>Urbano</td>\n      <td>67</td>\n      <td>34</td>\n      <td>Mujer</td>\n      <td>2</td>\n      <td>...</td>\n      <td>12.0</td>\n      <td>Media humanista completa</td>\n      <td>No</td>\n      <td>220000.0</td>\n      <td>300000</td>\n      <td>220000.0</td>\n      <td>300000</td>\n      <td>150000.0</td>\n      <td>150000.0</td>\n      <td>240586.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.101100e+11</td>\n      <td>2</td>\n      <td>6</td>\n      <td>Región de Tarapacá</td>\n      <td>Iquique</td>\n      <td>Urbano</td>\n      <td>67</td>\n      <td>4</td>\n      <td>Mujer</td>\n      <td>2</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>Sin educación formal</td>\n      <td>NaN</td>\n      <td>80000.0</td>\n      <td>300000</td>\n      <td>80000.0</td>\n      <td>300000</td>\n      <td>NaN</td>\n      <td>150000.0</td>\n      <td>240586.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.101100e+11</td>\n      <td>2</td>\n      <td>31</td>\n      <td>Región de Tarapacá</td>\n      <td>Iquique</td>\n      <td>Urbano</td>\n      <td>67</td>\n      <td>5</td>\n      <td>Mujer</td>\n      <td>3</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>Básica incompleta</td>\n      <td>NaN</td>\n      <td>25000.0</td>\n      <td>941583</td>\n      <td>25000.0</td>\n      <td>941583</td>\n      <td>NaN</td>\n      <td>891583.0</td>\n      <td>439170.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.101100e+11</td>\n      <td>1</td>\n      <td>32</td>\n      <td>Región de Tarapacá</td>\n      <td>Iquique</td>\n      <td>Urbano</td>\n      <td>67</td>\n      <td>45</td>\n      <td>Hombre</td>\n      <td>3</td>\n      <td>...</td>\n      <td>15.0</td>\n      <td>Técnico nivel superior incompleta</td>\n      <td>Sí</td>\n      <td>889500.0</td>\n      <td>941583</td>\n      <td>889500.0</td>\n      <td>941583</td>\n      <td>889500.0</td>\n      <td>891583.0</td>\n      <td>439170.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.101100e+11</td>\n      <td>3</td>\n      <td>30</td>\n      <td>Región de Tarapacá</td>\n      <td>Iquique</td>\n      <td>Urbano</td>\n      <td>67</td>\n      <td>19</td>\n      <td>Mujer</td>\n      <td>3</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>No sabe</td>\n      <td>No</td>\n      <td>27083.0</td>\n      <td>941583</td>\n      <td>27083.0</td>\n      <td>941583</td>\n      <td>2083.0</td>\n      <td>891583.0</td>\n      <td>439170.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\nUna vez cargados los datos, debemos proceder a su limpieza y\nexploración, para ser preparados para analizarlos. De esto se tratará la\nsiguiente sesión del curso.\n\n**Ejemplo: Datos desde la API del banco mundial** Primero siga este\nejemplo practico de importar datos, luego será facil responder la\npregunta anterior.\n\n::: {#5f34db8d .cell execution_count=2}\n``` {.python .cell-code}\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb # para instalar: conda install pandas-datareader  o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. En este caso revisare de PIB (GDP en ingés), pero se pueden explorar muchas más opciones.\n\nwb.search('gdp')\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>unit</th>\n      <th>source</th>\n      <th>sourceNote</th>\n      <th>sourceOrganization</th>\n      <th>topics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>688</th>\n      <td>6.0.GDP_current</td>\n      <td>GDP (current $)</td>\n      <td></td>\n      <td>LAC Equity Lab</td>\n      <td>GDP is the sum of gross value added by all res...</td>\n      <td>b'World Development Indicators (World Bank)'</td>\n      <td>Economy &amp; Growth</td>\n    </tr>\n    <tr>\n      <th>689</th>\n      <td>6.0.GDP_growth</td>\n      <td>GDP growth (annual %)</td>\n      <td></td>\n      <td>LAC Equity Lab</td>\n      <td>Annual percentage growth rate of GDP at market...</td>\n      <td>b'World Development Indicators (World Bank)'</td>\n      <td>Economy &amp; Growth</td>\n    </tr>\n    <tr>\n      <th>690</th>\n      <td>6.0.GDP_usd</td>\n      <td>GDP (constant 2005 $)</td>\n      <td></td>\n      <td>LAC Equity Lab</td>\n      <td>GDP is the sum of gross value added by all res...</td>\n      <td>b'World Development Indicators (World Bank)'</td>\n      <td>Economy &amp; Growth</td>\n    </tr>\n    <tr>\n      <th>691</th>\n      <td>6.0.GDPpc_constant</td>\n      <td>GDP per capita, PPP (constant 2011 internation...</td>\n      <td></td>\n      <td>LAC Equity Lab</td>\n      <td>GDP per capita based on purchasing power parit...</td>\n      <td>b'World Development Indicators (World Bank)'</td>\n      <td>Economy &amp; Growth</td>\n    </tr>\n    <tr>\n      <th>1503</th>\n      <td>BG.GSR.NFSV.GD.ZS</td>\n      <td>Trade in services (% of GDP)</td>\n      <td></td>\n      <td>World Development Indicators</td>\n      <td>Trade in services is the sum of service export...</td>\n      <td>b'International Monetary Fund, Balance of Paym...</td>\n      <td>Economy &amp; Growth ; Private Sector ; Trade</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16626</th>\n      <td>UIS.XUNIT.GDPCAP.23.FSGOV</td>\n      <td>Initial government funding per secondary stude...</td>\n      <td></td>\n      <td>Education Statistics</td>\n      <td>Total general (local, regional and central, cu...</td>\n      <td>b'UNESCO Institute for Statistics'</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16627</th>\n      <td>UIS.XUNIT.GDPCAP.23.FSHH</td>\n      <td>Initial household funding per secondary studen...</td>\n      <td></td>\n      <td>Education Statistics</td>\n      <td>Total payments of households (pupils, students...</td>\n      <td>b'UNESCO Institute for Statistics'</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16628</th>\n      <td>UIS.XUNIT.GDPCAP.3.FSGOV</td>\n      <td>Initial government funding per upper secondary...</td>\n      <td></td>\n      <td>Education Statistics</td>\n      <td>Total general (local, regional and central, cu...</td>\n      <td>b'UNESCO Institute for Statistics'</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16629</th>\n      <td>UIS.XUNIT.GDPCAP.5T8.FSGOV</td>\n      <td>Initial government funding per tertiary studen...</td>\n      <td></td>\n      <td>Education Statistics</td>\n      <td>Total general (local, regional and central, cu...</td>\n      <td>b'UNESCO Institute for Statistics'</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16630</th>\n      <td>UIS.XUNIT.GDPCAP.5T8.FSHH</td>\n      <td>Initial household funding per tertiary student...</td>\n      <td></td>\n      <td>Education Statistics</td>\n      <td>Total payments of households (pupils, students...</td>\n      <td>b'UNESCO Institute for Statistics'</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>540 rows × 7 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#f955e4fa .cell execution_count=3}\n``` {.python .cell-code}\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>iso3c</th>\n      <th>iso2c</th>\n      <th>name</th>\n      <th>region</th>\n      <th>adminregion</th>\n      <th>incomeLevel</th>\n      <th>lendingType</th>\n      <th>capitalCity</th>\n      <th>longitude</th>\n      <th>latitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABW</td>\n      <td>AW</td>\n      <td>Aruba</td>\n      <td>Latin America &amp; Caribbean</td>\n      <td></td>\n      <td>High income</td>\n      <td>Not classified</td>\n      <td>Oranjestad</td>\n      <td>-70.0167</td>\n      <td>12.5167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AFE</td>\n      <td>ZH</td>\n      <td>Africa Eastern and Southern</td>\n      <td>Aggregates</td>\n      <td></td>\n      <td>Aggregates</td>\n      <td>Aggregates</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AFG</td>\n      <td>AF</td>\n      <td>Afghanistan</td>\n      <td>South Asia</td>\n      <td>South Asia</td>\n      <td>Low income</td>\n      <td>IDA</td>\n      <td>Kabul</td>\n      <td>69.1761</td>\n      <td>34.5228</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AFR</td>\n      <td>A9</td>\n      <td>Africa</td>\n      <td>Aggregates</td>\n      <td></td>\n      <td>Aggregates</td>\n      <td>Aggregates</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AFW</td>\n      <td>ZI</td>\n      <td>Africa Western and Central</td>\n      <td>Aggregates</td>\n      <td></td>\n      <td>Aggregates</td>\n      <td>Aggregates</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nObservar que este es un data frame con dos índices: pais y año. Para\nmayor referencia coo tratar este tipo de datos ver en\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\n\nObtengamos un data frame con los datos de Chile, entre 1980 y 2020.\n\n::: {#4fd081d5 .cell execution_count=4}\n``` {.python .cell-code}\n#sabemos que queremos Chile, asi que busquemos su info\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB\n```\n:::\n:::\n\n\n::: {#4d31c46d .cell execution_count=5}\n``` {.python .cell-code}\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>NY.GDP.PCAP.KD</th>\n    </tr>\n    <tr>\n      <th>country</th>\n      <th>year</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Chile</th>\n      <th>2020</th>\n      <td>12741.157507</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>13761.374474</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>13906.770558</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>13615.523858</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>13644.623261</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nSi quisieramos, por simplicidad quedarnos solo con el indice del año y\nreordenar el dataframe:\n\n::: {#5eb4076c .cell execution_count=6}\n``` {.python .cell-code}\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el análisis es para un solo país\nreversed_df.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NY.GDP.PCAP.KD</th>\n    </tr>\n    <tr>\n      <th>year</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1980</th>\n      <td>4694.337113</td>\n    </tr>\n    <tr>\n      <th>1981</th>\n      <td>4928.563103</td>\n    </tr>\n    <tr>\n      <th>1982</th>\n      <td>4322.647868</td>\n    </tr>\n    <tr>\n      <th>1983</th>\n      <td>4047.790234</td>\n    </tr>\n    <tr>\n      <th>1984</th>\n      <td>4154.496068</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAhora, realicemos un grafico rápido con nuestros datos:\n\n::: {#d194e4d0 .cell execution_count=7}\n``` {.python .cell-code}\n# Graficamos\n\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'Año')\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nText(0.5, 0, 'Año')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](sesion1_notas_files/figure-ipynb/cell-8-output-2.png){}\n:::\n:::\n\n\n::: callout-warning\n#### Taller 1: Pregunta 1 - Bajando y formateando datos del Banco Mundial\\*\\*\n\nReplique el ejemplo práctico de importar datos desde la API del Banco\nMundial y empezar la base para su análisis de series de tiempo.\n\nImporte usted la serie de GDP total Y Percapita para otro país serie\ndesde la API del Banco mundial, muestre sus principales características\ny realice un grafico.\n\n**¿pareciera haber tendencias?**\n:::\n\n### Preguntando a los datos\n\n¿Cómo plantear preguntas y formular hipótesis en el contexto del\nanálisis de datos? El proceso de análisis comienza con la curiosidad y/o\nnecesidad.cLa formulación de preguntas relevantes que se puedan\nresponder mediante la exploración y el examen de los datos disponibles.\n\nInicia con la identificación de áreas de interés y la formulación de\npreguntas específicas relacionadas con esos temas. Estas preguntas\npueden surgir de la necesidad de resolver un problema, entender un\nfenómeno o explorar patrones en los datos. Un buen planteamiento de\npreguntas es crucial, ya que guiará todo el proceso de análisis.\n\n![El proceso de abstraer la realidad](img/abstraccionpng.png)\n\n**Preguntas e hipótesis:**\n\nUna hipótesis es una afirmación, verificable con evidencia. En este\nsentido, para toda pregunta podemos responderla mediante hipótesis.\n\nEn particular, para responder a las preguntas en el contexto de datos,\nes común formular hipótesis nulas y alternativas.\n\nLa hipótesis nula es aquella que propone que algun parámetro toma cierto\nvalor. Este generlamente es un punto de verdad. Si bien, con datos no\npodemos corroborar que algo es cierto, si podemos dar evidencia de que\nno es cierto. En general, planteamos el problema de tal manera que\npodamos rechazar la hipótesis nula, en favor de otra que llamamos\nalternatiba.\n\nQuizas, la hipotesis nula más famosa es la prueba de \"significancia\". En\nesta se propone que un parámetro (muchas veces un efecto, o correlación)\nes 0, es decir, plantea que no hay efecto o relación entre las\nvariables, mientras que la hipótesis alternativa sugiere que sí existe\nuna relación o efecto significativo.\n\nEstas hipótesis son fundamentales para establecer una base objetiva para\nel análisis y para evaluar las evidencias encontradas en los datos. El\nproceso de plantear preguntas y formular hipótesis es el primer paso en\nel análisis de datos, ya que establece una guía clara para el enfoque y\nla dirección del trabajo. **Al identificar preguntas y establecer\nhipótesis, se crea un marco sólido que orientará la exploración y el\nanálisis de los datos disponibles.**\n\n::: callout-warning\n#### Taller 1: Pregunta 2 - Investigando sobre países:\n\nConsidere que tenemos los datos del banco mundial, del país que\nselecciono anteriormente, y desea aprender sobre alguna caracterpistica\nde dicho pais en el periodo.\n\nEscriba una pregunta de investigación que se pueda responder con los\ndatos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué\nhipótesis podria responder su pregunta?\n:::\n\n## Respondiendo desde los datos\n\n### Inferencia estadística\n\nInferencia se refiere al proceso de hacer generalizaciones de una\npoblación a partir de una muestra de esa población. En particular, la\nidea es que si tenemos un conjunto de datos (muestra) obtenido de una\npoblación más grande, el cual es representativo de esta, podemos\nutilizar métodos estadísticos para sacar conclusiones sobre las\ncaracterísticas y propiedades de esa población en su totalidad.\n\n![Población y Muestra](img/poblacion_muestra.png)\n\nEl proceso de inferencia estadística se basa en el principio de que una\nmuestra bien seleccionada puede proporcionar información valiosa sobre\nla población en general. Mediante el análisis de la muestra, podemos\nestimar parámetros poblacionales, como la media, la proporción o la\ndesviación estándar, y también podemos construir intervalos de confianza\npara estimar el rango dentro del cual se espera que se encuentren estos\nparámetros.\n\nEl uso de la inferencia estadística es fundamental, especialmente si es\nimpracticable o costoso analizar cada elemento de una población en\nparticular. Por ejemplo, en lugar de encuestar a todos los ciudadanos de\nun país, es mucho más factible encuestar una muestra representativa y\nutilizar esa información para hacer suposiciones sobre la opinión de la\npoblación en general.\n\n#### Estadígrafos y el Teorema del Límite central\n\nEntonces, en cada muestra que tenemos podemos calcular aproximaciones a\nlos parámetros poblacionales de interés. Estos son los llamados\n**estadísgrafos** ![Estadigrafos](img/estadigrafos.png)\n\nDado que por cada muestra que tenemos, vamos a calcular un estadígrafo\neste es en si mismo una variable aleatoria. Tiene su propia\ndistribución, media y varianza!\n\n![](img/distribuciones_estadigrafos.png)\n\nEl estadígrafo más conocido es el promedio o media muestral.\n\n![Estadigrafos más comunes](img/tabla_estadigrafoscomunes.png)\n\nCada estimador es una función de la muestra, por ende para cada muestra\nque tengamos obtendremos un valor numérico específico para el estimador.\nPor este motivo, cuando estamos trabajando con una única muestra\nespecífica, tenemos un único valor del estimador, o estimador puntual.\n\nNunca (o casi nunca) podemos conocer el valor verdadero de los\nparámetros en la población, por lo cual un primer camino tentador es\nusar el estimador puntual para tomar una decisión. Como nunca podemos\nconocer el verdadero parámetro, tampoco podemos saber a ciencia cierta\nsi el estimador puntual es cercano a este.\n\n**¿Cómo conectamos estadpigrafos y parámetros?**\n\nEl teorema del límite central, nos dice que, bajo ciertas condiciones,\nla distribución de las medias muestrales de una población se aproxima a\nuna distribución normal a medida que el tamaño de la muestra aumenta,\nindependientemente de la forma de la distribución original de la\npoblación. Este teorema es esencial en inferencia estadística y tiene\namplias aplicaciones en análisis de datos y toma de decisiones.\n\n![La media muestral se distribuye normal, sin importar la distribución\nde la variable subyacente](img/promedio_TLC.png)\n\nFormalmente, el Teorema del Límite Central establece lo siguiente:\n\n$$\\bar{x} \\sim_a N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) $$\n\nSupongamos que tenemos una población con media μ y desviación estándar σ\nfinitas. Si tomamos muestras aleatorias de tamaño n de esta población y\ncalculamos la media muestral de cada muestra, entonces, a medida que n\ntiende a infinito, la distribución de estas medias muestrales se\naproximará a una distribución normal con media μ y desviación estándar\nσ/√n.\n\nEn otras palabras, sin importar la distribución original de la\npoblación, cuando el tamaño de la muestra es suficientemente grande, la\ndistribución de las medias muestrales seguirá una forma de campana\nsimilar a la distribución normal. Este resultado es fundamental para\nrealizar inferencias sobre la población a partir de muestras, ya que nos\npermite aplicar métodos basados en la distribución normal incluso cuando\nla población original no sigue una distribución normal.\n\n#### Error estándar\n\n-   Corresponde a un estimador de la desviación estándar del estimador.\n\n-   Identifica que tan lejos estamos del verdadero valor poblacional.\n\n-   Para la **media muestral**:\n\n$$ SE = \\frac{S_y}{\\sqrt{n}}$$\n\nSe utiliza para evaluar a los estimadores, mediante *pruebas de\nhipotesis* y construir *intervalos de confianza*\n\n-   Si se conoce un estimador y su desviación estándar, podemos saber\n    qué tan precisa es la estimación (mucha o poca varianza), pero no\n    podemos saber si el estimador está cercano o no a su valor verdadero\n    en la población (el cual no conocemos).\n\n-   Nunca (o casi nunca) podemos conocer el valor verdadero de los\n    parámetros en la población.\n\n-   Sí se puede construir un conjunto de valores que contienen el\n    parámetro poblacional con alguna probabilidad (llamada el nivel de\n    confianza).\n\n-   Un intervalo de confianza contiene los posibles valores del\n    estimador, entre un límite inferior y un límite superior, con cierta\n    probabilidad.\n\n#### Inferencia sobre Estadígrafos y parámetros - Conectados por el Teorema del Límite central\n\n$$ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)$$\n\n-   Con este teorema, podemos construir inferencia de \\mu a partir de\n    \\bar{x} indirectamente.\n    -   Intervalos de confianza\n    -   Pruebas de hipótesis\n    -   p-valor\n\n### Intervalos de confianza\n\nUna primera manera de aproximarnos a los parámetros poblacionales\n(particularmente a la esperanza) es mediante la construcción de\nintervalos de confianza.\n\n![](img/intervalo_dist.png)\n\nUn intervalo de confianza contiene los posibles valores del estimador,\nentre un límite inferior y un límite superior, con cierta probabilidad.\n\n![](img/intervalo_dist2.png)\n\n¿De dónde saco los valores críticos?\n\nLos valores críticos de una distribución los obtenemos de una tabla de\ndistribución o para calcular podemos usar excel, R o en python:\n\n```         \nscipy.stats.t.isf(alpha, n-p)\n```\n\nSi estamos trabajando con dos colas, usar alpha/2 porque la probabilidad\nde error la estamos repartiendo a ambas colas.\n\n::: callout-note\n#### \\[Matemáticamente\\] Caso 1: Varianza conocida\n\nSumpongamos que tenemos una muestra aleatoria: $y_1, y_2, \\dots, y_n$ de\nuna población $Y\\sim N(\\mu, \\sigma^2)$\n\n-   La media muestral $\\bar{y}= \\frac{1}{n}\\sum_{i=1}{n}y_i$\n    -   Su esperanza es: $E(\\bar{y}) =\\mu$\n    -   Su varianza es: $var(\\bar{y}) = \\frac{\\sigma^2}{n}$\n    -   se distribuye normal, tal que podemos estandarrizar: \\$\n        \\frac{\\bar{Y} - \\mu_y }{ \\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1) \\$\n\nEntonces, podemos describir que:\n\n\\$ P( -1.96 \\< \\frac{\\bar{Y} - \\mu_y }{ \\frac{\\sigma}{\\sqrt{n}}} \\< 1.96\n) = 0.95 \\$\\$\n\n\\$ P( \\bar{y}-\\frac{1.96\\sigma }{\\sqrt{n}} \\< \\mu \\< \\bar{y} +\n\\frac{1.96\\sigma }{\\sqrt{n}} ) = 0.95 \\$\n\n-   este intervalo es aleatorio, porque $\\bar{y}$ es diferente en cada\n    muestra.\n\n-   para el 95% de las muestras elatorias, el intervalo construido de\n    esta manera contendrá a $\\mu$\n:::\n\n::: callout-note\n#### \\[Matemáticamente\\] Caso 2: Varianza desconocida\n\n-   Supongamos que tenemos una muestra aleatoria $y_1, y_2, \\dots, y_n$\n    de una población $y\\sim N(\\mu, \\sigma^2 )$\n\n-   Usamos la estimación de la desviación estándar muestral:\n    $$ S_y = \\sqrt{\\frac{1}{n-1} \\sum{i=1}{n}(y_i - \\bar{y})^2} $$\n\n-   Y si estandarizamos $\\bar{y}$:\n    $$ \\frac{\\bar{Y} - \\mu_y }{ \\frac{S}{\\sqrt{n}}} \\sim t_{n-1} $$\n\n-   Podeos constrir un intervalo de la porbabilidad de estar al 95 con\n    el valor critico c adecuado a los grados de libertad n-1:\n\n$$ P( -c <   \\frac{\\bar{Y} - \\mu_y }{ \\frac{ S}{\\sqrt{n}}}  < c  ) =0.95  $$\n\n$$ P(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}} < \\mu < \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) 0.95$$\n\n-   Si llamamos al error estandar SE: $SE(\\bar{y})=\\frac{S}{\\sqrt{n}}$\n\n-   El IC es:\n    $$ (\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) $$\n:::\n\n### El intervalo es una muestra aleatoria\n\nEsto quiere decir que para cada muestra podemos construir un intervalo.\n\nAsí como el estimador es una variable aleatoria, esto también es cierto\npara los intervalos de confianza. Por eso también se les llama\nintervalos aleatorios, ya que con diferentes muestras obtendremos un\ndiferente estimador e intervalo.\n\nPor ende, supongamos que contamos con 20 muestras, entonces\nconstruiremos 20 intervalos de confanza diferentes para los 20\nestimadores puntuales.\n\n![](img/intervalo_confianza1.png)\n\n**Interpretación de intervalo de confianza**\n\nPensemos en un 95% de confianza (un valor usual). Esto quiere decir, que\nsi se repitiera este ejercicio muchas veces y construyéramos un\nintervalo de esta forma el 95% de ellos contendría el verdadero\nparámetro poblacional.\n\nUn elemento importante a considerar es que esto no significa que con 95%\nde certeza el parámetro está exactamente en estos valores. Por ejemplo,\nal 95% de confianza con 20 intervalos 19 contendrán el parámetro.\n\n![](img/intervalo_confianza2.png)\n\n### Pruebas de hipótesis\n\n-   Una forma de verificar hipotesis sobre los parámetros es mediante el\n    contraste de hipótesis.\n\nEmpezamos suponiendo que hay una distribución conocida para el\nestadígrafo, centrada en un valor específico. Y nos preguntamos, si esto\nfuea verdad ¿qué tan probable es la muestra que tengo?\n\n-   Llamamos la hipótesis a probar Ho, y su alternativa H1.\n\n![](img/prueba_hip2.png)\n\n![](img/prueba_hip3.png)\n\n### Errores y P-valor\n\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:\n\n![](img/prueba_hip.png)\n\n-   Tipo I: Rechazar Ho cuando es cierta\n-   Tipo II: No rechazar Ho cuando es falsa.\n\n![](img/prueba_errores.png)\n\n![](img/prueba_errores2.png)\n\n-   Se elige nivel de significancia de contraste (α) = probabilidad de\n    cometer error Tipo I. Típicamente α = 0,01, 0,05, 0,10.\n\n-   Para contrastar una hipótesis con su alternativa, debemos elegir:\n\n    1.  Un estadístico de contraste\n    2.  Una regla de rechazo, la cual depende de un valor crítico.\n\n    ![](https://www.dropbox.com/s/7cme0p3q8fyiep2/ic.png?raw=1)\n\n### Prueba de significancia\n\nDefinimos la prueba de hipótesis de **significancia** como aquella que\nindica si un estimador $\\hat{T}$ es 0.\n\n$$ H_0: T =0\\text{ vs }H_1: T \\neq 0 $$\n\nEl **Valor de probabilidad (ó p-valor)** es el nivel probabilidad más\nalto para el cual no podemos rechazar la hipótesis nula de la prueba de\nsignificancia.\n\nEjemplo: \\$H_0: \\mu=0 \\$ y en la muestra especifica t= 1.52:\n\n$$ P-valor = P(T>1.52 \\vert h_0)= 1- \\phi(1.52)=0.0065$$\n\n-   el mayor nivel de significancia estadistica al cual no rechazamos\n    $H_0$ es 6.5%\n\n-   la probabilidad de observar un velor $T\\geq 1.52$ cuando $H_0$ es\n    cierta es en un 6.5 de las muestras.\n\n-   P-valores bajos dan evidencia en contra de $H_0$, ya que la\n    probabilidad de observarlo si $H_0$ es cierta es bajo.\n\n### Ejemplo de aplicación: Peso de los Pingüinos Palmer\n\nConsideremos los datos de los *pingüinos Palmer*. Los datos \"Palmer\nPenguins\" son un conjunto que detalla medidas morfológicas y\ncaracterísticas de tres especies de pingüinos: Adelie, Gentoo y\nChinstrap. Recopilados por el Dr. Bill Link y su equipo. (Horst AM, Hill\nAP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica)\npenguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0,\nhttps://allisonhorst.github.io/palmerpenguins/index.html)\n\n![](https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png)\n\n::: {#dfdacb71 .cell execution_count=8}\n``` {.python .cell-code}\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_length_mm</th>\n      <th>bill_depth_mm</th>\n      <th>flipper_length_mm</th>\n      <th>body_mass_g</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEn el contexto de los pingüinos y el peso de su población, podríamos\ntomar una muestra de pingüinos y calcular un intervalo de confianza para\nel peso promedio. Esto nos daría una estimación del peso promedio de la\npoblación total, junto con la confianza en que este valor estimado es\npreciso.\n\nEs importante tener en cuenta que el proceso de inferencia estadística\nse basa en suposiciones y en el uso adecuado de técnicas estadísticas.\n\nLa elección de la muestra, la interpretación de los resultados y el\nnivel de confianza seleccionado son aspectos cruciales para realizar\ninferencias precisas y significativas.\n\nRelicemos algunos ejemplos de pruebas de hipótesis, sobre el peso de los\npingüinos.\n\nPrimero calcularemos el promedio muestral y lo veremos en el contexto de\nlos datos observados:\n\n::: {#510f737d .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los pingüinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribución del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribución de Peso de Pingüinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion1_notas_files/figure-ipynb/cell-10-output-1.png){}\n:::\n:::\n\n\nNuestra idea de la inferencia, es aprovechar las propiedades del\npromedio muestral. De que es el promedio muestral el que se distribuye\nnormal, su media es la media poblacional y conocemos sus\ncaracterísticas.\n\nPor ejemplo, consideremos que de esta población de pingüinos obtenemos\n1000 muestras de 50 individuos cada una. Si graficamos sus medias,\npodremos ver que estas se distribuyen aproximadamente normal.\n\n-   Si reducimos el tamaño de muestra, más nos alejamos de la\n    distribución normal.\n-   Si reducimos el número de repeticiones tambieé.\n\n::: {#4a8656bf .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\n\n# Definir el tamaño de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y cálculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gráfico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribución de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion1_notas_files/figure-ipynb/cell-11-output-1.png){}\n:::\n:::\n\n\n#### Intervalo de confianza\n\nObtengamos una muestra y calculemos un intervalo de confianza:\n\n::: {#8d85d83c .cell execution_count=11}\n``` {.python .cell-code}\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n# Obtener una muestra simple de 40 pingüinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error estándar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviación estándar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntervalo de Confianza para el Peso:\n(4065.7802580212597, 4526.71974197874)\n```\n:::\n:::\n\n\nEl resultado será un rango de valores dentro del cual es probable que se\nencuentre el verdadero peso promedio de los pingüinos en la población,\ncon un nivel de confianza del 95%.\n\n¿Como nos fue? ¿Contiene al verdadero valor?\n\n### Comparaciones de grupos\n\nAhora consideremos que tenemos grupos que queremos comparar.\n\nSi hacemos una grafica de distribución de tamaño por especie y sexo,\npodriamos empezar a analizar diferencias entre los grupos.\n\n::: {#6ab845d0 .cell execution_count=12}\n``` {.python .cell-code}\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\ntabla_doble_entrada\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>sex</th>\n      <th>Promedio</th>\n      <th>Varianza</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Female</td>\n      <td>3368.835616</td>\n      <td>72565.639269</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Male</td>\n      <td>4043.493151</td>\n      <td>120278.253425</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chinstrap</td>\n      <td>Female</td>\n      <td>3527.205882</td>\n      <td>81415.441176</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Chinstrap</td>\n      <td>Male</td>\n      <td>3938.970588</td>\n      <td>131143.605169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Gentoo</td>\n      <td>Female</td>\n      <td>4679.741379</td>\n      <td>79286.335451</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Gentoo</td>\n      <td>Male</td>\n      <td>5484.836066</td>\n      <td>98068.306011</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la\nespecie Adelie, para diferentes sexos:\n\n**Pregunta de Prueba de Hipótesis:**\n\n¿Existe una diferencia significativa en el peso promedio entre los\npingüinos machos y las pingüinas hembras en la especie \"Adelie\"?\n\n-   Hipótesis Nula (H0):\n\nNo hay diferencia significativa en el peso promedio entre los pingüinos\nmachos y las pingüinas hembras en la especie \"Adelie\".\n\n-   Hipótesis Alternativa (H1):\n\nExiste una diferencia significativa en el peso promedio entre los\npingüinos machos y las pingüinas hembras en la especie \"Adelie\".\n\nPara probar esta hipótesis, podrías utilizar una prueba de hipótesis\npara comparar las medias de las muestras de peso de los pingüinos machos\ny hembras en la especie \"Adelie\". Esto te permitiría determinar si la\ndiferencia observada en el peso promedio es lo suficientemente grande\ncomo para considerarse estadísticamente significativa.\n\n::: {#497ddeb2 .cell execution_count=13}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribución de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribución de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](sesion1_notas_files/figure-ipynb/cell-14-output-2.png){}\n:::\n:::\n\n\nA simple vista podriamos pensar ambos grupos son diferentes. Es más\nclaro si dibujamos el promedio muestral observado.\n\n::: {#5e7a33fc .cell execution_count=14}\n``` {.python .cell-code}\n# Crear un gráfico de densidad con líneas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion1_notas_files/figure-ipynb/cell-15-output-1.png){}\n:::\n:::\n\n\nSi construimos una prueba t de diferencia de medias:\n\n::: {#5a02aac3 .cell execution_count=15}\n``` {.python .cell-code}\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estadística t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gráfico de comparación de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Machos y Hembras de Pingüinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstadística t: 13.126285923485874\nValor p: 6.402319748031793e-26\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](sesion1_notas_files/figure-ipynb/cell-16-output-2.png){}\n:::\n:::\n\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes\nIslas. Para esto podriamos usar una prueba ANOVA.\n\nEn este código, primero cargamos el conjunto de datos \"Penguins\" y luego\ncreamos dos subconjuntos separados para machos y hembras. Después,\nutilizamos la función stats.f_oneway() para realizar una prueba ANOVA\npara comparar los pesos entre hembras y machos. El resultado incluye la\nestadística F y el valor p.\n\nEl valor p nos indica si hay una diferencia significativa entre los\ngrupos. Si el valor p es menor que un umbral de significancia (por\nejemplo, 0.05), podríamos rechazar la hipótesis nula y concluir que hay\nuna diferencia significativa en el peso entre hembras y machos de\ndiferentes islas.\n\nRecuerda que, antes de realizar una prueba ANOVA, es importante\nverificar las suposiciones necesarias, como la normalidad y la\nhomogeneidad de varianzas en los grupos. Si estas suposiciones no se\ncumplen, podría ser necesario considerar otras pruebas estadísticas o\ntransformaciones de los datos.\n\n::: {#6d83e0d8 .cell execution_count=16}\n``` {.python .cell-code}\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estadística F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstadística F: 72.96098633250911\nValor p: 4.897246751596325e-16\n```\n:::\n:::\n\n\n::: {#008b68e0 .cell execution_count=17}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Calcular las medias de peso por género e isla\nmedias_peso = penguins.groupby(['species', 'island', 'sex'])['body_mass_g'].mean().reset_index()\n\n# Crear un gráfico de barras con puntos y intervalos de confianza\nplt.figure(figsize=(10, 6))\nsns.barplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', errorbar='sd', palette=['pink', 'blue'])\n#sns.boxplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Hembras y Machos por Isla')\nplt.xlabel('Especie e Isla')\nplt.ylabel('Media de Peso (g)')\nplt.legend(title='Sexo')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion1_notas_files/figure-ipynb/cell-18-output-1.png){}\n:::\n:::\n\n\n### Experimentos Aleatorios y pruebas A/B\n\nUn experimento estadístico es un enfoque científico que busca establecer\nrelaciones de causalidad y obtener conclusiones sobre cómo ciertas\nvariables afectan a otras. Los experimentos estadísticos se diseñan para\nmanipular deliberadamente una o más variables independientes y observar\nlos efectos que tienen sobre una variable dependiente. Al controlar y\nmanipular las variables de interés, los experimentos permiten a los\ninvestigadores hacer afirmaciones más sólidas sobre las relaciones\ncausales.\n\nUna prueba A/B, también conocida como prueba de división, es una técnica\nutilizada en la investigación y el análisis para comparar dos variantes\no grupos con el fin de determinar cuál de ellos produce un mejor\nresultado en términos de rendimiento, efectividad o preferencia. En una\nprueba A/B, se selecciona un grupo de muestra y se divide en dos grupos,\nuno que experimenta la variante \"A\" (por ejemplo, una versión actual) y\notro que experimenta la variante \"B\" (por ejemplo, una versión\nmodificada). Luego, se recopilan datos y se comparan los resultados de\nambos grupos para determinar cuál variante es más efectiva. Las pruebas\nA/B son comunes en marketing, diseño de productos y desarrollo web para\ntomar decisiones informadas sobre mejoras y optimizaciones.\n\nLas pruebas A/B es son ampliamente utilizado en diversas áreas, como el\nmarketing, la investigación de usuarios y el diseño de productos. En una\nprueba A/B, se seleccionan dos grupos de muestra: uno experimenta la\nversión original (A) y el otro experimenta una variante modificada (B).\nLa idea detrás de una prueba A/B es evaluar si la variante B produce un\nefecto significativamente diferente en una métrica de interés en\ncomparación con la variante A.\n\nMediante la asignación aleatoria de los participantes a los grupos A y\nB, y al controlar las condiciones en las que se les presenta cada\nvariante, se reduce la posibilidad de sesgos y se permite un análisis\ncausal más confiable. Al comparar las diferencias observadas en los\nresultados entre los grupos A y B, es posible inferir si la variante B\ntiene un impacto significativo en la variable de interés.\n\nSin embargo, es importante tener en cuenta que aunque las pruebas A/B\nproporcionan evidencia de asociación causal, no garantizan que la\ncausalidad sea absoluta. Otros factores no controlados pueden influir en\nlos resultados. Para obtener una comprensión más completa de la\ncausalidad, los experimentos controlados aleatorizados y el uso de\nmétodos de diseño experimental sólidos son esenciales. Las pruebas A/B\nson una herramienta poderosa para explorar causas y efectos en\ncondiciones controladas y analizar el rendimiento relativo de diferentes\nopciones.\n\nVeamos un ejemplo en la práctica. Este es parte del ejercicio de\naplicación.\n\n## Caso: **Aplicación de A/B testing para promoción de Marketing**\n\n### Enunciado\n\nImaginemos que trabajamos en una empresa de e-commerce que vende\nproductos electrónicos y queremos aumentar las ventas en una línea de\nproductos específica, como teléfonos móviles.\n\nPara ello, decidimos utilizar una promoción de ventas basada en una\nruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\n\nPara implementar la promoción, primero seleccionamos aleatoriamente un\ngrupo de clientes y les enviamos un correo electrónico con un enlace a\nla ruleta lúdica. Al hacer clic en el enlace, los clientes son\nredirigidos a una página en la que pueden girar la ruleta y ganar un\ndescuento en su próxima compra.\n\nVamos a pensar que los clientes son asignados a uno de los siguientes\ngrupos: - Control: no les da una promoción (mala suerte, intentalo otra\nvez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2:\nUn complemento gratuito (carcasa) que tiene un costo para la empresa\nsimilar al descuento.\n\n### Creación de los datos\n\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\n\nEste código creará un conjunto de datos con 400 observaciones (200 en el\ngrupo de control y 200 en el grupo de tratamiento), donde se simulan\nlascompras de cada usuario.\n\n::: {#c4088582 .cell execution_count=18}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grupo</th>\n      <th>tipo_tratamiento</th>\n      <th>ventas</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Control</td>\n      <td>Control</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Control</td>\n      <td>Control</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Control</td>\n      <td>Control</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Control</td>\n      <td>Control</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Control</td>\n      <td>Control</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>Treatment</td>\n      <td>Treatment 2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>Treatment</td>\n      <td>Treatment 2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>Treatment</td>\n      <td>Treatment 2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>Treatment</td>\n      <td>Treatment 2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>Treatment</td>\n      <td>Treatment 2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 3 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: callout-warning\n#### Taller 1: Pregunta 3 -Ejemplo AB test en Marketing:\n\nConsidere que tenemos los datos del banco mundial, del país que\nselecciono anteriormente, y desea aprender sobre alguna caracterpistica\nde dicho pais en el periodo.\n\nEscriba una pregunta de investigación que se pueda responder con los\ndatos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué\nhipótesis podria responder su pregunta?\n\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\n1.  Describa los resultados de la promocion para los diferentes grupos,\n    en terminos de estadisticas descriptivas.\n2.  Compare visualmente los resultados de los diferentes grupos.\n3.  ¿Fue la promocion efectiva? Use una prueba de hipotesis para\n    analizar el grupo tratado y de control.\n4.  ¿Cual de las promociones fue más efectiva? Use una prueba ANOVA.\n:::\n\n### Buenas prácticas en análisis de datos\n\n#### Importancia de la Adquisición y Almacenamiento de Datos\n\nLa adquisición y el almacenamiento de datos son los cimientos sobre los\ncuales se construye todo el proceso de análisis. La calidad y la\nconfiabilidad de los datos que obtengamos son fundamentales para\nasegurarnos de que los resultados y conclusiones que extraigamos sean\nprecisos y relevantes. En esta sección, exploraremos la importancia de\nesta etapa y cómo afecta todo el flujo de trabajo de la ciencia de\ndatos.\n\nGarantía de Calidad y Fiabilidad en la Obtención de Datos: Obtener datos\nconfiables es el primer paso para garantizar que nuestras conclusiones\nsean sólidas. La calidad de los datos está relacionada con la precisión,\nintegridad y consistencia de la información que recopilamos. Asegurarnos\nde que los datos sean precisos desde el principio minimiza la\nposibilidad de errores en análisis posteriores. Exploraremos técnicas y\nprácticas para verificar la calidad de los datos y cómo mitigar posibles\nfuentes de error.\n\nExploración de Diferentes Fuentes de Datos y su Impacto en los\nResultados: En el mundo actual, los datos provienen de diversas fuentes:\nbases de datos, encuestas, sensores, redes sociales, entre otros. Cada\nfuente tiene sus propias características y potenciales sesgos.\nComprender las diferencias entre estas fuentes y cómo pueden influir en\nlos resultados es crucial para tomar decisiones informadas. Analizaremos\nejemplos de cómo la elección de la fuente de datos puede afectar las\nconclusiones y cómo evaluar la confiabilidad de las fuentes.\n\n#### Metodologías de Levantamiento y Adquisición de Datos:\n\nEl proceso de obtención de datos implica una planificación cuidadosa.\nExploraremos diversas metodologías utilizadas para recopilar datos,\ndesde encuestas y experimentos hasta scraping de datos en línea. Cada\nmetodología tiene sus propias ventajas y desventajas, y es importante\nseleccionar la más adecuada para los objetivos del análisis.\nDiscutiremos cómo diseñar encuestas efectivas, cómo considerar la ética\nen la recopilación de datos y cómo aprovechar las fuentes de datos\nexistentes.\n\nEsta sección nos proporcionará una base sólida para comprender cómo\nadquirir y almacenar datos de manera efectiva y confiable. Una vez que\ncomprendamos cómo obtener datos de calidad, podremos avanzar con\nconfianza en las etapas posteriores del proceso de análisis, sabiendo\nque estamos trabajando con una base sólida y confiable.\n\n#### Desafíos y Consideraciones:\n\nA medida que ingresamos al emocionante mundo del análisis de datos, nos\nencontramos con una serie de desafíos y consideraciones que debemos\nabordar de manera efectiva para garantizar el éxito de nuestro proyecto.\nEstos desafíos abarcan desde la protección de la privacidad de los datos\nhasta las complejidades de la limpieza y transformación durante la etapa\nde preparación.\n\n#### Privacidad y Seguridad de los Datos:\n\nUno de los aspectos más críticos en el análisis de datos es la\nprivacidad y seguridad de la información. Los datos pueden contener\ninformación sensible y personal, y es esencial proteger la\nconfidencialidad de las personas y organizaciones involucradas.\nExploraremos prácticas y regulaciones para garantizar que los datos se\nmanejen de manera ética y legal. Discutiremos cómo anonimizar los datos,\nutilizar técnicas de enmascaramiento y seguir las mejores prácticas para\nresguardar la privacidad de los individuos.\n\n#### Limpieza y Transformación durante la Preparación de Datos:\n\nLa etapa de preparación de datos es crucial para asegurarse de que los\ndatos sean aptos para el análisis. Sin embargo, este proceso no está\nexento de desafíos. Los datos pueden contener valores faltantes,\nduplicados y errores que deben abordarse de manera adecuada.\nExploraremos técnicas para identificar y manejar valores atípicos y\nfaltantes, así como la importancia de la normalización y estandarización\nde los datos. Aprenderemos cómo transformar los datos en un formato\nadecuado para el análisis, incluida la reorganización de variables y la\ncreación de nuevas características. En resumen, enfrentamos una serie de\ndesafíos y consideraciones clave en nuestro viaje hacia el análisis de\ndatos significativo. Desde la protección de la privacidad hasta la\npreparación efectiva de los datos, abordar estos desafíos de manera\nadecuada es esencial para garantizar que nuestras conclusiones sean\nsólidas, confiables y éticas.\n\n#### Reproducibilidad y Control de Versiones (GIT):\n\nKey ideas:\n\n-   Una documentacion detallada del analisis, de las desiciones tomadas.\n    -   Notebooks pueden ser una buena herramienta inicial.\n-   Importancia de mantener un registro de los cambios en los datos.\n-   Uso de sistemas de control de versiones como GIT para rastrear\n    cambios.\n-   Aplicación de control de versiones en proyectos de preparación de\n    datos.\n\nLa reproducibilidad y el control de versiones son componentes\nfundamentales para garantizar la integridad y la transparencia en el\nanálisis de datos. Además de mantener un registro detallado de las\ndecisiones tomadas durante el proceso, el uso de sistemas de control de\nversiones como GIT se vuelve esencial para mantener la trazabilidad y la\ncolaboración efectiva en proyectos de preparación y análisis de datos.\n\n**Documentación Detallada del Análisis y Uso de Notebooks:** Una\ndocumentación exhaustiva del análisis es esencial para comprender el\nflujo de trabajo, las decisiones tomadas y las transformaciones\naplicadas a los datos. Los notebooks, como Jupyter Notebooks, ofrecen\nuna herramienta excepcional para lograr esto. En cada celda de un\nnotebook, es posible combinar explicaciones en lenguaje natural con\ncódigo ejecutable y visualizaciones. Esto permite registrar no solo el\nqué y el cómo, sino también el porqué detrás de cada paso.\n\n**Importancia de Mantener un Registro de los Cambios en los Datos:**\nCada decisión tomada durante la preparación y el análisis de datos puede\ntener un impacto significativo en los resultados finales. Mantener un\nregistro detallado de estas decisiones, desde la limpieza de datos hasta\nla creación de variables derivadas, es crucial para comprender cómo se\nobtuvieron ciertos resultados. Una documentación precisa y detallada\npermite a otros analistas validar y replicar el análisis en el futuro.\n\n**Uso de Sistemas de Control de Versiones como GIT para Rastrear\nCambios:**\n\nGIT, un sistema de control de versiones ampliamente utilizado, no solo\nse aplica al desarrollo de software, sino que también es una herramienta\npoderosa en el análisis de datos. Permite rastrear cada modificación\nrealizada en el código y en los documentos, incluidos los notebooks.\nCada cambio es registrado como un \"commit\", lo que proporciona un\nhistorial completo y auditable de las transformaciones realizadas en los\ndatos.\n\n![Un esquema de git por Allison Horst\n@allison_horst](img/git_flujo_allison.jpeg)\n\n**Aplicación de Control de Versiones en Proyectos de Preparación de\nDatos:**\\* La aplicación de GIT en proyectos de preparación de datos\nagrega un nivel adicional de transparencia y colaboración. Los\nrepositorios de GIT almacenan no solo los datos originales, sino también\nlos notebooks y scripts utilizados en el proceso. Esto permite a los\nanalistas colaborar en un entorno controlado y mantener un historial de\ncambios. En caso de que surjan problemas o se necesite retroceder en el\ntiempo, GIT ofrece la capacidad de volver a versiones anteriores de\nmanera segura.\n\nLa combinación de documentación detallada a través de notebooks y el uso\nde sistemas de control de versiones como GIT proporciona una base sólida\npara el análisis de datos reproducible y transparente. Esto no solo\nfacilita la comprensión y validación de los resultados, sino que también\nfomenta la colaboración y la mejora continua en proyectos de preparación\ny análisis de datos.\n\n::: callout-tip\n#### Actividad de proyecto - Inicio reproducible\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que\nsea reproducible y transparente.\n\nUno de los productos del proyecto es un notebook de reporte del\nanálisis. Para esto, iremos avanzando desde hoy.\n\n1.  Defina a su grupo e inscribase.\n2.  Cree un repositorio de Github en el cual van a trabajar, agregue a\n    todos los integrantes como colaboradores y a la profesora (usuario:\n    melanieoyarzun)\n3.  Cree el readme listando a los integrantes del grupo.\n4.  Definan con que base de datos les gustaría trabajar.\n5.  Propongan una o dos preguntas de investigación y las hipotesis que\n    las responderían.\n\nLa siguiente sesión, vamos a explorar los datos y empezar los primeros\npasos en su análisis.\n:::\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.10.9\n---\n",
    "supporting": [
      "sesion1_notas_files/figure-ipynb"
    ],
    "filters": []
  }
}