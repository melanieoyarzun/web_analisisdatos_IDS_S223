{
  "hash": "e2334e1978eab2656350d37c80657833",
  "result": {
    "markdown": "---\ninstitute: Magíster en Data Science - Universidad del Desarrollo\nsubtitle: 'Curso: Análisis de datos'\ntitle: 'Sesión 2: Preparando los datos'\nauthor: 'Phd (c) Melanie Oyarzún - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'\nformat:\n  html:\n    toc: true\n    html-math-method: mathml\n    embed-resources: true\n  ipynb: default\necho: true\neditor:\n  markdown:\n    wrap: 72\nexecute:\n  keep-ipynb: true\n  freeze: auto\ncode-link: true\n---\n\n# La preparación de los datos\n\n-   La preparación de datos es una fase esencial en el proceso de\n    análisis de datos que involucra una serie de actividades destinadas\n    a garantizar que los datos estén en condiciones óptimas para su\n    posterior análisis.\n\n    -   Extraccion de los datos\n    -   Limpieza\n    -   Transformación\n    -   Organización\n\nEsta fase implica la extracción de los datos, su limpieza,\ntransformación y organización de manera que sean coherentes, completos y\nadecuados para el análisis que se va a realizar.\n\n**Trash in , trash out**\n\n::: columns\n::: {.column width=\"60%\"}\nLa preparación de datos es crucial porque afecta directamente la calidad\ny confiabilidad de los resultados obtenidos en cualquier análisis\nposterior.\n:::\n\n::: {.column width=\"40%\"}\n![](img/trash.png)\n:::\n:::\n\n## Tipos de datos\n\nAntes de adentrarnos en la preparación de los datos propiamente tal, es\nimportante que revisemos los diferentes tipos de datos con los cuales\nnos podemos encontrar.\n\nEn especial, para que trabajemos desde un lenguaje común.\n\n**Los tipos de datos y sus estructuras**\n\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas\nposibles agrupaciones son en relación a:\n\n-   Estructura\n\n-   Tamaño\n\n-   Operacionalización\n\n-   Temporalidad y unidad de análisis.\n\n### Según estructura de los datos\n\n-   La estructura de un dato se refiere a su formato y codificación.\n\n![fuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data](img/img_sesion2/structuredvsunestructures.png)\n\n#### Datos estructurados\n\n-   Tablas son formas organizadas y ordenadas de datos.\n-   Conjuntos de tablas forman bases de datos estructuradas.\n-   Las entradas en tablas pueden ser texto o números.\n-   Los datos estructurados son recopilados y organizados intencionalmente.\n\n#### Datos No estructurados\n\n-   Los datos no estructurados son comunes en la sociedad moderna.\n-   Pueden ser multimedia, imágenes, audio, datos de sensores, texto,\n    etc.\n-   No siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\n-   **Medios enriquecidos:** Datos de medios y entretenimiento, datos de\n    vigilancia, datos geoespaciales, audio, datos meteorológicos\n\n-   **Colecciones de documentos digitalizados:** Facturas, registros,\n    correos electrónicos, aplicaciones de productividad\n\n-   **Internet de las cosas:** Datos de sensores, datos de teletipos.\n\n#### Estructurados vs no estructuraods\n\n-   Datos estructurados están en sistemas transaccionales y bases de\n    datos.\n-   No hay preferencia entre estructurados y no estructurados.\n-   Ambos tienen herramientas para acceder a la información, pero los no\n    estructurados son más abundantes.\n\n### Datos según tamaño\n\n-   **Big Data:** Se refiere a conjuntos de datos extremadamente grandes\n    y complejos que son difíciles de gestionar, procesar y analizar con\n    herramientas y métodos tradicionales. Big Data involucra terabytes o\n    incluso petabytes de información y generalmente requiere tecnologías\n    y enfoques especializados para extraer conocimientos significativos.\n\n-   **Small Data:** Se refiere a conjuntos de datos más pequeños y\n    manejables en comparación con Big Data. Estos conjuntos de datos son\n    más accesibles y pueden ser procesados y analizados utilizando\n    herramientas y métodos convencionales. A menudo, Small Data se\n    centra en obtener información valiosa de fuentes limitadas y\n    específicas.\n\n### Datos según tipo y su operacionalización\n\n-   Las variables en análisis de datos pueden ser cuantitativas o\n    cualitativas.\n    -   Las cuantitativas representan números medibles, como medidas o\n        cantidades.\n    -   Las cualitativas representan cualidades y se subdividen en\n        nominales (sin orden) y ordinales (con orden).\n        -   Para operacionalizar variables cualitativas, asignamos\n            números o factores que representen categorías, facilitando\n            su análisis cuantitativo.\n\n### Datos según temporalidad y unidad de análisis.\n\nOtra manera de clasificar los datos con relación a la temporalidad en la\ncual son tomados y cuál es la unidad de análisis.\n\n-   Corte transversal\n-   serie de tiempo\n-   panel - datos longitudinales\n\n#### Corte transversal\n\n![](img/corte_transversal.png)\n\n#### Serie temproal\n\n![](img/serie_temporal.png)\n\n#### Panel\n\n![](img/panel.png)\n\n\n\n# Cargando los datos\n\n## Leyendo datos en Pandas\n\nEn `Pandas` podemos cargar una gran variedad de datos, a través de la\nfamilia de funciones `read_tipo`\n\n![](img/pandas_read.png)\n\n::: callout-tip\n## Algunos tipos de argumentos para funciones read\\_\\*\n\n-   **Indexing:** escoger una columna para indexar los datos, obtener\n    nombres de columnas del archivo o usuario\n-   **Type inference y data conversion:** automático o definido por el\n    usuario\n-   **Datetime parsing:** puede combinar información de múltiples\n    columnas\n-   **Iterating:** trabajar con archivos muy grandes\n-   **Unclean Data:** saltarse filas (por ejemplo, comentarios) o\n    trabajar con números con formato (por ejemplo, 1,000,345)\n-   **Memory:** `low_memory` indica que usemos pocos recursos (o no) del\n    sistema.\n:::\n\n## Formato CSV (valores separados por comas)\n\n-   La coma es un separador de campos, newline denota registros\n\n    -   `a,b,c,d,message`\n    -   `1,2,3,4,hello`\n    -   `5,6,7,8,world`\n    -   `9,10,11,12,foo`\n\n-   Puede tener un encabezado (`a,b,c,d,message`), pero no es requisito\n\n-   Sin tipo de información: no sabemos qué son las columnas (números,\n    strings, punto flotante, etc.)\n\n    -   Default: simplemente mantener todo como string\n    -   inferencia del tipo: descubrir qué tipo para transformar cada\n        columna basado en lo que parece ser\n\n-   ¿Y qué pasa con las comas en un valor?: doble comillas\n\n-   Se puede utilizar otros delimitadores (\\|, , )\n\n## CSV en pandas\n\n-   Lectura:\n    -   Básica: `df = pd.read_csv(fname)`\n    -   Utilizar diferente delimitador:\n        `df = pd.read_csv(fname, sep='\\t\\')`\n    -   Saltarse las primeras columnas:\n        `df = pd.read_csv(fname, skiprows=3)`\n-   Escritura:\n    -   Básica `df.to_csv()`\n    -   Cambiar el delimitador con sep kwarg:\n        `df.to_csv('example.dsv', sep='\\|')`\n    -   Cambiar la representación de missing value\n        `df.to_csv('example.dsv', na_rep='NULL')`\n\n## JSON (Java Script Object Notation)\n\n-   Un formato para datos web\n\n-   Aspecto muy similar a a diccionarios y listas python\n\n-   Ejemplo:\n    `{   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }`\n\n-   No hay variables sino elementos independientes, pero permite nulos\n    Valores: strings, arreglos, diccionarios, números, booleanos, o\n    nulos\n\n    -   Keys de diccionario deben ser strings\n    -   Signos de pregunta ayudan a diferenciar string o valores\n        numéricos\n\n## Lectura y escritura de JSON con pandas\n\n-   `pd.read_json(<filename>, orient=<orientation>)`\n-   `df.to_json(<filename>, orient=<orientation>)`\n\n## Orientaciones posibles de JSON\n\n-   *split*: tipo diccionario\n    `{index -> [index],  columns -> [columns],data -> [values]}`\n-   *records*: tipo lista\\\n    `[{column -> value}, ... , {column -> value}]`\n-   *index*: tipo diccionario `{index -> {column -> value}}`\n-   *columns:* tipo diccionario `{column -> {index -> value}}`\n-   *values:* solo los valores del arreglo\n\n## eXtensible Markup Language (XML)\n\n-   Formato más antiguo y auto descriptivo, con estructura jerárquica\n    anidada.\n-   Cada campo tiene tags\n-   Tiene un elemento inicial llamado root\n-   No tiene un método incorporado en Python.\n-   Se puede usar la librería `lxml` (también ElementTree)\n\n### Ejemplo XML\n\n![](img/xml.png)\n\n## Formatos binarios\n\n-   CSV, JSON y XML son todos formatos de texto\n\n-   ¿Qué es un formato binario?\n\n-   Pickle: Python's built-in serialization\n\n::: {#e7d33e75 .cell execution_count=1}\n``` {.python .cell-code}\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n```\n:::\n:::\n\n\n-   HDF5: Librería para almacenar gran cantidad de datos científicos\n\n    -   Formato jerárquico de datos\n\n    -   Interfaces in C, Java, MATLAB, etc.\n\n    -   Aguanta compresión\n\n    -   Usar pd.HDFStore para acceder\n\n    -   Comandos: read_hdf/to_hdf, se necesita especificar objeto\n\n    -   Excel: se necesita especificar la hoja cuando la hoja de cálculo\n        tiene múltiples hojas\n\n    -   pd.ExcelFile or pd.read_excel\n\n## Bases de datos relacionales\n\n![](img/datos_relacionales.png)\n\n-   Las bases de datos relacionales son similares a los marcos de datos\n    múltiples pero tienen muchas más características\n\n    -   vínculos entre tablas via *foreign keys*\n\n    -   *SQL* para crear, almacenar y *query* datos \n\n-   *sqlite3* es una base de datos simple con *built-in support in\n    python*\n\n-   Python tiene una *database API* que te permite acceder a la mayoría\n    de los sistemas de bases de datos a través de un *API* común\n\n-   Sintaxis similar (¡pero no igual!) de otros sistemas de bases de\n    datos (*MySQL, Microsoft SQL Server, Oracle*, etc.) \n\n-   *SQLAlchemy*: Paquete de python que se abstrae de las diferencias\n    entre distintos sistemas de bases de datos\n\n-   *SQLAlchemy* apoya la lectura de *queries* a *data frame*:\n\n    \\`import sqlalchemy as sqla\n\n    db = sqla.create_engine('sqlite:///mydata.sqlite')\n\n    pd.read_sql('select \\* from test', db)\\`\n\n# Dirty Data\n\n... pero, ¿y si los datos no son o están correctos/confiables/en formato\ncorrecto?\n\n## Dirty data: punto de vista estadístico\n\n-   Los datos son generados desde algún proceso\n-   Se quiere modelar el proceso, pero se tienen muestras no ideales:\n    -   Distorsión: algunas muestras se corrompieron por un proceso\n    -   Sesgo de selección: probabilidad de que una muestra dependa de\n        su valor\n    -   Censura de izquierda y derecha: Left and right censorship: los\n        usuarios van y vienen del escrutinio\n    -   Dependiencia: las muestras no son independientes (por ejemplo,\n        redes sociales)\n-   Puedes agregar/aumentar modelos para diferentes problemas, pero no\n    modelar todo\n-   Trade-off entre precisión y simplicidad\n\n## Dirty data: punto de vista experto en base de datos\n\n-   Se obtuvo un set de datos\n-   Algunos valores están missing, corrompidos, erróneos, duplicados\n-   Los resultados son absolutos (modelo relacional)\n-   Mejores respuestas provienen de mejorar la calidad de los valores en\n    el set de datos\n\n## Dirty Data: Punto de vista del experto en un área\n\n-   Algo se ve mal en los datos\n-   Algo se ve mal en la respuesta\n-   ¿Qué ocurrió?\n-   Los expertos en un área llevan un modelo implícito de los datos que\n    están testeando\n-   No siempre necesitas ser un experto en un área para hacer esto\n    -   ¿Puede una persona correr 500 km por hora?\n    -   ¿Puede una montaña en la Tierra estar a 50.000 metros sobre el\n        nivel del mar?\n    -   Utilizar sentido común\n\n## Dirty Data: Punto de vista del Data Scientist\n\n-   Combinación de los tres puntos de vista anteriores\n-   Todos los puntos de vista presentan problemas con los datos La meta\n    puede determinar las soluciones:\n    -   Valor de la mediana: no preocuparse mucho de los outliers muy\n        improbables\n    -   Generalmente, agregación es menos susceptible a los errores\n        numéricos\n    -   Ser cuidadoso, puede que los datos estén bien...\n\n## ¿Dónde se origina el dirty data?\n\n-   La fuente está mal, por ejemplo, una persona la ingresó de forma\n    incorrecta\n-   Las transformaciones corrompen los datos, por ejemplo, ciertos\n    valores fueron procesados de forma incorrecta debido a un software\n    bug\n-   La integración de diferentes sets de datos causa problemas\n-   Propagación del error: se magnifica un error\n\n## Problemas Dirty comunes:\n\n-   Problemas de separadores: por ejemplo, CSV sin respetar comillas\n    dobles\n-   Convenciones de nombres o denominaciones: NYC vs. New York\n-   Pérdida de campos requeridos, por ejemplo, key\n-   Representaciones diferentes: 2 vs. dos\n-   Datos truncados: \"Janice Keihanaikukauakahihuliheekahaunaele\" se\n    vuelve \"Janice - Keihanaikukauakahihuliheek\" en la licencia de\n    Hawaii\n-   Registros redundantes: pueden ser exactamente el mismo o tener\n    alguna superposición\n-   Problemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017\n\n# Preparando los datos\n\nMuchas veces los datos con los que queremos trabajar no están en el\nformato adecuado para los análisis que querenos realizar.\n\n## Data Wrangling\n\n-   Data wrangling: transformar datos en bruto a un formato más\n    significativo que pueda ser analizado mejor\n-   Data cleaning: deshacerse de datos imprecisos\n-   Data transformations: cambiar los datos de una representación a otra\n-   Data reshaping: reorganizar los datos\n-   Data merging: combinar dos sets de datos\n\n## Tidy Data\n\nEn este sentido, generalmente para análisis de *corte transversal*\nnuestro ideal es trabajar con *Tidy Data*\n\n**\"Tidy Data** is a standar way of mapping the **meaning** of a dataset\nto its structure¨\n\nUn tipo de estructura de datos útil para poder realizar modelos y\nanálisis, son los llamados datos ordenados o Tidy Data Propuesto por\n[Hadley Wickham (2014)](https://vita.had.co.nz/papers/tidy-data.pdf), se\nha vuelto un estándar deseable para analizar datos.\n\nSe caracteriza por que los datos son representados en tablas\nrectangulares, de tal manera que:\n\n-   Cada variable (feature) forma una columna.\n\n-   Cada observación (registro) forma una fila.\n\n-   Cada dato (valor) está en una celda de la tabla.\n\n![](img/tidy_data.png)\n\nCon Tidy data podemos tener una estandarización en el tratamiento de los\ndatos:\n\n![](img/tidydata_2.jpg)\n\nMuchas veces un flujo de trabajo **workflow** empieza limpiando y\nordenando los datos con el objetivo de que sean Tidy, para luego ser\nanalizados y finalmente, comunicados sus resultados.\n\n![](img/flujo_tidy.png)\n\n### ¿Porqué datos Tidy?\n\n-   Estandarizanción\n\nLos datos organizados te permiten ser más eficiente al utilizar\nherramientas existentes diseñadas específicamente para realizar las\ntareas que necesitas hacer, desde la selección de porciones de tus datos\nhasta la creación de mapas de tu área de estudio.\n\nUtilizar herramientas existentes te ahorra tener que construir todo\ndesde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual\npuede ser consume tiempo y desmotivante).\n\nAfortunadamente, existen muchas herramientas diseñadas específicamente\npara transformar datos desorganizados en datos organizados (por ejemplo,\nen el paquete tidyr). Al estar mejor preparado para transformar tus\ndatos en un formato organizado, podrás llegar más rápido a tus análisis\ny comenzar a responder las preguntas que estás planteando.\n\n![Fuente: https://allisonhorst.com/other-r-fun](img/tidydata_3.jpg)\n\n-   Facilitar la colaboración\n\nDado que nuestros colegas pueden emplear las mismas herramientas de\nmanera familiar. Tanto si consideramos a los colaboradores como\ncompañeros actuales, su futuro propio o futuros colegas, la organización\ny compartición de datos de manera coherente y predecible implica menos\najustes, tiempo y esfuerzo para todos.\n\n![Fuente: https://allisonhorst.com/other-r-fun](img/tidydata_4.jpg)\n\n-   Simplifica la reproducibilidad\n\nLos datos organizados también facilitan la reproducción de análisis, ya\nque son más fáciles de comprender, actualizar y reutilizar. Al utilizar\nherramientas que esperan datos organizados como entrada, puedes\nconstruir e iterar flujos de trabajo realmente potentes. Y cuando tienes\nentradas de datos adicionales, ¡no hay problema en volver a ejecutar tu\ncódigo!\n\n![Fuente: https://allisonhorst.com/other-r-fun](img/tidydata_5.jpg)\n\n### ¿Siempre Tidy?\n\n-   NO!\n\n-   Existen muchos otros tipos de estructuras de datos que no son tidy y\n    que eso no hace que no sean útiles o \"desordenadas\".\n\n-   Es importante tener en cuenta que siempre existen múltiples formas\n    de representar la misma información.\n\n**Existen dos principales motivos para utilizar otras estructuras de\ndatos:**\n\n-   Representaciones alternativas que tengan mucho mayor desempeño\n    computacional o ventajas en uso de memoria. Especialmente importante\n    al tratar con grandes datos.\n-   Algunos campos de estudio especializados tienen sus propias\n    convenciones, que pueden ser diferentes a estas, por ejemplo las\n    series temporales.\n\n# Tareas comunes en Data Cleaning\n\n-   Descartar e imputar missing data\n-   Remover duplicados.\n-   Modificar datos\n    -   mapear strings, expresiones aritméticas. Ejemplos:\n        -   Convertir strings de mayúsculas a minúsculas (upper/lower\n            case)\n        -   Convertir T en Fahrenheit a Celsius\n        -   Crear una nueva columna basada en la columna anterior.\n-   Reemplazar valores\n    -   (e.g. -999 → NaN). Usar método df\\['column'\\].replace()\n-   Restringir valores:\n    -   valores por encima o por debajo de los umbrales especificados se\n        establecen en un valor máximo/mínimo.\n\n::: {#ae7a1368 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', np.nan, 'Carlos'],\n        'Edad': [25, 30, np.nan, 30, 28]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar filas con valores faltantes\ndf = df.dropna()\n\n# Llenar valores faltantes con un valor específico\ndf['Edad'].fillna(0, inplace=True)\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Nombre  Edad\n0    Juan  25.0\n1     Ana  30.0\n4  Carlos  28.0\n```\n:::\n:::\n\n\n## Datos perdidos\n\n2 enfoques para lidiar con ellos:\n\n1.  Filtrar\n\n-   se pueden escoger filas o columnas\n\n2.  Llenar/ reemplazar :\n\n-   con un valor por default\n-   con un valor interpolados, otros\n\n![Missing en Pandas](img/pandas_missing.png)\n\n## Fitrando y limpiando\n\n-   Encontrar duplicados\n    -   `duplicated` : retorna una Series booleana , indicando si la\n        fila es un duplicado o no la primera instancia no es marcada\n        como un duplicado.\n-   Remover duplicados:\n    -   `drop_duplicates`: saca todas las filas donde duplicated es True\n    -   `keep`: Cuál de los valores es el que quiero mantener (first or\n        last)\n-   Puede recibir columnas específics para chequear por duplicados, e.g.\n    chequear solo la columna key.\n\n::: {#643cd5e9 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28\n```\n:::\n:::\n\n\n## Problemas de registro relacionados con textos\n\nTextos o strings, es muy común tener problemas de codificación, errores de tipeo, problemas de formato, etc.\n\nUno de los errores de registro más típico, tiene que ver con que los\ntextos/strings estan ingresados con errores.\n\n-   Una de las razones de la popularidad de Python es por su capacidad\n    de procesar texto/strings.\n\n-   `split(<delimiter>)` \"quiebra\" o separa un string en partes:\n    `s = \"12,13,14\"` `slist = s.split(',') # [\"12\", \"13\", \" 14\"]`\n\n-   `<delimiter>.join([<str>])`: Une varios strings por un delimitador\n    `\":\".join(slist) # \"12:13: 14”`\n\n-   strip(): remueve los espacios en blanco iniciales y finales\n    `[p.strip() for p in slist] # [\"12\", \"13\", \"14\"]`\n\n-   `replace(<from>,<to>)`: Cambia un substring por otro\n    `s.replace(',', ':') # \"12:13: 14”`\n\n-   upper()/lower(): casing `\"AbCd\".upper () # \"ABCD”`\n    `\"AbCd\".lower() # \"abcd\"`\n\n### Errores de formato\n\nA veces, los datos se almacenan en el formato incorrecto y necesitas\ncorregirlos.\n\n::: {#c2fefd0e .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_43937/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\n```\n:::\n:::\n\n\n### Transformando strings\n\n-   index()encuentra donde ocurre primero un substring(error si no lo\n    encuentra):\n\n::: {#f1d802dd .cell execution_count=5}\n``` {.python .cell-code}\ns = \"12,13, 14\"\ns.index(',') # 2\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n2\n```\n:::\n:::\n\n\n::: {#2b6bdeb6 .cell execution_count=6}\n``` {.python .cell-code}\n#s.index(':') # ValueError raised\n```\n:::\n\n\n-   `find(<str>)`: Lo mismo que index pero -1 si no es encontrado\n\n::: {#75136ea8 .cell execution_count=7}\n``` {.python .cell-code}\ns = \"12,13, 14\"\ns.find(',') # 2\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n2\n```\n:::\n:::\n\n\n::: {#7370af53 .cell execution_count=8}\n``` {.python .cell-code}\ns.find(':') # -1\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n-1\n```\n:::\n:::\n\n\n-   `startswith()` or `endswith()`: chequeo booleano para la ocurrencia\n    de un string\n\n::: {#3d07bdd0 .cell execution_count=9}\n``` {.python .cell-code}\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nFalse\n```\n:::\n:::\n\n\n### Métodos para strings\n\n![](img/metodos_strings.png)\n\n![](img/regular_expressions.png)\n\nCualquier columna o serie puede tener métodos string (e.g. replace,\nsplit) aplicado a la serie completa\n\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual\nlo hace rápido) Se debe usar .str.<method_name> (es importante el .str).\n\nEjemplo:\n\n::: {#5e899512 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object\n```\n:::\n:::\n\n\n## Transformaciones del formato de los datos\n\n1.  Reshapes\n2.  Eliminar/elegir columnas\n3.  Re codificar: variables dummies\n4.  Unir datasets\n\n### Eliminar/ elegir columnas\n\nA veces, queremos deshacernos de columnas que no son relevantes para\nnuestro análisis.\n\n::: {#0791c5d9 .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n```\n:::\n:::\n\n\n### Reshapes/re formato\n\n-   El \"reshape\" o remodelado de datos se refiere al proceso de\n    reorganizar la estructura de un conjunto de datos, cambiando su\n    disposición de filas y columnas para adaptarse a un formato\n    específico.\n-   Esto puede implicar la transformación de datos de un formato largo\n    (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el\n    análisis, la visualización o la aplicación de modelos estadísticos.\n-   El \"reshape\" es comúnmente realizado en la manipulación de datos\n    utilizando bibliotecas como Pandas en Python o funciones de pivote\n    en hojas de cálculo.\n\n![](img/reshape.png)\n\n::: {#02f24a39 .cell execution_count=12}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n```\n:::\n:::\n\n\n### Crear variables dummies / dicotómicas\n\n-   Muy útil para representar información cualitativa.\n-   Muy usando en análisis de regresión y machine learning.\n-   Queremos tomar valores posibles y mapearlos a uno o más indicadores\n    que toman valor 0 ó 1.\n-   Se pueden generar mediante `pd.get_dummies(df[‘key’])`\n\n![](img/onehot_dogs.png)\n\n::: {#6700fff0 .cell execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0\n```\n:::\n:::\n\n\n## Unir datasets\n\nMuchas veces tenemos varios dataset que queremos unir. Existen dos\nmétodos principales:\n\n1.  Apliar/concatenar/append Consiste en agregar un dataset a\n    continuación de otro, para esto tienen que tener la misma forma.\n\n2.  Uniones Merge / Join Las uniones combinan DataFrames utilizando\n    columnas en común como clave. Puedes realizar diferentes tipos de\n    uniones, como \"inner\", \"outer\", \"left\" y \"right\"\n\n![](img/joins.png)\n\n### 1. Apilar dataframes\n\nCreemos los dataframes:\n\n::: {#010c46df .cell execution_count=14}\n``` {.python .cell-code}\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n```\n:::\n:::\n\n\nHagamos el apilar. Atención a los index\n\n::: {#348123cf .cell execution_count=15}\n``` {.python .cell-code}\ndf.append(df2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_43937/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P</th>\n      <th>Q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n![](img/append1.png)\n\nUsemos ignorar index\n\n::: {#958b43b3 .cell execution_count=16}\n``` {.python .cell-code}\ndf.append(df2, ignore_index=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_43937/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P</th>\n      <th>Q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n![](img/append2.png)\n\n### 2. Unir dataframes\n\nUtilizando la función pd.merge().\n\n::: {#2abcd81a .cell execution_count=17}\n``` {.python .cell-code}\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnión interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUnión externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0\n```\n:::\n:::\n\n\n## Transformaciones estadísticas:\n\n### 1.  Outliers\n\nUn valor atípico, también conocido como valor outlier en inglés, es un\npunto de datos que difiere significativamente del patrón general de los\ndemás datos en un conjunto. Estos valores son inusuales en relación con\nel resto de la distribución de los datos y pueden ser considerablemente\nmás altos o más bajos que los valores típicos del conjunto.\n\nLa identificación de valores atípicos es importante por varias\nrazones: 1. Calidad de los datos 2. Impacto en estadísticas y modelos 3.\nAnomalías y problemas reales 4. Toma de decisiones informada\n\nVeamos un ejemplo:\n\n::: {#42cd8143 .cell execution_count=18}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] < lower_limit) | (df['Valor'] > upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores atípicos:\n   Valor\n6    200\n```\n:::\n:::\n\n\n### 2. Estandarización de Datos\n\nLa estandarización es un proceso importante en el análisis de datos y el\naprendizaje automático. Consiste en transformar los valores de una\nvariable de manera que tengan una media de cero y una desviación\nestándar de uno. Esta transformación se logra mediante la fórmula:\n\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n\nDonde \\$ x \\$ es el valor original, \\$ \\mu \\$ es la media de los valores\ny \\$ \\sigma \\$ es la desviación estándar.\n\nLa estandarización es especialmente útil cuando se tienen variables con\ndiferentes escalas y distribuciones, ya que permite compararlas de\nmanera equitativa y mejorar la eficacia de los algoritmos de aprendizaje\nautomático al mitigar el impacto de las diferencias en las magnitudes de\nlas características.\n\n::: panel-tabset\n### Pandas\n\n::: {#37280dd1 .cell execution_count=19}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n```\n:::\n:::\n\n\n### Numpy\n\n::: {#d521728d .cell execution_count=20}\n``` {.python .cell-code}\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]\n```\n:::\n:::\n\n\n:::\n\n### 3. Normalización de Datos\n\nLa normalización es un proceso esencial en el análisis de datos y el\naprendizaje automático. Consiste en ajustar los valores de una variable\npara que se encuentren dentro de un rango específico, generalmente entre\n0 y 1. La fórmula matemática utilizada para la normalización es:\n\n$$ x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} $$\n\nDonde \\$ x \\$ es el valor original y \\$x\\_{\\text{norm}} \\$ es el valor\nnormalizado.\n\nLa normalización es beneficiosa para igualar la escala de las variables\ny mejorar el rendimiento de los modelos que son sensibles a la magnitud\nde las características.\n\n::: panel-tabset\n### Pandas\n\n::: {#7ae07100 .cell execution_count=21}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n```\n:::\n:::\n\n\n### Numpy\n\n::: {#d83c7fb9 .cell execution_count=22}\n``` {.python .cell-code}\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]\n```\n:::\n:::\n\n\n:::\n\n\n# Taller de aplicación\n\n::: callout-tip\n## **Taller de aplicación 1: Pregunta 4**\n\nVamos a usar una situación ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de dentificación revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy rápidamente y en otras, vamos a tener mayor énfasis.\n\n**Pregunta de investigación** \n\n> Asistir a cursos de verano mejora los resultados académicos?\n\nPara responder esta pregunta, usaremos unos datos **ficticios y simulados**\n\nEl **escenario ficticio** es el siguiente:\n\n -   Para un conjunto de colegios en una comuna, existe la opción de\n     asistir a un curso de verano intensivo durante el verano entre 5 y\n     6to básico.\n -   El curso de verano se enfoca en mejorar las habilidades académicas\n     de preparar la prueba de admisión a la universidad vigente (PSU en\n     ese momento)\n -   El curso de verano es gratuito, pero para ser matriculados\n     requiere que los padres se involucren en un proceso.\n -   Estamos interesados en testear el impacto de la participación en\n     el curso en los resultados académicos de los estudiantes.\n\n **Datos ficticios dispobibles**\n\n Los datos estan disponibles en [github](https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data)\n\n 1.  school_data_1.csv\n\n -   Usamos esta data para ejemplificar como cargar data guardada en\n     formato csv.\n -   Este dataset tiene información sobre cada individuo (con\n     identificador id), la escuela a la que asiste, un indicador si\n     participó en el curso de verano, sexo, ingreso del hogar (en\n     logaritmo), educación de los padres, resultados en una prueba\n     estandarizada que se realiza a nivel de la comuna tanto para el\n     año 5 como para el año 6.\n\n 2.  school_data_2.dta\n\n -   Usamos esta data para ejemplificar como cargar data guardada en\n     formato STATA.\n -   Este dataset tiene información de cada individuo (con\n     identificador id).\n -   Este dataset tiene la información si el individuo recibió la carta\n     de invitación para participar del curso de verano.\n\n 3.  school_data_3.xlsx\n\n -   Usamos este dataset para practicar como cargar datos guardados en\n     formato Microsoft Excel.\n -   Este dataset incluye datos sobre cada individuo (con identificador\n     id)\n -   Este dataset tiene información de rendimiento académico antes y  después del curso de verano.\n\n **Objetivos:**\n\n La idea de este taller es poner en práctica los primeros pasos para un\n análisis:\n \n 1. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n 2. También exploraremos los datos, usaremos estadísticas descriptivas que nos permita entender los datos e identificar problemas.\n\n Sigue los detalles en el siguiente link [Taller 1 - Pregunta 4 : Cursos de verano](taller1_aplicacion_educ.qmd)\n:::\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.10.9\n---\n",
    "supporting": [
      "sesion2_notas_files/figure-ipynb"
    ],
    "filters": []
  }
}