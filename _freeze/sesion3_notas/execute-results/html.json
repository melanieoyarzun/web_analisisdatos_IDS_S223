{
  "hash": "3e2aa02e774b0cc569a536713e101edc",
  "result": {
    "markdown": "---\ntitle: 'Sesión 3: Introducción al Análisis de Regresión'\ninstitute: Magíster en Data Science - Universidad del Desarrollo\nsubtitle: 'Curso: Análisis de datos'\nauthor: 'Phd (c) Melanie Oyarzún - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'\nformat:\n  html:\n    toc: true\n    html-math-method: mathml\n    embed-resources: true\n  ipynb: default\necho: true\neditor:\n  markdown:\n    wrap: 72\nexecute:\n  keep-ipynb: true\n  freeze: auto\ncode-link: true\n---\n\n# Visión general al análisis de regresión\n\nEn las aplicaciones de la ciencia de datos, es muy común estar interesado en la relación entre dos o más variables.\n\nEl análisis de regresión es una técnica en la cual buscamos encontrar una función que pueda describir la relación observada en los datos entre dos o mas variables.\n\nPor ejemplo, una persona podría querer relacionar los pesos de los individuos con sus alturas…\n\n  - ¿Son los más altos más pesados? \n  -  …y¿cuánto más pesados?\n\nPensemos en el caso más sencillo: una **regresión lineal simple** o univariada. Tenemos una variable que deseamos explicar o predecir (Y)\ncomo función de otra (X).\n\nPara esto, buscamos la pendiente e intercepto de una funciónla recta de la forma:\n\n$$Y = \\alpha + \\beta X$$\n\nque se ajuste mejor al conjunto de datos con los que se cuenta.\n\ndonde $X$ es la variable explicativa e $Y$ es la variable dependiente.  La pendiente de la recta es $b$, y $a$ es la intersección (el valor de $y$ cuando $x = 0$).\n\n<img src=\"./img/img_sesion3/gif_regresion2.gif\" width=\"600\">\n\n\nPara esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistemática\no que se puede explicar directamente con una o más variables independientes (Xs o regresores) y otra que es no sistemática o error\n($\\mu$ o $epsilon$) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fenómeno.\n\n![](img/img_sesion3/gif_regresion1.gif)\n\nLa parte sistemática entonces la describimos con una forma funcional, que depende de otras variables o regresores.\n\nEsta forma funcional puede ser lineal univariada, lineal múltiple o no lineal. El tipo de forma funcional, definirá el tipo de regresión de la que estemos hablando.\n\nVentajas del análisis de regersión: es facil describir cuantitaivamente una relación.\n\nEsquemáticamente, los elementos son:\n\n![](img/img_sesion3/regresion_esquema.png)\n\n\n## Usos de las regresiones\n\nLas regresiones tienen tres principales usos:\n\n- Describir un fenómeno\n- Probar hipótesis sobre ciertas teorías\n- Realizar predicciones \n\n## Regresión simple y scatterplot\n\nPor ejemplo, pensemos en la relación entre los años de educación y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente economía.\n\nPodriamos pensar que ambas variables se encuentras relacionadas.\n\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\n::: {#16ce75ff .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 años\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_vivienda</th>\n      <th>folio</th>\n      <th>id_persona</th>\n      <th>region</th>\n      <th>area</th>\n      <th>nse</th>\n      <th>expr</th>\n      <th>tot_per_h</th>\n      <th>edad</th>\n      <th>sexo</th>\n      <th>pco1_a</th>\n      <th>e3</th>\n      <th>o6</th>\n      <th>o8</th>\n      <th>y1</th>\n      <th>ytrabajocor</th>\n      <th>esc</th>\n      <th>desercion</th>\n      <th>educ</th>\n      <th>contrato</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>1</td>\n      <td>Región de Ñuble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>43</td>\n      <td>3</td>\n      <td>72</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>Básica incompleta</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>2</td>\n      <td>Región de Ñuble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>43</td>\n      <td>3</td>\n      <td>67</td>\n      <td>1. Hombre</td>\n      <td>Sí</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>Básica incompleta</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>3</td>\n      <td>Región de Ñuble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>44</td>\n      <td>3</td>\n      <td>40</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No sabe</td>\n      <td>411242.0</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>Técnico nivel superior completo</td>\n      <td>No sabe</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000902</td>\n      <td>100090201</td>\n      <td>1</td>\n      <td>Región de Ñuble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>51</td>\n      <td>4</td>\n      <td>56</td>\n      <td>1. Hombre</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No sabe</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000902</td>\n      <td>100090201</td>\n      <td>2</td>\n      <td>Región de Ñuble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>51</td>\n      <td>4</td>\n      <td>25</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.0</td>\n      <td>Deserción</td>\n      <td>Media humanista completa</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\n::: {#60f60dc7 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregación\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por región\n\ncasen_2022_region.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region</th>\n      <th>ytrabajocor</th>\n      <th>esc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Región de Tarapacá</td>\n      <td>658026.6250</td>\n      <td>11.679582</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Región de Antofagasta</td>\n      <td>791351.8125</td>\n      <td>11.833934</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Región de Atacama</td>\n      <td>666128.3125</td>\n      <td>11.126735</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Región de Coquimbo</td>\n      <td>656137.8750</td>\n      <td>10.973584</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Región de Valparaíso</td>\n      <td>611298.1250</td>\n      <td>11.559877</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nRealicemos un scatter sencillo:\n\n::: panel-tabset\n\n## matplotlib\n\n::: {#fb4cfe6a .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por región)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-4-output-1.png){width=619 height=450}\n:::\n:::\n\n\n## seaborn\n\n::: {#79d7e7ea .cell execution_count=4}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-5-output-1.png){width=619 height=449}\n:::\n:::\n\n\n## seaborn + linea de regresion\n\n::: {#b012938e .cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Intervalo de Confianza')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-6-output-1.png){width=619 height=450}\n:::\n:::\n\n\n## Con codigos de region\n\n::: {#c8c7e300 .cell execution_count=6}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de región a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la última palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Etiquetas de Región (Última Palabra)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-7-output-1.png){width=657 height=453}\n:::\n:::\n\n\n:::\n\n\nPodemos ver que se aprecia una relación positiva: a mayor escolaridad promedio, mayor salario promedio por región.\n\n## Especificación\n\nLlamamos especifiación al precisar la relación entre las variables que deseamos estimar.\n\nEn nuestro caso, la función base que queremos entender es entre salario y educación:\n\n$$ \\text{Salario} = f(Educacion))$$\n\nEste es una relación teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales:\n- agregar el error aleatorio\n- especificar una forma funcional\n- definir una forma de medir las variables en los datos\n\nEn nuestro caso, entonces el modelo especificado sería:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i$$\n\n## Interpretación\n\nCon nuestro modelo especificado:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i$$\n\nPodemos interpretar $\\beta$ y $alpha$:\n\n- $\\beta = \\frac{\\partial ingr}{\\partial educ}$: un año adiciónal de educación, en cuanto incrementa el salario (si nada más cambia) \n\n- $\\alpha$ valor esperado de y, si x=0...\n  \n## Modelo poblaciónal y estimación\n\nEste modelo especificado esta definido en la población:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i$$\n\npero necesitamos calcularlo con la muestra.... por lo cual tenemos estimadores para los coeficientes poblacionales!\n\n$$\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{años educación}_i $$\n\n\n## Modelo poblaciónal y estimación\n\nEl método más comun de estimación es el de los **mínimos cuadrados ordinarios**. Veremos detalles sobre la estimación, supuestos, propiedades estadísticas la proxima sesión.\n\nPor ahora, pensaremos que es el método que busca la línea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresión).\n\n$$ \\hat{\\mu}_i= y_i-\\hat{y}_i$$\n\n::: {#abedf6f2 .cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuración del estilo del gráfico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gráfico de dispersión con la línea de regresión\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresión lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el término constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar líneas que conecten cada punto a la línea de regresión\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # Línea que conecta el punto a la línea de regresión\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresión y residuos')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-8-output-1.png){width=622 height=455}\n:::\n:::\n\n\nEs decir, minimiza $\\sum_{i}^{n} \\hat{\\mu}_i $\n\n## Modelo estimado\n\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\n::: panel-tabset\n\n## Agrupados por región\n\n::: {#b045c813 .cell execution_count=8}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:33:51   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n>=20 ... continuing anyway, n=16\n\n```\n:::\n:::\n\n\n## Todos los datos\n\n::: {#8bd67891 .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n```\n:::\n:::\n\n\n:::\n\nPodemos ver que un año adicional de educación ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n\n¿y la constante, como la podemos interpretar?\n\n## Modelos simples y múltiples\n\nMuchas veces una sola variable no es suficiente para describir bien un fenómeno. Necesitamos incluir más variables.\n\nEsto puede ser:\n- Una nueva variable\n- Una forma funcional no lineal de la variable ya incluida\n\nNuestra interpretación del modelo no cambia, solo que ahora efectivamente estamos **controlando** por otros factores.\n\nProbemos, agregar edad al modelo:\n\n::: {#cbbcea24 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n```\n:::\n:::\n\n\nEs muy usual, agregar edad al cuadrado.... para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer...\n\n::: {#c7d644f2 .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n## Un poco más sobre interpretación\n\n\nLos principales elementos que hay que interpretar en un modelo de regresión lineal son los coeficientes de los predictores:\n\n- $\\beta_0$  es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta $y$, cuando todos los predictores son cero.\n\n- $\\beta_j$ los coeficientes de regresión parcial de cada predictor **indican el cambio promedio esperado de la variable respuesta  𝑦  al incrementar en una unidad de la variable predictora  $x_j$, manteniéndose constantes el resto de variables. (\"Ceteris paribus\"))**\n\nLa magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.\n\n![](img/img_sesion3/unidad_medida.png)\n\nPara poder determinar qué impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviación estándar) las variables predictoras previo ajuste del modelo. En este caso, $\\beta_0$ se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y  $\\beta_j$ el cambio promedio esperado de la variable respuesta al incrementar en una desviación estándar la variable predictora  $x_j$, manteniéndose constantes el resto de variables.\n\nSi bien los coeficientes de regresión suelen ser el primer objetivo de la interpretación de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condición de normalidad...etc.). Estos últimos suelen ser tratados con poco detalle cuando el único objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta. \n\n## Causalidad, regresión y correlación\n\n**Importante tener en cuenta**\n\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relación entre las variables de interés. Esto no implica necesariamente que una variable **cause** la otra (por ejemplo, puntajes más altos en la PSU **no causan** calificaciones superiores en la universidad), pero existe alguna asociación significativa entre las dos variables.\n\nUn diagrama de dispersión puede ser una herramienta útil para determinar la fuerza de la relación entre dos variables. Si parece no haber asociación entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersión no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresión lineal a los datos probablemente no proporcionará un modelo útil.\n\nUna valiosa medida numérica de asociación entre dos variables es el coeficiente de correlación, que es un valor entre -1 y 1 que indica la fuerza de la asociación de los datos observados para las dos variables.\n\n\n# Una perspectiva histórica:\n\nEL origen de la técnica, podemos remontarlo a la genética.\n\nFrancis Galton estudió la variación y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolectó y estudió datos de altura de familias para tratar de entender la herencia. **Mientras hacía esto, desarrolló los conceptos de correlación y regresión.**\n\nPor supuesto, en el momento en que se recogieron estos datos, nuestro conocimiento de la genética era bastante limitado en comparación con lo\nque conocemos hoy en día. Una pregunta muy específica que Galton trató de responder fue:\n\n    ¿qué tan bien podemos predecir la estatura de un niño basado en la estatura de los padres? \n\nLa técnica que desarrolló para responder a esta pregunta, la regresión, también puede aplicarse en muchas otras circunstancias.\n\nNota histórica: Galton hizo importantes contribuciones a la estadística y la genética, pero también fue uno de los primeros defensores de la\neugenesia, un movimiento filosófico científicamente defectuoso favorecido por muchos biólogos de la época de Galton pero con terribles\nconsecuencias históricas.\n\n<img src=\"./img/img_sesion3/galton.png\" width=\"350\">\n\n## Estudio de caso: ¿es hereditaria la altura?\n\nTenemos acceso a los datos de altura de familias recolectado por Galton, a través del paquete `HistData`. Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n::: {#ba994c79 .cell execution_count=12}\n``` {.python .cell-code}\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\ngalton_data.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family</th>\n      <th>father</th>\n      <th>mother</th>\n      <th>midparentHeight</th>\n      <th>children</th>\n      <th>childNum</th>\n      <th>gender</th>\n      <th>childHeight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>1</td>\n      <td>male</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>2</td>\n      <td>female</td>\n      <td>69.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>3</td>\n      <td>female</td>\n      <td>69.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>4</td>\n      <td>female</td>\n      <td>69.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPara imitar el análisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n::: {#237b9b27 .cell execution_count=13}\n``` {.python .cell-code}\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>father</th>\n      <th>son</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>78.5</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>75.5</td>\n      <td>73.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>75.0</td>\n      <td>71.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75.0</td>\n      <td>68.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEn los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.\n\nSupongamos que se nos pidiera que resumiéramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones están bien aproximadas por la distribución normal, podríamos usar los dos promedios y dos desviaciones estándar como resúmenes:\n\n::: {#9296ed49 .cell execution_count=14}\n``` {.python .cell-code}\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nresumen_estadistico.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>promedio_padre</th>\n      <th>sd_padre</th>\n      <th>promedio_hijo</th>\n      <th>sd_hijo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69.098883</td>\n      <td>2.546555</td>\n      <td>69.263687</td>\n      <td>2.567837</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nSin embargo, este resumen no describe una característica importante de\nlos datos: **la tendencia de que cuanto más alto es el padre, más alto es el hijo.**\n\n::: {#2e2b094f .cell execution_count=15}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gráfico de dispersión con línea de regresión\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relación entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gráfico\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-16-output-1.png){width=824 height=529}\n:::\n:::\n\n\nAprenderemos que el **coeficiente de correlación** es un resumen\ninformativo de cómo dos variables se mueven juntas y luego veremos cómo\nesto puede ser usado para predecir una variable usando la otra, en **una\nregresión**.\n\n\n## Taller de aplicación 2: Caso aplicación: Cursos de Verano\n\n::: callout-tip\n## **Taller de aplicación 2: Pregunta 1**\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que queríamos responder:\n\n> Asistir a cursos de verano mejora los resultados académicos?\n\n1.  Plantee un modelo de regresión que con los datos disponibles quisieramos estimar.\n2.  Grafique la dispersión y la recta de regresión estimada.\n3.  Estime el modelo simple e interprete\n\n:::\n\n## ¿Regresión?... pero ¿Y la correlación?\n\n\n- Ambos están muy relacionados.\n- Aprenderemos que el coeficiente de correlación es un resumen informativo de cómo dos variables se mueven juntas…\n- y luego veremos cómo esto puede ser usado para predecir una variable usando la otra y modelado en una regresión\n\n::: {#06ba155b .cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-17-output-1.png){width=812 height=529}\n:::\n:::\n\n\n## El coeficiente de correlación\n\nEl coeficiente de correlación se define para una lista de pares $(x_1,y_1),...(x_n,y_n)$  como la media de los productos de los valores normalizados:\n\n$$\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n$$\n\nDónde $\\mu$ son promedios y $\\sigma$ son desviaciones estándar. La letra griega para r, $\\rho$ se utiliza comúnmente en los libros de estadística para denotar la correlación, porque es la primera letra de regresión. Pronto aprenderemos sobre la conexión entre correlación y regresión. \n\nPodemos representar la fórmula anterior con el código usando:\n\n`rho <- np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))`\n\nPodemos representar la fórmula anterior con el siguiente código usando:\n\n::: {#a6755e41 .cell execution_count=17}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aquí\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aquí\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9999999999999998\n```\n:::\n:::\n\n\nLa correlación entre las alturas del padre y del hijo es de aproximadamente $0,4$:\n\n::: {#a937cd60 .cell execution_count=18}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviación estándar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviación Estándar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviación Estándar de la Altura del Padre:\", sd_father)\n\n# Crear el gráfico de dispersión\nplt.scatter(galton_heights['father'], StandardScaler().fit_transform(galton_heights[['father']]))\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Padre Estandarizada\")\nplt.title(\"Relación entre Altura del Padre y Altura del Padre Estandarizada\")\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviación Estándar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviación Estándar de la Altura del Padre: 2.546555038637643\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-19-output-2.png){width=589 height=455}\n:::\n:::\n\n\nLa correlación entre las alturas del padre y del hijo es de aproximadamente $0,4$.\n\n::: {#81114510 .cell execution_count=19}\n``` {.python .cell-code}\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlación:\", correlation_coefficient)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoeficiente de Correlación: 0.42699639842017706\n```\n:::\n:::\n\n\n::: {#ca665697 .cell execution_count=20}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Generar datos simulados usando la biblioteca faux\ndat = pd.DataFrame(np.random.multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([1, 1, 1, 1, 1, 1]), size=100),\n                   columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n\nprint(dat)\n\n# Calcular la correlación entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           A         B         C         D         E         F\n0  -0.487216 -0.103054 -0.406291  0.144734  0.482508 -1.065099\n1  -2.619254 -0.482619  0.373571 -0.031262 -0.059186  1.284221\n2  -0.606478  0.793955 -0.923926 -0.833852  0.038484  0.057210\n3   0.748645  0.858455  0.849438  0.973093 -0.139669  0.295479\n4  -1.394435  0.086160  0.131287  0.053497 -0.113966  1.327921\n..       ...       ...       ...       ...       ...       ...\n95  0.066146 -0.052701  0.776452  0.885253  0.336985  1.836124\n96 -0.189615  0.674505  0.660422  0.862998 -1.177144  0.924969\n97 -1.168625  1.464250  0.373704 -0.818466 -1.300497 -0.431909\n98 -0.177136  0.473261  0.702792  3.293837  0.889548  0.447885\n99  0.637694 -0.315696  0.463798 -0.975720 -1.828953  2.008513\n\n[100 rows x 6 columns]\n0.4922242607689742\n```\n:::\n:::\n\n\nPara ver cómo se ven los datos para los diferentes valores de $\\rho$ aquí hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\n![image](img/img_sesion3/g1.png)\n\n## La correlación de la muestra es una variable aleatoria\n\nAntes de continuar conectando la correlación con la regresión, recordemos la variabilidad aleatoria.\n\nEn la mayoría de las aplicaciones de la ciencia de datos, observamos datos que incluyen **variación aleatoria**. \n\nPor ejemplo, en muchos casos, no se observan datos para toda la población de interés, sino para una muestra aleatoria. Al igual que con el promedio y la desviación estándar, la **correlación de la muestra** es la estimación más comúnmente utilizada de la **correlación de la población**. Esto implica que la correlación que calculamos y usamos como resumen es una variable aleatoria.\n\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra población. Un genetista menos afortunado sólo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlación de la muestra se puede calcular con:\n\n::: {#1fd11f26 .cell execution_count=21}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tamaño 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlación entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlación en la Muestra:\", correlation_coefficient)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoeficiente de Correlación en la Muestra: 0.4105312650311907\n```\n:::\n:::\n\n\nR es una variable aleatoria. Podemos ejecutar una simulación de Monte Carlo para ver su distribución:\n\n* Nota: el objetivo principal de la simulación de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir cómo van a evolucionar.\n\n::: {#b66cac31 .cell execution_count=22}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlación\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlación\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlación\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-23-output-1.png){width=597 height=455}\n:::\n:::\n\n\nVemos que el valor esperado de R es la correlación de la población:\n\n::: {#d9ce7b2b .cell execution_count=23}\n``` {.python .cell-code}\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlación:\", mean_R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedia de Coeficientes de Correlación: 0.42583844932004655\n```\n:::\n:::\n\n\ny que tiene un error estándar relativamente alto en relación con el rango de valores que puede tomar R:\n\n::: {#58d1adf2 .cell execution_count=24}\n``` {.python .cell-code}\nsd_R = np.std(R)\nprint(\"Desviación Estándar de Coeficientes de Correlación:\", sd_R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDesviación Estándar de Coeficientes de Correlación: 0.053483610252486324\n```\n:::\n:::\n\n\nPor lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\n\nAdemás, tenga en cuenta que debido a que la correlación de la muestra es un promedio de extracciones independientes, el teorema del límite central realmente funciona. Por lo tanto, para $N$ lo suficientemente grande la distribución de $R$ es aproximadamente normal con el valor esperado $\\rho$. La desviación estándar, que es algo compleja de derivar, es: $\\sqrt{\\frac{1-r^2}{N-2}}$.\n\nEn nuestro ejemplo, $N=25$ no parece ser lo suficientemente grande para que la aproximación sea buena:\n\n* Nota: El gráfico Q-Q, o gráfico cuantitativo, es una herramienta gráfica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribución teórica como una Normal o exponencial. Por ejemplo, si realizamos un análisis estadístico que asume que nuestra variable dependiente está Normalmente distribuida, podemos usar un gráfico Q-Q-Normal para verificar esa suposición. https://data.library.virginia.edu/understanding-q-q-plots/\n\n::: {#2421a93d .cell execution_count=25}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlación\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tamaño de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gráfico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Teóricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gráfico QQ-plot para los Coeficientes de Correlación\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # Línea de referencia\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-26-output-1.png){width=527 height=529}\n:::\n:::\n\n\nSi N aumenta verás que la distribución converge a una normal.\n\n\n## La correlación no siempre es un resumen útil\n\nLa correlación no siempre es un buen resumen de la relación entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlación de 0,82:\n\n![image](img/img_sesion3/g2.png)\n\nLa correlación sólo tiene sentido en un contexto particular. Para ayudarnos a entender cuándo es que la correlación es significativa como estadística de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudará a motivar y definir la regresión lineal. Comenzamos demostrando cómo la correlación puede ser útil para la predicción.\n\n# Correlación no es causalidad\n\nLa asociación no es causalidad es quizás la lección más importante que se aprende en una clase de estadística. Hay muchas razones por las que una variable $X$ puede correlacionarse con una variable $Y$ sin tener ningún efecto directo sobre $Y$. Aquí examinamos tres maneras comunes que pueden llevar a una mala interpretación de los datos.\n\n## Correlación espuria\nEl siguiente ejemplo cómico subraya que la correlación no es causalidad. Muestra una fuerte correlación entre las tasas de divorcio y el consumo de margarina.\n\n\n![image](img/img_sesion3/notcausa.png)\n\n\n(Acá pueden encontrar más http://tylervigen.com/old-version.html)\n\n¿Significa esto que la margarina causa divorcios? ¿O los divorcios hacen que la gente coma más margarina? Por supuesto que la respuesta a estas dos preguntas es no. Esto es sólo un ejemplo de lo que llamamos una correlación espuria.\n\n\nLos casos presentados en el sitio de correlación espuria son todos casos de lo que generalmente se denomina dragado de datos (data dredging), pesca de datos (data fishing) o espionaje de datos (data snooping). Es básicamente una forma de lo que en los EE.UU. se llama \"cherry picking\". Un ejemplo de dragado de datos sería si miras a través de muchos resultados producidos por un proceso aleatorio y escoges el que muestra una relación que apoya una teoría que se quiere defender.\n\n# La paradoja de Simpson\n\nSe llama paradoja porque vemos el signo de la correlación cambiar cuando comparamos toda la data y estratos específicos. Como ejemplo ilustrativo, supongamos que tiene tres variables aleatorias $X$, $Y$ y $Z$ y que observamos realizaciones de estas. Aquí está el gráfico de observaciones simuladas para $X$ y $Y$ a lo largo de la correlación de la muestra:\n\n<img src=\"./img/img_sesion3/simp1.png\" width=\"600\">\n\n\nPuedes ver que $X$ e $Y$ están negativamente correlacionados. Sin embargo, una vez que estratificamos por $Z$ (mostrado en diferentes colores abajo) emerge otro patrón:\n\n<img src=\"./img/img_sesion3/simp2.png\" width=\"600\">\n\n\nEs realmente $Z$ que está negativamente correlacionado con $X$. Si estratificamos por $Z$ las variables $X$ e $Y$ están en realidad correlacionados positivamente como se ha visto en el gráfico anterior.\n\n# Expectativas condicionales\n\nSupongamos que nos piden que adivinemos la altura de un hijo seleccionado al azar y no sabemos la altura de su padre. Debido a que la distribución de las alturas de los hijos es aproximadamente normal, sabemos que la altura media, $69.2$, es el valor con la mayor proporción y sería la predicción con mayores posibilidades de minimizar el error. Pero, ¿y si nos dicen que el padre es más alto que el promedio, digamos que mide 72 pulgadas de alto, todavía esperaríamos que la altura más probable del hijo sea 69.2 pulgadas?\n\nResulta que si pudiéramos recolectar datos de un gran número de padres que miden 72 pulgadas, la distribución de las alturas de sus hijos sería normalmente distribuida. Esto implica que el promedio de la distribución calculada en este subconjunto sería nuestra mejor predicción.\n\nEn general, llamamos a este enfoque condicional. La idea general es que estratificamos una población en grupos y calculamos resúmenes en cada grupo. Por lo tanto, el condicionamiento está relacionado con el concepto de estratificación descrito. \n\nPara proporcionar una descripción matemática del condicionamiento, considere que tenemos una población de pares de valores  $(x_1,y_1),...,(x_n,y_n)$, por ejemplo, todas las alturas de padre e hijo en Inglaterra. Sabemos que si se toma un par al azar $(X,Y)$ el valor esperado y el mejor predictor de $Y$ es $E(Y)=\\mu_y$, el promedio de la población: $1/n \\sum_{i=y}^{n}y_i$. Sin embargo, ya no estamos interesados en la población en general, sino sólo en el subconjunto de la población con un valor específico, $72$ pulgadas. Este subconjunto de la población, es también una población y por lo tanto se aplican los mismos principios y propiedades que hemos aprendido. El $y_i$ en la subpoblación tienen una distribución, denominada distribución condicional, y esta distribución tiene un valor esperado, denominado expectativa condicional. En nuestro ejemplo, la expectativa condicional es la estatura promedio de todos los hijos en Inglaterra con padres de 72 pulgadas. La notación estadística es para la expectativa condicional es:\n\n\\begin{equation}\nE(Y|X=x)\n\\end{equation}\n\ncon $x$ representando el valor fijo que define ese subconjunto, por ejemplo 72 pulgadas. Del mismo modo, se indica la desviación estándar de los estratos con:\n\n\\begin{equation}\nSD(Y|X=x)=\\sqrt{Var(Y|x=x)}\n\\end{equation}\n\n\nPorque la expectativa condicional $E(Y|X=x)$ es el mejor predictor para la variable aleatoria $Y$ para un individuo en los estratos definidos por  $X=x$ muchos de los desafíos de la ciencia de datos se reducen a la estimación de esta cantidad. La desviación estándar condicional cuantifica la precisión de la predicción.\n\nEn el ejemplo que hemos estado considerando, estamos interesados en calcular la altura promedio del hijo condicionada a que el padre tenga 72 pulgadas de altura. Queremos estimar $E(Y|X=72)$ usando la muestra recolectada por Galton. \n\nAnteriormente aprendimos que el promedio de la muestra es el enfoque preferido para estimar el promedio de la población. Sin embargo, un desafío al usar este enfoque para estimar las expectativas condicionales es que para los datos continuos no tenemos muchos puntos de datos que coincidan exactamente con un valor de nuestra muestra. Por ejemplo, sólo tenemos:\n\n::: {#3990ecaa .cell execution_count=26}\n``` {.python .cell-code}\ncount_72 = (galton_heights['father'] == 72).sum()\nprint(\"Cantidad de registros con valor 72 en la columna 'father':\", count_72)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de registros con valor 72 en la columna 'father': 8\n```\n:::\n:::\n\n\npadres que miden exactamente 72 pulgadas. Si cambiamos el número a 72.5, obtenemos aún menos puntos de datos:\n\n::: {#a6fa811d .cell execution_count=27}\n``` {.python .cell-code}\ncount_725 = (galton_heights['father'] == 72.5).sum()\nprint(\"Cantidad de registros con valor 72.5 en la columna 'father':\", count_725)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de registros con valor 72.5 en la columna 'father': 1\n```\n:::\n:::\n\n\nUna forma práctica de mejorar estas estimaciones de las expectativas condicionales, es definir estratos con valores similares de $x$. En nuestro ejemplo, podemos redondear las alturas paternas a la pulgada más cercana y asumir que todas son de 72 pulgadas. Si hacemos esto, terminamos con la siguiente predicción para el hijo de un padre que mide 72 pulgadas de alto:\n\n::: {#c09d6939 .cell execution_count=28}\n``` {.python .cell-code}\nconditional_avg = galton_heights[galton_heights['father'].round() == 72]['son'].mean()\nprint(\"Promedio condicional para father == 72:\", conditional_avg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPromedio condicional para father == 72: 70.90714285714286\n```\n:::\n:::\n\n\nEn este código, filtramos el DataFrame \"galton_heights\" para obtener las filas donde el valor redondeado de \"father\" es igual a 72. Luego, calculamos el promedio de la columna \"son\" en las filas filtradas y almacenamos el resultado en la variable \"conditional_avg\". Finalmente, imprimimos el promedio condicional calculado.\n\n\nNote que un padre de 72 pulgadas es más alto que el promedio -- específicamente, 72 - 69.1/2.5 = 1.1 desviaciones estándar más alto que el padre promedio. Nuestra predicción, $70.5$, es también más alta que el promedio, pero sólo $0.49$ desviaciones estándar más grandes que el hijo promedio. Los hijos de padres de 72 pulgadas han regresado algunos a la estatura promedio. Observamos que la reducción en el número de SD más altas es de alrededor de $0.5$, lo que resulta ser la correlación. Como veremos en una sección posterior, esto no es una coincidencia.\n\nSi queremos hacer una predicción de cualquier altura, no sólo de 72, podríamos aplicar el mismo enfoque a cada estrato. La estratificación seguida de los boxplots nos permite ver la distribución de cada grupo:\n\n::: {#4666bd64 .cell execution_count=29}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Crear una nueva columna 'father_strata' con los valores redondeados de 'father'\ngalton_heights['father_strata'] = galton_heights['father'].round().astype(int)\n\n# Crear el gráfico de boxplots\nplt.figure(figsize=(10, 6))  # Tamaño del gráfico\nsns.boxplot(data=galton_heights, x='father_strata', y='son')\n\n# Agregar puntos para mostrar las medias condicionadas\nsns.swarmplot(data=galton_heights, x='father_strata', y='son', color='black', size=4)\n\nplt.xlabel('father_strata')\nplt.ylabel('son')\nplt.title('Boxplots de son condicionado por father_strata con Medias Condicionadas')\nplt.xticks(rotation=45)  # Rotar etiquetas del eje x si es necesario\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-30-output-1.png){width=824 height=536}\n:::\n:::\n\n\nNo es de extrañar que los centros de los grupos aumenten con la altura.\n\n::: {#ef7007f3 .cell execution_count=30}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n# Crear un gráfico de puntos para mostrar el promedio condicional por \"father\"\nplt.figure(figsize=(10, 6))\nplt.scatter(conditional_avg_by_father['father'], conditional_avg_by_father['son'], color='blue')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres\")\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-31-output-1.png){width=812 height=529}\n:::\n:::\n\n\nAdemás, estos centros parecen seguir una relación lineal. A continuación se presentan los promedios de cada grupo. Si tenemos en cuenta que estos promedios son variables aleatorias con errores estándar, los datos son consistentes con estos puntos siguiendo una línea recta:\n\n::: {#49f995a5 .cell execution_count=31}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n\nconditional_avg_by_father.head()\n\n\n# Crear un gráfico de puntos con ajuste de regresión lineal\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='father', y='son', data=conditional_avg_by_father, color='blue')\nsns.regplot(x='father', y='son', data=conditional_avg_by_father, scatter=False, color='orange')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres con Regresión Lineal\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-32-output-1.png){width=812 height=529}\n:::\n:::\n\n\nEl hecho de que estos promedios condicionales sigan una línea no es una coincidencia. En la siguiente sección, explicamos que la línea que siguen estos promedios es lo que llamamos la línea de regresión, que mejora la precisión de nuestras estimaciones. Sin embargo, no siempre es apropiado estimar las expectativas condicionales con la línea de regresión, por lo que también describimos la justificación teórica de Galton para usar la línea de regresión.\n\n\n# La línea de regresión\n\nSi estamos prediciendo una variable aleatoria $Y$ conociendo el valor de otra variable $X=x$ usando una línea de regresión, entonces predecimos que **para cada desviación estándar, $\\sigma_x$ que $x$ aumenta por encima de la media $\\mu_x$, $Y$ incrementa $\\rho$ veces la desviación estándar $\\sigma_Y$ sobre el promedio $\\mu_Y$**, con $\\rho$ la correlación entre $X$ e $Y$. Por lo tanto, la formula de la regresión es:\n\n$$\n\\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right)=\\rho \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\n$$\n\nLo que podemos reescribir como:\n\n$$\nY=\\mu_Y + \\rho \\big(\\frac{x-\\mu_X}{\\sigma_X}\\big) \\sigma_Y\n$$\n\nSi existe una correlación perfecta, la línea de regresión predice un aumento que corresponde al mismo número de desviacones estándar. Si hay correlación 0, entonces no usamos $x$ en absoluto en la predicción y simplemente predecimos el promedio $\\mu_Y$. Para valores entre 0 y 1, la predicción se encuentra en un punto intermedio. Si la correlación es negativa, predecimos una reducción en lugar de un aumento.\n\nNótese que si la correlación es positiva e inferior a 1, nuestra predicción está más cerca (en unidades estándar) de la altura media que de lo que el valor utilizado para predecir, $x$, está del promedio de los $x$. Por eso lo llamamos regresión: el hijo regresa a la estatura media. De hecho, el título del artículo de Galton era: Regresión a la mediocridad en la estatura hereditaria (Regression toward mediocrity in hereditary stature.). \n\nPara añadir líneas de regresión a los gráficos, necesitaremos la fórmula anterior en la forma: $y=b+mx$, con pendiente $m=\\rho \\sigma_y / \\sigma_x$ e intercepto $b=\\mu_y - m \\mu_x$\n\nAquí agregamos la línea de regresión a la data original.\n\n::: {#581cf1fa .cell execution_count=32}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cálculo de las medias y desviaciones estándar\nmu_x = galton_heights['father'].mean()\nmu_y = galton_heights['son'].mean()\ns_x = galton_heights['father'].std()\ns_y = galton_heights['son'].std()\n\n# Cálculo del coeficiente de correlación\nr = galton_heights['father'].corr(galton_heights['son'])\n\n# Cálculo de la pendiente y el intercepto para la línea de regresión\nm = r * s_y / s_x\nb = mu_y - m * mu_x\n\n# Configuración del tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gráfico de dispersión con línea de regresión\nsns.scatterplot(x='father', y='son', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father', y='son', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Son Height\")\nplt.title(\"Relación entre Altura de Padres e Hijos con Línea de Regresión\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-33-output-1.png){width=824 height=529}\n:::\n:::\n\n\nLa fórmula de regresión implica que si primero estandarizamos las variables, es decir, restamos el promedio y dividimos por la desviación estándar, entonces la línea de regresión tiene intercepto 0 y pendiente igual a la correlación $\\rho$. Aquí está la misma gráfica, pero usando unidades estándar:\n\n::: {#95718684 .cell execution_count=33}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Estandarizar las variables 'father' y 'son'\ngalton_heights['father_standardized'] = (galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std()\ngalton_heights['son_standardized'] = (galton_heights['son'] - galton_heights['son'].mean()) / galton_heights['son'].std()\n\n# Calcular la correlación de las variables estandarizadas\nr = galton_heights['father_standardized'].corr(galton_heights['son_standardized'])\n\n# Configuración del tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gráfico de dispersión con línea de regresión\nsns.scatterplot(x='father_standardized', y='son_standardized', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father_standardized', y='son_standardized', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height (Standardized)\")\nplt.ylabel(\"Son Height (Standardized)\")\nplt.title(\"Relación Estandarizada entre Altura de Padres e Hijos con Línea de Regresión (Intercepto = 0, Pendiente = Correlación)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-34-output-1.png){width=874 height=529}\n:::\n:::\n\n\n# La regresión mejora la precisión\n\nComparemos los dos enfoques de predicción que hemos presentado:\n\n1) Redondee las alturas del padre a la pulgada más cercana, estratifique y luego tome el promedio.\n2) Calcula la línea de regresión y úsala para predecir.\n    \nUsamos un muestreo de simulación de Monte Carlo $N=50$ familias:\n\n::: {#9be17081 .cell execution_count=34}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n# Definimos B (número de simulaciones) y N (tamaño de la muestra).\n\nB= 1000\nN=50\n\n# Configuración de la semilla aleatoria para reproducibilidad\nnp.random.seed(10)\n\n# Inicializar listas para almacenar los resultados de las simulaciones\nconditional_avg = []\nregression_prediction = []\n\n# Realizar simulaciones de Monte Carlo\nfor _ in range(B):\n    # Seleccionar una muestra aleatoria de tamaño N\n    dat = galton_heights.sample(n=N)\n    \n    # Calcular la media condicional (Enfoque 1)\n    conditional_avg.append(dat[dat['father'].round() == 72]['son'].mean())\n    \n    # Calcular la predicción de regresión (Enfoque 2)\n    mu_x = dat['father'].mean()\n    mu_y = dat['son'].mean()\n    s_x = dat['father'].std()\n    s_y = dat['son'].std()\n    r = dat['father'].corr(dat['son'])\n    regression_prediction.append(mu_y + r * (72 - mu_x) / (s_x / s_y))\n\n# Calcular las estadísticas descriptivas de las simulaciones\nmean_conditional_avg = np.mean(conditional_avg)\nmean_regression_prediction = np.mean(regression_prediction)\nstd_conditional_avg = np.std(conditional_avg, ddof=1)\nstd_regression_prediction = np.std(regression_prediction, ddof=1)\n\n# Imprimir resultados\nprint(\"Valor Esperado (Media) - Enfoque 1 (Media Condicional):\", mean_conditional_avg)\nprint(\"Valor Esperado (Media) - Enfoque 2 (Predicción de Regresión):\", mean_regression_prediction)\nprint(\"Error Estándar - Enfoque 1 (Media Condicional):\", std_conditional_avg)\nprint(\"Error Estándar - Enfoque 2 (Predicción de Regresión):\", std_regression_prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValor Esperado (Media) - Enfoque 1 (Media Condicional): nan\nValor Esperado (Media) - Enfoque 2 (Predicción de Regresión): 70.54175180600582\nError Estándar - Enfoque 1 (Media Condicional): nan\nError Estándar - Enfoque 2 (Predicción de Regresión): 0.4203567940735428\n```\n:::\n:::\n\n\nAunque el valor esperado de estas dos variables aleatorias es casi el mismo, el error estándar para la predicción de regresión es sustancialmente menor.\n\nPor lo tanto, la línea de regresión es mucho más estable que la media condicional. Hay una razón intuitiva para ello. El promedio condicional se calcula en un subconjunto relativamente pequeño: los padres que miden alrededor de 72 pulgadas de alto. De hecho, en algunas de las permutaciones no tenemos datos. La regresión siempre utiliza todos los datos.\n\nEntonces, ¿por qué no usar siempre la regresión para predecir? Porque no siempre es apropiado. Por ejemplo, Anscombe proporcionó casos en los que los datos no tienen una relación lineal. Entonces, ¿está justificado usar la línea de regresión para predecir? Galton contestó esto de forma afirmativa para los datos de altura.\n\n\n\n# Definición matemática\n\n\nEl modelo de regresión lineal (Legendre, Gauss, Galton y Pearson) considera que, dado un conjunto de observaciones $\\{y_i, x_{i1},...,x_{np}\\}^{n}_{i=1}$ , la media  $𝜇$  de la variable respuesta  $𝑦$  se relaciona de forma lineal con la o las variables regresoras  $𝑥_1$ ... $x_p$  acorde a la ecuación:\n\n$\\mu_y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + ... + \\beta_p x_{p}$\n \nEl resultado de esta ecuación se conoce como la línea de regresión poblacional, y recoge la relación entre los predictores y la media de la variable respuesta.\n\nOtra definición que se encuentra con frecuencia en los libros de estadística es:\n\n$y_i= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} +\\epsilon_i$\n \nEn este caso, se está haciendo referencia al valor de  𝑦  para una observación  𝑖  concreta. El valor de una observación puntual nunca va a ser exactamente igual al promedio, de ahí que se añada el término de error  $\\epsilon$.\n\n\nEn ambos casos, la interpretación de los elementos del modelo es la misma:\n\n- $\\beta_0$: es la ordenada en el origen, se corresponde con el valor promedio de la variable respuesta  $y$  cuando todos los predictores son cero.\n\n- $\\beta_j$: es el efecto promedio que tiene sobre la variable respuesta el incremento en una unidad de la variable predictora  $x_j$, manteniéndose constantes el resto de variables. Se conocen como coeficientes de regresión.\n\n- $\\epsilon$: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo. Recoge el efecto de todas aquellas variables que influyen en $y$ pero que no se incluyen en el modelo como predictores.\n\nEn la gran mayoría de casos, los valores $\\beta_0$ y $\\beta_j$ poblacionales se desconocen, por lo que, a partir de una muestra, se obtienen sus estimaciones  $\\hat{\\beta_0}$ y $\\hat{\\beta_j}$. **Ajustar el modelo consiste en estimar, a partir de los datos disponibles, los valores de los coeficientes de regresión que maximizan la verosimilitud (likelihood), es decir, los que dan lugar al modelo que con mayor probabilidad puede haber generado los datos observados.**\n\nEl método empleado con más frecuencia es el ajuste por mínimos cuadrados ordinarios (OLS), que identifica como mejor modelo la recta (o plano si es regresión múltiple) que minimiza la suma de las desviaciones verticales entre cada dato de entrenamiento y la recta, elevadas al cuadrado.\n\n# Interpretación del modelo\n\n\nLos principales elementos que hay que interpretar en un modelo de regresión lineal son los coeficientes de los predictores:\n\n- $\\beta_0$  es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta $y$, cuando todos los predictores son cero.\n\n- $\\beta_j$ los coeficientes de regresión parcial de cada predictor **indican el cambio promedio esperado de la variable respuesta  𝑦  al incrementar en una unidad de la variable predictora  $x_j$, manteniéndose constantes el resto de variables. (\"Ceteris paribus\"))**\n\nLa magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.\n\nPara poder determinar qué impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviación estándar) las variables predictoras previo ajuste del modelo. En este caso, $\\beta_0$ se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y  $\\beta_j$ el cambio promedio esperado de la variable respuesta al incrementar en una desviación estándar la variable predictora  $x_j$, manteniéndose constantes el resto de variables.\n\nSi bien los coeficientes de regresión suelen ser el primer objetivo de la interpretación de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condición de normalidad...etc.). Estos últimos suelen ser tratados con poco detalle cuando el único objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta. \n\n\n\n# Significado \"lineal\"\n\nEl término \"lineal\" en los modelos de regresión hace referencia al hecho de que los parámetros se incorporan en la ecuación de forma lineal, no a que necesariamente la relación entre cada predictor y la variable respuesta tenga que seguir un patrón lineal.\n\nLa siguiente ecuación muestra un modelo lineal en el que el predictor  𝑥1  no es lineal respecto a y:\n\n$y = \\beta_0 + \\beta_1x_1 + \\beta_2log(x_1) + \\epsilon$\n\n\n<img src=\"./img/img_sesion3/im1.png\" width=\"400\">\n\n\nEn contraposición, el siguiente no es un modelo lineal:\n\n$y = \\beta_0 + \\beta_1x_1^{\\beta_2} + \\epsilon$\n\n \nEn ocasiones, algunas relaciones no-lineales pueden transformarse de forma que se pueden expresar de manera lineal:\n\n- Modelo no-lineal a estimar: $y = \\beta_0x_1^{\\beta_1}\\epsilon$\n\n- Solucion: pasamos todo a logaritmos:\n\n$log(y)=log(\\beta_0) + \\beta_1log(x_1) + log(\\epsilon)$\n        \n$y^{'}=\\beta_0^{'}+\\beta_1x_1^{'} + \\epsilon^{'}$\n\n- Estimar el modelo y extraer los coeficientes.\n\n- Volvera a la forma funcional incial exponenciando los logaritmos.\n    - $\\beta_1$ es explicito.\n    - $\\beta_0^{'}=log(\\beta_0)=> exp(log(\\beta_0))$\n\n### Distribución normal bivariable (avanzado)\n\nhttps://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal_multivariante\n\nLa correlación y la pendiente de regresión son estadísticas de resumen ampliamente utilizadas, pero a menudo se utilizan o interpretan erróneamente. Los ejemplos de Anscombe proporcionan casos excesivamente simplificados de conjuntos de datos en los que resumir con correlación sería un error. Pero hay muchos más ejemplos de la vida real.\n\nLa principal forma en que motivamos el uso de la correlación involucra lo que se llama la distribución normal bivariada.\n\nCuando un par de variables aleatorias son aproximadas por la distribución normal bivariada, las gráficas de dispersión se ven como óvalos. Pueden ser delgadas (alta correlación) o en forma de círculo (sin correlación).\n\n![image](img/img_sesion3/g1.png)\n\n\nUna forma más técnica de definir la distribución normal bivariada es la siguiente: si $X$ es una variable aleatoria normalmente distribuida, $Y$ es también una variable aleatoria normalmente distribuida, y la distribución condicional de $Y$ para cualquier $X=x$ es aproximadamente normal, entonces el par es aproximadamente normal bivariado.\n\n![image](img/img_sesion3/g3.png)\n\n\nSi pensamos que los datos de altura están bien aproximados por la distribución normal bivariada, entonces deberíamos ver la aproximación normal para cada estrato. Aquí estratificamos las alturas del hijo por las alturas paternas estandarizadas y vemos que la suposición parece mantenerse:\n\n::: {#4c941f0c .cell execution_count=35}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Estandarizar las alturas del padre y crear una columna 'z_father'\ngalton_heights['z_father'] = np.round((galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std())\n\n# Filtrar las alturas del padre dentro del rango [-2, 2]\ngalton_heights_filtered = galton_heights[galton_heights['z_father'].isin([-2, -1, 0, 1, 2])]\n\n\ngalton_heights_filtered.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>father</th>\n      <th>son</th>\n      <th>father_strata</th>\n      <th>father_standardized</th>\n      <th>son_standardized</th>\n      <th>z_father</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>75.0</td>\n      <td>71.0</td>\n      <td>75</td>\n      <td>2.304857</td>\n      <td>0.658278</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75.0</td>\n      <td>70.5</td>\n      <td>75</td>\n      <td>2.304857</td>\n      <td>0.472130</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>75.0</td>\n      <td>72.0</td>\n      <td>75</td>\n      <td>2.304857</td>\n      <td>1.030575</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74.0</td>\n      <td>73.0</td>\n      <td>74</td>\n      <td>1.914905</td>\n      <td>1.402871</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>74.0</td>\n      <td>74.0</td>\n      <td>74</td>\n      <td>1.914905</td>\n      <td>1.775168</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#f2a0fdf7 .cell execution_count=36}\n``` {.python .cell-code}\n# Configurar el tamaño de la figura y el estilo de Seaborn\nplt.figure(figsize=(5, 3))\nsns.set(style='whitegrid')\n\n# Crear gráficos QQ-plot utilizando un bucle for para cada estrato\nfor z_father_value, group_data in galton_heights_filtered.groupby('z_father'):\n    plt.subplot(2, 3, int(z_father_value) + 3)  # Convertir z_father_value a entero\n    stats.probplot(group_data['son'], plot=plt, fit=True, rvalue=True)\n    plt.title(f\"Estrato z_father={z_father_value}\")\n\n# Ajustar títulos y etiquetas\nplt.suptitle(\"QQ-Plots de las Alturas del Hijo por Estrato de Alturas del Padre Estandarizadas\")\nplt.tight_layout()\n\n# Mostrar el gráfico\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-37-output-1.png){width=702 height=284}\n:::\n:::\n\n\nAhora volvemos a definir la correlación. Galton utilizó estadísticas matemáticas para demostrar que, cuando dos variables siguen una distribución normal bivariada, el cálculo de la línea de regresión es equivalente al cálculo de las expectativas condicionales. Esto implica que, si nuestros datos son aproximadamente bivariados, la línea de regresión es equivalente a la probabilidad condicional. Por lo tanto, podemos obtener una estimación mucho más estable del valor de expectación condicional, encontrando la línea de regresión y usándola para predecir.\n\nEn resumen, si nuestros datos son aproximadamente bivariados, entonces la expectativa condicional, la mejor predicción de $Y$ dado que conocemos el valor de $X$ está dada por la línea de regresión.\n\n$$\nY_i=\\beta_0 + \\beta_1 x_i +U_i\n$$\n\nDe aquí facilmente podemos intuir algunos de los supuestos que deben cumplirse al implementar una regresión (y que estudiaremos en detalle en la siguiente sesion):\n\n\n   1) Normalidad: $u_i \\sim Normal$\n   \n   2) Linealidad: Los residuos se distribuyen sin forma alrededor del cero $E(u_i)=0$\n   \n   3) Homocedasticidad: La variabilidad de los residuos es similar para todos los $x_i$, $V(u_i)=\\sigma^2$\n   \n   4) No existen resudios atípicos.\n  \n   5) Independecia: Los residuos, ($u_i$), son independientes \n\n## Varianza explicada\nLa teoría de la normalidad bivariada también nos dice que la desviación estándar de la distribución condicional descrita anteriormente es:\n\n$$\nSD(Y|X=x)=\\sigma_Y\\sqrt{1-\\rho^2}\n$$\n\n\nPara ver por qué esto es intuitivo, note que sin condicionamiento, $SD(Y)=\\sigma_Y$ estamos viendo la variabilidad de todos los hijos. Pero una vez que los condicionamos, sólo estamos viendo la variabilidad de los hijos con un padre que mide 72 pulgadas de alto. Este grupo tenderá a ser \"algo más\" alto (que el promedio), por lo que la desviación estándar se reduce. \n\nEspecíficamente, se reduce a $\\sqrt{1-\\rho^2}=\\sqrt{1-0.25}=0.86$ de lo que era originalmente. Podríamos decir que la estatura del padre \"explica\" el 14% de la variabilidad de estatura del hijo.\n\n\n\nLa frase \"$X$ explica tal o cual porcentaje de la variabilidad\" se utiliza comúnmente en papers académicos. En este caso, este porcentaje se refiere realmente a la desviación (SD al cuadrado). Por lo tanto, si los datos son normales bivariados, la varianza se reduce en $1-\\rho^2$ por lo que decimos que $X$ explica $1-(1-\\rho^2)=\\rho^2$ (la correlación al cuadrado) de la varianza.\n\n\nPero es importante recordar que la afirmación de \"varianza explicada\" sólo tiene sentido cuando los datos se aproximan mediante una distribución normal bivariada.\n\n\n## Cuidado: hay dos líneas de regresión\n\nCalculamos una línea de regresión para predecir la altura del hijo desde la altura del padre. \n\nUsamos estos cálculos:\n\n::: {#05f307f5 .cell execution_count=37}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviación estándar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviación estándar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlación entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.4282667416279721\n2.5644170903922188\n2.6860317955777377\n69.08938547486034\n69.23184357541899\n```\n:::\n:::\n\n\n::: {#d8d2834a .cell execution_count=38}\n``` {.python .cell-code}\n# Calcular la pendiente de la primera línea de regresión\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera línea de regresión\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npendiente 0.44857682836034635\nconstante 38.23994616574076\n```\n:::\n:::\n\n\nLo que nos da la función $E(Y|X=x)=28,8+0.44x$.\n\n¿Y si queremos predecir la estatura del padre basándonos en la del hijo? \n\nEs importante saber que esto no se determina calculando la función inversa!.\n\nNecesitamos computar $E(X∣Y=y)$. Dado que los datos son aproximadamente normales bivariados, la teoría descrita anteriormente nos dice que esta expectativa condicional seguirá una línea con pendiente e intercepto:\n\n::: {#090d3fc6 .cell execution_count=39}\n``` {.python .cell-code}\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npendiente 0.4088762289729847\nconstante 40.782130348895464\n```\n:::\n:::\n\n\nDe nuevo vemos una regresión a la media: la predicción para el padre está más cerca de la media del padre que lo que estan las alturas del hijo $y$ al promedio del hijo.\n\nAquí hay un gráfico que muestra las dos líneas de regresión:\n\n::: {#0836d929 .cell execution_count=40}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gráfico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-41-output-1.png){width=812 height=529}\n:::\n:::\n\n\ncon azul para la predicción de las alturas del hijo con las alturas del padre y rojo para la predicción de las alturas del padre con las alturas del hijo.\n\n::: callout-tip\n\n## Taller aplicacción 2: Altura de padres e hijos\n\n\n1) Cargue los datos de `GaltonFamilies` desde el HistData. Los niños de cada familia están ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado `galton_heights` seleccionando niños y niñas al azar. (HINT: use `sample`).\n\n2) Haga una gráfica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\n\n3) Calcular la correlación para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\n\n4) Plotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\n\n5) Obtener el modelo de regresión e interpretar los coeficientes.\n:::\n\n",
    "supporting": [
      "sesion3_notas_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}