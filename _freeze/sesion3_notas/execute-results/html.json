{
  "hash": "3e2aa02e774b0cc569a536713e101edc",
  "result": {
    "markdown": "---\ntitle: 'Sesi贸n 3: Introducci贸n al An谩lisis de Regresi贸n'\ninstitute: Mag铆ster en Data Science - Universidad del Desarrollo\nsubtitle: 'Curso: An谩lisis de datos'\nauthor: 'Phd (c) Melanie Oyarz煤n - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'\nformat:\n  html:\n    toc: true\n    html-math-method: mathml\n    embed-resources: true\n  ipynb: default\necho: true\neditor:\n  markdown:\n    wrap: 72\nexecute:\n  keep-ipynb: true\n  freeze: auto\ncode-link: true\n---\n\n# Visi贸n general al an谩lisis de regresi贸n\n\nEn las aplicaciones de la ciencia de datos, es muy com煤n estar interesado en la relaci贸n entre dos o m谩s variables.\n\nEl an谩lisis de regresi贸n es una t茅cnica en la cual buscamos encontrar una funci贸n que pueda describir la relaci贸n observada en los datos entre dos o mas variables.\n\nPor ejemplo, una persona podr铆a querer relacionar los pesos de los individuos con sus alturas\n\n  - 驴Son los m谩s altos m谩s pesados? \n  -  y驴cu谩nto m谩s pesados?\n\nPensemos en el caso m谩s sencillo: una **regresi贸n lineal simple** o univariada. Tenemos una variable que deseamos explicar o predecir (Y)\ncomo funci贸n de otra (X).\n\nPara esto, buscamos la pendiente e intercepto de una funci贸nla recta de la forma:\n\n$$Y = \\alpha + \\beta X$$\n\nque se ajuste mejor al conjunto de datos con los que se cuenta.\n\ndonde $X$ es la variable explicativa e $Y$ es la variable dependiente.  La pendiente de la recta es $b$, y $a$ es la intersecci贸n (el valor de $y$ cuando $x = 0$).\n\n<img src=\"./img/img_sesion3/gif_regresion2.gif\" width=\"600\">\n\n\nPara esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistem谩tica\no que se puede explicar directamente con una o m谩s variables independientes (Xs o regresores) y otra que es no sistem谩tica o error\n($\\mu$ o $epsilon$) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fen贸meno.\n\n![](img/img_sesion3/gif_regresion1.gif)\n\nLa parte sistem谩tica entonces la describimos con una forma funcional, que depende de otras variables o regresores.\n\nEsta forma funcional puede ser lineal univariada, lineal m煤ltiple o no lineal. El tipo de forma funcional, definir谩 el tipo de regresi贸n de la que estemos hablando.\n\nVentajas del an谩lisis de regersi贸n: es facil describir cuantitaivamente una relaci贸n.\n\nEsquem谩ticamente, los elementos son:\n\n![](img/img_sesion3/regresion_esquema.png)\n\n\n## Usos de las regresiones\n\nLas regresiones tienen tres principales usos:\n\n- Describir un fen贸meno\n- Probar hip贸tesis sobre ciertas teor铆as\n- Realizar predicciones \n\n## Regresi贸n simple y scatterplot\n\nPor ejemplo, pensemos en la relaci贸n entre los a帽os de educaci贸n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom铆a.\n\nPodriamos pensar que ambas variables se encuentras relacionadas.\n\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\n::: {#16ce75ff .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a帽os\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_vivienda</th>\n      <th>folio</th>\n      <th>id_persona</th>\n      <th>region</th>\n      <th>area</th>\n      <th>nse</th>\n      <th>expr</th>\n      <th>tot_per_h</th>\n      <th>edad</th>\n      <th>sexo</th>\n      <th>pco1_a</th>\n      <th>e3</th>\n      <th>o6</th>\n      <th>o8</th>\n      <th>y1</th>\n      <th>ytrabajocor</th>\n      <th>esc</th>\n      <th>desercion</th>\n      <th>educ</th>\n      <th>contrato</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>1</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>43</td>\n      <td>3</td>\n      <td>72</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>B谩sica incompleta</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>2</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>43</td>\n      <td>3</td>\n      <td>67</td>\n      <td>1. Hombre</td>\n      <td>S铆</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>B谩sica incompleta</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000901</td>\n      <td>100090101</td>\n      <td>3</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>44</td>\n      <td>3</td>\n      <td>40</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No sabe</td>\n      <td>411242.0</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>T茅cnico nivel superior completo</td>\n      <td>No sabe</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000902</td>\n      <td>100090201</td>\n      <td>1</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>51</td>\n      <td>4</td>\n      <td>56</td>\n      <td>1. Hombre</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No sabe</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000902</td>\n      <td>100090201</td>\n      <td>2</td>\n      <td>Regi贸n de uble</td>\n      <td>Rural</td>\n      <td>Bajo-medio</td>\n      <td>51</td>\n      <td>4</td>\n      <td>25</td>\n      <td>2. Mujer</td>\n      <td>No</td>\n      <td>2. No</td>\n      <td>2. No</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12.0</td>\n      <td>Deserci贸n</td>\n      <td>Media humanista completa</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\n::: {#60f60dc7 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregaci贸n\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por regi贸n\n\ncasen_2022_region.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region</th>\n      <th>ytrabajocor</th>\n      <th>esc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Regi贸n de Tarapac谩</td>\n      <td>658026.6250</td>\n      <td>11.679582</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Regi贸n de Antofagasta</td>\n      <td>791351.8125</td>\n      <td>11.833934</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Regi贸n de Atacama</td>\n      <td>666128.3125</td>\n      <td>11.126735</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Regi贸n de Coquimbo</td>\n      <td>656137.8750</td>\n      <td>10.973584</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Regi贸n de Valpara铆so</td>\n      <td>611298.1250</td>\n      <td>11.559877</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nRealicemos un scatter sencillo:\n\n::: panel-tabset\n\n## matplotlib\n\n::: {#fb4cfe6a .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por regi贸n)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-4-output-1.png){width=619 height=450}\n:::\n:::\n\n\n## seaborn\n\n::: {#79d7e7ea .cell execution_count=4}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-5-output-1.png){width=619 height=449}\n:::\n:::\n\n\n## seaborn + linea de regresion\n\n::: {#b012938e .cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L铆nea de Regresi贸n y Intervalo de Confianza')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-6-output-1.png){width=619 height=450}\n:::\n:::\n\n\n## Con codigos de region\n\n::: {#c8c7e300 .cell execution_count=6}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de regi贸n a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la 煤ltima palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L铆nea de Regresi贸n y Etiquetas de Regi贸n (ltima Palabra)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-7-output-1.png){width=657 height=453}\n:::\n:::\n\n\n:::\n\n\nPodemos ver que se aprecia una relaci贸n positiva: a mayor escolaridad promedio, mayor salario promedio por regi贸n.\n\n## Especificaci贸n\n\nLlamamos especifiaci贸n al precisar la relaci贸n entre las variables que deseamos estimar.\n\nEn nuestro caso, la funci贸n base que queremos entender es entre salario y educaci贸n:\n\n$$ \\text{Salario} = f(Educacion))$$\n\nEste es una relaci贸n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales:\n- agregar el error aleatorio\n- especificar una forma funcional\n- definir una forma de medir las variables en los datos\n\nEn nuestro caso, entonces el modelo especificado ser铆a:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a帽os educaci贸n}_i + \\mu_i$$\n\n## Interpretaci贸n\n\nCon nuestro modelo especificado:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a帽os educaci贸n}_i + \\mu_i$$\n\nPodemos interpretar $\\beta$ y $alpha$:\n\n- $\\beta = \\frac{\\partial ingr}{\\partial educ}$: un a帽o adici贸nal de educaci贸n, en cuanto incrementa el salario (si nada m谩s cambia) \n\n- $\\alpha$ valor esperado de y, si x=0...\n  \n## Modelo poblaci贸nal y estimaci贸n\n\nEste modelo especificado esta definido en la poblaci贸n:\n\n$$ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a帽os educaci贸n}_i + \\mu_i$$\n\npero necesitamos calcularlo con la muestra.... por lo cual tenemos estimadores para los coeficientes poblacionales!\n\n$$\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{a帽os educaci贸n}_i $$\n\n\n## Modelo poblaci贸nal y estimaci贸n\n\nEl m茅todo m谩s comun de estimaci贸n es el de los **m铆nimos cuadrados ordinarios**. Veremos detalles sobre la estimaci贸n, supuestos, propiedades estad铆sticas la proxima sesi贸n.\n\nPor ahora, pensaremos que es el m茅todo que busca la l铆nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi贸n).\n\n$$ \\hat{\\mu}_i= y_i-\\hat{y}_i$$\n\n::: {#abedf6f2 .cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuraci贸n del estilo del gr谩fico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gr谩fico de dispersi贸n con la l铆nea de regresi贸n\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresi贸n lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t茅rmino constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar l铆neas que conecten cada punto a la l铆nea de regresi贸n\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # L铆nea que conecta el punto a la l铆nea de regresi贸n\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresi贸n y residuos')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-8-output-1.png){width=622 height=455}\n:::\n:::\n\n\nEs decir, minimiza $\\sum_{i}^{n} \\hat{\\mu}_i $\n\n## Modelo estimado\n\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\n::: panel-tabset\n\n## Agrupados por regi贸n\n\n::: {#b045c813 .cell execution_count=8}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t茅rmino constante\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:33:51   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n>=20 ... continuing anyway, n=16\n\n```\n:::\n:::\n\n\n## Todos los datos\n\n::: {#8bd67891 .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t茅rmino constante\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n```\n:::\n:::\n\n\n:::\n\nPodemos ver que un a帽o adicional de educaci贸n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n\n驴y la constante, como la podemos interpretar?\n\n## Modelos simples y m煤ltiples\n\nMuchas veces una sola variable no es suficiente para describir bien un fen贸meno. Necesitamos incluir m谩s variables.\n\nEsto puede ser:\n- Una nueva variable\n- Una forma funcional no lineal de la variable ya incluida\n\nNuestra interpretaci贸n del modelo no cambia, solo que ahora efectivamente estamos **controlando** por otros factores.\n\nProbemos, agregar edad al modelo:\n\n::: {#cbbcea24 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t茅rmino constante\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n```\n:::\n:::\n\n\nEs muy usual, agregar edad al cuadrado.... para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer...\n\n::: {#c7d644f2 .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t茅rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresi贸n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n## Un poco m谩s sobre interpretaci贸n\n\n\nLos principales elementos que hay que interpretar en un modelo de regresi贸n lineal son los coeficientes de los predictores:\n\n- $\\beta_0$  es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta $y$, cuando todos los predictores son cero.\n\n- $\\beta_j$ los coeficientes de regresi贸n parcial de cada predictor **indican el cambio promedio esperado de la variable respuesta    al incrementar en una unidad de la variable predictora  $x_j$, manteni茅ndose constantes el resto de variables. (\"Ceteris paribus\"))**\n\nLa magnitud de cada coeficiente parcial de regresi贸n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est谩 asociada con la importancia de cada predictor.\n\n![](img/img_sesion3/unidad_medida.png)\n\nPara poder determinar qu茅 impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviaci贸n est谩ndar) las variables predictoras previo ajuste del modelo. En este caso, $\\beta_0$ se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y  $\\beta_j$ el cambio promedio esperado de la variable respuesta al incrementar en una desviaci贸n est谩ndar la variable predictora  $x_j$, manteni茅ndose constantes el resto de variables.\n\nSi bien los coeficientes de regresi贸n suelen ser el primer objetivo de la interpretaci贸n de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condici贸n de normalidad...etc.). Estos 煤ltimos suelen ser tratados con poco detalle cuando el 煤nico objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta. \n\n## Causalidad, regresi贸n y correlaci贸n\n\n**Importante tener en cuenta**\n\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relaci贸n entre las variables de inter茅s. Esto no implica necesariamente que una variable **cause** la otra (por ejemplo, puntajes m谩s altos en la PSU **no causan** calificaciones superiores en la universidad), pero existe alguna asociaci贸n significativa entre las dos variables.\n\nUn diagrama de dispersi贸n puede ser una herramienta 煤til para determinar la fuerza de la relaci贸n entre dos variables. Si parece no haber asociaci贸n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi贸n no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresi贸n lineal a los datos probablemente no proporcionar谩 un modelo 煤til.\n\nUna valiosa medida num茅rica de asociaci贸n entre dos variables es el coeficiente de correlaci贸n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci贸n de los datos observados para las dos variables.\n\n\n# Una perspectiva hist贸rica:\n\nEL origen de la t茅cnica, podemos remontarlo a la gen茅tica.\n\nFrancis Galton estudi贸 la variaci贸n y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolect贸 y estudi贸 datos de altura de familias para tratar de entender la herencia. **Mientras hac铆a esto, desarroll贸 los conceptos de correlaci贸n y regresi贸n.**\n\nPor supuesto, en el momento en que se recogieron estos datos, nuestro conocimiento de la gen茅tica era bastante limitado en comparaci贸n con lo\nque conocemos hoy en d铆a. Una pregunta muy espec铆fica que Galton trat贸 de responder fue:\n\n    驴qu茅 tan bien podemos predecir la estatura de un ni帽o basado en la estatura de los padres? \n\nLa t茅cnica que desarroll贸 para responder a esta pregunta, la regresi贸n, tambi茅n puede aplicarse en muchas otras circunstancias.\n\nNota hist贸rica: Galton hizo importantes contribuciones a la estad铆stica y la gen茅tica, pero tambi茅n fue uno de los primeros defensores de la\neugenesia, un movimiento filos贸fico cient铆ficamente defectuoso favorecido por muchos bi贸logos de la 茅poca de Galton pero con terribles\nconsecuencias hist贸ricas.\n\n<img src=\"./img/img_sesion3/galton.png\" width=\"350\">\n\n## Estudio de caso: 驴es hereditaria la altura?\n\nTenemos acceso a los datos de altura de familias recolectado por Galton, a trav茅s del paquete `HistData`. Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n::: {#ba994c79 .cell execution_count=12}\n``` {.python .cell-code}\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\ngalton_data.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family</th>\n      <th>father</th>\n      <th>mother</th>\n      <th>midparentHeight</th>\n      <th>children</th>\n      <th>childNum</th>\n      <th>gender</th>\n      <th>childHeight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>1</td>\n      <td>male</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>2</td>\n      <td>female</td>\n      <td>69.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>3</td>\n      <td>female</td>\n      <td>69.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001</td>\n      <td>78.5</td>\n      <td>67.0</td>\n      <td>75.43</td>\n      <td>4</td>\n      <td>4</td>\n      <td>female</td>\n      <td>69.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPara imitar el an谩lisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n::: {#237b9b27 .cell execution_count=13}\n``` {.python .cell-code}\n# Filtrar por g茅nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>father</th>\n      <th>son</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>78.5</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>75.5</td>\n      <td>73.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>75.0</td>\n      <td>71.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75.0</td>\n      <td>68.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nEn los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.\n\nSupongamos que se nos pidiera que resumi茅ramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones est谩n bien aproximadas por la distribuci贸n normal, podr铆amos usar los dos promedios y dos desviaciones est谩ndar como res煤menes:\n\n::: {#9296ed49 .cell execution_count=14}\n``` {.python .cell-code}\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nresumen_estadistico.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>promedio_padre</th>\n      <th>sd_padre</th>\n      <th>promedio_hijo</th>\n      <th>sd_hijo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>69.098883</td>\n      <td>2.546555</td>\n      <td>69.263687</td>\n      <td>2.567837</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nSin embargo, este resumen no describe una caracter铆stica importante de\nlos datos: **la tendencia de que cuanto m谩s alto es el padre, m谩s alto es el hijo.**\n\n::: {#2e2b094f .cell execution_count=15}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tama帽o de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g茅nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gr谩fico de dispersi贸n con l铆nea de regresi贸n\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relaci贸n entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gr谩fico\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-16-output-1.png){width=824 height=529}\n:::\n:::\n\n\nAprenderemos que el **coeficiente de correlaci贸n** es un resumen\ninformativo de c贸mo dos variables se mueven juntas y luego veremos c贸mo\nesto puede ser usado para predecir una variable usando la otra, en **una\nregresi贸n**.\n\n\n## Taller de aplicaci贸n 2: Caso aplicaci贸n: Cursos de Verano\n\n::: callout-tip\n## **Taller de aplicaci贸n 2: Pregunta 1**\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que quer铆amos responder:\n\n> Asistir a cursos de verano mejora los resultados acad茅micos?\n\n1.  Plantee un modelo de regresi贸n que con los datos disponibles quisieramos estimar.\n2.  Grafique la dispersi贸n y la recta de regresi贸n estimada.\n3.  Estime el modelo simple e interprete\n\n:::\n\n## 驴Regresi贸n?... pero 驴Y la correlaci贸n?\n\n\n- Ambos est谩n muy relacionados.\n- Aprenderemos que el coeficiente de correlaci贸n es un resumen informativo de c贸mo dos variables se mueven juntas\n- y luego veremos c贸mo esto puede ser usado para predecir una variable usando la otra y modelado en una regresi贸n\n\n::: {#06ba155b .cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-17-output-1.png){width=812 height=529}\n:::\n:::\n\n\n## El coeficiente de correlaci贸n\n\nEl coeficiente de correlaci贸n se define para una lista de pares $(x_1,y_1),...(x_n,y_n)$  como la media de los productos de los valores normalizados:\n\n$$\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n$$\n\nD贸nde $\\mu$ son promedios y $\\sigma$ son desviaciones est谩ndar. La letra griega para r, $\\rho$ se utiliza com煤nmente en los libros de estad铆stica para denotar la correlaci贸n, porque es la primera letra de regresi贸n. Pronto aprenderemos sobre la conexi贸n entre correlaci贸n y regresi贸n. \n\nPodemos representar la f贸rmula anterior con el c贸digo usando:\n\n`rho <- np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))`\n\nPodemos representar la f贸rmula anterior con el siguiente c贸digo usando:\n\n::: {#a6755e41 .cell execution_count=17}\n``` {.python .cell-code}\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aqu铆\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aqu铆\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9999999999999998\n```\n:::\n:::\n\n\nLa correlaci贸n entre las alturas del padre y del hijo es de aproximadamente $0,4$:\n\n::: {#a937cd60 .cell execution_count=18}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g茅nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviaci贸n est谩ndar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviaci贸n Est谩ndar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviaci贸n Est谩ndar de la Altura del Padre:\", sd_father)\n\n# Crear el gr谩fico de dispersi贸n\nplt.scatter(galton_heights['father'], StandardScaler().fit_transform(galton_heights[['father']]))\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Padre Estandarizada\")\nplt.title(\"Relaci贸n entre Altura del Padre y Altura del Padre Estandarizada\")\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviaci贸n Est谩ndar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviaci贸n Est谩ndar de la Altura del Padre: 2.546555038637643\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-19-output-2.png){width=589 height=455}\n:::\n:::\n\n\nLa correlaci贸n entre las alturas del padre y del hijo es de aproximadamente $0,4$.\n\n::: {#81114510 .cell execution_count=19}\n``` {.python .cell-code}\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlaci贸n:\", correlation_coefficient)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoeficiente de Correlaci贸n: 0.42699639842017706\n```\n:::\n:::\n\n\n::: {#ca665697 .cell execution_count=20}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Generar datos simulados usando la biblioteca faux\ndat = pd.DataFrame(np.random.multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([1, 1, 1, 1, 1, 1]), size=100),\n                   columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n\nprint(dat)\n\n# Calcular la correlaci贸n entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           A         B         C         D         E         F\n0  -0.487216 -0.103054 -0.406291  0.144734  0.482508 -1.065099\n1  -2.619254 -0.482619  0.373571 -0.031262 -0.059186  1.284221\n2  -0.606478  0.793955 -0.923926 -0.833852  0.038484  0.057210\n3   0.748645  0.858455  0.849438  0.973093 -0.139669  0.295479\n4  -1.394435  0.086160  0.131287  0.053497 -0.113966  1.327921\n..       ...       ...       ...       ...       ...       ...\n95  0.066146 -0.052701  0.776452  0.885253  0.336985  1.836124\n96 -0.189615  0.674505  0.660422  0.862998 -1.177144  0.924969\n97 -1.168625  1.464250  0.373704 -0.818466 -1.300497 -0.431909\n98 -0.177136  0.473261  0.702792  3.293837  0.889548  0.447885\n99  0.637694 -0.315696  0.463798 -0.975720 -1.828953  2.008513\n\n[100 rows x 6 columns]\n0.4922242607689742\n```\n:::\n:::\n\n\nPara ver c贸mo se ven los datos para los diferentes valores de $\\rho$ aqu铆 hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\n![image](img/img_sesion3/g1.png)\n\n## La correlaci贸n de la muestra es una variable aleatoria\n\nAntes de continuar conectando la correlaci贸n con la regresi贸n, recordemos la variabilidad aleatoria.\n\nEn la mayor铆a de las aplicaciones de la ciencia de datos, observamos datos que incluyen **variaci贸n aleatoria**. \n\nPor ejemplo, en muchos casos, no se observan datos para toda la poblaci贸n de inter茅s, sino para una muestra aleatoria. Al igual que con el promedio y la desviaci贸n est谩ndar, la **correlaci贸n de la muestra** es la estimaci贸n m谩s com煤nmente utilizada de la **correlaci贸n de la poblaci贸n**. Esto implica que la correlaci贸n que calculamos y usamos como resumen es una variable aleatoria.\n\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra poblaci贸n. Un genetista menos afortunado s贸lo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlaci贸n de la muestra se puede calcular con:\n\n::: {#1fd11f26 .cell execution_count=21}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tama帽o 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlaci贸n entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlaci贸n en la Muestra:\", correlation_coefficient)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoeficiente de Correlaci贸n en la Muestra: 0.4105312650311907\n```\n:::\n:::\n\n\nR es una variable aleatoria. Podemos ejecutar una simulaci贸n de Monte Carlo para ver su distribuci贸n:\n\n* Nota: el objetivo principal de la simulaci贸n de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir c贸mo van a evolucionar.\n\n::: {#b66cac31 .cell execution_count=22}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlaci贸n\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlaci贸n\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlaci贸n\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-23-output-1.png){width=597 height=455}\n:::\n:::\n\n\nVemos que el valor esperado de R es la correlaci贸n de la poblaci贸n:\n\n::: {#d9ce7b2b .cell execution_count=23}\n``` {.python .cell-code}\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlaci贸n:\", mean_R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedia de Coeficientes de Correlaci贸n: 0.42583844932004655\n```\n:::\n:::\n\n\ny que tiene un error est谩ndar relativamente alto en relaci贸n con el rango de valores que puede tomar R:\n\n::: {#58d1adf2 .cell execution_count=24}\n``` {.python .cell-code}\nsd_R = np.std(R)\nprint(\"Desviaci贸n Est谩ndar de Coeficientes de Correlaci贸n:\", sd_R)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDesviaci贸n Est谩ndar de Coeficientes de Correlaci贸n: 0.053483610252486324\n```\n:::\n:::\n\n\nPor lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\n\nAdem谩s, tenga en cuenta que debido a que la correlaci贸n de la muestra es un promedio de extracciones independientes, el teorema del l铆mite central realmente funciona. Por lo tanto, para $N$ lo suficientemente grande la distribuci贸n de $R$ es aproximadamente normal con el valor esperado $\\rho$. La desviaci贸n est谩ndar, que es algo compleja de derivar, es: $\\sqrt{\\frac{1-r^2}{N-2}}$.\n\nEn nuestro ejemplo, $N=25$ no parece ser lo suficientemente grande para que la aproximaci贸n sea buena:\n\n* Nota: El gr谩fico Q-Q, o gr谩fico cuantitativo, es una herramienta gr谩fica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribuci贸n te贸rica como una Normal o exponencial. Por ejemplo, si realizamos un an谩lisis estad铆stico que asume que nuestra variable dependiente est谩 Normalmente distribuida, podemos usar un gr谩fico Q-Q-Normal para verificar esa suposici贸n. https://data.library.virginia.edu/understanding-q-q-plots/\n\n::: {#2421a93d .cell execution_count=25}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlaci贸n\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tama帽o de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gr谩fico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Te贸ricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gr谩fico QQ-plot para los Coeficientes de Correlaci贸n\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # L铆nea de referencia\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-26-output-1.png){width=527 height=529}\n:::\n:::\n\n\nSi N aumenta ver谩s que la distribuci贸n converge a una normal.\n\n\n## La correlaci贸n no siempre es un resumen 煤til\n\nLa correlaci贸n no siempre es un buen resumen de la relaci贸n entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlaci贸n de 0,82:\n\n![image](img/img_sesion3/g2.png)\n\nLa correlaci贸n s贸lo tiene sentido en un contexto particular. Para ayudarnos a entender cu谩ndo es que la correlaci贸n es significativa como estad铆stica de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudar谩 a motivar y definir la regresi贸n lineal. Comenzamos demostrando c贸mo la correlaci贸n puede ser 煤til para la predicci贸n.\n\n# Correlaci贸n no es causalidad\n\nLa asociaci贸n no es causalidad es quiz谩s la lecci贸n m谩s importante que se aprende en una clase de estad铆stica. Hay muchas razones por las que una variable $X$ puede correlacionarse con una variable $Y$ sin tener ning煤n efecto directo sobre $Y$. Aqu铆 examinamos tres maneras comunes que pueden llevar a una mala interpretaci贸n de los datos.\n\n## Correlaci贸n espuria\nEl siguiente ejemplo c贸mico subraya que la correlaci贸n no es causalidad. Muestra una fuerte correlaci贸n entre las tasas de divorcio y el consumo de margarina.\n\n\n![image](img/img_sesion3/notcausa.png)\n\n\n(Ac谩 pueden encontrar m谩s http://tylervigen.com/old-version.html)\n\n驴Significa esto que la margarina causa divorcios? 驴O los divorcios hacen que la gente coma m谩s margarina? Por supuesto que la respuesta a estas dos preguntas es no. Esto es s贸lo un ejemplo de lo que llamamos una correlaci贸n espuria.\n\n\nLos casos presentados en el sitio de correlaci贸n espuria son todos casos de lo que generalmente se denomina dragado de datos (data dredging), pesca de datos (data fishing) o espionaje de datos (data snooping). Es b谩sicamente una forma de lo que en los EE.UU. se llama \"cherry picking\". Un ejemplo de dragado de datos ser铆a si miras a trav茅s de muchos resultados producidos por un proceso aleatorio y escoges el que muestra una relaci贸n que apoya una teor铆a que se quiere defender.\n\n# La paradoja de Simpson\n\nSe llama paradoja porque vemos el signo de la correlaci贸n cambiar cuando comparamos toda la data y estratos espec铆ficos. Como ejemplo ilustrativo, supongamos que tiene tres variables aleatorias $X$, $Y$ y $Z$ y que observamos realizaciones de estas. Aqu铆 est谩 el gr谩fico de observaciones simuladas para $X$ y $Y$ a lo largo de la correlaci贸n de la muestra:\n\n<img src=\"./img/img_sesion3/simp1.png\" width=\"600\">\n\n\nPuedes ver que $X$ e $Y$ est谩n negativamente correlacionados. Sin embargo, una vez que estratificamos por $Z$ (mostrado en diferentes colores abajo) emerge otro patr贸n:\n\n<img src=\"./img/img_sesion3/simp2.png\" width=\"600\">\n\n\nEs realmente $Z$ que est谩 negativamente correlacionado con $X$. Si estratificamos por $Z$ las variables $X$ e $Y$ est谩n en realidad correlacionados positivamente como se ha visto en el gr谩fico anterior.\n\n# Expectativas condicionales\n\nSupongamos que nos piden que adivinemos la altura de un hijo seleccionado al azar y no sabemos la altura de su padre. Debido a que la distribuci贸n de las alturas de los hijos es aproximadamente normal, sabemos que la altura media, $69.2$, es el valor con la mayor proporci贸n y ser铆a la predicci贸n con mayores posibilidades de minimizar el error. Pero, 驴y si nos dicen que el padre es m谩s alto que el promedio, digamos que mide 72 pulgadas de alto, todav铆a esperar铆amos que la altura m谩s probable del hijo sea 69.2 pulgadas?\n\nResulta que si pudi茅ramos recolectar datos de un gran n煤mero de padres que miden 72 pulgadas, la distribuci贸n de las alturas de sus hijos ser铆a normalmente distribuida. Esto implica que el promedio de la distribuci贸n calculada en este subconjunto ser铆a nuestra mejor predicci贸n.\n\nEn general, llamamos a este enfoque condicional. La idea general es que estratificamos una poblaci贸n en grupos y calculamos res煤menes en cada grupo. Por lo tanto, el condicionamiento est谩 relacionado con el concepto de estratificaci贸n descrito. \n\nPara proporcionar una descripci贸n matem谩tica del condicionamiento, considere que tenemos una poblaci贸n de pares de valores  $(x_1,y_1),...,(x_n,y_n)$, por ejemplo, todas las alturas de padre e hijo en Inglaterra. Sabemos que si se toma un par al azar $(X,Y)$ el valor esperado y el mejor predictor de $Y$ es $E(Y)=\\mu_y$, el promedio de la poblaci贸n: $1/n \\sum_{i=y}^{n}y_i$. Sin embargo, ya no estamos interesados en la poblaci贸n en general, sino s贸lo en el subconjunto de la poblaci贸n con un valor espec铆fico, $72$ pulgadas. Este subconjunto de la poblaci贸n, es tambi茅n una poblaci贸n y por lo tanto se aplican los mismos principios y propiedades que hemos aprendido. El $y_i$ en la subpoblaci贸n tienen una distribuci贸n, denominada distribuci贸n condicional, y esta distribuci贸n tiene un valor esperado, denominado expectativa condicional. En nuestro ejemplo, la expectativa condicional es la estatura promedio de todos los hijos en Inglaterra con padres de 72 pulgadas. La notaci贸n estad铆stica es para la expectativa condicional es:\n\n\\begin{equation}\nE(Y|X=x)\n\\end{equation}\n\ncon $x$ representando el valor fijo que define ese subconjunto, por ejemplo 72 pulgadas. Del mismo modo, se indica la desviaci贸n est谩ndar de los estratos con:\n\n\\begin{equation}\nSD(Y|X=x)=\\sqrt{Var(Y|x=x)}\n\\end{equation}\n\n\nPorque la expectativa condicional $E(Y|X=x)$ es el mejor predictor para la variable aleatoria $Y$ para un individuo en los estratos definidos por  $X=x$ muchos de los desaf铆os de la ciencia de datos se reducen a la estimaci贸n de esta cantidad. La desviaci贸n est谩ndar condicional cuantifica la precisi贸n de la predicci贸n.\n\nEn el ejemplo que hemos estado considerando, estamos interesados en calcular la altura promedio del hijo condicionada a que el padre tenga 72 pulgadas de altura. Queremos estimar $E(Y|X=72)$ usando la muestra recolectada por Galton. \n\nAnteriormente aprendimos que el promedio de la muestra es el enfoque preferido para estimar el promedio de la poblaci贸n. Sin embargo, un desaf铆o al usar este enfoque para estimar las expectativas condicionales es que para los datos continuos no tenemos muchos puntos de datos que coincidan exactamente con un valor de nuestra muestra. Por ejemplo, s贸lo tenemos:\n\n::: {#3990ecaa .cell execution_count=26}\n``` {.python .cell-code}\ncount_72 = (galton_heights['father'] == 72).sum()\nprint(\"Cantidad de registros con valor 72 en la columna 'father':\", count_72)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de registros con valor 72 en la columna 'father': 8\n```\n:::\n:::\n\n\npadres que miden exactamente 72 pulgadas. Si cambiamos el n煤mero a 72.5, obtenemos a煤n menos puntos de datos:\n\n::: {#a6fa811d .cell execution_count=27}\n``` {.python .cell-code}\ncount_725 = (galton_heights['father'] == 72.5).sum()\nprint(\"Cantidad de registros con valor 72.5 en la columna 'father':\", count_725)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCantidad de registros con valor 72.5 en la columna 'father': 1\n```\n:::\n:::\n\n\nUna forma pr谩ctica de mejorar estas estimaciones de las expectativas condicionales, es definir estratos con valores similares de $x$. En nuestro ejemplo, podemos redondear las alturas paternas a la pulgada m谩s cercana y asumir que todas son de 72 pulgadas. Si hacemos esto, terminamos con la siguiente predicci贸n para el hijo de un padre que mide 72 pulgadas de alto:\n\n::: {#c09d6939 .cell execution_count=28}\n``` {.python .cell-code}\nconditional_avg = galton_heights[galton_heights['father'].round() == 72]['son'].mean()\nprint(\"Promedio condicional para father == 72:\", conditional_avg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPromedio condicional para father == 72: 70.90714285714286\n```\n:::\n:::\n\n\nEn este c贸digo, filtramos el DataFrame \"galton_heights\" para obtener las filas donde el valor redondeado de \"father\" es igual a 72. Luego, calculamos el promedio de la columna \"son\" en las filas filtradas y almacenamos el resultado en la variable \"conditional_avg\". Finalmente, imprimimos el promedio condicional calculado.\n\n\nNote que un padre de 72 pulgadas es m谩s alto que el promedio -- espec铆ficamente, 72 - 69.1/2.5 = 1.1 desviaciones est谩ndar m谩s alto que el padre promedio. Nuestra predicci贸n, $70.5$, es tambi茅n m谩s alta que el promedio, pero s贸lo $0.49$ desviaciones est谩ndar m谩s grandes que el hijo promedio. Los hijos de padres de 72 pulgadas han regresado algunos a la estatura promedio. Observamos que la reducci贸n en el n煤mero de SD m谩s altas es de alrededor de $0.5$, lo que resulta ser la correlaci贸n. Como veremos en una secci贸n posterior, esto no es una coincidencia.\n\nSi queremos hacer una predicci贸n de cualquier altura, no s贸lo de 72, podr铆amos aplicar el mismo enfoque a cada estrato. La estratificaci贸n seguida de los boxplots nos permite ver la distribuci贸n de cada grupo:\n\n::: {#4666bd64 .cell execution_count=29}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Crear una nueva columna 'father_strata' con los valores redondeados de 'father'\ngalton_heights['father_strata'] = galton_heights['father'].round().astype(int)\n\n# Crear el gr谩fico de boxplots\nplt.figure(figsize=(10, 6))  # Tama帽o del gr谩fico\nsns.boxplot(data=galton_heights, x='father_strata', y='son')\n\n# Agregar puntos para mostrar las medias condicionadas\nsns.swarmplot(data=galton_heights, x='father_strata', y='son', color='black', size=4)\n\nplt.xlabel('father_strata')\nplt.ylabel('son')\nplt.title('Boxplots de son condicionado por father_strata con Medias Condicionadas')\nplt.xticks(rotation=45)  # Rotar etiquetas del eje x si es necesario\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-30-output-1.png){width=824 height=536}\n:::\n:::\n\n\nNo es de extra帽ar que los centros de los grupos aumenten con la altura.\n\n::: {#ef7007f3 .cell execution_count=30}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n# Crear un gr谩fico de puntos para mostrar el promedio condicional por \"father\"\nplt.figure(figsize=(10, 6))\nplt.scatter(conditional_avg_by_father['father'], conditional_avg_by_father['son'], color='blue')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres\")\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-31-output-1.png){width=812 height=529}\n:::\n:::\n\n\nAdem谩s, estos centros parecen seguir una relaci贸n lineal. A continuaci贸n se presentan los promedios de cada grupo. Si tenemos en cuenta que estos promedios son variables aleatorias con errores est谩ndar, los datos son consistentes con estos puntos siguiendo una l铆nea recta:\n\n::: {#49f995a5 .cell execution_count=31}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n\nconditional_avg_by_father.head()\n\n\n# Crear un gr谩fico de puntos con ajuste de regresi贸n lineal\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='father', y='son', data=conditional_avg_by_father, color='blue')\nsns.regplot(x='father', y='son', data=conditional_avg_by_father, scatter=False, color='orange')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres con Regresi贸n Lineal\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-32-output-1.png){width=812 height=529}\n:::\n:::\n\n\nEl hecho de que estos promedios condicionales sigan una l铆nea no es una coincidencia. En la siguiente secci贸n, explicamos que la l铆nea que siguen estos promedios es lo que llamamos la l铆nea de regresi贸n, que mejora la precisi贸n de nuestras estimaciones. Sin embargo, no siempre es apropiado estimar las expectativas condicionales con la l铆nea de regresi贸n, por lo que tambi茅n describimos la justificaci贸n te贸rica de Galton para usar la l铆nea de regresi贸n.\n\n\n# La l铆nea de regresi贸n\n\nSi estamos prediciendo una variable aleatoria $Y$ conociendo el valor de otra variable $X=x$ usando una l铆nea de regresi贸n, entonces predecimos que **para cada desviaci贸n est谩ndar, $\\sigma_x$ que $x$ aumenta por encima de la media $\\mu_x$, $Y$ incrementa $\\rho$ veces la desviaci贸n est谩ndar $\\sigma_Y$ sobre el promedio $\\mu_Y$**, con $\\rho$ la correlaci贸n entre $X$ e $Y$. Por lo tanto, la formula de la regresi贸n es:\n\n$$\n\\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right)=\\rho \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\n$$\n\nLo que podemos reescribir como:\n\n$$\nY=\\mu_Y + \\rho \\big(\\frac{x-\\mu_X}{\\sigma_X}\\big) \\sigma_Y\n$$\n\nSi existe una correlaci贸n perfecta, la l铆nea de regresi贸n predice un aumento que corresponde al mismo n煤mero de desviacones est谩ndar. Si hay correlaci贸n 0, entonces no usamos $x$ en absoluto en la predicci贸n y simplemente predecimos el promedio $\\mu_Y$. Para valores entre 0 y 1, la predicci贸n se encuentra en un punto intermedio. Si la correlaci贸n es negativa, predecimos una reducci贸n en lugar de un aumento.\n\nN贸tese que si la correlaci贸n es positiva e inferior a 1, nuestra predicci贸n est谩 m谩s cerca (en unidades est谩ndar) de la altura media que de lo que el valor utilizado para predecir, $x$, est谩 del promedio de los $x$. Por eso lo llamamos regresi贸n: el hijo regresa a la estatura media. De hecho, el t铆tulo del art铆culo de Galton era: Regresi贸n a la mediocridad en la estatura hereditaria (Regression toward mediocrity in hereditary stature.). \n\nPara a帽adir l铆neas de regresi贸n a los gr谩ficos, necesitaremos la f贸rmula anterior en la forma: $y=b+mx$, con pendiente $m=\\rho \\sigma_y / \\sigma_x$ e intercepto $b=\\mu_y - m \\mu_x$\n\nAqu铆 agregamos la l铆nea de regresi贸n a la data original.\n\n::: {#581cf1fa .cell execution_count=32}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# C谩lculo de las medias y desviaciones est谩ndar\nmu_x = galton_heights['father'].mean()\nmu_y = galton_heights['son'].mean()\ns_x = galton_heights['father'].std()\ns_y = galton_heights['son'].std()\n\n# C谩lculo del coeficiente de correlaci贸n\nr = galton_heights['father'].corr(galton_heights['son'])\n\n# C谩lculo de la pendiente y el intercepto para la l铆nea de regresi贸n\nm = r * s_y / s_x\nb = mu_y - m * mu_x\n\n# Configuraci贸n del tama帽o de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gr谩fico de dispersi贸n con l铆nea de regresi贸n\nsns.scatterplot(x='father', y='son', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father', y='son', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Son Height\")\nplt.title(\"Relaci贸n entre Altura de Padres e Hijos con L铆nea de Regresi贸n\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-33-output-1.png){width=824 height=529}\n:::\n:::\n\n\nLa f贸rmula de regresi贸n implica que si primero estandarizamos las variables, es decir, restamos el promedio y dividimos por la desviaci贸n est谩ndar, entonces la l铆nea de regresi贸n tiene intercepto 0 y pendiente igual a la correlaci贸n $\\rho$. Aqu铆 est谩 la misma gr谩fica, pero usando unidades est谩ndar:\n\n::: {#95718684 .cell execution_count=33}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Estandarizar las variables 'father' y 'son'\ngalton_heights['father_standardized'] = (galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std()\ngalton_heights['son_standardized'] = (galton_heights['son'] - galton_heights['son'].mean()) / galton_heights['son'].std()\n\n# Calcular la correlaci贸n de las variables estandarizadas\nr = galton_heights['father_standardized'].corr(galton_heights['son_standardized'])\n\n# Configuraci贸n del tama帽o de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gr谩fico de dispersi贸n con l铆nea de regresi贸n\nsns.scatterplot(x='father_standardized', y='son_standardized', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father_standardized', y='son_standardized', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height (Standardized)\")\nplt.ylabel(\"Son Height (Standardized)\")\nplt.title(\"Relaci贸n Estandarizada entre Altura de Padres e Hijos con L铆nea de Regresi贸n (Intercepto = 0, Pendiente = Correlaci贸n)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-34-output-1.png){width=874 height=529}\n:::\n:::\n\n\n# La regresi贸n mejora la precisi贸n\n\nComparemos los dos enfoques de predicci贸n que hemos presentado:\n\n1) Redondee las alturas del padre a la pulgada m谩s cercana, estratifique y luego tome el promedio.\n2) Calcula la l铆nea de regresi贸n y 煤sala para predecir.\n    \nUsamos un muestreo de simulaci贸n de Monte Carlo $N=50$ familias:\n\n::: {#9be17081 .cell execution_count=34}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n# Definimos B (n煤mero de simulaciones) y N (tama帽o de la muestra).\n\nB= 1000\nN=50\n\n# Configuraci贸n de la semilla aleatoria para reproducibilidad\nnp.random.seed(10)\n\n# Inicializar listas para almacenar los resultados de las simulaciones\nconditional_avg = []\nregression_prediction = []\n\n# Realizar simulaciones de Monte Carlo\nfor _ in range(B):\n    # Seleccionar una muestra aleatoria de tama帽o N\n    dat = galton_heights.sample(n=N)\n    \n    # Calcular la media condicional (Enfoque 1)\n    conditional_avg.append(dat[dat['father'].round() == 72]['son'].mean())\n    \n    # Calcular la predicci贸n de regresi贸n (Enfoque 2)\n    mu_x = dat['father'].mean()\n    mu_y = dat['son'].mean()\n    s_x = dat['father'].std()\n    s_y = dat['son'].std()\n    r = dat['father'].corr(dat['son'])\n    regression_prediction.append(mu_y + r * (72 - mu_x) / (s_x / s_y))\n\n# Calcular las estad铆sticas descriptivas de las simulaciones\nmean_conditional_avg = np.mean(conditional_avg)\nmean_regression_prediction = np.mean(regression_prediction)\nstd_conditional_avg = np.std(conditional_avg, ddof=1)\nstd_regression_prediction = np.std(regression_prediction, ddof=1)\n\n# Imprimir resultados\nprint(\"Valor Esperado (Media) - Enfoque 1 (Media Condicional):\", mean_conditional_avg)\nprint(\"Valor Esperado (Media) - Enfoque 2 (Predicci贸n de Regresi贸n):\", mean_regression_prediction)\nprint(\"Error Est谩ndar - Enfoque 1 (Media Condicional):\", std_conditional_avg)\nprint(\"Error Est谩ndar - Enfoque 2 (Predicci贸n de Regresi贸n):\", std_regression_prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValor Esperado (Media) - Enfoque 1 (Media Condicional): nan\nValor Esperado (Media) - Enfoque 2 (Predicci贸n de Regresi贸n): 70.54175180600582\nError Est谩ndar - Enfoque 1 (Media Condicional): nan\nError Est谩ndar - Enfoque 2 (Predicci贸n de Regresi贸n): 0.4203567940735428\n```\n:::\n:::\n\n\nAunque el valor esperado de estas dos variables aleatorias es casi el mismo, el error est谩ndar para la predicci贸n de regresi贸n es sustancialmente menor.\n\nPor lo tanto, la l铆nea de regresi贸n es mucho m谩s estable que la media condicional. Hay una raz贸n intuitiva para ello. El promedio condicional se calcula en un subconjunto relativamente peque帽o: los padres que miden alrededor de 72 pulgadas de alto. De hecho, en algunas de las permutaciones no tenemos datos. La regresi贸n siempre utiliza todos los datos.\n\nEntonces, 驴por qu茅 no usar siempre la regresi贸n para predecir? Porque no siempre es apropiado. Por ejemplo, Anscombe proporcion贸 casos en los que los datos no tienen una relaci贸n lineal. Entonces, 驴est谩 justificado usar la l铆nea de regresi贸n para predecir? Galton contest贸 esto de forma afirmativa para los datos de altura.\n\n\n\n# Definici贸n matem谩tica\n\n\nEl modelo de regresi贸n lineal (Legendre, Gauss, Galton y Pearson) considera que, dado un conjunto de observaciones $\\{y_i, x_{i1},...,x_{np}\\}^{n}_{i=1}$ , la media  $$  de la variable respuesta  $$  se relaciona de forma lineal con la o las variables regresoras  $_1$ ... $x_p$  acorde a la ecuaci贸n:\n\n$\\mu_y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + ... + \\beta_p x_{p}$\n \nEl resultado de esta ecuaci贸n se conoce como la l铆nea de regresi贸n poblacional, y recoge la relaci贸n entre los predictores y la media de la variable respuesta.\n\nOtra definici贸n que se encuentra con frecuencia en los libros de estad铆stica es:\n\n$y_i= \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} +\\epsilon_i$\n \nEn este caso, se est谩 haciendo referencia al valor de    para una observaci贸n    concreta. El valor de una observaci贸n puntual nunca va a ser exactamente igual al promedio, de ah铆 que se a帽ada el t茅rmino de error  $\\epsilon$.\n\n\nEn ambos casos, la interpretaci贸n de los elementos del modelo es la misma:\n\n- $\\beta_0$: es la ordenada en el origen, se corresponde con el valor promedio de la variable respuesta  $y$  cuando todos los predictores son cero.\n\n- $\\beta_j$: es el efecto promedio que tiene sobre la variable respuesta el incremento en una unidad de la variable predictora  $x_j$, manteni茅ndose constantes el resto de variables. Se conocen como coeficientes de regresi贸n.\n\n- $\\epsilon$: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo. Recoge el efecto de todas aquellas variables que influyen en $y$ pero que no se incluyen en el modelo como predictores.\n\nEn la gran mayor铆a de casos, los valores $\\beta_0$ y $\\beta_j$ poblacionales se desconocen, por lo que, a partir de una muestra, se obtienen sus estimaciones  $\\hat{\\beta_0}$ y $\\hat{\\beta_j}$. **Ajustar el modelo consiste en estimar, a partir de los datos disponibles, los valores de los coeficientes de regresi贸n que maximizan la verosimilitud (likelihood), es decir, los que dan lugar al modelo que con mayor probabilidad puede haber generado los datos observados.**\n\nEl m茅todo empleado con m谩s frecuencia es el ajuste por m铆nimos cuadrados ordinarios (OLS), que identifica como mejor modelo la recta (o plano si es regresi贸n m煤ltiple) que minimiza la suma de las desviaciones verticales entre cada dato de entrenamiento y la recta, elevadas al cuadrado.\n\n# Interpretaci贸n del modelo\n\n\nLos principales elementos que hay que interpretar en un modelo de regresi贸n lineal son los coeficientes de los predictores:\n\n- $\\beta_0$  es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta $y$, cuando todos los predictores son cero.\n\n- $\\beta_j$ los coeficientes de regresi贸n parcial de cada predictor **indican el cambio promedio esperado de la variable respuesta    al incrementar en una unidad de la variable predictora  $x_j$, manteni茅ndose constantes el resto de variables. (\"Ceteris paribus\"))**\n\nLa magnitud de cada coeficiente parcial de regresi贸n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est谩 asociada con la importancia de cada predictor.\n\nPara poder determinar qu茅 impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviaci贸n est谩ndar) las variables predictoras previo ajuste del modelo. En este caso, $\\beta_0$ se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y  $\\beta_j$ el cambio promedio esperado de la variable respuesta al incrementar en una desviaci贸n est谩ndar la variable predictora  $x_j$, manteni茅ndose constantes el resto de variables.\n\nSi bien los coeficientes de regresi贸n suelen ser el primer objetivo de la interpretaci贸n de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condici贸n de normalidad...etc.). Estos 煤ltimos suelen ser tratados con poco detalle cuando el 煤nico objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta. \n\n\n\n# Significado \"lineal\"\n\nEl t茅rmino \"lineal\" en los modelos de regresi贸n hace referencia al hecho de que los par谩metros se incorporan en la ecuaci贸n de forma lineal, no a que necesariamente la relaci贸n entre cada predictor y la variable respuesta tenga que seguir un patr贸n lineal.\n\nLa siguiente ecuaci贸n muestra un modelo lineal en el que el predictor  1  no es lineal respecto a y:\n\n$y = \\beta_0 + \\beta_1x_1 + \\beta_2log(x_1) + \\epsilon$\n\n\n<img src=\"./img/img_sesion3/im1.png\" width=\"400\">\n\n\nEn contraposici贸n, el siguiente no es un modelo lineal:\n\n$y = \\beta_0 + \\beta_1x_1^{\\beta_2} + \\epsilon$\n\n \nEn ocasiones, algunas relaciones no-lineales pueden transformarse de forma que se pueden expresar de manera lineal:\n\n- Modelo no-lineal a estimar: $y = \\beta_0x_1^{\\beta_1}\\epsilon$\n\n- Solucion: pasamos todo a logaritmos:\n\n$log(y)=log(\\beta_0) + \\beta_1log(x_1) + log(\\epsilon)$\n        \n$y^{'}=\\beta_0^{'}+\\beta_1x_1^{'} + \\epsilon^{'}$\n\n- Estimar el modelo y extraer los coeficientes.\n\n- Volvera a la forma funcional incial exponenciando los logaritmos.\n    - $\\beta_1$ es explicito.\n    - $\\beta_0^{'}=log(\\beta_0)=> exp(log(\\beta_0))$\n\n### Distribuci贸n normal bivariable (avanzado)\n\nhttps://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal_multivariante\n\nLa correlaci贸n y la pendiente de regresi贸n son estad铆sticas de resumen ampliamente utilizadas, pero a menudo se utilizan o interpretan err贸neamente. Los ejemplos de Anscombe proporcionan casos excesivamente simplificados de conjuntos de datos en los que resumir con correlaci贸n ser铆a un error. Pero hay muchos m谩s ejemplos de la vida real.\n\nLa principal forma en que motivamos el uso de la correlaci贸n involucra lo que se llama la distribuci贸n normal bivariada.\n\nCuando un par de variables aleatorias son aproximadas por la distribuci贸n normal bivariada, las gr谩ficas de dispersi贸n se ven como 贸valos. Pueden ser delgadas (alta correlaci贸n) o en forma de c铆rculo (sin correlaci贸n).\n\n![image](img/img_sesion3/g1.png)\n\n\nUna forma m谩s t茅cnica de definir la distribuci贸n normal bivariada es la siguiente: si $X$ es una variable aleatoria normalmente distribuida, $Y$ es tambi茅n una variable aleatoria normalmente distribuida, y la distribuci贸n condicional de $Y$ para cualquier $X=x$ es aproximadamente normal, entonces el par es aproximadamente normal bivariado.\n\n![image](img/img_sesion3/g3.png)\n\n\nSi pensamos que los datos de altura est谩n bien aproximados por la distribuci贸n normal bivariada, entonces deber铆amos ver la aproximaci贸n normal para cada estrato. Aqu铆 estratificamos las alturas del hijo por las alturas paternas estandarizadas y vemos que la suposici贸n parece mantenerse:\n\n::: {#4c941f0c .cell execution_count=35}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Estandarizar las alturas del padre y crear una columna 'z_father'\ngalton_heights['z_father'] = np.round((galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std())\n\n# Filtrar las alturas del padre dentro del rango [-2, 2]\ngalton_heights_filtered = galton_heights[galton_heights['z_father'].isin([-2, -1, 0, 1, 2])]\n\n\ngalton_heights_filtered.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>father</th>\n      <th>son</th>\n      <th>father_strata</th>\n      <th>father_standardized</th>\n      <th>son_standardized</th>\n      <th>z_father</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>75.0</td>\n      <td>71.0</td>\n      <td>75</td>\n      <td>2.304857</td>\n      <td>0.658278</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75.0</td>\n      <td>70.5</td>\n      <td>75</td>\n      <td>2.304857</td>\n      <td>0.472130</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>75.0</td>\n      <td>72.0</td>\n      <td>75</td>\n      <td>2.304857</td>\n      <td>1.030575</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74.0</td>\n      <td>73.0</td>\n      <td>74</td>\n      <td>1.914905</td>\n      <td>1.402871</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>74.0</td>\n      <td>74.0</td>\n      <td>74</td>\n      <td>1.914905</td>\n      <td>1.775168</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#f2a0fdf7 .cell execution_count=36}\n``` {.python .cell-code}\n# Configurar el tama帽o de la figura y el estilo de Seaborn\nplt.figure(figsize=(5, 3))\nsns.set(style='whitegrid')\n\n# Crear gr谩ficos QQ-plot utilizando un bucle for para cada estrato\nfor z_father_value, group_data in galton_heights_filtered.groupby('z_father'):\n    plt.subplot(2, 3, int(z_father_value) + 3)  # Convertir z_father_value a entero\n    stats.probplot(group_data['son'], plot=plt, fit=True, rvalue=True)\n    plt.title(f\"Estrato z_father={z_father_value}\")\n\n# Ajustar t铆tulos y etiquetas\nplt.suptitle(\"QQ-Plots de las Alturas del Hijo por Estrato de Alturas del Padre Estandarizadas\")\nplt.tight_layout()\n\n# Mostrar el gr谩fico\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-37-output-1.png){width=702 height=284}\n:::\n:::\n\n\nAhora volvemos a definir la correlaci贸n. Galton utiliz贸 estad铆sticas matem谩ticas para demostrar que, cuando dos variables siguen una distribuci贸n normal bivariada, el c谩lculo de la l铆nea de regresi贸n es equivalente al c谩lculo de las expectativas condicionales. Esto implica que, si nuestros datos son aproximadamente bivariados, la l铆nea de regresi贸n es equivalente a la probabilidad condicional. Por lo tanto, podemos obtener una estimaci贸n mucho m谩s estable del valor de expectaci贸n condicional, encontrando la l铆nea de regresi贸n y us谩ndola para predecir.\n\nEn resumen, si nuestros datos son aproximadamente bivariados, entonces la expectativa condicional, la mejor predicci贸n de $Y$ dado que conocemos el valor de $X$ est谩 dada por la l铆nea de regresi贸n.\n\n$$\nY_i=\\beta_0 + \\beta_1 x_i +U_i\n$$\n\nDe aqu铆 facilmente podemos intuir algunos de los supuestos que deben cumplirse al implementar una regresi贸n (y que estudiaremos en detalle en la siguiente sesion):\n\n\n   1) Normalidad: $u_i \\sim Normal$\n   \n   2) Linealidad: Los residuos se distribuyen sin forma alrededor del cero $E(u_i)=0$\n   \n   3) Homocedasticidad: La variabilidad de los residuos es similar para todos los $x_i$, $V(u_i)=\\sigma^2$\n   \n   4) No existen resudios at铆picos.\n  \n   5) Independecia: Los residuos, ($u_i$), son independientes \n\n## Varianza explicada\nLa teor铆a de la normalidad bivariada tambi茅n nos dice que la desviaci贸n est谩ndar de la distribuci贸n condicional descrita anteriormente es:\n\n$$\nSD(Y|X=x)=\\sigma_Y\\sqrt{1-\\rho^2}\n$$\n\n\nPara ver por qu茅 esto es intuitivo, note que sin condicionamiento, $SD(Y)=\\sigma_Y$ estamos viendo la variabilidad de todos los hijos. Pero una vez que los condicionamos, s贸lo estamos viendo la variabilidad de los hijos con un padre que mide 72 pulgadas de alto. Este grupo tender谩 a ser \"algo m谩s\" alto (que el promedio), por lo que la desviaci贸n est谩ndar se reduce. \n\nEspec铆ficamente, se reduce a $\\sqrt{1-\\rho^2}=\\sqrt{1-0.25}=0.86$ de lo que era originalmente. Podr铆amos decir que la estatura del padre \"explica\" el 14% de la variabilidad de estatura del hijo.\n\n\n\nLa frase \"$X$ explica tal o cual porcentaje de la variabilidad\" se utiliza com煤nmente en papers acad茅micos. En este caso, este porcentaje se refiere realmente a la desviaci贸n (SD al cuadrado). Por lo tanto, si los datos son normales bivariados, la varianza se reduce en $1-\\rho^2$ por lo que decimos que $X$ explica $1-(1-\\rho^2)=\\rho^2$ (la correlaci贸n al cuadrado) de la varianza.\n\n\nPero es importante recordar que la afirmaci贸n de \"varianza explicada\" s贸lo tiene sentido cuando los datos se aproximan mediante una distribuci贸n normal bivariada.\n\n\n## Cuidado: hay dos l铆neas de regresi贸n\n\nCalculamos una l铆nea de regresi贸n para predecir la altura del hijo desde la altura del padre. \n\nUsamos estos c谩lculos:\n\n::: {#05f307f5 .cell execution_count=37}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviaci贸n est谩ndar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviaci贸n est谩ndar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlaci贸n entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.4282667416279721\n2.5644170903922188\n2.6860317955777377\n69.08938547486034\n69.23184357541899\n```\n:::\n:::\n\n\n::: {#d8d2834a .cell execution_count=38}\n``` {.python .cell-code}\n# Calcular la pendiente de la primera l铆nea de regresi贸n\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera l铆nea de regresi贸n\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npendiente 0.44857682836034635\nconstante 38.23994616574076\n```\n:::\n:::\n\n\nLo que nos da la funci贸n $E(Y|X=x)=28,8+0.44x$.\n\n驴Y si queremos predecir la estatura del padre bas谩ndonos en la del hijo? \n\nEs importante saber que esto no se determina calculando la funci贸n inversa!.\n\nNecesitamos computar $E(XY=y)$. Dado que los datos son aproximadamente normales bivariados, la teor铆a descrita anteriormente nos dice que esta expectativa condicional seguir谩 una l铆nea con pendiente e intercepto:\n\n::: {#090d3fc6 .cell execution_count=39}\n``` {.python .cell-code}\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npendiente 0.4088762289729847\nconstante 40.782130348895464\n```\n:::\n:::\n\n\nDe nuevo vemos una regresi贸n a la media: la predicci贸n para el padre est谩 m谩s cerca de la media del padre que lo que estan las alturas del hijo $y$ al promedio del hijo.\n\nAqu铆 hay un gr谩fico que muestra las dos l铆neas de regresi贸n:\n\n::: {#0836d929 .cell execution_count=40}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gr谩fico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-html/cell-41-output-1.png){width=812 height=529}\n:::\n:::\n\n\ncon azul para la predicci贸n de las alturas del hijo con las alturas del padre y rojo para la predicci贸n de las alturas del padre con las alturas del hijo.\n\n::: callout-tip\n\n## Taller aplicacci贸n 2: Altura de padres e hijos\n\n\n1) Cargue los datos de `GaltonFamilies` desde el HistData. Los ni帽os de cada familia est谩n ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado `galton_heights` seleccionando ni帽os y ni帽as al azar. (HINT: use `sample`).\n\n2) Haga una gr谩fica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\n\n3) Calcular la correlaci贸n para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\n\n4) Plotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\n\n5) Obtener el modelo de regresi贸n e interpretar los coeficientes.\n:::\n\n",
    "supporting": [
      "sesion3_notas_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}