{
  "hash": "1c67bacb954ef7de6faf438fa5080222",
  "result": {
    "markdown": "---\ntitle: 'Sesión 3: Introducción al Análisis de Regresión'\ninstitute: Magíster en Data Science - Universidad del Desarrollo\nsubtitle: 'Curso: Análisis de datos'\nauthor: 'Phd (c) Melanie Oyarzún - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'\nformat:\n  html:\n    toc: true\n    html-math-method: mathml\n    embed-resources: true\n  ipynb: default\necho: true\neditor:\n  markdown:\n    wrap: 72\nexecute:\n  keep-ipynb: true\n  freeze: auto\ncode-link: true\n---\n\n# El análisis de regresión\n\nEn las aplicaciones de la ciencia de datos, es muy común estar interesado en la relación entre dos o más variables.\n\nEl análisis de regresión es una técnica en la cual buscamos encontrar una función que pueda describir la relación observada en los datos entre dos o mas variables.\n\nPor ejemplo, una persona podría querer relacionar los pesos de los individuos con sus alturas… \n- ¿Son los más altos más pesados? \n-  …y¿cuánto más pesados?\n\nPensemos en el caso más sencillo: una **regresión lineal simple** o univariada. Tenemos una variable que deseamos explicar o predecir (Y)\ncomo función de otra (X).\n\nPara esto, buscamos la pendiente e intercepto de una funciónla recta de la forma:\n\n$Y = \\alpha + \\beta X$\n\nque se ajuste mejor al conjunto de datos con los que se cuenta.\n\ndonde $X$ es la variable explicativa e $Y$ es la variable dependiente.  La pendiente de la recta es $b$, y $a$ es la intersección (el valor de $y$ cuando $x = 0$).\n\n![](attachment:img/img_sesion3/gif_regresion2.gif)\n\nPara esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistemática\no que se puede explicar directamente con una o más variables independientes (Xs o regresores) y otra que es no sistemática o error\n($\\mu$ o $epsilon$) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fenómeno.\n\n![](attachment:img/img_sesion3/gif_regresion1.gif)\n\nLa parte sistemática entonces la describimos con una forma funcional, que depende de otras variables o regresores.\n\nEsta forma funcional puede ser lineal univariada, lineal múltiple o no lineal. El tipo de forma funcional, definirá el tipo de regresión de la que estemos hablando.\n\nVentajas del análisis de regersión: es facil decsribir cuantitaivamente una rlación.\n\nEsquemáticamente, los elementos son:\n\n![](attachment:img/img_sesion3/regresion_esquema.png)\n\n**Importante tener en cuenta**\n\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relación entre las variables de interés. Esto no implica necesariamente que una variable **cause** la otra (por ejemplo, puntajes más altos en la PSU **no\ncausan** calificaciones superiores en la universidad), pero existe alguna asociación significativa entre las dos variables.\n\nUn diagrama de dispersión puede ser una herramienta útil para determinar la fuerza de la relación entre dos variables. Si parece no haber\nasociación entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersión no indica ninguna tendencia creciente o\ndecreciente), entonces ajustar un modelo de regresión lineal a los datos probablemente no proporcionará un modelo útil.\n\nUna valiosa medida numérica de asociación entre dos variables es el coeficiente de correlación, que es un valor entre -1 y 1 que indica la fuerza de la asociación de los datos observados para las dos variables.\n\n## Una perspectiva histórica:\n\nEL origen de la técnica, podemos remontarlo a la genética.\n\nFrancis Galton estudió la variación y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolectó y estudió datos de altura de familias para tratar de entender la herencia. **Mientras hacía esto, desarrolló los conceptos de correlación y regresión.**\n\nPor supuesto, en el momento en que se recogieron estos datos, nuestro conocimiento de la genética era bastante limitado en comparación con lo\nque conocemos hoy en día. Una pregunta muy específica que Galton trató de responder fue:\n\n    ¿qué tan bien podemos predecir la estatura de un niño basado en la estatura de los padres? \n\nLa técnica que desarrolló para responder a esta pregunta, la regresión, también puede aplicarse en muchas otras circunstancias.\n\nNota histórica: Galton hizo importantes contribuciones a la estadística y la genética, pero también fue uno de los primeros defensores de la\neugenesia, un movimiento filosófico científicamente defectuoso favorecido por muchos biólogos de la época de Galton pero con terribles\nconsecuencias históricas.\n\n<img src=\"./img/img_sesion3/galton.png\" width=\"400\">\n\n## Estudio de caso: ¿es hereditaria la altura?\n\nTenemos acceso a los datos de altura de familias recolectado por Galton, a través del paquete `HistData`. Estos datos contienen las alturas de\nvarias docenas de familias: madres, padres, hijas e hijos.\n\n::: {#c892c95c .cell execution_count=1}\n``` {.python .cell-code}\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\nprint(galton_data.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  family  father  mother  midparentHeight  children  childNum  gender  \\\n0    001    78.5    67.0            75.43         4         1    male   \n1    001    78.5    67.0            75.43         4         2  female   \n2    001    78.5    67.0            75.43         4         3  female   \n3    001    78.5    67.0            75.43         4         4  female   \n4    002    75.5    66.5            73.66         4         1    male   \n\n   childHeight  \n0         73.2  \n1         69.2  \n2         69.0  \n3         69.0  \n4         73.5  \n```\n:::\n:::\n\n\nPara imitar el análisis de Galton, crearemos un conjunto de datos con\nlas alturas de los padres y un hijo de cada familia seleccionado al\nazar:\n\n::: {#27dc8b55 .cell execution_count=2}\n``` {.python .cell-code}\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\nprint(galton_heights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     father   son\n0      78.5  73.2\n1      75.5  72.5\n2      75.0  71.0\n3      75.0  68.5\n4      75.0  72.0\n..      ...   ...\n174    64.0  70.5\n175    64.0  64.5\n176    64.0  66.0\n177    62.0  64.0\n178    62.5  66.5\n\n[179 rows x 2 columns]\n```\n:::\n:::\n\n\nEn los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.\n\nSupongamos que se nos pidiera que resumiéramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones están bien aproximadas por la distribución normal, podríamos usar los dos promedios y dos desviaciones estándar como resúmenes:\n\n::: {#45d2bf0b .cell execution_count=3}\n``` {.python .cell-code}\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nprint(resumen_estadistico)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   promedio_padre  sd_padre  promedio_hijo   sd_hijo\n0       69.098883  2.546555      69.429609  2.800856\n```\n:::\n:::\n\n\nSin embargo, este resumen no describe una característica importante de\nlos datos: **la tendencia de que cuanto más alto es el padre, más alto\nes el hijo.**\n\n::: {#ab53f67d .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gráfico de dispersión con línea de regresión\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relación entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gráfico\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](sesion3_notas_files/figure-ipynb/cell-5-output-1.png){}\n:::\n:::\n\n\nAprenderemos que el **coeficiente de correlación** es un resumen\ninformativo de cómo dos variables se mueven juntas y luego veremos cómo\nesto puede ser usado para predecir una variable usando la otra, en **una\nregresión**.\n\n::: callout-note\n## Taller de aplicación 2: Caso aplicación: Cursos de Verano\n\n> **Taller de aplicación 2: Pregunta 1**\n>\n> Considere los datos trabajados en el taller 1, sobre los cursos de\n> verano. Recordemos la pregunta que queríamos responder:\n>\n> Asistir a cursos de verano mejora los resultados académicos?\n\n-   Plantee un modelo de regresión con los datos disponibles y estímelo.\n-   Grafique la dispoersión y la recta de regresión estimada.\n\n:::\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n---\n",
    "supporting": [
      "sesion3_notas_files/figure-ipynb"
    ],
    "filters": []
  }
}