---
institute: "Mag√≠ster en Data Science - Universidad del Desarrollo"
subtitle: "Curso: An√°lisis de datos"
title: "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n"
author: "Phd (c) Melanie Oyarz√∫n - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)"
format:
  revealjs:
    logo: logo_udd.png
    footer: "Curso An√°lisis de Datos - Sesi√≥n 3"
    transition: fade
    background-transition: fade
    fontsize: 1.8em
    theme: [simple, custom.scss]
    chalkboard: 
        theme: whiteboard
        boardmarker-width: 5
        buttons: true
    progress: true
    incremental: true 
    scrollable: true
code-link: true    
editor: 
  markdown: 
    wrap: 100
jupyter: python3
echo: true
output-location: fragment
execute: 
  freeze: auto
toc-depth: 2
df-print: paged
---

<style>
  table {
    font-size: 0.7em; /* Reducir el tama√±o de fuente en las tablas al 70% del tama√±o base */
  }
</style>


## El an√°lisis de regresi√≥n

- En las aplicaciones de la ciencia de datos, es muy com√∫n estar interesado en la relaci√≥n entre dos o m√°s variables.
  
- El an√°lisis de regresi√≥n es una t√©cnica en la cual buscamos encontrar una funci√≥n que pueda describir la relaci√≥n observada en los datos entre dos o mas variables.
- Por ejemplo, podr√≠amos querer relacionar los pesos de los individuos con sus alturas...
  -  ¬øson los m√°s altos, m√°s pesados?
  -  y... ¬øcu√°nto m√°s pesados?

## El an√°lisis de regresi√≥n: Regresi√≥n Simple

- Caso m√°s sencillo: univariada o **regresi√≥n lineal simple**. 
  - Una variable que deseamos explicar o predecir (Y) como funci√≥n de otra (X).
  - Buscamos la pendiente e intercepto de una funci√≥nla recta de la forma:

. . .

  $$Y = \alpha + \beta X$$
  
  donde:

  - Y es la variale dependiente o que deseamos entender
  - X es la variable independiente
  - $\beta$ es la pendiente de la recta
  - $\alpha$ es la constante o intersecci√≥n (el valor de y cuando x=0)

## El an√°lisis de regresi√≥n

Buscamos los **coeficientes** de la funci√≥n entre Y y X: **constante** y **pendiente**

```{=html}
<img src="./img/img_sesion3/gif_regresion2.gif"  style="display: block; margin: 0 auto;">
```

## El an√°lisis de regresi√≥n

Para esto, pensamos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: 

- una que es **sistem√°tica** o que se puede explicar directamente con una o m√°s variables independientes (Xs o regresores) 

- y otra que es **no sistem√°tica** o error ($\mu$ o $epsilon$) , que es aquella parte que no se puede explicar  y representa a la aleatoriedad del fen√≥meno.

. . .

```{=html}
<img src="./img/img_sesion3/gif_regresion1.gif" style="display: block; margin: 0 auto;">

```
## El an√°lisis de regresi√≥n

```{=html}
<img src="./img/img_sesion3/gif_regresion1.gif" width="600" style="display: block; margin: 0 auto;">

```
. . . 

La parte sistem√°tica entonces la describimos con una **forma funcional**, que depende de otras variables o regresores. 

. . .

Esta forma funcional puede:

- ser lineal univariada,
-  lineal m√∫ltiple o 
-  no lineal. 

. . .
El tipo de forma funcional, definir√° el tipo de regresi√≥n de la que estemos hablando.

## El an√°lisis de regresi√≥n

Ventajas del an√°lisis de regersi√≥n: es facil decsribir cuantitaivamente una rlaci√≥n.

Esquem√°ticamente, los elementos son:

![](img/img_sesion3/regresion_esquema.png)

## ¬øPara qu√© hacer regresiones?

Podemos pensar en tres uso, al menos, del an√°lisis d eregresi√≥n:

- Describir cuantitativamente una relaci√≥n emp√≠rica
- Probar hip√≥tesis sobre ciertas teor√≠as
- Realizar predicciones 


## Regresi√≥n simple y scatterplot

- Por ejemplo, pensemos en la relaci√≥n entre los a√±os de educaci√≥n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom√≠a.

- Usemos un subconjunto de datos de la encuesta CASEN 2022.

. . . 
```{python}

import pandas as pd
# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a√±os

casen_2022 = pd.read_stata("data/small_casen2022.dta")
# casen_2022 = pd.read_stata("https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)

casen_2022.head()

```

## Regresi√≥n simple y scatterplot

Y lo agruparemos por regi√≥n, para facilitar el ejemplo:

. . .

```{python}
import pandas as pd

# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'

# Agrupar por 'region' y aplicar funciones de agregaci√≥n
casen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()

# Ahora contiene los resultados agregados por regi√≥n

casen_2022_region.head()
```

## Regresi√≥n simple y scatterplot

Realicemos un scatter sencillo:

::: panel-tabset

## matplotlib

```{python}
import matplotlib.pyplot as plt

# Suponiendo que casen_2022 es tu DataFrame
plt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)
plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot entre ytrabajocor y esc (por regi√≥n)')
plt.show()

```
## seaborn

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

# Suponiendo que casen_2022 es tu DataFrame
sns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')
plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot entre ytrabajocor y esc')
plt.show()

```
## seaborn + linea de regresion


```{python}
import seaborn as sns
import matplotlib.pyplot as plt

# Suponiendo que casen_2022 es tu DataFrame
sns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza
plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot con L√≠nea de Regresi√≥n y Intervalo de Confianza')
plt.show()
```

## Con codigos de region

```{python}

import seaborn as sns
import matplotlib.pyplot as plt

# Suponiendo que casen_2022_region es tu DataFrame
sns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza

# Procesar y agregar etiquetas de regi√≥n a los puntos
for i, label in enumerate(casen_2022_region['region']):
    last_word = label.split()[-1]  # Obtener la √∫ltima palabra de la etiqueta
    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')

plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot con L√≠nea de Regresi√≥n y Etiquetas de Regi√≥n (√öltima Palabra)')
plt.show()

```
:::


Podemos ver que se aprecia una relaci√≥n positiva: a mayor escolaridad promedio, mayor salario promedio por regi√≥n.


## Especificaci√≥n

Llamamos especifiaci√≥n al precisar la relaci√≥n entre las variables que deseamos estimar.
. . . 

En nuestro caso, la funci√≥n base que queremos entender es entre salario y educaci√≥n:
. . . 

$$ \text{Salario} = f(Educacion))$$

. . . 

Este es una relaci√≥n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales:

- agregar el error aleatorio
- especificar una forma funcional
- definir una forma de medir las variables en los datos

. . .

En nuestro caso, entonces el modelo especificado ser√≠a:

. . . 
$$ \text{ingreso del trabajo}_i = \alpha + \beta \text{a√±os educaci√≥n}_i + \mu_i$$

## Interpretaci√≥n

Con nuestro modelo especificado:

$$ \text{ingreso del trabajo}_i = \alpha + \beta \text{a√±os educaci√≥n}_i + \mu_i$$

Podemos interpretar $\beta$ y $alpha$:

- $\beta = \frac{\partial ingr}{\partial educ}$: un a√±o adici√≥nal de educaci√≥n, en cuanto incrementa el salario (si nada m√°s cambia) 

- $\alpha$ valor esperado de y, si x=0...
  
## Modelo poblaci√≥nal y estimaci√≥n

Este modelo especificado esta definido en la poblaci√≥n:

$$ \text{ingreso del trabajo}_i = \alpha + \beta \text{a√±os educaci√≥n}_i + \mu_i$$

pero necesitamos calcularlo con la muestra.... por lo cual tenemos estimadores para los coeficientes poblacionales!

$$\hat{ \text{ingreso del trabajo}}_i = \hat{\alpha} + \hat{\beta} \text{a√±os educaci√≥n}_i $$


## Modelo poblaci√≥nal y estimaci√≥n

El m√©todo m√°s comun de estimaci√≥n es el de los **m√≠nimos cuadrados ordinarios**. Veremos detalles sobre la estimaci√≥n, supuestos, propiedades estad√≠sticas la proxima sesi√≥n.

Por ahora, pensaremos que es el m√©todo que busca la l√≠nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi√≥n).

$$ \hat{\mu}_i= y_i-\hat{y}_i$$

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm

# Suponiendo que casen_2022_region es tu DataFrame
sns.set(style='whitegrid')  # Configuraci√≥n del estilo del gr√°fico

# Agregar una columna de constante al DataFrame
casen_2022_region['constante'] = 1

# Crear el gr√°fico de dispersi√≥n con la l√≠nea de regresi√≥n
sns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza

# Ajustar el modelo de regresi√≥n lineal
y = casen_2022_region['ytrabajocor']
X = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t√©rmino constante
modelo = sm.OLS(y, X).fit()

# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo
casen_2022_region['ytrabajocor_pred'] = modelo.predict(X)

# Agregar l√≠neas que conecten cada punto a la l√≠nea de regresi√≥n
for i in range(len(casen_2022_region)):
    x_point = casen_2022_region['esc'][i]
    y_point = casen_2022_region['ytrabajocor'][i]
    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo
    
    # L√≠nea que conecta el punto a la l√≠nea de regresi√≥n
    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')

plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Lineas de regresi√≥n y residuos')
plt.show()

```

Es decir, minimiza $$\sum_{i}^{n} \hat{\mu}_i $$

## Modelo estimado

Por ahora, solo estimaremos el modelo directamente usando statsmodels

::: panel-tabset

## Agrupados por regi√≥n

```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),
# y que puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())


```

## Todos los datos


```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),
# y que puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())


```
:::

Podemos ver que un a√±o adicional de educaci√≥n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.

¬øy la constante, como la podemos interpretar?

## Modelos simples y m√∫ltiples

Muchas veces una sola variable no es suficiente para describir bien un fen√≥meno. Necesitamos incluir m√°s variables.

Esto puede ser:

- Una nueva variable
- Una forma funcional no lineal de la variable ya incluida

Nuestra interpretaci√≥n del modelo no cambia, solo que ahora efectivamente estamos **controlando** por otros factores.

## Modelos simples y m√∫ltiples

Probemos, agregar edad al modelo:

$$ \text{ingreso del trabajo}_i = \alpha + \beta_1 \text{a√±os educaci√≥n}_i + \beta_2 \text{edad}_i + \mu_i$$


```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),
# y que puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t√©rmino constante

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())


```

## Modelos simples y m√∫ltiples

Es muy usual, agregar edad al cuadrado.... para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer...

$$ \text{ingreso del trabajo}_i = \alpha + \beta_1 \text{a√±os educaci√≥n}_i + \beta_2 \text{edad}_i  + \beta_3 \text{edad}^2_i + \mu_i$$



```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),
# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Agregar una columna con 'edad' al cuadrado
casen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())

```
## Ya no es lineal el modelo?

Ojo! La linealidad es en los par√°metros, no en las variables. 

La siguiente ecuaci√≥n muestra un modelo lineal en el que el predictor  ùë•1  no es lineal respecto a y:

$y = \beta_0 + \beta_1x_1 + \beta_2log(x_1) + \epsilon$


<img src="./img/img_sesion3/im1.png" width="400">

En contraposici√≥n, el siguiente no es un modelo lineal:

$y = \beta_0 + \beta_1x_1^{\beta_2} + \epsilon$

## Ya no es lineal el modelo?

En ocasiones, algunas relaciones no-lineales pueden transformarse de forma que se pueden expresar de manera lineal:

- Modelo no-lineal a estimar: $y = \beta_0x_1^{\beta_1}\epsilon$

- Solucion: pasamos todo a logaritmos:

$$log(y)=log(\beta_0) + \beta_1log(x_1) + log(\epsilon)$$
        
$$y^{'}=\beta_0^{'}+\beta_1x_1^{'} + \epsilon^{'}$$

- Estimar el modelo y extraer los coeficientes.

- Volvera a la forma funcional incial exponenciando los logaritmos.
    - $\beta_1$ es explicito.
    - $\beta_0^{'}=log(\beta_0)=> exp(log(\beta_0))$

## Un poco m√°s sobre interpretaci√≥n

Elementos clave en la interpretaci√≥n de un modelo de regresi√≥n lineal:

- $\beta_0$: Ordenada en el origen, valor esperado de $y$ cuando todos los predictores son cero.
- $\beta_j$: Coeficientes de regresi√≥n parcial de cada predictor, representan el cambio promedio esperado en $y$ al aumentar en una unidad $x_j$, manteniendo otros predictores constantes ("ceteris paribus").

## Un poco m√°s sobre interpretaci√≥n: Magnitud

Los coeficientes est√°n medidos en las unidades que se est√° trabajando.

![](img/img_sesion3/unidad_medida.png)

**Importancia de coeficientes parciales estandarizados:**
- Se obtienen al estandarizar las variables predictoras antes del ajuste del modelo.
- $\beta_0$ refleja el valor esperado de $y$ cuando los predictores est√°n en su promedio.
- $\beta_j$ indica el cambio promedio esperado en $y$ al aumentar en una desviaci√≥n est√°ndar $x_j$, manteniendo otros predictores constantes.


## Causalidad, regresi√≥n y correlaci√≥n

**Importante tener en cuenta:**

- Antes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero **si existe o no una relaci√≥n entre las variables** de inter√©s.
- Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes m√°s altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociaci√≥n significativa entre las dos variables.
- Un diagrama de dispersi√≥n puede ser una herramienta √∫til para determinar la fuerza de la relaci√≥n entre dos variables.
  
## Causalidad, regresi√≥n y correlaci√≥n

**Importante tener en cuenta:**

- Si parece no haber asociaci√≥n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi√≥n no indica ninguna tendencia creciente o decreciente),

  - entonces ajustar un modelo de regresi√≥n lineal a los datos probablemente no proporcionar√° un modelo √∫til.
  - Una valiosa medida num√©rica de asociaci√≥n entre dos variables es el coeficiente de correlaci√≥n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci√≥n de los datos observados para las dos variables.


## Una perspectiva hist√≥rica:

- El origen de la t√©cnica, podemos remontarlo a la gen√©tica.

- Francis Galton estudi√≥ la variaci√≥n y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolect√≥ y estudi√≥ datos de altura de familias para tratar de entender la herencia. **Mientras hac√≠a esto, desarroll√≥ los conceptos de correlaci√≥n y regresi√≥n.**
- Pregunta: **¬øqu√© tan bien podemos predecir la estatura de un ni√±o basado en la estatura de los padres?**
- La t√©cnica que desarroll√≥ para responder a esta pregunta, la regresi√≥n, tambi√©n puede aplicarse en muchas otras circunstancias.

## Una perspectiva hist√≥rica:

::: columns
:::column
<img src="./img/img_sesion3/galton.png" width="400">
:::

:::column
**Nota hist√≥rica:**

- Galton hizo importantes contribuciones a la estad√≠stica y la gen√©tica‚Ä¶
- pero tambi√©n fue uno de los primeros defensores de la **eugenesia**‚Ä¶
- un movimiento filos√≥fico cient√≠ficamente defectuoso favorecido por muchos bi√≥logos de la √©poca de Galton, pero con terribles consecuencias hist√≥ricas.
:::
:::

## Estudio de caso: ¬øes hereditaria la altura?

- Tenemos acceso a los datos de altura de familias recolectado por Galton, a trav√©s del paquete `HistData`.

- Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.

. . .

:::{.panel-tabset}
## Cargar datos
```{python}
#| output: false
# Cargamos los paquetes que vamos a usar
import statsmodels.api as sm
import pandas as pd
import numpy as np
import seaborn as sns

# Si no tiene stats models, instalar: pip install statsmodels

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Mostrar las primeras filas del DataFrame
print(galton_data.head(4))
```

## Tabla de datos

```{python}
#| echo: false
# Cargamos los paquetes que vamos a usar
import statsmodels.api as sm
import pandas as pd
import numpy as np
import seaborn as sns

# Si no tiene stats models, instalar: pip install statsmodels

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Mostrar las primeras filas del DataFrame
galton_data.head(4)
```
:::

## An√°lisis de caso: ¬øes hereditaria la altura?

Para imitar el an√°lisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:

. . .

:::{.panel-tabset}
## Cargar datos
```{python}
#| output: false

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

galton_heights.head()

```

## Tabla de datos

```{python}
#| echo: false
# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

galton_heights.head()
```
:::


## Estudio de caso: ¬øes hereditaria la altura?

- Supongamos que se nos pidiera que resumi√©ramos (describieramos) los datos de padres e hijos.

- Como ambas distribuciones est√°n aproximadas por la distribuci√≥n normal, podr√≠amos usar los dos promedios y dos desviaciones est√°ndar como res√∫menes:

. . .

```{python}
promedio_padre = galton_heights['father'].mean()
sd_padre = galton_heights['father'].std()
promedio_hijo = galton_heights['son'].mean()
sd_hijo = galton_heights['son'].std()

resumen_estadistico = pd.DataFrame({
    'promedio_padre': [promedio_padre],
    'sd_padre': [sd_padre],
    'promedio_hijo': [promedio_hijo],
    'sd_hijo': [sd_hijo]
})

print(resumen_estadistico)
```

Sin embargo, este resumen no describe una caracter√≠stica importante de los datos: 

**la tendencia de que cuanto m√°s alto es el padre, m√°s alto es el hijo.**

## Estudio de caso: ¬øes hereditaria la altura?

:::{.panel-tabset}
## Code
```{python}
#| output: false

import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar el tama√±o de la figura
plt.figure(figsize=(10, 6))

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.set(style="whitegrid")
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)
sns.regplot(data=galton_heights, x='father', y='son', scatter=False)

plt.xlabel("Altura del Padre")
plt.ylabel("Altura del Hijo")
plt.title("Relaci√≥n entre Altura del Padre y Altura del Hijo")

# Mostrar el gr√°fico
plt.show()
```

## Plot

```{python}
#| echo: false
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar el tama√±o de la figura
plt.figure(figsize=(10, 6))

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.set(style="whitegrid")
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)
sns.regplot(data=galton_heights, x='father', y='son', scatter=False)

plt.xlabel("Altura del Padre")
plt.ylabel("Altura del Hijo")
plt.title("Relaci√≥n entre Altura del Padre y Altura del Hijo")

# Mostrar el gr√°fico
plt.show()
```

:::

## ¬øRegresi√≥n? ¬øY la correlaci√≥n?

::: columns
::: column
- Ambos est√°n muy relacionados.
- Aprenderemos que el coeficiente de correlaci√≥n es un resumen informativo de c√≥mo dos variables se mueven juntas‚Ä¶
- y luego veremos c√≥mo esto puede ser usado para predecir una variable usando la otra y modelado en una regresi√≥n
:::

::: column

```{python}
#| echo: false
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar el tama√±o de la figura
plt.figure(figsize=(10, 6))

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.set(style="whitegrid")
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)
sns.regplot(data=galton_heights, x='father', y='son', scatter=False)

plt.xlabel("Altura del Padre")
plt.ylabel("Altura del Hijo")
plt.title("Relaci√≥n entre Altura del Padre y Altura del Hijo")

# Mostrar el gr√°fico
plt.show()
```

:::

:::

## Taller de aplicaci√≥n 2: 
### Caso aplicaci√≥n: Cursos de Verano

::: callout-tip
## **Taller de aplicaci√≥n 2: Pregunta 1**

- Considere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que quer√≠amos responder:

- **Asistir a cursos de verano mejora los resultados acad√©micos?**

1.  Plantee un modelo de regresi√≥n con los datos disponibles  que deseamos estimar.
2.   Grafique la dispersi√≥n y la recta de regresi√≥n estimada.

:::

## El coeficiente de correlaci√≥n

El coeficiente de correlaci√≥n se define para una lista de pares $(x_1,y_1),...(x_n,y_n)$  como la media de los productos de los valores normalizados:

$$
\rho = \frac{1}{n}\sum_{i=1}^{n} \big(\frac{x_i-\mu_x }{\sigma_x}\big)\big(\frac{y_i-\mu_y}{\sigma_y}\big)
$$

D√≥nde $\mu$ son promedios y $\sigma$ son desviaciones est√°ndar. La letra griega para r, $\rho$ se utiliza com√∫nmente en los libros de estad√≠stica para denotar la correlaci√≥n, porque es la primera letra de regresi√≥n. Pronto aprenderemos sobre la conexi√≥n entre correlaci√≥n y regresi√≥n. 

Podemos representar la f√≥rmula anterior con el c√≥digo R usando:

`rho <- mean(scale(x) * scale(y))`

La correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente $0,4$:

## El coeficiente de correlaci√≥n


Podemos representar la f√≥rmula anterior con el siguiente c√≥digo usando:

```{python}
import numpy as np

x = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aqu√≠
y = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aqu√≠

rho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))

print(rho)

```

## El coeficiente de correlaci√≥n

La correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente $0,4$.

```{python}
correlation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]
print("Coeficiente de Correlaci√≥n:", correlation_coefficient)

```


```{python}

import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

# Calcular la media y la desviaci√≥n est√°ndar de la altura del padre
mean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()
sd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()

mean_father = galton_heights['father'].mean()
sd_father = galton_heights['father'].std()

print("Media de la Altura del Padre (Estandarizada):", mean_scaled_father)
print("Desviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada):", sd_scaled_father)
print("Media de la Altura del Padre:", mean_father)
print("Desviaci√≥n Est√°ndar de la Altura del Padre:", sd_father)
```

## El coeficiente de correlaci√≥n


```{python}
# Calcular la correlaci√≥n entre father y son usando una muestra de galton_heights
R = galton_heights.sample(n=75, replace=True).corr().loc["father", "son"]
print(R)

```

Para ver c√≥mo se ven los datos para los diferentes valores de $\rho$ aqu√≠ hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:

![image](img/img_sesion3/g1.png)

## La correlaci√≥n es variable aleatoria

Antes de continuar conectando la correlaci√≥n con la regresi√≥n, recordemos la variabilidad aleatoria.

En la mayor√≠a de las aplicaciones de la ciencia de datos, observamos datos que incluyen **variaci√≥n aleatoria**.


A modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra poblaci√≥n. Un genetista menos afortunado s√≥lo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlaci√≥n de la muestra se puede calcular con:


```{python}

import pandas as pd

# Seleccionar una muestra aleatoria de tama√±o 75 con reemplazo
R = galton_heights.sample(n=75, replace=True)

# Calcular el coeficiente de correlaci√≥n entre las columnas "father" y "son"
correlation_coefficient = R[['father', 'son']].corr().iloc[0, 1]

print("Coeficiente de Correlaci√≥n en la Muestra:", correlation_coefficient)


```

## 


R es una variable aleatoria. Podemos ejecutar una simulaci√≥n de Monte Carlo para ver su distribuci√≥n:

* Nota: el objetivo principal de la simulaci√≥n de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir c√≥mo van a evolucionar.


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

B = 1000
N = 100
R = np.zeros(B)

for i in range(B):
    sample = galton_heights.sample(n=N, replace=False)
    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]
    R[i] = correlation_coefficient

# Crear un histograma de los coeficientes de correlaci√≥n
plt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')
plt.xlabel("Coeficiente de Correlaci√≥n")
plt.ylabel("Frecuencia")
plt.title("Histograma de Coeficientes de Correlaci√≥n")
plt.show()

```

## 


Vemos que el valor esperado de R es la correlaci√≥n de la poblaci√≥n:


```{python}
mean_R = np.mean(R)
print("Media de Coeficientes de Correlaci√≥n:", mean_R)

```

y que tiene un error est√°ndar relativamente alto en relaci√≥n con el rango de valores que puede tomar R:


```{python}
sd_R = np.std(R)
print("Desviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n:", sd_R)

```

## 

- Por lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.

- Adem√°s, tenga en cuenta que debido a que la correlaci√≥n de la muestra es un promedio de extracciones independientes, el teorema del l√≠mite central realmente funciona. 
- Por lo tanto, para $N$ lo suficientemente grande la distribuci√≥n de $R$ es aproximadamente normal con el valor esperado $\rho$. 
- La desviaci√≥n est√°ndar, que es algo compleja de derivar, es: $\sqrt{\frac{1-r^2}{N-2}}$.

##

- En nuestro ejemplo, $N=25$ no parece ser lo suficientemente grande para que la aproximaci√≥n sea buena


-Si N aumenta ver√°s que la distribuci√≥n converge a una normal.

* Nota: El gr√°fico Q-Q, o gr√°fico cuantitativo, es una herramienta gr√°fica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribuci√≥n te√≥rica como una Normal o exponencial.


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Crear un DataFrame con los coeficientes de correlaci√≥n
df_R = pd.DataFrame({'R': R})

# Calcular la media y el tama√±o de la muestra
mean_R = np.mean(R)
N = len(R)

# Crear el gr√°fico QQ-plot
plt.figure(figsize=(6, 6))
stats.probplot(df_R['R'], dist='norm', plot=plt)
plt.xlabel("Cuantiles Te√≥ricos")
plt.ylabel("Cuantiles de R")
plt.title("Gr√°fico QQ-plot para los Coeficientes de Correlaci√≥n")
plt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # L√≠nea de referencia
plt.show()

```

## La correlaci√≥n no siempre es un resumen √∫til

La correlaci√≥n no siempre es un buen resumen de la relaci√≥n entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlaci√≥n de 0,82:

![image](img/img_sesion3/g2.png)

La correlaci√≥n s√≥lo tiene sentido en un contexto particular. Para ayudarnos a entender cu√°ndo es que la correlaci√≥n es significativa como estad√≠stica de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudar√° a motivar y definir la regresi√≥n lineal. Comenzamos demostrando c√≥mo la correlaci√≥n puede ser √∫til para la predicci√≥n.

# Correlaci√≥n no es causalidad

La asociaci√≥n no es causalidad es quiz√°s la lecci√≥n m√°s importante que se aprende en una clase de estad√≠stica. Hay muchas razones por las que una variable $X$ puede correlacionarse con una variable $Y$ sin tener ning√∫n efecto directo sobre $Y$. Aqu√≠ examinamos tres maneras comunes que pueden llevar a una mala interpretaci√≥n de los datos.

## Correlaci√≥n espuria

Vemos una fuerte correlaci√≥n entre las tasas de divorcio y el consumo de margarina.


![image](img/img_sesion3/notcausa.png)


(Ac√° pueden encontrar m√°s http://tylervigen.com/old-version.html)

- ¬øSignifica esto que la margarina causa divorcios?  
- ¬øO los divorcios hacen que la gente coma m√°s margarina? 

## La paradoja de Simpson

- Se llama paradoja porque vemos el signo de la correlaci√≥n cambiar cuando comparamos toda la data y estratos espec√≠ficos. 

- Como ejemplo ilustrativo, supongamos que tiene tres variables aleatorias $X$, $Y$ y $Z$ y que observamos realizaciones de estas. 
- Aqu√≠ est√° el gr√°fico de observaciones simuladas para $X$ y $Y$ a lo largo de la correlaci√≥n de la muestra:

. . .

<img src="./img/img_sesion3/simp1.png" width="600">

## La paradoja de Simpson


- Puedes ver que $X$ e $Y$ est√°n negativamente correlacionados. 
- Sin embargo, una vez que estratificamos por $Z$ (mostrado en diferentes colores abajo) emerge otro patr√≥n:

. . . 

<img src="./img/img_sesion3/simp2.png" width="600">

## La paradoja de Simpson

<img src="./img/img_sesion3/simp2.png" width="600">

- Es realmente $Z$ que est√° negativamente correlacionado con $X$. 
- Si estratificamos por $Z$ las variables $X$ e $Y$ est√°n en realidad correlacionados positivamente como se ha visto en el gr√°fico anterior.

## Expectativas condicionales

- Supongamos que nos piden que adivinemos la altura de un hijo seleccionado al azar y no sabemos la altura de su padre.

- Debido a que la distribuci√≥n de las alturas de los hijos es aproximadamente normal, sabemos que la altura media, $69.2$, es el valor con la mayor proporci√≥n y ser√≠a la predicci√≥n con mayores posibilidades de minimizar el error.

- Pero, ¬øy si nos dicen que el padre es m√°s alto que el promedio, digamos que mide 72 pulgadas de alto, todav√≠a esperar√≠amos que la altura m√°s probable del hijo sea 69.2 pulgadas?

## expectativas condicionales

- Resulta que si pudi√©ramos recolectar datos de un gran n√∫mero de padres que miden 72 pulgadas...
  - la distribuci√≥n de las alturas de sus hijos ser√≠a normalmente distribuida. 
  - Esto implica que el promedio de la distribuci√≥n calculada en este subconjunto ser√≠a nuestra mejor predicci√≥n.

## expectativas condicionales

- En general, llamamos a este enfoque condicional. 
- La idea general es que estratificamos una poblaci√≥n en grupos y calculamos res√∫menes en cada grupo. 
- Por lo tanto, el condicionamiento est√° relacionado con el concepto de estratificaci√≥n descrito. 

- Porque la expectativa condicional $E(Y|X=x)$ es el mejor predictor para la variable aleatoria $Y$ para un individuo en los estratos definidos por  $X=x$ muchos de los desaf√≠os de la ciencia de datos se reducen a la estimaci√≥n de esta cantidad.

## Expectativas condicionales


- En el ejemplo que hemos estado considerando, estamos interesados en calcular la altura promedio del hijo condicionada a que el padre tenga 72 pulgadas de altura. 

- Queremos estimar $E(Y|X=72)$ usando la muestra recolectada por Galton. 

- ¬øCuantos padres miden 72?

. . . 

```{python}
count_72 = (galton_heights['father'] == 72).sum()
print("Cantidad de registros con valor 72 en la columna 'father':", count_72)

```


- Si cambiamos el n√∫mero a 72.5, obtenemos a√∫n menos puntos de datos:

. . .

```{python}
count_725 = (galton_heights['father'] == 72.5).sum()
print("Cantidad de registros con valor 72.5 en la columna 'father':", count_725)
```

## Expectativas condicionales

- Una forma pr√°ctica de mejorar estas estimaciones de las expectativas condicionales, es definir estratos con valores similares de $x$.
- En nuestro ejemplo, podemos redondear las alturas paternas a la pulgada m√°s cercana y asumir que todas son de 72 pulgadas. 
- Si hacemos esto, terminamos con la siguiente predicci√≥n para el hijo de un padre que mide 72 pulgadas de alto:


```{python}

conditional_avg = galton_heights[galton_heights['father'].round() == 72]['son'].mean()
print("Promedio condicional para father == 72:", conditional_avg)

```

## Expectativas condicionales

- Note que un padre de 72 pulgadas es m√°s alto que el promedio -- espec√≠ficamente, 72 - 69.1/2.5 = 1.1 desviaciones est√°ndar m√°s alto que el padre promedio. 
- Nuestra predicci√≥n, $70.5$, es tambi√©n m√°s alta que el promedio, pero s√≥lo $0.49$ desviaciones est√°ndar m√°s grandes que el hijo promedio. 
- Los hijos de padres de 72 pulgadas han regresado algunos a la estatura promedio.
-  Observamos que la reducci√≥n en el n√∫mero de SD m√°s altas es de alrededor de $0.5$, lo que resulta ser la correlaci√≥n. 
-  Como veremos en una secci√≥n posterior, esto no es una coincidencia.

## Expectativas condicionales

- Si queremos hacer una predicci√≥n de cualquier altura, no s√≥lo de 72, podr√≠amos aplicar el mismo enfoque a cada estrato. 
- La estratificaci√≥n seguida de los boxplots nos permite ver la distribuci√≥n de cada grupo:


```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.

# Crear una nueva columna 'father_strata' con los valores redondeados de 'father'
galton_heights['father_strata'] = galton_heights['father'].round().astype(int)

# Crear el gr√°fico de boxplots
plt.figure(figsize=(10, 6))  # Tama√±o del gr√°fico
sns.boxplot(data=galton_heights, x='father_strata', y='son')

# Agregar puntos para mostrar las medias condicionadas
sns.swarmplot(data=galton_heights, x='father_strata', y='son', color='black', size=4)

plt.xlabel('father_strata')
plt.ylabel('son')
plt.title('Boxplots de son condicionado por father_strata con Medias Condicionadas')
plt.xticks(rotation=45)  # Rotar etiquetas del eje x si es necesario

plt.show()


```

## Expectativas condicionales

No es de extra√±ar que los centros de los grupos aumenten con la altura.

. . .
```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Redondear los valores de la columna "father"
galton_heights['father'] = galton_heights['father'].round()

# Calcular el promedio condicional de "son" para cada valor de "father"
conditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()

# Crear un gr√°fico de puntos para mostrar el promedio condicional por "father"
plt.figure(figsize=(10, 6))
plt.scatter(conditional_avg_by_father['father'], conditional_avg_by_father['son'], color='blue')
plt.xlabel("Father Height")
plt.ylabel("Conditional Son Height Average")
plt.title("Promedio Condicional de Alturas de Hijos por Altura de Padres")
plt.show()


```

- Adem√°s, estos centros parecen seguir una relaci√≥n lineal. 

## Expectativas condicionales

- A continuaci√≥n se presentan los promedios de cada grupo. 
- Si tenemos en cuenta que estos promedios son variables aleatorias con errores est√°ndar, los datos son consistentes con estos puntos siguiendo una l√≠nea recta:


```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Redondear los valores de la columna "father"
galton_heights['father'] = galton_heights['father'].round()

# Calcular el promedio condicional de "son" para cada valor de "father"
conditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()


conditional_avg_by_father.head()


# Crear un gr√°fico de puntos con ajuste de regresi√≥n lineal
plt.figure(figsize=(10, 6))
sns.scatterplot(x='father', y='son', data=conditional_avg_by_father, color='blue')
sns.regplot(x='father', y='son', data=conditional_avg_by_father, scatter=False, color='orange')
plt.xlabel("Father Height")
plt.ylabel("Conditional Son Height Average")
plt.title("Promedio Condicional de Alturas de Hijos por Altura de Padres con Regresi√≥n Lineal")
plt.show()

```

## Expectativas condicionales

- El hecho de que estos promedios condicionales sigan una l√≠nea **no es una coincidencia**. 
- En la siguiente secci√≥n, explicamos que la l√≠nea que siguen estos promedios es lo que llamamos la l√≠nea de regresi√≥n, que mejora la precisi√≥n de nuestras estimaciones. 
- Sin embargo, no siempre es apropiado estimar las expectativas condicionales con la l√≠nea de regresi√≥n, por lo que tambi√©n describimos la justificaci√≥n te√≥rica de Galton para usar la l√≠nea de regresi√≥n.



## La l√≠nea/recta de regresi√≥n

- Si estamos prediciendo una variable aleatoria $Y$ conociendo el valor de otra variable $X=x$ usando una l√≠nea de regresi√≥n, entonces predecimos que **para cada desviaci√≥n est√°ndar, $\sigma_x$ que $x$ aumenta por encima de la media $\mu_x$, $Y$ incrementa $\rho$ veces la desviaci√≥n est√°ndar $\sigma_Y$ sobre el promedio $\mu_Y$**, con $\rho$ la correlaci√≥n entre $X$ e $Y$. Por lo tanto, la formula de la regresi√≥n es:

$$
\left( \frac{Y-\mu_Y}{\sigma_Y} \right)=\rho \left(\frac{x-\mu_X}{\sigma_X}\right)
$$

Lo que podemos reescribir como:

$$
Y=\mu_Y + \rho \big(\frac{x-\mu_X}{\sigma_X}\big) \sigma_Y
$$

## La l√≠nea/recta de regresi√≥n

- Si existe una correlaci√≥n perfecta, la l√≠nea de regresi√≥n predice un aumento que corresponde al mismo n√∫mero de desviacones est√°ndar. 

- Si hay correlaci√≥n 0, entonces no usamos $x$ en absoluto en la predicci√≥n y simplemente predecimos el promedio $\mu_Y$. 
- Para valores entre 0 y 1, la predicci√≥n se encuentra en un punto intermedio. 
- Si la correlaci√≥n es negativa, predecimos una reducci√≥n en lugar de un aumento.

## Regresi√≥n a la media

- N√≥tese que si la correlaci√≥n es positiva e inferior a 1, nuestra predicci√≥n est√° m√°s cerca (en unidades est√°ndar) de la altura media que de lo que el valor utilizado para predecir, $x$, est√° del promedio de los $x$. 
- Por eso lo llamamos regresi√≥n: el hijo regresa a la estatura media.
-  De hecho, el t√≠tulo del art√≠culo de Galton era: Regresi√≥n a la mediocridad en la estatura hereditaria (Regression toward mediocrity in hereditary stature.). 

## La l√≠nea/recta de regresi√≥n

- Para a√±adir l√≠neas de regresi√≥n a los gr√°ficos, necesitaremos la f√≥rmula anterior en la forma: $y=b+mx$, con pendiente $m=\rho \sigma_y / \sigma_x$ e intercepto $b=\mu_y - m \mu_x$

- Aqu√≠ agregamos la l√≠nea de regresi√≥n a la data original.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# C√°lculo de las medias y desviaciones est√°ndar
mu_x = galton_heights['father'].mean()
mu_y = galton_heights['son'].mean()
s_x = galton_heights['father'].std()
s_y = galton_heights['son'].std()

# C√°lculo del coeficiente de correlaci√≥n
r = galton_heights['father'].corr(galton_heights['son'])

# C√°lculo de la pendiente y el intercepto para la l√≠nea de regresi√≥n
m = r * s_y / s_x
b = mu_y - m * mu_x

# Configuraci√≥n del tama√±o de la figura
plt.figure(figsize=(10, 6))

# Crear un gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.scatterplot(x='father', y='son', data=galton_heights, alpha=0.5, size=3)
sns.regplot(x='father', y='son', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})
plt.xlabel("Father Height")
plt.ylabel("Son Height")
plt.title("Relaci√≥n entre Altura de Padres e Hijos con L√≠nea de Regresi√≥n")
plt.show()

```

## La l√≠nea/recta de regresi√≥n

- La f√≥rmula de regresi√≥n implica que si primero estandarizamos las variables, es decir, restamos el promedio y dividimos por la desviaci√≥n est√°ndar, entonces la l√≠nea de regresi√≥n tiene intercepto 0 y pendiente igual a la correlaci√≥n $\rho$. 
- Aqu√≠ est√° la misma gr√°fica, pero usando unidades est√°ndar:

. . .

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.

# Estandarizar las variables 'father' y 'son'
galton_heights['father_standardized'] = (galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std()
galton_heights['son_standardized'] = (galton_heights['son'] - galton_heights['son'].mean()) / galton_heights['son'].std()

# Calcular la correlaci√≥n de las variables estandarizadas
r = galton_heights['father_standardized'].corr(galton_heights['son_standardized'])

# Configuraci√≥n del tama√±o de la figura
plt.figure(figsize=(10, 6))

# Crear un gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.scatterplot(x='father_standardized', y='son_standardized', data=galton_heights, alpha=0.5, size=3)
sns.regplot(x='father_standardized', y='son_standardized', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})
plt.xlabel("Father Height (Standardized)")
plt.ylabel("Son Height (Standardized)")
plt.title("Relaci√≥n Estandarizada entre Altura de Padres e Hijos con L√≠nea de Regresi√≥n (Intercepto = 0, Pendiente = Correlaci√≥n)")
plt.show()

```


# Regresi√≥n: Definici√≥n matem√°tica


El modelo de regresi√≥n lineal (Legendre, Gauss, Galton y Pearson) considera que, dado un conjunto de observaciones $\{y_i, x_{i1},...,x_{np}\}^{n}_{i=1}$ , la media  $ùúá$  de la variable respuesta  $ùë¶$  se relaciona de forma lineal con la o las variables regresoras  $ùë•_1$ ... $x_p$  acorde a la ecuaci√≥n:

$$\mu_y = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + ... + \beta_p x_{p}$$
 
El resultado de esta ecuaci√≥n se conoce como la l√≠nea de regresi√≥n poblacional, y recoge la relaci√≥n entre los predictores y la media de la variable respuesta.

# Regresi√≥n: Definici√≥n matem√°tica


- Otra definici√≥n que se encuentra con frecuencia en los libros de estad√≠stica es:

$$y_i= \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} +\epsilon_i$$
 
- En este caso, se est√° haciendo referencia al valor de  ùë¶  para una observaci√≥n  ùëñ  concreta. El valor de una observaci√≥n puntual nunca va a ser exactamente igual al promedio, de ah√≠ que se a√±ada el t√©rmino de error  $\epsilon$.

## Interpretaci√≥n:


En ambos casos, la interpretaci√≥n de los elementos del modelo es la misma:

- $\beta_0$: es la ordenada en el origen, se corresponde con el valor promedio de la variable respuesta  $y$  cuando todos los predictores son cero.

- $\beta_j$: es el efecto promedio que tiene sobre la variable respuesta el incremento en una unidad de la variable predictora  $x_j$, manteni√©ndose constantes el resto de variables. Se conocen como coeficientes de regresi√≥n.

- $\epsilon$: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo. Recoge el efecto de todas aquellas variables que influyen en $y$ pero que no se incluyen en el modelo como predictores.

## Interpretaci√≥n:

- En la gran mayor√≠a de casos, los valores $\beta_0$ y $\beta_j$ poblacionales se desconocen, por lo que, a partir de una muestra, se obtienen sus estimaciones  $\hat{\beta_0}$ y $\hat{\beta_j}$. 
- **Ajustar el modelo consiste en estimar, a partir de los datos disponibles, los valores de los coeficientes de regresi√≥n que maximizan la verosimilitud (likelihood), es decir, los que dan lugar al modelo que con mayor probabilidad puede haber generado los datos observados.**

- El m√©todo empleado con m√°s frecuencia es el **ajuste por m√≠nimos cuadrados ordinarios (OLS)**
  - que identifica como mejor modelo la recta (o plano si es regresi√≥n m√∫ltiple) 
  - que minimiza la suma de las desviaciones verticales entre cada dato de entrenamiento y la recta, elevadas al cuadrado.

## Magnitud y significancia

- La magnitud de cada coeficiente parcial de regresi√≥n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est√° asociada con la importancia de cada predictor.
- Una buena practica es estandarizar


## Cuidado: hay dos l√≠neas de regresi√≥n

Calculamos una l√≠nea de regresi√≥n para predecir la altura del hijo desde la altura del padre. 

Usamos estos c√°lculos:

```{python}

import numpy as np

# Calcular la media de las alturas del padre
mu_x = galton_heights['father'].mean()

# Calcular la media de las alturas del hijo
mu_y = galton_heights['son'].mean()

# Calcular la desviaci√≥n est√°ndar de las alturas del padre
s_x = galton_heights['father'].std()

# Calcular la desviaci√≥n est√°ndar de las alturas del hijo
s_y = galton_heights['son'].std()

# Calcular el coeficiente de correlaci√≥n entre las alturas del padre y el hijo
r = galton_heights['father'].corr(galton_heights['son'])



print(r)
print(s_x)
print(s_y)
print(mu_x)
print(mu_y)
```


```{python}
# Calcular la pendiente de la primera l√≠nea de regresi√≥n
m_1 = r * s_y / s_x

# Calcular el intercepto de la primera l√≠nea de regresi√≥n
b_1 = mu_y - m_1 * mu_x

print("pendiente", m_1)
print("constante", b_1)
```



## Cuidado: hay dos l√≠neas de regresi√≥n

- ¬øY si queremos predecir la estatura del padre bas√°ndonos en la del hijo? 

- Es importante saber que esto no se determina calculando la funci√≥n inversa!.

- Necesitamos computar $E(X‚à£Y=y)$. Dado que los datos son aproximadamente normales bivariados, la teor√≠a descrita anteriormente nos dice que esta expectativa condicional seguir√° una l√≠nea con pendiente e intercepto:

```{python}
m_2 = r * s_x / s_y
b_2 = mu_x - m_2 * mu_y

print("pendiente", m_2)
print("constante", b_2)
```

## Cuidado: hay dos l√≠neas de regresi√≥n

- Aqu√≠ hay un gr√°fico que muestra las dos l√≠neas de regresi√≥n:

- con azul para la predicci√≥n de las alturas del hijo con las alturas del padre y rojo para la predicci√≥n de las alturas del padre con las alturas del hijo.


::: panel-tabset

## Codigo
```{python}
#| output: false
import matplotlib.pyplot as plt
import seaborn as sns

# Crear el gr√°fico utilizando Matplotlib y Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)
plt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')
plt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')
plt.legend()
plt.xlabel('Father Height')
plt.ylabel('Son Height')
plt.title('Scatter Plot with Regression Lines')
plt.show()

```
## plot

```{python}
#| echo: false
import matplotlib.pyplot as plt
import seaborn as sns

# Crear el gr√°fico utilizando Matplotlib y Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)
plt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')
plt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')
plt.legend()
plt.xlabel('Father Height')
plt.ylabel('Son Height')
plt.title('Scatter Plot with Regression Lines')
plt.show()

```

:::


## Taller aplicaci√≥n 2: ALtura de madres, padres, hijos e hijas
::: callout-tip

## Taller aplicacci√≥n 2: Altura de madres, padres, hijos e hijas


1) Cargue los datos de `GaltonFamilies` desde el HistData. Los ni√±os de cada familia est√°n ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado `galton_heights` seleccionando ni√±os y ni√±as al azar. (HINT: use `sample`).

2) Haga una gr√°fica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.

3) Calcular la correlaci√≥n para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.

4) Plotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).

5) Obtener el modelo de regresi√≥n e interpretar los coeficientes.
:::