---
title: 'Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n'
institute: "Mag√≠ster en Data Science - Universidad del Desarrollo"
subtitle: "Curso: An√°lisis de datos"
author: "Phd (c) Melanie Oyarz√∫n - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)"
format:
#  ipynb: default
  html:
    toc: true
    html-math-method: mathml
    embed-resources: true
#   revealjs:
#     logo: logo_udd.png
#     footer: "Curso An√°lisis de Datos - Sesi√≥n 1 "
#     transition: fade
#     background-transition: fade
#     theme: simple
#     chalkboard: 
#         theme: whiteboard
#         boardmarker-width: 5
#         buttons: true
#     progress: true
#     incremental: true
  ipynb: default
echo: true
editor: 
  markdown: 
    wrap: 72
jupyter: python3
execute:
  keep-ipynb: true
  freeze: auto
code-link: true
---

# Visi√≥n general al an√°lisis de regresi√≥n

En las aplicaciones de la ciencia de datos, es muy com√∫n estar interesado en la relaci√≥n entre dos o m√°s variables.

El an√°lisis de regresi√≥n es una t√©cnica en la cual buscamos encontrar una funci√≥n que pueda describir la relaci√≥n observada en los datos entre dos o mas variables.

Por ejemplo, una persona podr√≠a querer relacionar los pesos de los individuos con sus alturas‚Ä¶

  - ¬øSon los m√°s altos m√°s pesados? 
  -  ‚Ä¶y¬øcu√°nto m√°s pesados?

Pensemos en el caso m√°s sencillo: una **regresi√≥n lineal simple** o univariada. Tenemos una variable que deseamos explicar o predecir (Y)
como funci√≥n de otra (X).

Para esto, buscamos la pendiente e intercepto de una funci√≥nla recta de la forma:

$$Y = \alpha + \beta X$$

que se ajuste mejor al conjunto de datos con los que se cuenta.

donde $X$ es la variable explicativa e $Y$ es la variable dependiente.  La pendiente de la recta es $b$, y $a$ es la intersecci√≥n (el valor de $y$ cuando $x = 0$).

<img src="./img/img_sesion3/gif_regresion2.gif" width="600">


Para esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistem√°tica
o que se puede explicar directamente con una o m√°s variables independientes (Xs o regresores) y otra que es no sistem√°tica o error
($\mu$ o $epsilon$) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fen√≥meno.

![](img/img_sesion3/gif_regresion1.gif)

La parte sistem√°tica entonces la describimos con una forma funcional, que depende de otras variables o regresores.

Esta forma funcional puede ser lineal univariada, lineal m√∫ltiple o no lineal. El tipo de forma funcional, definir√° el tipo de regresi√≥n de la que estemos hablando.

Ventajas del an√°lisis de regersi√≥n: es facil describir cuantitaivamente una relaci√≥n.

Esquem√°ticamente, los elementos son:

![](img/img_sesion3/regresion_esquema.png)


## Usos de las regresiones

Las regresiones tienen tres principales usos:

- Describir un fen√≥meno
- Probar hip√≥tesis sobre ciertas teor√≠as
- Realizar predicciones 

## Regresi√≥n simple y scatterplot

Por ejemplo, pensemos en la relaci√≥n entre los a√±os de educaci√≥n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom√≠a.

Podriamos pensar que ambas variables se encuentras relacionadas.

Usemos un subconjunto de datos de la encuesta CASEN 2022.

```{python}

import pandas as pd
# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a√±os

casen_2022 = pd.read_stata("data/small_casen2022.dta")
# casen_2022 = pd.read_stata("https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)

casen_2022.head()

```

Y lo agrparemos por ragion, para facilitar el ejemplo:

```{python}
import pandas as pd

# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'

# Agrupar por 'region' y aplicar funciones de agregaci√≥n
casen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()

# Ahora contiene los resultados agregados por regi√≥n

casen_2022_region.head()
```


Realicemos un scatter sencillo:

::: panel-tabset

## matplotlib

```{python}
import matplotlib.pyplot as plt

# Suponiendo que casen_2022 es tu DataFrame
plt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)
plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot entre ytrabajocor y esc (por regi√≥n)')
plt.show()

```
## seaborn

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

# Suponiendo que casen_2022 es tu DataFrame
sns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')
plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot entre ytrabajocor y esc')
plt.show()

```
## seaborn + linea de regresion


```{python}
import seaborn as sns
import matplotlib.pyplot as plt

# Suponiendo que casen_2022 es tu DataFrame
sns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza
plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot con L√≠nea de Regresi√≥n y Intervalo de Confianza')
plt.show()
```

## Con codigos de region

```{python}

import seaborn as sns
import matplotlib.pyplot as plt

# Suponiendo que casen_2022_region es tu DataFrame
sns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza

# Procesar y agregar etiquetas de regi√≥n a los puntos
for i, label in enumerate(casen_2022_region['region']):
    last_word = label.split()[-1]  # Obtener la √∫ltima palabra de la etiqueta
    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')

plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Scatter Plot con L√≠nea de Regresi√≥n y Etiquetas de Regi√≥n (√öltima Palabra)')
plt.show()

```
:::


Podemos ver que se aprecia una relaci√≥n positiva: a mayor escolaridad promedio, mayor salario promedio por regi√≥n.

## Especificaci√≥n

Llamamos especifiaci√≥n al precisar la relaci√≥n entre las variables que deseamos estimar.

En nuestro caso, la funci√≥n base que queremos entender es entre salario y educaci√≥n:

$$ \text{Salario} = f(Educacion))$$

Este es una relaci√≥n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales:
- agregar el error aleatorio
- especificar una forma funcional
- definir una forma de medir las variables en los datos

En nuestro caso, entonces el modelo especificado ser√≠a:

$$ \text{ingreso del trabajo}_i = \alpha + \beta \text{a√±os educaci√≥n}_i + \mu_i$$

## Interpretaci√≥n

Con nuestro modelo especificado:

$$ \text{ingreso del trabajo}_i = \alpha + \beta \text{a√±os educaci√≥n}_i + \mu_i$$

Podemos interpretar $\beta$ y $alpha$:

- $\beta = \frac{\partial ingr}{\partial educ}$: un a√±o adici√≥nal de educaci√≥n, en cuanto incrementa el salario (si nada m√°s cambia) 

- $\alpha$ valor esperado de y, si x=0...
  
## Modelo poblaci√≥nal y estimaci√≥n

Este modelo especificado esta definido en la poblaci√≥n:

$$ \text{ingreso del trabajo}_i = \alpha + \beta \text{a√±os educaci√≥n}_i + \mu_i$$

pero necesitamos calcularlo con la muestra.... por lo cual tenemos estimadores para los coeficientes poblacionales!

$$\hat{ \text{ingreso del trabajo}}_i = \hat{\alpha} + \hat{\beta} \text{a√±os educaci√≥n}_i $$


## Modelo poblaci√≥nal y estimaci√≥n

El m√©todo m√°s comun de estimaci√≥n es el de los **m√≠nimos cuadrados ordinarios**. Veremos detalles sobre la estimaci√≥n, supuestos, propiedades estad√≠sticas la proxima sesi√≥n.

Por ahora, pensaremos que es el m√©todo que busca la l√≠nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi√≥n).

$$ \hat{\mu}_i= y_i-\hat{y}_i$$

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm

# Suponiendo que casen_2022_region es tu DataFrame
sns.set(style='whitegrid')  # Configuraci√≥n del estilo del gr√°fico

# Agregar una columna de constante al DataFrame
casen_2022_region['constante'] = 1

# Crear el gr√°fico de dispersi√≥n con la l√≠nea de regresi√≥n
sns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza

# Ajustar el modelo de regresi√≥n lineal
y = casen_2022_region['ytrabajocor']
X = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t√©rmino constante
modelo = sm.OLS(y, X).fit()

# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo
casen_2022_region['ytrabajocor_pred'] = modelo.predict(X)

# Agregar l√≠neas que conecten cada punto a la l√≠nea de regresi√≥n
for i in range(len(casen_2022_region)):
    x_point = casen_2022_region['esc'][i]
    y_point = casen_2022_region['ytrabajocor'][i]
    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo
    
    # L√≠nea que conecta el punto a la l√≠nea de regresi√≥n
    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')

plt.ylabel('ytrabajocor')
plt.xlabel('esc')
plt.title('Lineas de regresi√≥n y residuos')
plt.show()

```

Es decir, minimiza $\sum_{i}^{n} \hat{\mu}_i $

## Modelo estimado

Por ahora, solo estimaremos el modelo directamente usando statsmodels

::: panel-tabset

## Agrupados por regi√≥n

```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),
# y que puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())


```

## Todos los datos


```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),
# y que puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())


```
:::

Podemos ver que un a√±o adicional de educaci√≥n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.

¬øy la constante, como la podemos interpretar?

## Modelos simples y m√∫ltiples

Muchas veces una sola variable no es suficiente para describir bien un fen√≥meno. Necesitamos incluir m√°s variables.

Esto puede ser:
- Una nueva variable
- Una forma funcional no lineal de la variable ya incluida

Nuestra interpretaci√≥n del modelo no cambia, solo que ahora efectivamente estamos **controlando** por otros factores.

Probemos, agregar edad al modelo:


```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),
# y que puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t√©rmino constante

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())


```

Es muy usual, agregar edad al cuadrado.... para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer...


```{python}
import pandas as pd
import statsmodels.api as sm

# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),
# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.

# Eliminar filas con valores NaN
casen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])

# Agregar una columna de constantes para el t√©rmino constante en el modelo
casen_2022_clean['constante'] = 1

# Agregar una columna con 'edad' al cuadrado
casen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2

# Definir las variables dependiente e independiente
y = casen_2022_clean['ytrabajocor']
X = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'

# Ajustar el modelo de regresi√≥n lineal
modelo = sm.OLS(y, X).fit()

# Imprimir un resumen del modelo
print(modelo.summary())

```

## Un poco m√°s sobre interpretaci√≥n


Los principales elementos que hay que interpretar en un modelo de regresi√≥n lineal son los coeficientes de los predictores:

- $\beta_0$  es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta $y$, cuando todos los predictores son cero.

- $\beta_j$ los coeficientes de regresi√≥n parcial de cada predictor **indican el cambio promedio esperado de la variable respuesta  ùë¶  al incrementar en una unidad de la variable predictora  $x_j$, manteni√©ndose constantes el resto de variables. ("Ceteris paribus"))**

La magnitud de cada coeficiente parcial de regresi√≥n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est√° asociada con la importancia de cada predictor.

![](img/img_sesion3/unidad_medida.png)

Para poder determinar qu√© impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviaci√≥n est√°ndar) las variables predictoras previo ajuste del modelo. En este caso, $\beta_0$ se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y  $\beta_j$ el cambio promedio esperado de la variable respuesta al incrementar en una desviaci√≥n est√°ndar la variable predictora  $x_j$, manteni√©ndose constantes el resto de variables.

Si bien los coeficientes de regresi√≥n suelen ser el primer objetivo de la interpretaci√≥n de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condici√≥n de normalidad...etc.). Estos √∫ltimos suelen ser tratados con poco detalle cuando el √∫nico objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta. 

## Causalidad, regresi√≥n y correlaci√≥n

**Importante tener en cuenta**

Antes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relaci√≥n entre las variables de inter√©s. Esto no implica necesariamente que una variable **cause** la otra (por ejemplo, puntajes m√°s altos en la PSU **no causan** calificaciones superiores en la universidad), pero existe alguna asociaci√≥n significativa entre las dos variables.

Un diagrama de dispersi√≥n puede ser una herramienta √∫til para determinar la fuerza de la relaci√≥n entre dos variables. Si parece no haber asociaci√≥n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi√≥n no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresi√≥n lineal a los datos probablemente no proporcionar√° un modelo √∫til.

Una valiosa medida num√©rica de asociaci√≥n entre dos variables es el coeficiente de correlaci√≥n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci√≥n de los datos observados para las dos variables.


# Una perspectiva hist√≥rica:

EL origen de la t√©cnica, podemos remontarlo a la gen√©tica.

Francis Galton estudi√≥ la variaci√≥n y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolect√≥ y estudi√≥ datos de altura de familias para tratar de entender la herencia. **Mientras hac√≠a esto, desarroll√≥ los conceptos de correlaci√≥n y regresi√≥n.**

Por supuesto, en el momento en que se recogieron estos datos, nuestro conocimiento de la gen√©tica era bastante limitado en comparaci√≥n con lo
que conocemos hoy en d√≠a. Una pregunta muy espec√≠fica que Galton trat√≥ de responder fue:

    ¬øqu√© tan bien podemos predecir la estatura de un ni√±o basado en la estatura de los padres? 

La t√©cnica que desarroll√≥ para responder a esta pregunta, la regresi√≥n, tambi√©n puede aplicarse en muchas otras circunstancias.

Nota hist√≥rica: Galton hizo importantes contribuciones a la estad√≠stica y la gen√©tica, pero tambi√©n fue uno de los primeros defensores de la
eugenesia, un movimiento filos√≥fico cient√≠ficamente defectuoso favorecido por muchos bi√≥logos de la √©poca de Galton pero con terribles
consecuencias hist√≥ricas.

<img src="./img/img_sesion3/galton.png" width="350">

## Estudio de caso: ¬øes hereditaria la altura?

Tenemos acceso a los datos de altura de familias recolectado por Galton, a trav√©s del paquete `HistData`. Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.


```{python}
# Cargamos los paquetes que vamos a usar
import statsmodels.api as sm
import pandas as pd
import numpy as np
import seaborn as sns

# Si no tiene stats models, instalar: pip install statsmodels

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Mostrar las primeras filas del DataFrame
galton_data.head(4)
```

Para imitar el an√°lisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:

```{python}

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

galton_heights.head(4)
```

En los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.

Supongamos que se nos pidiera que resumi√©ramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones est√°n bien aproximadas por la distribuci√≥n normal, podr√≠amos usar los dos promedios y dos desviaciones est√°ndar como res√∫menes:

```{python}
promedio_padre = galton_heights['father'].mean()
sd_padre = galton_heights['father'].std()
promedio_hijo = galton_heights['son'].mean()
sd_hijo = galton_heights['son'].std()

resumen_estadistico = pd.DataFrame({
    'promedio_padre': [promedio_padre],
    'sd_padre': [sd_padre],
    'promedio_hijo': [promedio_hijo],
    'sd_hijo': [sd_hijo]
})

resumen_estadistico.head()
```

Sin embargo, este resumen no describe una caracter√≠stica importante de
los datos: **la tendencia de que cuanto m√°s alto es el padre, m√°s alto es el hijo.**

```{python}
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar el tama√±o de la figura
plt.figure(figsize=(10, 6))

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.set(style="whitegrid")
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)
sns.regplot(data=galton_heights, x='father', y='son', scatter=False)

plt.xlabel("Altura del Padre")
plt.ylabel("Altura del Hijo")
plt.title("Relaci√≥n entre Altura del Padre y Altura del Hijo")

# Mostrar el gr√°fico
plt.show()
```

Aprenderemos que el **coeficiente de correlaci√≥n** es un resumen
informativo de c√≥mo dos variables se mueven juntas y luego veremos c√≥mo
esto puede ser usado para predecir una variable usando la otra, en **una
regresi√≥n**.


## Taller de aplicaci√≥n 2: Caso aplicaci√≥n: Cursos de Verano

::: callout-tip
## **Taller de aplicaci√≥n 2: Pregunta 1**

Considere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que quer√≠amos responder:

> Asistir a cursos de verano mejora los resultados acad√©micos?

1.  Plantee un modelo de regresi√≥n que con los datos disponibles quisieramos estimar.
2.  Grafique la dispersi√≥n y la recta de regresi√≥n estimada.
3.  Estime el modelo simple e interprete

:::

## ¬øRegresi√≥n?... pero ¬øY la correlaci√≥n?


- Ambos est√°n muy relacionados.
- Aprenderemos que el coeficiente de correlaci√≥n es un resumen informativo de c√≥mo dos variables se mueven juntas‚Ä¶
- y luego veremos c√≥mo esto puede ser usado para predecir una variable usando la otra y modelado en una regresi√≥n

```{python}
#| echo: false
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar el tama√±o de la figura
plt.figure(figsize=(10, 6))

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.set(style="whitegrid")
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)
sns.regplot(data=galton_heights, x='father', y='son', scatter=False)

plt.xlabel("Altura del Padre")
plt.ylabel("Altura del Hijo")
plt.title("Relaci√≥n entre Altura del Padre y Altura del Hijo")

# Mostrar el gr√°fico
plt.show()
```


## El coeficiente de correlaci√≥n

El coeficiente de correlaci√≥n se define para una lista de pares $(x_1,y_1),...(x_n,y_n)$  como la media de los productos de los valores normalizados:

$$
\rho = \frac{1}{n}\sum_{i=1}^{n} \big(\frac{x_i-\mu_x }{\sigma_x}\big)\big(\frac{y_i-\mu_y}{\sigma_y}\big)
$$

D√≥nde $\mu$ son promedios y $\sigma$ son desviaciones est√°ndar. La letra griega para r, $\rho$ se utiliza com√∫nmente en los libros de estad√≠stica para denotar la correlaci√≥n, porque es la primera letra de regresi√≥n. Pronto aprenderemos sobre la conexi√≥n entre correlaci√≥n y regresi√≥n. 

Podemos representar la f√≥rmula anterior con el c√≥digo usando:

`rho <- np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))`

Podemos representar la f√≥rmula anterior con el siguiente c√≥digo usando:

```{python}
import numpy as np

x = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aqu√≠
y = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aqu√≠

rho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))

print(rho)

```

La correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente $0,4$:


```{python}

import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Cargar el conjunto de datos GaltonFamilies
galton_data = sm.datasets.get_rdataset("GaltonFamilies", package="HistData").data

# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia
galton_heights = galton_data[galton_data['gender'] == 'male']\
    .groupby('family')\
    .apply(lambda group: group.sample(n=1))\
    .reset_index(drop=True)\
    .loc[:, ['father', 'childHeight']]\
    .rename(columns={'childHeight': 'son'})

# Calcular la media y la desviaci√≥n est√°ndar de la altura del padre
mean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()
sd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()

mean_father = galton_heights['father'].mean()
sd_father = galton_heights['father'].std()

print("Media de la Altura del Padre (Estandarizada):", mean_scaled_father)
print("Desviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada):", sd_scaled_father)
print("Media de la Altura del Padre:", mean_father)
print("Desviaci√≥n Est√°ndar de la Altura del Padre:", sd_father)

# Crear el gr√°fico de dispersi√≥n
plt.scatter(galton_heights['father'], StandardScaler().fit_transform(galton_heights[['father']]))
plt.xlabel("Altura del Padre")
plt.ylabel("Altura del Padre Estandarizada")
plt.title("Relaci√≥n entre Altura del Padre y Altura del Padre Estandarizada")
plt.show()


```

La correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente $0,4$.

```{python}
correlation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]
print("Coeficiente de Correlaci√≥n:", correlation_coefficient)

```


```{python}
import pandas as pd

# Generar datos simulados usando la biblioteca faux
dat = pd.DataFrame(np.random.multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([1, 1, 1, 1, 1, 1]), size=100),
                   columns=["A", "B", "C", "D", "E", "F"])

print(dat)

# Calcular la correlaci√≥n entre father y son usando una muestra de galton_heights
R = galton_heights.sample(n=75, replace=True).corr().loc["father", "son"]
print(R)


```

Para ver c√≥mo se ven los datos para los diferentes valores de $\rho$ aqu√≠ hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:

![image](img/img_sesion3/g1.png)

## La correlaci√≥n de la muestra es una variable aleatoria

Antes de continuar conectando la correlaci√≥n con la regresi√≥n, recordemos la variabilidad aleatoria.

En la mayor√≠a de las aplicaciones de la ciencia de datos, observamos datos que incluyen **variaci√≥n aleatoria**. 

Por ejemplo, en muchos casos, no se observan datos para toda la poblaci√≥n de inter√©s, sino para una muestra aleatoria. Al igual que con el promedio y la desviaci√≥n est√°ndar, la **correlaci√≥n de la muestra** es la estimaci√≥n m√°s com√∫nmente utilizada de la **correlaci√≥n de la poblaci√≥n**. Esto implica que la correlaci√≥n que calculamos y usamos como resumen es una variable aleatoria.

A modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra poblaci√≥n. Un genetista menos afortunado s√≥lo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlaci√≥n de la muestra se puede calcular con:


```{python}

import pandas as pd

# Seleccionar una muestra aleatoria de tama√±o 75 con reemplazo
R = galton_heights.sample(n=75, replace=True)

# Calcular el coeficiente de correlaci√≥n entre las columnas "father" y "son"
correlation_coefficient = R[['father', 'son']].corr().iloc[0, 1]

print("Coeficiente de Correlaci√≥n en la Muestra:", correlation_coefficient)


```

R es una variable aleatoria. Podemos ejecutar una simulaci√≥n de Monte Carlo para ver su distribuci√≥n:

* Nota: el objetivo principal de la simulaci√≥n de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir c√≥mo van a evolucionar.


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

B = 1000
N = 100
R = np.zeros(B)

for i in range(B):
    sample = galton_heights.sample(n=N, replace=False)
    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]
    R[i] = correlation_coefficient

# Crear un histograma de los coeficientes de correlaci√≥n
plt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')
plt.xlabel("Coeficiente de Correlaci√≥n")
plt.ylabel("Frecuencia")
plt.title("Histograma de Coeficientes de Correlaci√≥n")
plt.show()

```

Vemos que el valor esperado de R es la correlaci√≥n de la poblaci√≥n:


```{python}
mean_R = np.mean(R)
print("Media de Coeficientes de Correlaci√≥n:", mean_R)

```

y que tiene un error est√°ndar relativamente alto en relaci√≥n con el rango de valores que puede tomar R:


```{python}
sd_R = np.std(R)
print("Desviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n:", sd_R)

```

Por lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.

Adem√°s, tenga en cuenta que debido a que la correlaci√≥n de la muestra es un promedio de extracciones independientes, el teorema del l√≠mite central realmente funciona. Por lo tanto, para $N$ lo suficientemente grande la distribuci√≥n de $R$ es aproximadamente normal con el valor esperado $\rho$. La desviaci√≥n est√°ndar, que es algo compleja de derivar, es: $\sqrt{\frac{1-r^2}{N-2}}$.

En nuestro ejemplo, $N=25$ no parece ser lo suficientemente grande para que la aproximaci√≥n sea buena:

* Nota: El gr√°fico Q-Q, o gr√°fico cuantitativo, es una herramienta gr√°fica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribuci√≥n te√≥rica como una Normal o exponencial. Por ejemplo, si realizamos un an√°lisis estad√≠stico que asume que nuestra variable dependiente est√° Normalmente distribuida, podemos usar un gr√°fico Q-Q-Normal para verificar esa suposici√≥n. https://data.library.virginia.edu/understanding-q-q-plots/


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Crear un DataFrame con los coeficientes de correlaci√≥n
df_R = pd.DataFrame({'R': R})

# Calcular la media y el tama√±o de la muestra
mean_R = np.mean(R)
N = len(R)

# Crear el gr√°fico QQ-plot
plt.figure(figsize=(6, 6))
stats.probplot(df_R['R'], dist='norm', plot=plt)
plt.xlabel("Cuantiles Te√≥ricos")
plt.ylabel("Cuantiles de R")
plt.title("Gr√°fico QQ-plot para los Coeficientes de Correlaci√≥n")
plt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # L√≠nea de referencia
plt.show()

```

Si N aumenta ver√°s que la distribuci√≥n converge a una normal.


## La correlaci√≥n no siempre es un resumen √∫til

La correlaci√≥n no siempre es un buen resumen de la relaci√≥n entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlaci√≥n de 0,82:

![image](img/img_sesion3/g2.png)

La correlaci√≥n s√≥lo tiene sentido en un contexto particular. Para ayudarnos a entender cu√°ndo es que la correlaci√≥n es significativa como estad√≠stica de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudar√° a motivar y definir la regresi√≥n lineal. Comenzamos demostrando c√≥mo la correlaci√≥n puede ser √∫til para la predicci√≥n.

# Correlaci√≥n no es causalidad

La asociaci√≥n no es causalidad es quiz√°s la lecci√≥n m√°s importante que se aprende en una clase de estad√≠stica. Hay muchas razones por las que una variable $X$ puede correlacionarse con una variable $Y$ sin tener ning√∫n efecto directo sobre $Y$. Aqu√≠ examinamos tres maneras comunes que pueden llevar a una mala interpretaci√≥n de los datos.

## Correlaci√≥n espuria
El siguiente ejemplo c√≥mico subraya que la correlaci√≥n no es causalidad. Muestra una fuerte correlaci√≥n entre las tasas de divorcio y el consumo de margarina.


![image](img/img_sesion3/notcausa.png)


(Ac√° pueden encontrar m√°s http://tylervigen.com/old-version.html)

¬øSignifica esto que la margarina causa divorcios? ¬øO los divorcios hacen que la gente coma m√°s margarina? Por supuesto que la respuesta a estas dos preguntas es no. Esto es s√≥lo un ejemplo de lo que llamamos una correlaci√≥n espuria.


Los casos presentados en el sitio de correlaci√≥n espuria son todos casos de lo que generalmente se denomina dragado de datos (data dredging), pesca de datos (data fishing) o espionaje de datos (data snooping). Es b√°sicamente una forma de lo que en los EE.UU. se llama "cherry picking". Un ejemplo de dragado de datos ser√≠a si miras a trav√©s de muchos resultados producidos por un proceso aleatorio y escoges el que muestra una relaci√≥n que apoya una teor√≠a que se quiere defender.

# La paradoja de Simpson

Se llama paradoja porque vemos el signo de la correlaci√≥n cambiar cuando comparamos toda la data y estratos espec√≠ficos. Como ejemplo ilustrativo, supongamos que tiene tres variables aleatorias $X$, $Y$ y $Z$ y que observamos realizaciones de estas. Aqu√≠ est√° el gr√°fico de observaciones simuladas para $X$ y $Y$ a lo largo de la correlaci√≥n de la muestra:

<img src="./img/img_sesion3/simp1.png" width="600">


Puedes ver que $X$ e $Y$ est√°n negativamente correlacionados. Sin embargo, una vez que estratificamos por $Z$ (mostrado en diferentes colores abajo) emerge otro patr√≥n:

<img src="./img/img_sesion3/simp2.png" width="600">


Es realmente $Z$ que est√° negativamente correlacionado con $X$. Si estratificamos por $Z$ las variables $X$ e $Y$ est√°n en realidad correlacionados positivamente como se ha visto en el gr√°fico anterior.

# Expectativas condicionales

Supongamos que nos piden que adivinemos la altura de un hijo seleccionado al azar y no sabemos la altura de su padre. Debido a que la distribuci√≥n de las alturas de los hijos es aproximadamente normal, sabemos que la altura media, $69.2$, es el valor con la mayor proporci√≥n y ser√≠a la predicci√≥n con mayores posibilidades de minimizar el error. Pero, ¬øy si nos dicen que el padre es m√°s alto que el promedio, digamos que mide 72 pulgadas de alto, todav√≠a esperar√≠amos que la altura m√°s probable del hijo sea 69.2 pulgadas?

Resulta que si pudi√©ramos recolectar datos de un gran n√∫mero de padres que miden 72 pulgadas, la distribuci√≥n de las alturas de sus hijos ser√≠a normalmente distribuida. Esto implica que el promedio de la distribuci√≥n calculada en este subconjunto ser√≠a nuestra mejor predicci√≥n.

En general, llamamos a este enfoque condicional. La idea general es que estratificamos una poblaci√≥n en grupos y calculamos res√∫menes en cada grupo. Por lo tanto, el condicionamiento est√° relacionado con el concepto de estratificaci√≥n descrito. 

Para proporcionar una descripci√≥n matem√°tica del condicionamiento, considere que tenemos una poblaci√≥n de pares de valores  $(x_1,y_1),...,(x_n,y_n)$, por ejemplo, todas las alturas de padre e hijo en Inglaterra. Sabemos que si se toma un par al azar $(X,Y)$ el valor esperado y el mejor predictor de $Y$ es $E(Y)=\mu_y$, el promedio de la poblaci√≥n: $1/n \sum_{i=y}^{n}y_i$. Sin embargo, ya no estamos interesados en la poblaci√≥n en general, sino s√≥lo en el subconjunto de la poblaci√≥n con un valor espec√≠fico, $72$ pulgadas. Este subconjunto de la poblaci√≥n, es tambi√©n una poblaci√≥n y por lo tanto se aplican los mismos principios y propiedades que hemos aprendido. El $y_i$ en la subpoblaci√≥n tienen una distribuci√≥n, denominada distribuci√≥n condicional, y esta distribuci√≥n tiene un valor esperado, denominado expectativa condicional. En nuestro ejemplo, la expectativa condicional es la estatura promedio de todos los hijos en Inglaterra con padres de 72 pulgadas. La notaci√≥n estad√≠stica es para la expectativa condicional es:

\begin{equation}
E(Y|X=x)
\end{equation}

con $x$ representando el valor fijo que define ese subconjunto, por ejemplo 72 pulgadas. Del mismo modo, se indica la desviaci√≥n est√°ndar de los estratos con:

\begin{equation}
SD(Y|X=x)=\sqrt{Var(Y|x=x)}
\end{equation}


Porque la expectativa condicional $E(Y|X=x)$ es el mejor predictor para la variable aleatoria $Y$ para un individuo en los estratos definidos por  $X=x$ muchos de los desaf√≠os de la ciencia de datos se reducen a la estimaci√≥n de esta cantidad. La desviaci√≥n est√°ndar condicional cuantifica la precisi√≥n de la predicci√≥n.

En el ejemplo que hemos estado considerando, estamos interesados en calcular la altura promedio del hijo condicionada a que el padre tenga 72 pulgadas de altura. Queremos estimar $E(Y|X=72)$ usando la muestra recolectada por Galton. 

Anteriormente aprendimos que el promedio de la muestra es el enfoque preferido para estimar el promedio de la poblaci√≥n. Sin embargo, un desaf√≠o al usar este enfoque para estimar las expectativas condicionales es que para los datos continuos no tenemos muchos puntos de datos que coincidan exactamente con un valor de nuestra muestra. Por ejemplo, s√≥lo tenemos:

```{python}
count_72 = (galton_heights['father'] == 72).sum()
print("Cantidad de registros con valor 72 en la columna 'father':", count_72)

```

padres que miden exactamente 72 pulgadas. Si cambiamos el n√∫mero a 72.5, obtenemos a√∫n menos puntos de datos:

```{python}
count_725 = (galton_heights['father'] == 72.5).sum()
print("Cantidad de registros con valor 72.5 en la columna 'father':", count_725)
```

Una forma pr√°ctica de mejorar estas estimaciones de las expectativas condicionales, es definir estratos con valores similares de $x$. En nuestro ejemplo, podemos redondear las alturas paternas a la pulgada m√°s cercana y asumir que todas son de 72 pulgadas. Si hacemos esto, terminamos con la siguiente predicci√≥n para el hijo de un padre que mide 72 pulgadas de alto:


```{python}

conditional_avg = galton_heights[galton_heights['father'].round() == 72]['son'].mean()
print("Promedio condicional para father == 72:", conditional_avg)

```

En este c√≥digo, filtramos el DataFrame "galton_heights" para obtener las filas donde el valor redondeado de "father" es igual a 72. Luego, calculamos el promedio de la columna "son" en las filas filtradas y almacenamos el resultado en la variable "conditional_avg". Finalmente, imprimimos el promedio condicional calculado.


Note que un padre de 72 pulgadas es m√°s alto que el promedio -- espec√≠ficamente, 72 - 69.1/2.5 = 1.1 desviaciones est√°ndar m√°s alto que el padre promedio. Nuestra predicci√≥n, $70.5$, es tambi√©n m√°s alta que el promedio, pero s√≥lo $0.49$ desviaciones est√°ndar m√°s grandes que el hijo promedio. Los hijos de padres de 72 pulgadas han regresado algunos a la estatura promedio. Observamos que la reducci√≥n en el n√∫mero de SD m√°s altas es de alrededor de $0.5$, lo que resulta ser la correlaci√≥n. Como veremos en una secci√≥n posterior, esto no es una coincidencia.

Si queremos hacer una predicci√≥n de cualquier altura, no s√≥lo de 72, podr√≠amos aplicar el mismo enfoque a cada estrato. La estratificaci√≥n seguida de los boxplots nos permite ver la distribuci√≥n de cada grupo:


```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.

# Crear una nueva columna 'father_strata' con los valores redondeados de 'father'
galton_heights['father_strata'] = galton_heights['father'].round().astype(int)

# Crear el gr√°fico de boxplots
plt.figure(figsize=(10, 6))  # Tama√±o del gr√°fico
sns.boxplot(data=galton_heights, x='father_strata', y='son')

# Agregar puntos para mostrar las medias condicionadas
sns.swarmplot(data=galton_heights, x='father_strata', y='son', color='black', size=4)

plt.xlabel('father_strata')
plt.ylabel('son')
plt.title('Boxplots de son condicionado por father_strata con Medias Condicionadas')
plt.xticks(rotation=45)  # Rotar etiquetas del eje x si es necesario

plt.show()


```


No es de extra√±ar que los centros de los grupos aumenten con la altura.


```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Redondear los valores de la columna "father"
galton_heights['father'] = galton_heights['father'].round()

# Calcular el promedio condicional de "son" para cada valor de "father"
conditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()

# Crear un gr√°fico de puntos para mostrar el promedio condicional por "father"
plt.figure(figsize=(10, 6))
plt.scatter(conditional_avg_by_father['father'], conditional_avg_by_father['son'], color='blue')
plt.xlabel("Father Height")
plt.ylabel("Conditional Son Height Average")
plt.title("Promedio Condicional de Alturas de Hijos por Altura de Padres")
plt.show()


```

Adem√°s, estos centros parecen seguir una relaci√≥n lineal. A continuaci√≥n se presentan los promedios de cada grupo. Si tenemos en cuenta que estos promedios son variables aleatorias con errores est√°ndar, los datos son consistentes con estos puntos siguiendo una l√≠nea recta:


```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Redondear los valores de la columna "father"
galton_heights['father'] = galton_heights['father'].round()

# Calcular el promedio condicional de "son" para cada valor de "father"
conditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()


conditional_avg_by_father.head()


# Crear un gr√°fico de puntos con ajuste de regresi√≥n lineal
plt.figure(figsize=(10, 6))
sns.scatterplot(x='father', y='son', data=conditional_avg_by_father, color='blue')
sns.regplot(x='father', y='son', data=conditional_avg_by_father, scatter=False, color='orange')
plt.xlabel("Father Height")
plt.ylabel("Conditional Son Height Average")
plt.title("Promedio Condicional de Alturas de Hijos por Altura de Padres con Regresi√≥n Lineal")
plt.show()

```


El hecho de que estos promedios condicionales sigan una l√≠nea no es una coincidencia. En la siguiente secci√≥n, explicamos que la l√≠nea que siguen estos promedios es lo que llamamos la l√≠nea de regresi√≥n, que mejora la precisi√≥n de nuestras estimaciones. Sin embargo, no siempre es apropiado estimar las expectativas condicionales con la l√≠nea de regresi√≥n, por lo que tambi√©n describimos la justificaci√≥n te√≥rica de Galton para usar la l√≠nea de regresi√≥n.


# La l√≠nea de regresi√≥n

Si estamos prediciendo una variable aleatoria $Y$ conociendo el valor de otra variable $X=x$ usando una l√≠nea de regresi√≥n, entonces predecimos que **para cada desviaci√≥n est√°ndar, $\sigma_x$ que $x$ aumenta por encima de la media $\mu_x$, $Y$ incrementa $\rho$ veces la desviaci√≥n est√°ndar $\sigma_Y$ sobre el promedio $\mu_Y$**, con $\rho$ la correlaci√≥n entre $X$ e $Y$. Por lo tanto, la formula de la regresi√≥n es:

$$
\left( \frac{Y-\mu_Y}{\sigma_Y} \right)=\rho \left(\frac{x-\mu_X}{\sigma_X}\right)
$$

Lo que podemos reescribir como:

$$
Y=\mu_Y + \rho \big(\frac{x-\mu_X}{\sigma_X}\big) \sigma_Y
$$

Si existe una correlaci√≥n perfecta, la l√≠nea de regresi√≥n predice un aumento que corresponde al mismo n√∫mero de desviacones est√°ndar. Si hay correlaci√≥n 0, entonces no usamos $x$ en absoluto en la predicci√≥n y simplemente predecimos el promedio $\mu_Y$. Para valores entre 0 y 1, la predicci√≥n se encuentra en un punto intermedio. Si la correlaci√≥n es negativa, predecimos una reducci√≥n en lugar de un aumento.

N√≥tese que si la correlaci√≥n es positiva e inferior a 1, nuestra predicci√≥n est√° m√°s cerca (en unidades est√°ndar) de la altura media que de lo que el valor utilizado para predecir, $x$, est√° del promedio de los $x$. Por eso lo llamamos regresi√≥n: el hijo regresa a la estatura media. De hecho, el t√≠tulo del art√≠culo de Galton era: Regresi√≥n a la mediocridad en la estatura hereditaria (Regression toward mediocrity in hereditary stature.). 

Para a√±adir l√≠neas de regresi√≥n a los gr√°ficos, necesitaremos la f√≥rmula anterior en la forma: $y=b+mx$, con pendiente $m=\rho \sigma_y / \sigma_x$ e intercepto $b=\mu_y - m \mu_x$

Aqu√≠ agregamos la l√≠nea de regresi√≥n a la data original.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# C√°lculo de las medias y desviaciones est√°ndar
mu_x = galton_heights['father'].mean()
mu_y = galton_heights['son'].mean()
s_x = galton_heights['father'].std()
s_y = galton_heights['son'].std()

# C√°lculo del coeficiente de correlaci√≥n
r = galton_heights['father'].corr(galton_heights['son'])

# C√°lculo de la pendiente y el intercepto para la l√≠nea de regresi√≥n
m = r * s_y / s_x
b = mu_y - m * mu_x

# Configuraci√≥n del tama√±o de la figura
plt.figure(figsize=(10, 6))

# Crear un gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.scatterplot(x='father', y='son', data=galton_heights, alpha=0.5, size=3)
sns.regplot(x='father', y='son', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})
plt.xlabel("Father Height")
plt.ylabel("Son Height")
plt.title("Relaci√≥n entre Altura de Padres e Hijos con L√≠nea de Regresi√≥n")
plt.show()

```

La f√≥rmula de regresi√≥n implica que si primero estandarizamos las variables, es decir, restamos el promedio y dividimos por la desviaci√≥n est√°ndar, entonces la l√≠nea de regresi√≥n tiene intercepto 0 y pendiente igual a la correlaci√≥n $\rho$. Aqu√≠ est√° la misma gr√°fica, pero usando unidades est√°ndar:

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.

# Estandarizar las variables 'father' y 'son'
galton_heights['father_standardized'] = (galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std()
galton_heights['son_standardized'] = (galton_heights['son'] - galton_heights['son'].mean()) / galton_heights['son'].std()

# Calcular la correlaci√≥n de las variables estandarizadas
r = galton_heights['father_standardized'].corr(galton_heights['son_standardized'])

# Configuraci√≥n del tama√±o de la figura
plt.figure(figsize=(10, 6))

# Crear un gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n
sns.scatterplot(x='father_standardized', y='son_standardized', data=galton_heights, alpha=0.5, size=3)
sns.regplot(x='father_standardized', y='son_standardized', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})
plt.xlabel("Father Height (Standardized)")
plt.ylabel("Son Height (Standardized)")
plt.title("Relaci√≥n Estandarizada entre Altura de Padres e Hijos con L√≠nea de Regresi√≥n (Intercepto = 0, Pendiente = Correlaci√≥n)")
plt.show()

```


# La regresi√≥n mejora la precisi√≥n

Comparemos los dos enfoques de predicci√≥n que hemos presentado:

1) Redondee las alturas del padre a la pulgada m√°s cercana, estratifique y luego tome el promedio.
2) Calcula la l√≠nea de regresi√≥n y √∫sala para predecir.
    
Usamos un muestreo de simulaci√≥n de Monte Carlo $N=50$ familias:

```{python}

import pandas as pd
import numpy as np

# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.
# Definimos B (n√∫mero de simulaciones) y N (tama√±o de la muestra).

B= 1000
N=50

# Configuraci√≥n de la semilla aleatoria para reproducibilidad
np.random.seed(10)

# Inicializar listas para almacenar los resultados de las simulaciones
conditional_avg = []
regression_prediction = []

# Realizar simulaciones de Monte Carlo
for _ in range(B):
    # Seleccionar una muestra aleatoria de tama√±o N
    dat = galton_heights.sample(n=N)
    
    # Calcular la media condicional (Enfoque 1)
    conditional_avg.append(dat[dat['father'].round() == 72]['son'].mean())
    
    # Calcular la predicci√≥n de regresi√≥n (Enfoque 2)
    mu_x = dat['father'].mean()
    mu_y = dat['son'].mean()
    s_x = dat['father'].std()
    s_y = dat['son'].std()
    r = dat['father'].corr(dat['son'])
    regression_prediction.append(mu_y + r * (72 - mu_x) / (s_x / s_y))

# Calcular las estad√≠sticas descriptivas de las simulaciones
mean_conditional_avg = np.mean(conditional_avg)
mean_regression_prediction = np.mean(regression_prediction)
std_conditional_avg = np.std(conditional_avg, ddof=1)
std_regression_prediction = np.std(regression_prediction, ddof=1)

# Imprimir resultados
print("Valor Esperado (Media) - Enfoque 1 (Media Condicional):", mean_conditional_avg)
print("Valor Esperado (Media) - Enfoque 2 (Predicci√≥n de Regresi√≥n):", mean_regression_prediction)
print("Error Est√°ndar - Enfoque 1 (Media Condicional):", std_conditional_avg)
print("Error Est√°ndar - Enfoque 2 (Predicci√≥n de Regresi√≥n):", std_regression_prediction)

```

Aunque el valor esperado de estas dos variables aleatorias es casi el mismo, el error est√°ndar para la predicci√≥n de regresi√≥n es sustancialmente menor.

Por lo tanto, la l√≠nea de regresi√≥n es mucho m√°s estable que la media condicional. Hay una raz√≥n intuitiva para ello. El promedio condicional se calcula en un subconjunto relativamente peque√±o: los padres que miden alrededor de 72 pulgadas de alto. De hecho, en algunas de las permutaciones no tenemos datos. La regresi√≥n siempre utiliza todos los datos.

Entonces, ¬øpor qu√© no usar siempre la regresi√≥n para predecir? Porque no siempre es apropiado. Por ejemplo, Anscombe proporcion√≥ casos en los que los datos no tienen una relaci√≥n lineal. Entonces, ¬øest√° justificado usar la l√≠nea de regresi√≥n para predecir? Galton contest√≥ esto de forma afirmativa para los datos de altura.



# Definici√≥n matem√°tica


El modelo de regresi√≥n lineal (Legendre, Gauss, Galton y Pearson) considera que, dado un conjunto de observaciones $\{y_i, x_{i1},...,x_{np}\}^{n}_{i=1}$ , la media  $ùúá$  de la variable respuesta  $ùë¶$  se relaciona de forma lineal con la o las variables regresoras  $ùë•_1$ ... $x_p$  acorde a la ecuaci√≥n:

$\mu_y = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + ... + \beta_p x_{p}$
 
El resultado de esta ecuaci√≥n se conoce como la l√≠nea de regresi√≥n poblacional, y recoge la relaci√≥n entre los predictores y la media de la variable respuesta.

Otra definici√≥n que se encuentra con frecuencia en los libros de estad√≠stica es:

$y_i= \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} +\epsilon_i$
 
En este caso, se est√° haciendo referencia al valor de  ùë¶  para una observaci√≥n  ùëñ  concreta. El valor de una observaci√≥n puntual nunca va a ser exactamente igual al promedio, de ah√≠ que se a√±ada el t√©rmino de error  $\epsilon$.


En ambos casos, la interpretaci√≥n de los elementos del modelo es la misma:

- $\beta_0$: es la ordenada en el origen, se corresponde con el valor promedio de la variable respuesta  $y$  cuando todos los predictores son cero.

- $\beta_j$: es el efecto promedio que tiene sobre la variable respuesta el incremento en una unidad de la variable predictora  $x_j$, manteni√©ndose constantes el resto de variables. Se conocen como coeficientes de regresi√≥n.

- $\epsilon$: es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo. Recoge el efecto de todas aquellas variables que influyen en $y$ pero que no se incluyen en el modelo como predictores.

En la gran mayor√≠a de casos, los valores $\beta_0$ y $\beta_j$ poblacionales se desconocen, por lo que, a partir de una muestra, se obtienen sus estimaciones  $\hat{\beta_0}$ y $\hat{\beta_j}$. **Ajustar el modelo consiste en estimar, a partir de los datos disponibles, los valores de los coeficientes de regresi√≥n que maximizan la verosimilitud (likelihood), es decir, los que dan lugar al modelo que con mayor probabilidad puede haber generado los datos observados.**

El m√©todo empleado con m√°s frecuencia es el ajuste por m√≠nimos cuadrados ordinarios (OLS), que identifica como mejor modelo la recta (o plano si es regresi√≥n m√∫ltiple) que minimiza la suma de las desviaciones verticales entre cada dato de entrenamiento y la recta, elevadas al cuadrado.

# Interpretaci√≥n del modelo


Los principales elementos que hay que interpretar en un modelo de regresi√≥n lineal son los coeficientes de los predictores:

- $\beta_0$  es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta $y$, cuando todos los predictores son cero.

- $\beta_j$ los coeficientes de regresi√≥n parcial de cada predictor **indican el cambio promedio esperado de la variable respuesta  ùë¶  al incrementar en una unidad de la variable predictora  $x_j$, manteni√©ndose constantes el resto de variables. ("Ceteris paribus"))**

La magnitud de cada coeficiente parcial de regresi√≥n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est√° asociada con la importancia de cada predictor.

Para poder determinar qu√© impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviaci√≥n est√°ndar) las variables predictoras previo ajuste del modelo. En este caso, $\beta_0$ se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y  $\beta_j$ el cambio promedio esperado de la variable respuesta al incrementar en una desviaci√≥n est√°ndar la variable predictora  $x_j$, manteni√©ndose constantes el resto de variables.

Si bien los coeficientes de regresi√≥n suelen ser el primer objetivo de la interpretaci√≥n de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condici√≥n de normalidad...etc.). Estos √∫ltimos suelen ser tratados con poco detalle cuando el √∫nico objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta. 



# Significado "lineal"

El t√©rmino "lineal" en los modelos de regresi√≥n hace referencia al hecho de que los par√°metros se incorporan en la ecuaci√≥n de forma lineal, no a que necesariamente la relaci√≥n entre cada predictor y la variable respuesta tenga que seguir un patr√≥n lineal.

La siguiente ecuaci√≥n muestra un modelo lineal en el que el predictor  ùë•1  no es lineal respecto a y:

$y = \beta_0 + \beta_1x_1 + \beta_2log(x_1) + \epsilon$


<img src="./img/img_sesion3/im1.png" width="400">


En contraposici√≥n, el siguiente no es un modelo lineal:

$y = \beta_0 + \beta_1x_1^{\beta_2} + \epsilon$

 
En ocasiones, algunas relaciones no-lineales pueden transformarse de forma que se pueden expresar de manera lineal:

- Modelo no-lineal a estimar: $y = \beta_0x_1^{\beta_1}\epsilon$

- Solucion: pasamos todo a logaritmos:

$log(y)=log(\beta_0) + \beta_1log(x_1) + log(\epsilon)$
        
$y^{'}=\beta_0^{'}+\beta_1x_1^{'} + \epsilon^{'}$

- Estimar el modelo y extraer los coeficientes.

- Volvera a la forma funcional incial exponenciando los logaritmos.
    - $\beta_1$ es explicito.
    - $\beta_0^{'}=log(\beta_0)=> exp(log(\beta_0))$

### Distribuci√≥n normal bivariable (avanzado)

https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal_multivariante

La correlaci√≥n y la pendiente de regresi√≥n son estad√≠sticas de resumen ampliamente utilizadas, pero a menudo se utilizan o interpretan err√≥neamente. Los ejemplos de Anscombe proporcionan casos excesivamente simplificados de conjuntos de datos en los que resumir con correlaci√≥n ser√≠a un error. Pero hay muchos m√°s ejemplos de la vida real.

La principal forma en que motivamos el uso de la correlaci√≥n involucra lo que se llama la distribuci√≥n normal bivariada.

Cuando un par de variables aleatorias son aproximadas por la distribuci√≥n normal bivariada, las gr√°ficas de dispersi√≥n se ven como √≥valos. Pueden ser delgadas (alta correlaci√≥n) o en forma de c√≠rculo (sin correlaci√≥n).

![image](img/img_sesion3/g1.png)


Una forma m√°s t√©cnica de definir la distribuci√≥n normal bivariada es la siguiente: si $X$ es una variable aleatoria normalmente distribuida, $Y$ es tambi√©n una variable aleatoria normalmente distribuida, y la distribuci√≥n condicional de $Y$ para cualquier $X=x$ es aproximadamente normal, entonces el par es aproximadamente normal bivariado.

![image](img/img_sesion3/g3.png)


Si pensamos que los datos de altura est√°n bien aproximados por la distribuci√≥n normal bivariada, entonces deber√≠amos ver la aproximaci√≥n normal para cada estrato. Aqu√≠ estratificamos las alturas del hijo por las alturas paternas estandarizadas y vemos que la suposici√≥n parece mantenerse:


```{python}
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns

# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.

# Estandarizar las alturas del padre y crear una columna 'z_father'
galton_heights['z_father'] = np.round((galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std())

# Filtrar las alturas del padre dentro del rango [-2, 2]
galton_heights_filtered = galton_heights[galton_heights['z_father'].isin([-2, -1, 0, 1, 2])]


galton_heights_filtered.head()
```

```{python}

# Configurar el tama√±o de la figura y el estilo de Seaborn
plt.figure(figsize=(5, 3))
sns.set(style='whitegrid')

# Crear gr√°ficos QQ-plot utilizando un bucle for para cada estrato
for z_father_value, group_data in galton_heights_filtered.groupby('z_father'):
    plt.subplot(2, 3, int(z_father_value) + 3)  # Convertir z_father_value a entero
    stats.probplot(group_data['son'], plot=plt, fit=True, rvalue=True)
    plt.title(f"Estrato z_father={z_father_value}")

# Ajustar t√≠tulos y etiquetas
plt.suptitle("QQ-Plots de las Alturas del Hijo por Estrato de Alturas del Padre Estandarizadas")
plt.tight_layout()

# Mostrar el gr√°fico
plt.show()

```

Ahora volvemos a definir la correlaci√≥n. Galton utiliz√≥ estad√≠sticas matem√°ticas para demostrar que, cuando dos variables siguen una distribuci√≥n normal bivariada, el c√°lculo de la l√≠nea de regresi√≥n es equivalente al c√°lculo de las expectativas condicionales. Esto implica que, si nuestros datos son aproximadamente bivariados, la l√≠nea de regresi√≥n es equivalente a la probabilidad condicional. Por lo tanto, podemos obtener una estimaci√≥n mucho m√°s estable del valor de expectaci√≥n condicional, encontrando la l√≠nea de regresi√≥n y us√°ndola para predecir.

En resumen, si nuestros datos son aproximadamente bivariados, entonces la expectativa condicional, la mejor predicci√≥n de $Y$ dado que conocemos el valor de $X$ est√° dada por la l√≠nea de regresi√≥n.

$$
Y_i=\beta_0 + \beta_1 x_i +U_i
$$

De aqu√≠ facilmente podemos intuir algunos de los supuestos que deben cumplirse al implementar una regresi√≥n (y que estudiaremos en detalle en la siguiente sesion):


   1) Normalidad: $u_i \sim Normal$
   
   2) Linealidad: Los residuos se distribuyen sin forma alrededor del cero $E(u_i)=0$
   
   3) Homocedasticidad: La variabilidad de los residuos es similar para todos los $x_i$, $V(u_i)=\sigma^2$
   
   4) No existen resudios at√≠picos.
  
   5) Independecia: Los residuos, ($u_i$), son independientes 

## Varianza explicada
La teor√≠a de la normalidad bivariada tambi√©n nos dice que la desviaci√≥n est√°ndar de la distribuci√≥n condicional descrita anteriormente es:

$$
SD(Y|X=x)=\sigma_Y\sqrt{1-\rho^2}
$$


Para ver por qu√© esto es intuitivo, note que sin condicionamiento, $SD(Y)=\sigma_Y$ estamos viendo la variabilidad de todos los hijos. Pero una vez que los condicionamos, s√≥lo estamos viendo la variabilidad de los hijos con un padre que mide 72 pulgadas de alto. Este grupo tender√° a ser "algo m√°s" alto (que el promedio), por lo que la desviaci√≥n est√°ndar se reduce. 

Espec√≠ficamente, se reduce a $\sqrt{1-\rho^2}=\sqrt{1-0.25}=0.86$ de lo que era originalmente. Podr√≠amos decir que la estatura del padre "explica" el 14% de la variabilidad de estatura del hijo.



La frase "$X$ explica tal o cual porcentaje de la variabilidad" se utiliza com√∫nmente en papers acad√©micos. En este caso, este porcentaje se refiere realmente a la desviaci√≥n (SD al cuadrado). Por lo tanto, si los datos son normales bivariados, la varianza se reduce en $1-\rho^2$ por lo que decimos que $X$ explica $1-(1-\rho^2)=\rho^2$ (la correlaci√≥n al cuadrado) de la varianza.


Pero es importante recordar que la afirmaci√≥n de "varianza explicada" s√≥lo tiene sentido cuando los datos se aproximan mediante una distribuci√≥n normal bivariada.


## Cuidado: hay dos l√≠neas de regresi√≥n

Calculamos una l√≠nea de regresi√≥n para predecir la altura del hijo desde la altura del padre. 

Usamos estos c√°lculos:

```{python}

import numpy as np

# Calcular la media de las alturas del padre
mu_x = galton_heights['father'].mean()

# Calcular la media de las alturas del hijo
mu_y = galton_heights['son'].mean()

# Calcular la desviaci√≥n est√°ndar de las alturas del padre
s_x = galton_heights['father'].std()

# Calcular la desviaci√≥n est√°ndar de las alturas del hijo
s_y = galton_heights['son'].std()

# Calcular el coeficiente de correlaci√≥n entre las alturas del padre y el hijo
r = galton_heights['father'].corr(galton_heights['son'])



print(r)
print(s_x)
print(s_y)
print(mu_x)
print(mu_y)
```


```{python}
# Calcular la pendiente de la primera l√≠nea de regresi√≥n
m_1 = r * s_y / s_x

# Calcular el intercepto de la primera l√≠nea de regresi√≥n
b_1 = mu_y - m_1 * mu_x

print("pendiente", m_1)
print("constante", b_1)
```
Lo que nos da la funci√≥n $E(Y|X=x)=28,8+0.44x$.

¬øY si queremos predecir la estatura del padre bas√°ndonos en la del hijo? 

Es importante saber que esto no se determina calculando la funci√≥n inversa!.

Necesitamos computar $E(X‚à£Y=y)$. Dado que los datos son aproximadamente normales bivariados, la teor√≠a descrita anteriormente nos dice que esta expectativa condicional seguir√° una l√≠nea con pendiente e intercepto:

```{python}
m_2 = r * s_x / s_y
b_2 = mu_x - m_2 * mu_y

print("pendiente", m_2)
print("constante", b_2)
```

De nuevo vemos una regresi√≥n a la media: la predicci√≥n para el padre est√° m√°s cerca de la media del padre que lo que estan las alturas del hijo $y$ al promedio del hijo.

Aqu√≠ hay un gr√°fico que muestra las dos l√≠neas de regresi√≥n:

```{python}

import matplotlib.pyplot as plt
import seaborn as sns

# Crear el gr√°fico utilizando Matplotlib y Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)
plt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')
plt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')
plt.legend()
plt.xlabel('Father Height')
plt.ylabel('Son Height')
plt.title('Scatter Plot with Regression Lines')
plt.show()

```


con azul para la predicci√≥n de las alturas del hijo con las alturas del padre y rojo para la predicci√≥n de las alturas del padre con las alturas del hijo.

::: callout-tip

## Taller aplicacci√≥n 2: Altura de padres e hijos


1) Cargue los datos de `GaltonFamilies` desde el HistData. Los ni√±os de cada familia est√°n ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado `galton_heights` seleccionando ni√±os y ni√±as al azar. (HINT: use `sample`).

2) Haga una gr√°fica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.

3) Calcular la correlaci√≥n para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.

4) Plotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).

5) Obtener el modelo de regresi√≥n e interpretar los coeficientes.
:::