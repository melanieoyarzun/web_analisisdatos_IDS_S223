[
  {
    "objectID": "sesion0.html",
    "href": "sesion0.html",
    "title": "Presentación del curso",
    "section": "",
    "text": "Hola! Soy Melanie y seré la docente de este curso, en el que aprenderás los fundamentos del análisis de datos, tanto desde una perspectiva teórica como práctica (en Python).\nMe pueden contactar al mail melanie.oyarzun@udd.cl\n\n\nPuedes ver las slides de este documento en slides seseion introductoria\n\n\n\n\n\n\n\n\nRevisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0.html#links",
    "href": "sesion0.html#links",
    "title": "Presentación del curso",
    "section": "",
    "text": "Puedes ver las slides de este documento en slides seseion introductoria"
  },
  {
    "objectID": "sesion0.html#en-la-sesión-de-hoy",
    "href": "sesion0.html#en-la-sesión-de-hoy",
    "title": "Presentación del curso",
    "section": "",
    "text": "Revisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0.html#contexto-en-el-programa-de-magister",
    "href": "sesion0.html#contexto-en-el-programa-de-magister",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la línea de desarrollo de data science.\nEsta asignatura tributa, a través de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Magíster en Data Science:\n\nAplicar teorías, algoritmos, métodos, técnicas y herramientas básicas y avanzadas de Data Science para analizar, resolver y hacer una evaluación crítica de desafíos complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones.\nComunica efectivamente y argumenta sobre los resultados de su trabajo a públicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento ético, cumpliendo los protocolos y normas que guían su desempeño, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicación de estrategias para utilizar nuevo conocimiento en data science en su ámbito de desempeño."
  },
  {
    "objectID": "sesion0.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "href": "sesion0.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura (resultados de aprendizaje)",
    "text": "Objetivos de la asignatura (resultados de aprendizaje)\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el análisis de datos, utilizando lenguaje técnico afín.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso.\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer información y generar conclusiones basadas en evidencia.\nIdentificar las buenas prácticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0.html#resumen",
    "href": "sesion0.html#resumen",
    "title": "Presentación del curso",
    "section": "Resumen:",
    "text": "Resumen:"
  },
  {
    "objectID": "sesion0.html#detallado",
    "href": "sesion0.html#detallado",
    "title": "Presentación del curso",
    "section": "Detallado",
    "text": "Detallado\n\n\n\n\n\n\nSesión 1: Respondiendo Preguntas con datos\n\n\n\n\n\nFecha: 19 agosto\nObjetivos:\n\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados.\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos.\n\nContenidos:\n\nEl proceso de análisis de datos\n\nPlanteamiento de preguntas\nAdquision y almacenmiento de los datos\nPreparación de los datos\nUna visión general a las metodologías de análisis que veremos en el curso\n\nFormulación de Preguntas y Hipótesis:\n\nImportancia de definir preguntas claras y específicas.\nDiferenciación entre preguntas exploratorias y confirmatorias.\nCreación de hipótesis nulas y alternativas.\n\nHipótesis y Variables:\n\nIdentificación de variables independientes y dependientes.\nRelación entre hipótesis y variables a analizar.\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\nImportancia de la Adquisición y Almacenamiento de Datos:\n\nGarantía de calidad y fiabilidad en la obtención de datos.\nExploración de diferentes fuentes de datos y su impacto en los resultados.\nMetodologias de levantamiento y adquision\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\n\nBibliografia recomendada:\nActividades:\n\nTaller 1 (incluidas en slides 1)\nProyecto clase 1: Conformación de grupos, definición de temas, primeras hipótesis y datos.\n\n\n\n\n\n\n\n\n\n\nSesión 2:Preparando los datos\n\n\n\n\n\nFecha: 26 agosto\nObjetivos:\n\nComprender la importancia del proceso de preparación de datos para el análisis, reconociendo principios y enfoques clave junto con sus ventajas y desventajas.\nDesarrollar habilidades prácticas en la preparación de datos, identificando y abordando problemas comunes como valores faltantes, valores atípicos y formatos inconsistentes, así como enfoques de trabajo eficientes.\n\nContenidos:\n\nEl proceso de preparación de los datos\n\nSignificado y relevancia de la preparación de datos.\nEjemplos reales de cómo la falta de preparación puede afectar los resultados.\n\nPrincipios y enfoques\n\nExtract, Transform, Load (ETL): Proceso fundamental en la preparación de datos.\nData Wrangling: Técnicas para dar forma y estructura a los datos.\nDatos Tidy: Organización y reestructuración para un análisis eficaz.\n\nBuenas Prácticas en la Preparación de Datos\n\nDocumentación y Consistencia\n\nImportancia de la documentación detallada.\nMantener nomenclatura y convenciones consistentes.\n\nValidación y Verificación\n\nValidación cruzada y verificación de integridad.\nCumplir con reglas y restricciones esperadas.\n\nReproducibilidad y Versionado\n\nEntorno de trabajo reproducible (Jupyter Notebooks, R Markdown).\nUtilización de sistemas de control de versiones (GIT).\n\nComunicación y Validación Colaborativa\n\nComunicación clara de pasos y resultados.\nValidación intermedia con colaboradores para feedback.\n\nSeguridad y privacidad de los datos\n\nProblemas comunes presentes en datos\n\nValores faltantes: \n\nEstrategias para manejar valores faltantes.\nDecidir entre imputación, eliminación o conservación.\n\nValores atípicos\nNormalización y estandarización\nErrores de registro Bibliografia recomendada:\n\n\n\n“Practical Statistics for Data Scientists” (Capítulo 2).\n“Doing Data Science” (Capítulo 1).\n\nActividades de aplicación práctica:\n\nTaller 1: Limpieza y análisis descriptivo de datos en la practica con datos de educación (repasa elementos del curso anterior) (sesión 1)\nProyecto:\n\nInicie el proyecto, cree un documento notebook en el cual van a alojarsu proyecto\nExplorar los datos\nDiagnosticar problemas.\nLa hipótesis que pensamos, ¿tienen variables que pueda concretizarlas? ¿Qué variables usar?\n\n\n\n\n\n\n\n\n\n\n\nSesión 3: Introduccion al análisis de regresión\n\n\n\n\n\nFecha: 2 septiembre\nObjetivos:\n\nComprender los conceptos fundamentales del análisis de regresión lineal y su aplicación en la resolución de problemas.\nDesarrollar la habilidad de plantear modelos e interpretar los resultados obtenidos del análisis de regresión, para aplicarlos en la toma de decisiones.\n\nContenidos\n\nIntroducción al Análisis de Regresión:\n\nDefinición y concepto de regresión.\nUso y aplicabilidad en la toma de decisiones.\n\nRegresión Lineal Múltiple:\n\nExtensión del modelo de regresión a múltiples variables predictoras.\nEcuación de regresión lineal múltiple.\n\nInterpretación de Coeficientes:\n\nSignificado e interpretación de los coeficientes de regresión.\nInfluencia de las variables predictoras en la variable de respuesta.\n\nEvaluación de Modelos de Regresión:\n\nUso de medidas como el coeficiente de determinación (R²) y el error estándar de estimación.\nInterpretación de los resultados de evaluación.\n\nIncorporación de Variables Categóricas:\n\nTransformación de variables categóricas en variables numéricas.\nInterpretación de coeficientes para variables categóricas.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nPlantear modelos de regresión que implementen las hipótesis del proyecto\nEstimar e interpretar modelos\n\n\n\n\n\n\n\n\n\n\nSesión 4: Profundizando en Análisis de Regresión, Supuestos y Limitaciones\n\n\n\n\n\nFecha: 9 septiembre\nObjetivos:\n\nExplorar los supuestos y limitaciones asociados al análisis de regresión y desarrollar estrategias para manejar problemas comunes.\nAplicar estrategias prácticas para identificar y abordar problemas en el análisis de regresión.\n\nContenidos\n\nSupuestos en el Análisis de Regresión:\n\nIdentificación de supuestos clave: linealidad, independencia, homoscedasticidad y normalidad.\nSignificado de cada supuesto y su importancia en la interpretación de resultados.\n\nIdentificación de Problemas en la Regresión:\n\nIdentificación y manejo de outliers en los datos.\nReconocimiento de la heterocedasticidad y sus implicaciones.\nDetección de la no-normalidad de los residuos.\n\nEstrategias para Manejar Problemas:\n\nTransformación de variables para abordar problemas de linealidad.\nMétodos para reducir la influencia de outliers.\nUso de transformaciones para tratar la heterocedasticidad.\nPruebas y técnicas para verificar y mejorar la normalidad.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nRevisar supuestos de modelo de regresión\nDiscutir problemas de identificación y limitaciones\n\n\n\n\n\n\n\n\n\n\nSesión 5: Introduccion al análisis de series de tiempo\n\n\n\n\n\nFecha: 30 septiembre\nObjetivos:\n\nIdentificar las características y particularidades de los datos de series de tiempo, comprendiendo sus aplicaciones profesionales.\nRealizar un análisis exploratorio de una serie de tiempo, identificando características clave para su modelamiento.\n\nContenidos\n\nConceptos Básicos de Series de Tiempo:\n\nDefinición y características de una serie de tiempo.\nEjemplos de aplicaciones en distintos campos profesionales.\n\nParticularidades de los Datos Temporales:\n\nDependencia temporal y autocorrelación.\nTendencias, estacionalidad y ciclos.\n\nAplicaciones Profesionales:\n\nCasos de estudio en finanzas, economía, medicina y otros campos.\nCómo el análisis de series de tiempo puede brindar insights valiosos.\n\nBúsqueda y Reorganización de Datos Temporales:\n\nFuentes de datos para series de tiempo (bases de datos, APIs, archivos).\nImportancia de la temporalidad y el orden en los datos.\n\nVisualización y Exploración Inicial:\n\nGráficos de línea y dispersión para identificar tendencias y patrones.\nEstudio de estacionalidad y ciclos mediante gráficos.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 6: Modelando series temporales\n\n\n\n\n\nFecha: 7 occtubre\nObjetivos:\n\nComprender los conceptos y aplicaciones de los modelos ARIMA y VAR en el análisis de series temporales.\nEvaluar las ventajas y desventajas de los métodos estadísticos para el análisis de series de tiempo y seleccionar la técnica más adecuada.\n\nContenidos\n\nModelos ARIMA:\n\nDefinición y componentes de los modelos ARIMA.\nIdentificación, Estimación y Validación de un modelo ARIMA.\nUso de correlogramas y gráficos ACF/PACF para la identificación.\n\nModelos VAR (Vector Autoregressive):\n\nIntroducción a los modelos VAR y su aplicación.\nUso de matrices de coeficientes para representar relaciones entre variables.\n\nVentajas y Desventajas de los Métodos Estadísticos:\n\nUso de modelos estadísticos en comparación con otros enfoques.\nLimitaciones y supuestos asociados a los modelos ARIMA y VAR.\n\nSelección del Método Adecuado:\n\nCriterios para elegir entre modelos ARIMA y VAR.\nConsideraciones al evaluar las alternativas disponibles.\n\nOtros modelos\n\n\nGARCH\nSARIMA y SARIMAX\nAlisado exponencial\nCambio estructural\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 7: Principales lecciones para el análisis de datos y presentaciones de proyectos\n\n\n\n\n\nFecha: TBA\nObjetivos:\n\nCerrar el curso, poniendo en contexto las principales herramientas de análisis de datos.\nPresentar un proyecto de análisis de datos\nRecibir feedback y propuestas de mejoras, tanto del trabajo propio como el de sus compañeros.\n\nContenidos\nBibliografía recomendada\nActividades de aplicación práctica\n\nProyecto: Presentaciones finales"
  },
  {
    "objectID": "sesion3.html",
    "href": "sesion3.html",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Slides Sesión 3"
  },
  {
    "objectID": "sesion3.html#usos-de-las-regresiones",
    "href": "sesion3.html#usos-de-las-regresiones",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Usos de las regresiones",
    "text": "Usos de las regresiones\nLas regresiones tienen tres principales usos:\n\nDescribir un fenómeno\nProbar hipótesis sobre ciertas teorías\nRealizar predicciones"
  },
  {
    "objectID": "sesion3.html#regresión-simple-y-scatterplot",
    "href": "sesion3.html#regresión-simple-y-scatterplot",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Regresión simple y scatterplot",
    "text": "Regresión simple y scatterplot\nPor ejemplo, pensemos en la relación entre los años de educación y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente economía.\nPodriamos pensar que ambas variables se encuentras relacionadas.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 años\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nBásica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nSí\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nBásica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegión de Ñuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nTécnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserción\nMedia humanista completa\nNaN\n\n\n\n\n\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregación\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por región\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegión de Tarapacá\n658026.6250\n11.679582\n\n\n1\nRegión de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegión de Atacama\n666128.3125\n11.126735\n\n\n3\nRegión de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegión de Valparaíso\n611298.1250\n11.559877\n\n\n\n\n\n\n\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por región)')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de región a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la última palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Etiquetas de Región (Última Palabra)')\nplt.show()\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relación positiva: a mayor escolaridad promedio, mayor salario promedio por región."
  },
  {
    "objectID": "sesion3.html#especificación",
    "href": "sesion3.html#especificación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Especificación",
    "text": "Especificación\nLlamamos especifiación al precisar la relación entre las variables que deseamos estimar.\nEn nuestro caso, la función base que queremos entender es entre salario y educación:\n\\[ \\text{Salario} = f(Educacion))\\]\nEste es una relación teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales: - agregar el error aleatorio - especificar una forma funcional - definir una forma de medir las variables en los datos\nEn nuestro caso, entonces el modelo especificado sería:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\\]"
  },
  {
    "objectID": "sesion3.html#interpretación",
    "href": "sesion3.html#interpretación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Interpretación",
    "text": "Interpretación\nCon nuestro modelo especificado:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\\]\nPodemos interpretar \\(\\beta\\) y \\(alpha\\):\n\n\\(\\beta = \\frac{\\partial ingr}{\\partial educ}\\): un año adiciónal de educación, en cuanto incrementa el salario (si nada más cambia)\n\\(\\alpha\\) valor esperado de y, si x=0…"
  },
  {
    "objectID": "sesion3.html#modelo-poblaciónal-y-estimación",
    "href": "sesion3.html#modelo-poblaciónal-y-estimación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Modelo poblaciónal y estimación",
    "text": "Modelo poblaciónal y estimación\nEste modelo especificado esta definido en la población:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\\]\npero necesitamos calcularlo con la muestra…. por lo cual tenemos estimadores para los coeficientes poblacionales!\n\\[\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{años educación}_i \\]"
  },
  {
    "objectID": "sesion3.html#modelo-poblaciónal-y-estimación-1",
    "href": "sesion3.html#modelo-poblaciónal-y-estimación-1",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Modelo poblaciónal y estimación",
    "text": "Modelo poblaciónal y estimación\nEl método más comun de estimación es el de los mínimos cuadrados ordinarios. Veremos detalles sobre la estimación, supuestos, propiedades estadísticas la proxima sesión.\nPor ahora, pensaremos que es el método que busca la línea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresión).\n\\[ \\hat{\\mu}_i= y_i-\\hat{y}_i\\]\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuración del estilo del gráfico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gráfico de dispersión con la línea de regresión\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresión lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el término constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar líneas que conecten cada punto a la línea de regresión\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # Línea que conecta el punto a la línea de regresión\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresión y residuos')\nplt.show()\n\n\n\n\nEs decir, minimiza $_{i}^{n} _i $"
  },
  {
    "objectID": "sesion3.html#modelo-estimado",
    "href": "sesion3.html#modelo-estimado",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Modelo estimado",
    "text": "Modelo estimado\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regiónTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        13:57:56   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n&gt;=20 ... continuing anyway, n=16\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        13:57:56   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\nPodemos ver que un año adicional de educación ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¿y la constante, como la podemos interpretar?"
  },
  {
    "objectID": "sesion3.html#modelos-simples-y-múltiples",
    "href": "sesion3.html#modelos-simples-y-múltiples",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Modelos simples y múltiples",
    "text": "Modelos simples y múltiples\nMuchas veces una sola variable no es suficiente para describir bien un fenómeno. Necesitamos incluir más variables.\nEsto puede ser: - Una nueva variable - Una forma funcional no lineal de la variable ya incluida\nNuestra interpretación del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores.\nProbemos, agregar edad al modelo:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        13:57:56   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nEs muy usual, agregar edad al cuadrado…. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer…\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        13:57:56   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "sesion3.html#un-poco-más-sobre-interpretación",
    "href": "sesion3.html#un-poco-más-sobre-interpretación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Un poco más sobre interpretación",
    "text": "Un poco más sobre interpretación\nLos principales elementos que hay que interpretar en un modelo de regresión lineal son los coeficientes de los predictores:\n\n\\(\\beta_0\\) es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta \\(y\\), cuando todos los predictores son cero.\n\\(\\beta_j\\) los coeficientes de regresión parcial de cada predictor indican el cambio promedio esperado de la variable respuesta 𝑦 al incrementar en una unidad de la variable predictora \\(x_j\\), manteniéndose constantes el resto de variables. (“Ceteris paribus”))\n\nLa magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.\n\nPara poder determinar qué impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviación estándar) las variables predictoras previo ajuste del modelo. En este caso, \\(\\beta_0\\) se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y \\(\\beta_j\\) el cambio promedio esperado de la variable respuesta al incrementar en una desviación estándar la variable predictora \\(x_j\\), manteniéndose constantes el resto de variables.\nSi bien los coeficientes de regresión suelen ser el primer objetivo de la interpretación de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condición de normalidad…etc.). Estos últimos suelen ser tratados con poco detalle cuando el único objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta."
  },
  {
    "objectID": "sesion3.html#causalidad-regresión-y-correlación",
    "href": "sesion3.html#causalidad-regresión-y-correlación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Causalidad, regresión y correlación",
    "text": "Causalidad, regresión y correlación\nImportante tener en cuenta\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relación entre las variables de interés. Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes más altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociación significativa entre las dos variables.\nUn diagrama de dispersión puede ser una herramienta útil para determinar la fuerza de la relación entre dos variables. Si parece no haber asociación entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersión no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresión lineal a los datos probablemente no proporcionará un modelo útil.\nUna valiosa medida numérica de asociación entre dos variables es el coeficiente de correlación, que es un valor entre -1 y 1 que indica la fuerza de la asociación de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3.html#estudio-de-caso-es-hereditaria-la-altura",
    "href": "sesion3.html#estudio-de-caso-es-hereditaria-la-altura",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Estudio de caso: ¿es hereditaria la altura?",
    "text": "Estudio de caso: ¿es hereditaria la altura?\nTenemos acceso a los datos de altura de familias recolectado por Galton, a través del paquete HistData. Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\ngalton_data.head(4)\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\nmidparentHeight\nchildren\nchildNum\ngender\nchildHeight\n\n\n\n\n0\n001\n78.5\n67.0\n75.43\n4\n1\nmale\n73.2\n\n\n1\n001\n78.5\n67.0\n75.43\n4\n2\nfemale\n69.2\n\n\n2\n001\n78.5\n67.0\n75.43\n4\n3\nfemale\n69.0\n\n\n3\n001\n78.5\n67.0\n75.43\n4\n4\nfemale\n69.0\n\n\n\n\n\n\n\nPara imitar el análisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head(4)\n\n\n\n\n\n\n\n\nfather\nson\n\n\n\n\n0\n78.5\n73.2\n\n\n1\n75.5\n72.5\n\n\n2\n75.0\n71.0\n\n\n3\n75.0\n68.5\n\n\n\n\n\n\n\nEn los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.\nSupongamos que se nos pidiera que resumiéramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones están bien aproximadas por la distribución normal, podríamos usar los dos promedios y dos desviaciones estándar como resúmenes:\n\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nresumen_estadistico.head()\n\n\n\n\n\n\n\n\npromedio_padre\nsd_padre\npromedio_hijo\nsd_hijo\n\n\n\n\n0\n69.098883\n2.546555\n69.120112\n2.622362\n\n\n\n\n\n\n\nSin embargo, este resumen no describe una característica importante de los datos: la tendencia de que cuanto más alto es el padre, más alto es el hijo.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gráfico de dispersión con línea de regresión\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relación entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\nAprenderemos que el coeficiente de correlación es un resumen informativo de cómo dos variables se mueven juntas y luego veremos cómo esto puede ser usado para predecir una variable usando la otra, en una regresión."
  },
  {
    "objectID": "sesion3.html#taller-de-aplicación-2-caso-aplicación-cursos-de-verano",
    "href": "sesion3.html#taller-de-aplicación-2-caso-aplicación-cursos-de-verano",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Taller de aplicación 2: Caso aplicación: Cursos de Verano",
    "text": "Taller de aplicación 2: Caso aplicación: Cursos de Verano\n\n\n\n\n\n\nTaller de aplicación 2: Pregunta 1\n\n\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que queríamos responder:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\n\nPlantee un modelo de regresión que con los datos disponibles quisieramos estimar.\nGrafique la dispersión y la recta de regresión estimada.\nEstime el modelo simple e interprete"
  },
  {
    "objectID": "sesion3.html#regresión-pero-y-la-correlación",
    "href": "sesion3.html#regresión-pero-y-la-correlación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "¿Regresión?… pero ¿Y la correlación?",
    "text": "¿Regresión?… pero ¿Y la correlación?\n\nAmbos están muy relacionados.\nAprenderemos que el coeficiente de correlación es un resumen informativo de cómo dos variables se mueven juntas…\ny luego veremos cómo esto puede ser usado para predecir una variable usando la otra y modelado en una regresión"
  },
  {
    "objectID": "sesion3.html#el-coeficiente-de-correlación",
    "href": "sesion3.html#el-coeficiente-de-correlación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "El coeficiente de correlación",
    "text": "El coeficiente de correlación\nEl coeficiente de correlación se define para una lista de pares \\((x_1,y_1),...(x_n,y_n)\\) como la media de los productos de los valores normalizados:\n\\[\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n\\]\nDónde \\(\\mu\\) son promedios y \\(\\sigma\\) son desviaciones estándar. La letra griega para r, \\(\\rho\\) se utiliza comúnmente en los libros de estadística para denotar la correlación, porque es la primera letra de regresión. Pronto aprenderemos sobre la conexión entre correlación y regresión.\nPodemos representar la fórmula anterior con el código usando:\nrho &lt;- np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\nPodemos representar la fórmula anterior con el siguiente código usando:\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aquí\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aquí\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n\n0.9999999999999998\n\n\nLa correlación entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\):\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviación estándar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviación Estándar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviación Estándar de la Altura del Padre:\", sd_father)\n\n# Crear el gráfico de dispersión\nplt.scatter(galton_heights['father'], StandardScaler().fit_transform(galton_heights[['father']]))\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Padre Estandarizada\")\nplt.title(\"Relación entre Altura del Padre y Altura del Padre Estandarizada\")\nplt.show()\n\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviación Estándar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviación Estándar de la Altura del Padre: 2.546555038637643\n\n\n\n\n\nLa correlación entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\).\n\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlación:\", correlation_coefficient)\n\nCoeficiente de Correlación: 0.4868259925112461\n\n\n\nimport pandas as pd\n\n# Generar datos simulados usando la biblioteca faux\ndat = pd.DataFrame(np.random.multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([1, 1, 1, 1, 1, 1]), size=100),\n                   columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n\nprint(dat)\n\n# Calcular la correlación entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n           A         B         C         D         E         F\n0  -0.341648  0.116020  2.277212  0.292145  1.164207  1.454844\n1  -0.574601 -1.376076  0.849175 -0.331692  0.534056  0.616964\n2  -1.346341 -1.466495 -1.525634  0.074053  0.632263  0.373107\n3  -1.317879  1.225713 -0.363807  0.544034 -1.344250  1.198290\n4  -0.443862 -1.315814 -0.916430  0.419516 -0.771529  1.095978\n..       ...       ...       ...       ...       ...       ...\n95  0.063052 -0.305811 -0.986839  0.926453  0.191580 -2.430414\n96 -2.056289  1.048124  0.958648 -0.424067  1.561997 -0.168998\n97 -0.685922 -2.506629  2.436036 -1.473975 -1.937537 -0.549623\n98  1.008418  0.230475  0.260994  1.729545 -1.918848 -0.709950\n99 -1.046753  2.880363  0.154750  0.715915 -0.156903 -0.364088\n\n[100 rows x 6 columns]\n0.4075594018787953\n\n\nPara ver cómo se ven los datos para los diferentes valores de \\(\\rho\\) aquí hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\n\n\nimage"
  },
  {
    "objectID": "sesion3.html#la-correlación-de-la-muestra-es-una-variable-aleatoria",
    "href": "sesion3.html#la-correlación-de-la-muestra-es-una-variable-aleatoria",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "La correlación de la muestra es una variable aleatoria",
    "text": "La correlación de la muestra es una variable aleatoria\nAntes de continuar conectando la correlación con la regresión, recordemos la variabilidad aleatoria.\nEn la mayoría de las aplicaciones de la ciencia de datos, observamos datos que incluyen variación aleatoria.\nPor ejemplo, en muchos casos, no se observan datos para toda la población de interés, sino para una muestra aleatoria. Al igual que con el promedio y la desviación estándar, la correlación de la muestra es la estimación más comúnmente utilizada de la correlación de la población. Esto implica que la correlación que calculamos y usamos como resumen es una variable aleatoria.\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra población. Un genetista menos afortunado sólo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlación de la muestra se puede calcular con:\n\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tamaño 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlación entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlación en la Muestra:\", correlation_coefficient)\n\nCoeficiente de Correlación en la Muestra: 0.5461944437719961\n\n\nR es una variable aleatoria. Podemos ejecutar una simulación de Monte Carlo para ver su distribución:\n\nNota: el objetivo principal de la simulación de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir cómo van a evolucionar.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlación\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlación\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlación\")\nplt.show()\n\n\n\n\nVemos que el valor esperado de R es la correlación de la población:\n\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlación:\", mean_R)\n\nMedia de Coeficientes de Correlación: 0.48653710780288484\n\n\ny que tiene un error estándar relativamente alto en relación con el rango de valores que puede tomar R:\n\nsd_R = np.std(R)\nprint(\"Desviación Estándar de Coeficientes de Correlación:\", sd_R)\n\nDesviación Estándar de Coeficientes de Correlación: 0.051105220051558105\n\n\nPor lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\nAdemás, tenga en cuenta que debido a que la correlación de la muestra es un promedio de extracciones independientes, el teorema del límite central realmente funciona. Por lo tanto, para \\(N\\) lo suficientemente grande la distribución de \\(R\\) es aproximadamente normal con el valor esperado \\(\\rho\\). La desviación estándar, que es algo compleja de derivar, es: \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nEn nuestro ejemplo, \\(N=25\\) no parece ser lo suficientemente grande para que la aproximación sea buena:\n\nNota: El gráfico Q-Q, o gráfico cuantitativo, es una herramienta gráfica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribución teórica como una Normal o exponencial. Por ejemplo, si realizamos un análisis estadístico que asume que nuestra variable dependiente está Normalmente distribuida, podemos usar un gráfico Q-Q-Normal para verificar esa suposición. https://data.library.virginia.edu/understanding-q-q-plots/\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlación\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tamaño de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gráfico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Teóricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gráfico QQ-plot para los Coeficientes de Correlación\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # Línea de referencia\nplt.show()\n\n\n\n\nSi N aumenta verás que la distribución converge a una normal."
  },
  {
    "objectID": "sesion3.html#la-correlación-no-siempre-es-un-resumen-útil",
    "href": "sesion3.html#la-correlación-no-siempre-es-un-resumen-útil",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "La correlación no siempre es un resumen útil",
    "text": "La correlación no siempre es un resumen útil\nLa correlación no siempre es un buen resumen de la relación entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlación de 0,82:\n\n\n\nimage\n\n\nLa correlación sólo tiene sentido en un contexto particular. Para ayudarnos a entender cuándo es que la correlación es significativa como estadística de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudará a motivar y definir la regresión lineal. Comenzamos demostrando cómo la correlación puede ser útil para la predicción."
  },
  {
    "objectID": "sesion3.html#correlación-espuria",
    "href": "sesion3.html#correlación-espuria",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Correlación espuria",
    "text": "Correlación espuria\nEl siguiente ejemplo cómico subraya que la correlación no es causalidad. Muestra una fuerte correlación entre las tasas de divorcio y el consumo de margarina.\n\n\n\nimage\n\n\n(Acá pueden encontrar más http://tylervigen.com/old-version.html)\n¿Significa esto que la margarina causa divorcios? ¿O los divorcios hacen que la gente coma más margarina? Por supuesto que la respuesta a estas dos preguntas es no. Esto es sólo un ejemplo de lo que llamamos una correlación espuria.\nLos casos presentados en el sitio de correlación espuria son todos casos de lo que generalmente se denomina dragado de datos (data dredging), pesca de datos (data fishing) o espionaje de datos (data snooping). Es básicamente una forma de lo que en los EE.UU. se llama “cherry picking”. Un ejemplo de dragado de datos sería si miras a través de muchos resultados producidos por un proceso aleatorio y escoges el que muestra una relación que apoya una teoría que se quiere defender."
  },
  {
    "objectID": "sesion3.html#varianza-explicada",
    "href": "sesion3.html#varianza-explicada",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Varianza explicada",
    "text": "Varianza explicada\nLa teoría de la normalidad bivariada también nos dice que la desviación estándar de la distribución condicional descrita anteriormente es:\n\\[\nSD(Y|X=x)=\\sigma_Y\\sqrt{1-\\rho^2}\n\\]\nPara ver por qué esto es intuitivo, note que sin condicionamiento, \\(SD(Y)=\\sigma_Y\\) estamos viendo la variabilidad de todos los hijos. Pero una vez que los condicionamos, sólo estamos viendo la variabilidad de los hijos con un padre que mide 72 pulgadas de alto. Este grupo tenderá a ser “algo más” alto (que el promedio), por lo que la desviación estándar se reduce.\nEspecíficamente, se reduce a \\(\\sqrt{1-\\rho^2}=\\sqrt{1-0.25}=0.86\\) de lo que era originalmente. Podríamos decir que la estatura del padre “explica” el 14% de la variabilidad de estatura del hijo.\nLa frase “\\(X\\) explica tal o cual porcentaje de la variabilidad” se utiliza comúnmente en papers académicos. En este caso, este porcentaje se refiere realmente a la desviación (SD al cuadrado). Por lo tanto, si los datos son normales bivariados, la varianza se reduce en \\(1-\\rho^2\\) por lo que decimos que \\(X\\) explica \\(1-(1-\\rho^2)=\\rho^2\\) (la correlación al cuadrado) de la varianza.\nPero es importante recordar que la afirmación de “varianza explicada” sólo tiene sentido cuando los datos se aproximan mediante una distribución normal bivariada."
  },
  {
    "objectID": "sesion3.html#cuidado-hay-dos-líneas-de-regresión",
    "href": "sesion3.html#cuidado-hay-dos-líneas-de-regresión",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Cuidado: hay dos líneas de regresión",
    "text": "Cuidado: hay dos líneas de regresión\nCalculamos una línea de regresión para predecir la altura del hijo desde la altura del padre.\nUsamos estos cálculos:\n\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviación estándar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviación estándar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlación entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n\n0.489950313647045\n2.5644170903922188\n2.5850810242285687\n69.08938547486034\n69.2223463687151\n\n\n\n# Calcular la pendiente de la primera línea de regresión\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera línea de regresión\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n\npendiente 0.4938983067025557\nconstante 35.09921587156143\n\n\nLo que nos da la función \\(E(Y|X=x)=28,8+0.44x\\).\n¿Y si queremos predecir la estatura del padre basándonos en la del hijo?\nEs importante saber que esto no se determina calculando la función inversa!.\nNecesitamos computar \\(E(X∣Y=y)\\). Dado que los datos son aproximadamente normales bivariados, la teoría descrita anteriormente nos dice que esta expectativa condicional seguirá una línea con pendiente e intercepto:\n\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n\npendiente 0.486033879009441\nconstante 35.44497995513865\n\n\nDe nuevo vemos una regresión a la media: la predicción para el padre está más cerca de la media del padre que lo que estan las alturas del hijo \\(y\\) al promedio del hijo.\nAquí hay un gráfico que muestra las dos líneas de regresión:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gráfico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()\n\n\n\n\ncon azul para la predicción de las alturas del hijo con las alturas del padre y rojo para la predicción de las alturas del padre con las alturas del hijo.\n\n\n\n\n\n\nTaller aplicacción 2: Altura de padres e hijos\n\n\n\n\nCargue los datos de GaltonFamilies desde el HistData. Los niños de cada familia están ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado galton_heights seleccionando niños y niñas al azar. (HINT: use sample).\nHaga una gráfica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nCalcular la correlación para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nPlotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\nObtener el modelo de regresión e interpretar los coeficientes."
  },
  {
    "objectID": "sesion3_notas.html",
    "href": "sesion3_notas.html",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "En las aplicaciones de la ciencia de datos, es muy común estar interesado en la relación entre dos o más variables.\nEl análisis de regresión es una técnica en la cual buscamos encontrar una función que pueda describir la relación observada en los datos entre dos o mas variables.\nPor ejemplo, una persona podría querer relacionar los pesos de los individuos con sus alturas…\n\n¿Son los más altos más pesados?\n…y¿cuánto más pesados?\n\nPensemos en el caso más sencillo: una regresión lineal simple o univariada. Tenemos una variable que deseamos explicar o predecir (Y) como función de otra (X).\nPara esto, buscamos la pendiente e intercepto de una funciónla recta de la forma:\nY=α+βXY = \\alpha + \\beta X\nque se ajuste mejor al conjunto de datos con los que se cuenta.\ndonde XX es la variable explicativa e YY es la variable dependiente. La pendiente de la recta es bb, y aa es la intersección (el valor de yy cuando x=0x = 0).\n\nPara esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistemática o que se puede explicar directamente con una o más variables independientes (Xs o regresores) y otra que es no sistemática o error (μ\\mu o epsilonepsilon) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fenómeno.\n\nLa parte sistemática entonces la describimos con una forma funcional, que depende de otras variables o regresores.\nEsta forma funcional puede ser lineal univariada, lineal múltiple o no lineal. El tipo de forma funcional, definirá el tipo de regresión de la que estemos hablando.\nVentajas del análisis de regersión: es facil describir cuantitaivamente una relación.\nEsquemáticamente, los elementos son:\n\n\n\nLas regresiones tienen tres principales usos:\n\nDescribir un fenómeno\nProbar hipótesis sobre ciertas teorías\nRealizar predicciones\n\n\n\n\nPor ejemplo, pensemos en la relación entre los años de educación y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente economía.\nPodriamos pensar que ambas variables se encuentras relacionadas.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 años\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nBásica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nSí\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nBásica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegión de Ñuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nTécnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserción\nMedia humanista completa\nNaN\n\n\n\n\n\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregación\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por región\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegión de Tarapacá\n658026.6250\n11.679582\n\n\n1\nRegión de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegión de Atacama\n666128.3125\n11.126735\n\n\n3\nRegión de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegión de Valparaíso\n611298.1250\n11.559877\n\n\n\n\n\n\n\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por región)')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de región a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la última palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Etiquetas de Región (Última Palabra)')\nplt.show()\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relación positiva: a mayor escolaridad promedio, mayor salario promedio por región.\n\n\n\nLlamamos especifiación al precisar la relación entre las variables que deseamos estimar.\nEn nuestro caso, la función base que queremos entender es entre salario y educación:\nSalario=f(Educacion)) \\text{Salario} = f(Educacion))\nEste es una relación teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales: - agregar el error aleatorio - especificar una forma funcional - definir una forma de medir las variables en los datos\nEn nuestro caso, entonces el modelo especificado sería:\ningreso del trabajoi=α+βaños educacióni+μi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\n\n\n\nCon nuestro modelo especificado:\ningreso del trabajoi=α+βaños educacióni+μi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\nPodemos interpretar β\\beta y alphaalpha:\n\nβ=∂ingr∂educ\\beta = \\frac{\\partial ingr}{\\partial educ}: un año adiciónal de educación, en cuanto incrementa el salario (si nada más cambia)\nα\\alpha valor esperado de y, si x=0…\n\n\n\n\nEste modelo especificado esta definido en la población:\ningreso del trabajoi=α+βaños educacióni+μi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\npero necesitamos calcularlo con la muestra…. por lo cual tenemos estimadores para los coeficientes poblacionales!\ningreso del trabajôi=α̂+β̂años educacióni\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{años educación}_i \n\n\n\nEl método más comun de estimación es el de los mínimos cuadrados ordinarios. Veremos detalles sobre la estimación, supuestos, propiedades estadísticas la proxima sesión.\nPor ahora, pensaremos que es el método que busca la línea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresión).\nμ̂i=yi−ŷi \\hat{\\mu}_i= y_i-\\hat{y}_i\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuración del estilo del gráfico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gráfico de dispersión con la línea de regresión\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresión lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el término constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar líneas que conecten cada punto a la línea de regresión\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # Línea que conecta el punto a la línea de regresión\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresión y residuos')\nplt.show()\n\n\n\n\nEs decir, minimiza $_{i}^{n} _i $\n\n\n\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regiónTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:33:51   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n&gt;=20 ... continuing anyway, n=16\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\nPodemos ver que un año adicional de educación ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¿y la constante, como la podemos interpretar?\n\n\n\nMuchas veces una sola variable no es suficiente para describir bien un fenómeno. Necesitamos incluir más variables.\nEsto puede ser: - Una nueva variable - Una forma funcional no lineal de la variable ya incluida\nNuestra interpretación del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores.\nProbemos, agregar edad al modelo:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nEs muy usual, agregar edad al cuadrado…. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer…\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\nLos principales elementos que hay que interpretar en un modelo de regresión lineal son los coeficientes de los predictores:\n\nβ0\\beta_0 es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta yy, cuando todos los predictores son cero.\nβj\\beta_j los coeficientes de regresión parcial de cada predictor indican el cambio promedio esperado de la variable respuesta 𝑦 al incrementar en una unidad de la variable predictora xjx_j, manteniéndose constantes el resto de variables. (“Ceteris paribus”))\n\nLa magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.\n\nPara poder determinar qué impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviación estándar) las variables predictoras previo ajuste del modelo. En este caso, β0\\beta_0 se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y βj\\beta_j el cambio promedio esperado de la variable respuesta al incrementar en una desviación estándar la variable predictora xjx_j, manteniéndose constantes el resto de variables.\nSi bien los coeficientes de regresión suelen ser el primer objetivo de la interpretación de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condición de normalidad…etc.). Estos últimos suelen ser tratados con poco detalle cuando el único objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta.\n\n\n\nImportante tener en cuenta\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relación entre las variables de interés. Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes más altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociación significativa entre las dos variables.\nUn diagrama de dispersión puede ser una herramienta útil para determinar la fuerza de la relación entre dos variables. Si parece no haber asociación entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersión no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresión lineal a los datos probablemente no proporcionará un modelo útil.\nUna valiosa medida numérica de asociación entre dos variables es el coeficiente de correlación, que es un valor entre -1 y 1 que indica la fuerza de la asociación de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3_notas.html#usos-de-las-regresiones",
    "href": "sesion3_notas.html#usos-de-las-regresiones",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Las regresiones tienen tres principales usos:\n\nDescribir un fenómeno\nProbar hipótesis sobre ciertas teorías\nRealizar predicciones"
  },
  {
    "objectID": "sesion3_notas.html#regresión-simple-y-scatterplot",
    "href": "sesion3_notas.html#regresión-simple-y-scatterplot",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Por ejemplo, pensemos en la relación entre los años de educación y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente economía.\nPodriamos pensar que ambas variables se encuentras relacionadas.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 años\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nBásica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nSí\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nBásica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegión de Ñuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nTécnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserción\nMedia humanista completa\nNaN\n\n\n\n\n\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregación\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por región\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegión de Tarapacá\n658026.6250\n11.679582\n\n\n1\nRegión de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegión de Atacama\n666128.3125\n11.126735\n\n\n3\nRegión de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegión de Valparaíso\n611298.1250\n11.559877\n\n\n\n\n\n\n\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por región)')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de región a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la última palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Etiquetas de Región (Última Palabra)')\nplt.show()\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relación positiva: a mayor escolaridad promedio, mayor salario promedio por región."
  },
  {
    "objectID": "sesion3_notas.html#especificación",
    "href": "sesion3_notas.html#especificación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Llamamos especifiación al precisar la relación entre las variables que deseamos estimar.\nEn nuestro caso, la función base que queremos entender es entre salario y educación:\nSalario=f(Educacion)) \\text{Salario} = f(Educacion))\nEste es una relación teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales: - agregar el error aleatorio - especificar una forma funcional - definir una forma de medir las variables en los datos\nEn nuestro caso, entonces el modelo especificado sería:\ningreso del trabajoi=α+βaños educacióni+μi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i"
  },
  {
    "objectID": "sesion3_notas.html#interpretación",
    "href": "sesion3_notas.html#interpretación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Con nuestro modelo especificado:\ningreso del trabajoi=α+βaños educacióni+μi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\nPodemos interpretar β\\beta y alphaalpha:\n\nβ=∂ingr∂educ\\beta = \\frac{\\partial ingr}{\\partial educ}: un año adiciónal de educación, en cuanto incrementa el salario (si nada más cambia)\nα\\alpha valor esperado de y, si x=0…"
  },
  {
    "objectID": "sesion3_notas.html#modelo-poblaciónal-y-estimación",
    "href": "sesion3_notas.html#modelo-poblaciónal-y-estimación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Este modelo especificado esta definido en la población:\ningreso del trabajoi=α+βaños educacióni+μi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\npero necesitamos calcularlo con la muestra…. por lo cual tenemos estimadores para los coeficientes poblacionales!\ningreso del trabajôi=α̂+β̂años educacióni\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{años educación}_i"
  },
  {
    "objectID": "sesion3_notas.html#modelo-poblaciónal-y-estimación-1",
    "href": "sesion3_notas.html#modelo-poblaciónal-y-estimación-1",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "El método más comun de estimación es el de los mínimos cuadrados ordinarios. Veremos detalles sobre la estimación, supuestos, propiedades estadísticas la proxima sesión.\nPor ahora, pensaremos que es el método que busca la línea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresión).\nμ̂i=yi−ŷi \\hat{\\mu}_i= y_i-\\hat{y}_i\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuración del estilo del gráfico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gráfico de dispersión con la línea de regresión\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresión lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el término constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar líneas que conecten cada punto a la línea de regresión\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # Línea que conecta el punto a la línea de regresión\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresión y residuos')\nplt.show()\n\n\n\n\nEs decir, minimiza $_{i}^{n} _i $"
  },
  {
    "objectID": "sesion3_notas.html#modelo-estimado",
    "href": "sesion3_notas.html#modelo-estimado",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Por ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regiónTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:33:51   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n&gt;=20 ... continuing anyway, n=16\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\nPodemos ver que un año adicional de educación ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¿y la constante, como la podemos interpretar?"
  },
  {
    "objectID": "sesion3_notas.html#modelos-simples-y-múltiples",
    "href": "sesion3_notas.html#modelos-simples-y-múltiples",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Muchas veces una sola variable no es suficiente para describir bien un fenómeno. Necesitamos incluir más variables.\nEsto puede ser: - Una nueva variable - Una forma funcional no lineal de la variable ya incluida\nNuestra interpretación del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores.\nProbemos, agregar edad al modelo:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nEs muy usual, agregar edad al cuadrado…. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer…\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "sesion3_notas.html#un-poco-más-sobre-interpretación",
    "href": "sesion3_notas.html#un-poco-más-sobre-interpretación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Los principales elementos que hay que interpretar en un modelo de regresión lineal son los coeficientes de los predictores:\n\nβ0\\beta_0 es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta yy, cuando todos los predictores son cero.\nβj\\beta_j los coeficientes de regresión parcial de cada predictor indican el cambio promedio esperado de la variable respuesta 𝑦 al incrementar en una unidad de la variable predictora xjx_j, manteniéndose constantes el resto de variables. (“Ceteris paribus”))\n\nLa magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.\n\nPara poder determinar qué impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviación estándar) las variables predictoras previo ajuste del modelo. En este caso, β0\\beta_0 se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y βj\\beta_j el cambio promedio esperado de la variable respuesta al incrementar en una desviación estándar la variable predictora xjx_j, manteniéndose constantes el resto de variables.\nSi bien los coeficientes de regresión suelen ser el primer objetivo de la interpretación de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condición de normalidad…etc.). Estos últimos suelen ser tratados con poco detalle cuando el único objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta."
  },
  {
    "objectID": "sesion3_notas.html#causalidad-regresión-y-correlación",
    "href": "sesion3_notas.html#causalidad-regresión-y-correlación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Importante tener en cuenta\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relación entre las variables de interés. Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes más altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociación significativa entre las dos variables.\nUn diagrama de dispersión puede ser una herramienta útil para determinar la fuerza de la relación entre dos variables. Si parece no haber asociación entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersión no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresión lineal a los datos probablemente no proporcionará un modelo útil.\nUna valiosa medida numérica de asociación entre dos variables es el coeficiente de correlación, que es un valor entre -1 y 1 que indica la fuerza de la asociación de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3_notas.html#estudio-de-caso-es-hereditaria-la-altura",
    "href": "sesion3_notas.html#estudio-de-caso-es-hereditaria-la-altura",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Estudio de caso: ¿es hereditaria la altura?",
    "text": "Estudio de caso: ¿es hereditaria la altura?\nTenemos acceso a los datos de altura de familias recolectado por Galton, a través del paquete HistData. Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\ngalton_data.head(4)\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\nmidparentHeight\nchildren\nchildNum\ngender\nchildHeight\n\n\n\n\n0\n001\n78.5\n67.0\n75.43\n4\n1\nmale\n73.2\n\n\n1\n001\n78.5\n67.0\n75.43\n4\n2\nfemale\n69.2\n\n\n2\n001\n78.5\n67.0\n75.43\n4\n3\nfemale\n69.0\n\n\n3\n001\n78.5\n67.0\n75.43\n4\n4\nfemale\n69.0\n\n\n\n\n\n\n\nPara imitar el análisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head(4)\n\n\n\n\n\n\n\n\nfather\nson\n\n\n\n\n0\n78.5\n73.2\n\n\n1\n75.5\n73.5\n\n\n2\n75.0\n71.0\n\n\n3\n75.0\n68.5\n\n\n\n\n\n\n\nEn los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.\nSupongamos que se nos pidiera que resumiéramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones están bien aproximadas por la distribución normal, podríamos usar los dos promedios y dos desviaciones estándar como resúmenes:\n\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nresumen_estadistico.head()\n\n\n\n\n\n\n\n\npromedio_padre\nsd_padre\npromedio_hijo\nsd_hijo\n\n\n\n\n0\n69.098883\n2.546555\n69.263687\n2.567837\n\n\n\n\n\n\n\nSin embargo, este resumen no describe una característica importante de los datos: la tendencia de que cuanto más alto es el padre, más alto es el hijo.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gráfico de dispersión con línea de regresión\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relación entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\nAprenderemos que el coeficiente de correlación es un resumen informativo de cómo dos variables se mueven juntas y luego veremos cómo esto puede ser usado para predecir una variable usando la otra, en una regresión."
  },
  {
    "objectID": "sesion3_notas.html#taller-de-aplicación-2-caso-aplicación-cursos-de-verano",
    "href": "sesion3_notas.html#taller-de-aplicación-2-caso-aplicación-cursos-de-verano",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Taller de aplicación 2: Caso aplicación: Cursos de Verano",
    "text": "Taller de aplicación 2: Caso aplicación: Cursos de Verano\n\n\n\n\n\n\nTaller de aplicación 2: Pregunta 1\n\n\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que queríamos responder:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\n\nPlantee un modelo de regresión que con los datos disponibles quisieramos estimar.\nGrafique la dispersión y la recta de regresión estimada.\nEstime el modelo simple e interprete"
  },
  {
    "objectID": "sesion3_notas.html#regresión-pero-y-la-correlación",
    "href": "sesion3_notas.html#regresión-pero-y-la-correlación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "¿Regresión?… pero ¿Y la correlación?",
    "text": "¿Regresión?… pero ¿Y la correlación?\n\nAmbos están muy relacionados.\nAprenderemos que el coeficiente de correlación es un resumen informativo de cómo dos variables se mueven juntas…\ny luego veremos cómo esto puede ser usado para predecir una variable usando la otra y modelado en una regresión"
  },
  {
    "objectID": "sesion3_notas.html#el-coeficiente-de-correlación",
    "href": "sesion3_notas.html#el-coeficiente-de-correlación",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "El coeficiente de correlación",
    "text": "El coeficiente de correlación\nEl coeficiente de correlación se define para una lista de pares (x1,y1),...(xn,yn)(x_1,y_1),...(x_n,y_n) como la media de los productos de los valores normalizados:\nρ=1n∑i=1n(xi−μxσx)(yi−μyσy)\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n\nDónde μ\\mu son promedios y σ\\sigma son desviaciones estándar. La letra griega para r, ρ\\rho se utiliza comúnmente en los libros de estadística para denotar la correlación, porque es la primera letra de regresión. Pronto aprenderemos sobre la conexión entre correlación y regresión.\nPodemos representar la fórmula anterior con el código usando:\nrho &lt;- np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\nPodemos representar la fórmula anterior con el siguiente código usando:\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aquí\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aquí\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n\n0.9999999999999998\n\n\nLa correlación entre las alturas del padre y del hijo es de aproximadamente 0,40,4:\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviación estándar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviación Estándar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviación Estándar de la Altura del Padre:\", sd_father)\n\n# Crear el gráfico de dispersión\nplt.scatter(galton_heights['father'], StandardScaler().fit_transform(galton_heights[['father']]))\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Padre Estandarizada\")\nplt.title(\"Relación entre Altura del Padre y Altura del Padre Estandarizada\")\nplt.show()\n\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviación Estándar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviación Estándar de la Altura del Padre: 2.546555038637643\n\n\n\n\n\nLa correlación entre las alturas del padre y del hijo es de aproximadamente 0,40,4.\n\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlación:\", correlation_coefficient)\n\nCoeficiente de Correlación: 0.42699639842017706\n\n\n\nimport pandas as pd\n\n# Generar datos simulados usando la biblioteca faux\ndat = pd.DataFrame(np.random.multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([1, 1, 1, 1, 1, 1]), size=100),\n                   columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n\nprint(dat)\n\n# Calcular la correlación entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n           A         B         C         D         E         F\n0  -0.487216 -0.103054 -0.406291  0.144734  0.482508 -1.065099\n1  -2.619254 -0.482619  0.373571 -0.031262 -0.059186  1.284221\n2  -0.606478  0.793955 -0.923926 -0.833852  0.038484  0.057210\n3   0.748645  0.858455  0.849438  0.973093 -0.139669  0.295479\n4  -1.394435  0.086160  0.131287  0.053497 -0.113966  1.327921\n..       ...       ...       ...       ...       ...       ...\n95  0.066146 -0.052701  0.776452  0.885253  0.336985  1.836124\n96 -0.189615  0.674505  0.660422  0.862998 -1.177144  0.924969\n97 -1.168625  1.464250  0.373704 -0.818466 -1.300497 -0.431909\n98 -0.177136  0.473261  0.702792  3.293837  0.889548  0.447885\n99  0.637694 -0.315696  0.463798 -0.975720 -1.828953  2.008513\n\n[100 rows x 6 columns]\n0.4922242607689742\n\n\nPara ver cómo se ven los datos para los diferentes valores de ρ\\rho aquí hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\n\n\nimage"
  },
  {
    "objectID": "sesion3_notas.html#la-correlación-de-la-muestra-es-una-variable-aleatoria",
    "href": "sesion3_notas.html#la-correlación-de-la-muestra-es-una-variable-aleatoria",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "La correlación de la muestra es una variable aleatoria",
    "text": "La correlación de la muestra es una variable aleatoria\nAntes de continuar conectando la correlación con la regresión, recordemos la variabilidad aleatoria.\nEn la mayoría de las aplicaciones de la ciencia de datos, observamos datos que incluyen variación aleatoria.\nPor ejemplo, en muchos casos, no se observan datos para toda la población de interés, sino para una muestra aleatoria. Al igual que con el promedio y la desviación estándar, la correlación de la muestra es la estimación más comúnmente utilizada de la correlación de la población. Esto implica que la correlación que calculamos y usamos como resumen es una variable aleatoria.\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra población. Un genetista menos afortunado sólo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlación de la muestra se puede calcular con:\n\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tamaño 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlación entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlación en la Muestra:\", correlation_coefficient)\n\nCoeficiente de Correlación en la Muestra: 0.4105312650311907\n\n\nR es una variable aleatoria. Podemos ejecutar una simulación de Monte Carlo para ver su distribución:\n\nNota: el objetivo principal de la simulación de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir cómo van a evolucionar.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlación\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlación\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlación\")\nplt.show()\n\n\n\n\nVemos que el valor esperado de R es la correlación de la población:\n\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlación:\", mean_R)\n\nMedia de Coeficientes de Correlación: 0.42583844932004655\n\n\ny que tiene un error estándar relativamente alto en relación con el rango de valores que puede tomar R:\n\nsd_R = np.std(R)\nprint(\"Desviación Estándar de Coeficientes de Correlación:\", sd_R)\n\nDesviación Estándar de Coeficientes de Correlación: 0.053483610252486324\n\n\nPor lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\nAdemás, tenga en cuenta que debido a que la correlación de la muestra es un promedio de extracciones independientes, el teorema del límite central realmente funciona. Por lo tanto, para NN lo suficientemente grande la distribución de RR es aproximadamente normal con el valor esperado ρ\\rho. La desviación estándar, que es algo compleja de derivar, es: 1−r2N−2\\sqrt{\\frac{1-r^2}{N-2}}.\nEn nuestro ejemplo, N=25N=25 no parece ser lo suficientemente grande para que la aproximación sea buena:\n\nNota: El gráfico Q-Q, o gráfico cuantitativo, es una herramienta gráfica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribución teórica como una Normal o exponencial. Por ejemplo, si realizamos un análisis estadístico que asume que nuestra variable dependiente está Normalmente distribuida, podemos usar un gráfico Q-Q-Normal para verificar esa suposición. https://data.library.virginia.edu/understanding-q-q-plots/\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlación\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tamaño de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gráfico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Teóricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gráfico QQ-plot para los Coeficientes de Correlación\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # Línea de referencia\nplt.show()\n\n\n\n\nSi N aumenta verás que la distribución converge a una normal."
  },
  {
    "objectID": "sesion3_notas.html#la-correlación-no-siempre-es-un-resumen-útil",
    "href": "sesion3_notas.html#la-correlación-no-siempre-es-un-resumen-útil",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "La correlación no siempre es un resumen útil",
    "text": "La correlación no siempre es un resumen útil\nLa correlación no siempre es un buen resumen de la relación entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlación de 0,82:\n\n\n\nimage\n\n\nLa correlación sólo tiene sentido en un contexto particular. Para ayudarnos a entender cuándo es que la correlación es significativa como estadística de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudará a motivar y definir la regresión lineal. Comenzamos demostrando cómo la correlación puede ser útil para la predicción."
  },
  {
    "objectID": "sesion3_notas.html#correlación-espuria",
    "href": "sesion3_notas.html#correlación-espuria",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Correlación espuria",
    "text": "Correlación espuria\nEl siguiente ejemplo cómico subraya que la correlación no es causalidad. Muestra una fuerte correlación entre las tasas de divorcio y el consumo de margarina.\n\n\n\nimage\n\n\n(Acá pueden encontrar más http://tylervigen.com/old-version.html)\n¿Significa esto que la margarina causa divorcios? ¿O los divorcios hacen que la gente coma más margarina? Por supuesto que la respuesta a estas dos preguntas es no. Esto es sólo un ejemplo de lo que llamamos una correlación espuria.\nLos casos presentados en el sitio de correlación espuria son todos casos de lo que generalmente se denomina dragado de datos (data dredging), pesca de datos (data fishing) o espionaje de datos (data snooping). Es básicamente una forma de lo que en los EE.UU. se llama “cherry picking”. Un ejemplo de dragado de datos sería si miras a través de muchos resultados producidos por un proceso aleatorio y escoges el que muestra una relación que apoya una teoría que se quiere defender."
  },
  {
    "objectID": "sesion3_notas.html#varianza-explicada",
    "href": "sesion3_notas.html#varianza-explicada",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Varianza explicada",
    "text": "Varianza explicada\nLa teoría de la normalidad bivariada también nos dice que la desviación estándar de la distribución condicional descrita anteriormente es:\nSD(Y|X=x)=σY1−ρ2\nSD(Y|X=x)=\\sigma_Y\\sqrt{1-\\rho^2}\n\nPara ver por qué esto es intuitivo, note que sin condicionamiento, SD(Y)=σYSD(Y)=\\sigma_Y estamos viendo la variabilidad de todos los hijos. Pero una vez que los condicionamos, sólo estamos viendo la variabilidad de los hijos con un padre que mide 72 pulgadas de alto. Este grupo tenderá a ser “algo más” alto (que el promedio), por lo que la desviación estándar se reduce.\nEspecíficamente, se reduce a 1−ρ2=1−0.25=0.86\\sqrt{1-\\rho^2}=\\sqrt{1-0.25}=0.86 de lo que era originalmente. Podríamos decir que la estatura del padre “explica” el 14% de la variabilidad de estatura del hijo.\nLa frase “XX explica tal o cual porcentaje de la variabilidad” se utiliza comúnmente en papers académicos. En este caso, este porcentaje se refiere realmente a la desviación (SD al cuadrado). Por lo tanto, si los datos son normales bivariados, la varianza se reduce en 1−ρ21-\\rho^2 por lo que decimos que XX explica 1−(1−ρ2)=ρ21-(1-\\rho^2)=\\rho^2 (la correlación al cuadrado) de la varianza.\nPero es importante recordar que la afirmación de “varianza explicada” sólo tiene sentido cuando los datos se aproximan mediante una distribución normal bivariada."
  },
  {
    "objectID": "sesion3_notas.html#cuidado-hay-dos-líneas-de-regresión",
    "href": "sesion3_notas.html#cuidado-hay-dos-líneas-de-regresión",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "Cuidado: hay dos líneas de regresión",
    "text": "Cuidado: hay dos líneas de regresión\nCalculamos una línea de regresión para predecir la altura del hijo desde la altura del padre.\nUsamos estos cálculos:\n\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviación estándar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviación estándar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlación entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n\n0.4282667416279721\n2.5644170903922188\n2.6860317955777377\n69.08938547486034\n69.23184357541899\n\n\n\n# Calcular la pendiente de la primera línea de regresión\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera línea de regresión\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n\npendiente 0.44857682836034635\nconstante 38.23994616574076\n\n\nLo que nos da la función E(Y|X=x)=28,8+0.44xE(Y|X=x)=28,8+0.44x.\n¿Y si queremos predecir la estatura del padre basándonos en la del hijo?\nEs importante saber que esto no se determina calculando la función inversa!.\nNecesitamos computar E(X∣Y=y)E(X∣Y=y). Dado que los datos son aproximadamente normales bivariados, la teoría descrita anteriormente nos dice que esta expectativa condicional seguirá una línea con pendiente e intercepto:\n\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n\npendiente 0.4088762289729847\nconstante 40.782130348895464\n\n\nDe nuevo vemos una regresión a la media: la predicción para el padre está más cerca de la media del padre que lo que estan las alturas del hijo yy al promedio del hijo.\nAquí hay un gráfico que muestra las dos líneas de regresión:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gráfico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()\n\n\n\n\ncon azul para la predicción de las alturas del hijo con las alturas del padre y rojo para la predicción de las alturas del padre con las alturas del hijo.\n\n\n\n\n\n\nTaller aplicacción 2: Altura de padres e hijos\n\n\n\n\nCargue los datos de GaltonFamilies desde el HistData. Los niños de cada familia están ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado galton_heights seleccionando niños y niñas al azar. (HINT: use sample).\nHaga una gráfica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nCalcular la correlación para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nPlotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\nObtener el modelo de regresión e interpretar los coeficientes."
  },
  {
    "objectID": "talleres.html",
    "href": "talleres.html",
    "title": "Talleres de aplicación",
    "section": "",
    "text": "Vamos a trabajar 3 tralleres de aplicación:\n\n\n\nActividad\nEnunciado\nFecha de Entrega\n\n\n\n\nTaller 1\nTaller 1 . Enunciado\n15 de septiembre\n\n\nTaller 2\n\nTBA\n\n\nTaller 3\n\nTBA"
  },
  {
    "objectID": "taller1_enunciado.html",
    "href": "taller1_enunciado.html",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Este taller es un conjunto de actividades para poner en práctica y consolidar los contenidos revisados en la primera unidad del curso, en las sesiones 1 y 2.\n\nPuedes realizarlo individualmente o hasta en grupos de 3 personas.\nLa entrega es hasta el 15 de septiembre (para todo el crédito)\nDebes entregar un notebook en el que desarrolle las preguntas y no olvide concluir sus respuestas.\nSerá evaluado mediante la rúbrica de evaluación disponible."
  },
  {
    "objectID": "taller1_enunciado.html#instrucciones",
    "href": "taller1_enunciado.html#instrucciones",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Este taller es un conjunto de actividades para poner en práctica y consolidar los contenidos revisados en la primera unidad del curso, en las sesiones 1 y 2.\n\nPuedes realizarlo individualmente o hasta en grupos de 3 personas.\nLa entrega es hasta el 15 de septiembre (para todo el crédito)\nDebes entregar un notebook en el que desarrolle las preguntas y no olvide concluir sus respuestas.\nSerá evaluado mediante la rúbrica de evaluación disponible."
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-1---bajando-y-formateando-datos-del-banco-mundial",
    "href": "taller1_enunciado.html#pregunta-1---bajando-y-formateando-datos-del-banco-mundial",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "Pregunta 1 - Bajando y formateando datos del Banco Mundial",
    "text": "Pregunta 1 - Bajando y formateando datos del Banco Mundial\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿Pareciera haber tendencias? Explique."
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-2---investigando-sobre-países",
    "href": "taller1_enunciado.html#pregunta-2---investigando-sobre-países",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "Pregunta 2 - Investigando sobre países:",
    "text": "Pregunta 2 - Investigando sobre países:\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?"
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-3---caso-aplicación-ejemplo-ab-test-en-marketing",
    "href": "taller1_enunciado.html#pregunta-3---caso-aplicación-ejemplo-ab-test-en-marketing",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "Pregunta 3 - Caso aplicación: Ejemplo AB test en Marketing:",
    "text": "Pregunta 3 - Caso aplicación: Ejemplo AB test en Marketing:\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreación de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df.head(5)\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n\n\n\n\n\n\n\nPreguntas:\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-4---caso-de-aplicación-datos-de-educación",
    "href": "taller1_enunciado.html#pregunta-4---caso-de-aplicación-datos-de-educación",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "Pregunta 4 - Caso de aplicación datos de educación",
    "text": "Pregunta 4 - Caso de aplicación datos de educación\n\nPregunta de investigación\nNuestro objetivo es responder la siguiente pregunta ficticia de investigación:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\nContexto\nLa pregunta de investigación se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioeconómico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opción de asistir a un curso de verano intensivo durante el verano entre 5 y 6to básico.\nEl curso de verano se enfoca en mejorar las habilidades académicas de preparar la prueba de admisión a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participación en el curso en los resultados académicos de los estudiantes.\n\n\n\nDatos ficticios dispobibles\nLos datos estan disponibles en https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene información sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si participó en el curso de verano, sexo, ingreso del hogar (en logaritmo), educación de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el año 5 como para el año 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene información de cada individuo (con identificador id).\nEste dataset tiene la información si el individuo recibió la carta de invitación para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene información de rendimiento académico antes y después del curso de verano.\n\n\n\nObjetivos:\nLa idea de este taller es poner en práctica los primeros pasos para un análisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada. II. También exploraremos los datos, usaremos estadísticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre los pasos anteriores múltiples veces. También invlocura tomar desiciones por parte de los investigadores, por lo cual la documentación de esta fase es especialmente importante.\nEn nuestro caso, será bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser así.\n\n\nPasos que debe realizar:\n\n1. Preparación inicial de los datos\nTenemos 3 bases de datos con información diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta información, especialmente si estimaremos modelos en base a ésta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido debería tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de información del estudiante si recibió o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de unión person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que acá unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id única tiene un solo colegio, pero sirve de ejemplo en como usar más de una columna mediante.\nUsamos la función summary() para obtener una estadística descriptiva de las variables en el dataset unido.\n\n\n\n2. Limpieza de los datos\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera “tidy” (Según Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observación.\n\n(puedes profundizar y ver más ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observación.\nSi inspeccionamos el número de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del año 2 al 10. Este tipo de datos son de panel.\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y análisi preferimos que esté en formato long. Especialmente cuando modelamos incluyendo efectos fijos También es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos pivot_longer.\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selección de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podríamos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y número de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn último paso que haremos antes de hacer estadística decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviación estándar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto específico de la medida y es más facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estadística descriptiva.\n\n\n3. Estadística descriptiva\nHasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estadísticas descriptivas y también algunos graficos descriptivos.\n3.1 Tablas de etsádistocas descriptivas Incluya la media, la desviación estandar, la mediana, max y min, al menos.\n3.2 Gráficos de estadística descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  },
  {
    "objectID": "sesion2_slides.html#la-preparación-de-los-datos",
    "href": "sesion2_slides.html#la-preparación-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "La preparación de los datos",
    "text": "La preparación de los datos\n\nLa preparación de datos es una fase esencial en el proceso de análisis de datos que involucra una serie de actividades destinadas a garantizar que los datos estén en condiciones óptimas para su posterior análisis.\n\nExtraccion de los datos\nLimpieza\nTransformación\nOrganización"
  },
  {
    "objectID": "sesion2_slides.html#tipos-de-datos",
    "href": "sesion2_slides.html#tipos-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tipos de datos",
    "text": "Tipos de datos\n\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\n\nPodemos clasificar estos datos que recopilamos de muchas maneras:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis."
  },
  {
    "objectID": "sesion2_slides.html#estructura-de-los-datos",
    "href": "sesion2_slides.html#estructura-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Estructura de los datos",
    "text": "Estructura de los datos\n\nLa estructura de un dato se refiere a su formato y codificación."
  },
  {
    "objectID": "sesion2_slides.html#datos-estructurados",
    "href": "sesion2_slides.html#datos-estructurados",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos estructurados",
    "text": "Datos estructurados\n\nConjuntos de tablas forman bases de datos estructuradas.\nTablas son formas organizadas y ordenadas de datos.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente."
  },
  {
    "objectID": "sesion2_slides.html#datos-no-estructurados",
    "href": "sesion2_slides.html#datos-no-estructurados",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos No estructurados",
    "text": "Datos No estructurados\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido."
  },
  {
    "objectID": "sesion2_slides.html#algunos-ejemplos-de-datos-no-estructurados-son",
    "href": "sesion2_slides.html#algunos-ejemplos-de-datos-no-estructurados-son",
    "title": "Sesión 2: Preparando los datos",
    "section": "Algunos ejemplos de datos no estructurados son:",
    "text": "Algunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos."
  },
  {
    "objectID": "sesion2_slides.html#estructurados-vs-no-estructuraods",
    "href": "sesion2_slides.html#estructurados-vs-no-estructuraods",
    "title": "Sesión 2: Preparando los datos",
    "section": "Estructurados vs no estructuraods",
    "text": "Estructurados vs no estructuraods\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes."
  },
  {
    "objectID": "sesion2_slides.html#datos-según-tamano",
    "href": "sesion2_slides.html#datos-según-tamano",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según tamaño",
    "text": "Datos según tamaño\n\nBig Data:\n\nSe refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales.\nBig Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos."
  },
  {
    "objectID": "sesion2_slides.html#datos-según-tamano-1",
    "href": "sesion2_slides.html#datos-según-tamano-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según tamaño",
    "text": "Datos según tamaño\n\nSmall Data:\n\nSe refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data.\nEstos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales.\nA menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas."
  },
  {
    "objectID": "sesion2_slides.html#datos-según-tipo-y-su-operacionalizacion",
    "href": "sesion2_slides.html#datos-según-tipo-y-su-operacionalizacion",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según tipo y su operacionalización",
    "text": "Datos según tipo y su operacionalización\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\nContinuas y discretas"
  },
  {
    "objectID": "sesion2_slides.html#datos-según-temporalidad-y-unidad-de-analisis.",
    "href": "sesion2_slides.html#datos-según-temporalidad-y-unidad-de-analisis.",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según temporalidad y unidad de análisis.",
    "text": "Datos según temporalidad y unidad de análisis.\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nSerie de tiempo\nPanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_slides.html#corte-transversal",
    "href": "sesion2_slides.html#corte-transversal",
    "title": "Sesión 2: Preparando los datos",
    "section": "Corte transversal",
    "text": "Corte transversal"
  },
  {
    "objectID": "sesion2_slides.html#serie-temporal",
    "href": "sesion2_slides.html#serie-temporal",
    "title": "Sesión 2: Preparando los datos",
    "section": "Serie temporal",
    "text": "Serie temporal"
  },
  {
    "objectID": "sesion2_slides.html#panel",
    "href": "sesion2_slides.html#panel",
    "title": "Sesión 2: Preparando los datos",
    "section": "Panel",
    "text": "Panel"
  },
  {
    "objectID": "sesion2_slides.html#leyendo-datos-en-pandas",
    "href": "sesion2_slides.html#leyendo-datos-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo"
  },
  {
    "objectID": "sesion2_slides.html#leyendo-datos-en-pandas-1",
    "href": "sesion2_slides.html#leyendo-datos-en-pandas-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read:\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: automático o definido por el usuario\nDatetime parsing: puede combinar información de múltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con números con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2_slides.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2_slides.html#formato-csv-valores-separados-por-comas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que parece ser\n\n¿Y qué pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2_slides.html#csv-en-pandas",
    "href": "sesion2_slides.html#csv-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nBásica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nBásica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representación de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2_slides.html#json-java-script-object-notation",
    "href": "sesion2_slides.html#json-java-script-object-notation",
    "title": "Sesión 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, números, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores numéricos"
  },
  {
    "objectID": "sesion2_slides.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2_slides.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2_slides.html#orientaciones-posibles-de-json",
    "href": "sesion2_slides.html#orientaciones-posibles-de-json",
    "title": "Sesión 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2_slides.html#extensible-markup-language-xml",
    "href": "sesion2_slides.html#extensible-markup-language-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato más antiguo y auto descriptivo, con estructura jerárquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un método incorporado en Python.\nSe puede usar la librería lxml (también ElementTree)"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-xml",
    "href": "sesion2_slides.html#ejemplo-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo XML",
    "text": "Ejemplo XML"
  },
  {
    "objectID": "sesion2_slides.html#formatos-binarios",
    "href": "sesion2_slides.html#formatos-binarios",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¿Qué es un formato binario?\nPickle: Python’s built-in serialization\n\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\n\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}"
  },
  {
    "objectID": "sesion2_slides.html#formatos-binarios-1",
    "href": "sesion2_slides.html#formatos-binarios-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nHDF5: Librería para almacenar gran cantidad de datos científicos\n\nFormato jerárquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresión\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2_slides.html#bases-de-datos-relacionales",
    "href": "sesion2_slides.html#bases-de-datos-relacionales",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales"
  },
  {
    "objectID": "sesion2_slides.html#bases-de-datos-relacionales-1",
    "href": "sesion2_slides.html#bases-de-datos-relacionales-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\nLas bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\nvínculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos \n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayoría de los sistemas de bases de datos a través de un API común\nSintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.) \nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n\n\n\nimport sqlalchemy as sqla\ndb = sqla.create_engine('sqlite:///mydata.sqlite')\npd.read_sql('select \\* from test', db)"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-estadístico",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-estadístico",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista estadístico",
    "text": "Dirty data: punto de vista estadístico\n\nLos datos son generados desde algún proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsión: algunas muestras se corrompieron por un proceso\nSesgo de selección: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: hay valores que no observamos\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisión y simplicidad"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores están missing, corrompidos, erróneos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un área",
    "text": "Dirty Data: Punto de vista del experto en un área\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¿Qué ocurrió?\n\nLos expertos en un área llevan un modelo implícito de los datos que están testeando\n\nNo siempre necesitas ser un experto en un área para hacer esto\n\n¿Puede una persona correr 500 km por hora?\n¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido común"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinación de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregación es menos susceptible a los errores numéricos\nSer cuidadoso, puede que los datos estén bien…"
  },
  {
    "objectID": "sesion2_slides.html#dónde-se-origina-el-dirty-data",
    "href": "sesion2_slides.html#dónde-se-origina-el-dirty-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Dónde se origina el dirty data?",
    "text": "¿Dónde se origina el dirty data?\n\nLa fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integración de diferentes sets de datos causa problemas\nPropagación del error: se magnifica un error"
  },
  {
    "objectID": "sesion2_slides.html#problemas-dirty-comunes",
    "href": "sesion2_slides.html#problemas-dirty-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs. New York\nPérdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs. dos\nDatos truncados: “Janice Keihanaikukauakahihuliheekahaunaele” se vuelve “Janice - Keihanaikukauakahihuliheek” en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposición\nProblemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017"
  },
  {
    "objectID": "sesion2_slides.html#preparando-los-datos",
    "href": "sesion2_slides.html#preparando-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Preparando los datos",
    "text": "Preparando los datos\nMuchas veces los datos con los que queremos trabajar no están en el formato adecuado para los análisis que querenos realizar."
  },
  {
    "objectID": "sesion2_slides.html#data-wrangling",
    "href": "sesion2_slides.html#data-wrangling",
    "title": "Sesión 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato más significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representación a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2_slides.html#tidy-data",
    "href": "sesion2_slides.html#tidy-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para análisis de corte transversal nuestro ideal es trabajar con Tidy Data\n“Tidy Data is a standar way of mapping the meaning of a dataset to its structure¨\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un estándar deseable para analizar datos."
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-1",
    "href": "sesion2_slides.html#tidy-data-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observación (registro) forma una fila.\nCada dato (valor) está en una celda de la tabla."
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-2",
    "href": "sesion2_slides.html#tidy-data-2",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-3",
    "href": "sesion2_slides.html#tidy-data-3",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:"
  },
  {
    "objectID": "sesion2_slides.html#tidy-workflow",
    "href": "sesion2_slides.html#tidy-workflow",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy & workflow",
    "text": "Tidy & workflow\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados."
  },
  {
    "objectID": "sesion2_slides.html#porqué-datos-tidy",
    "href": "sesion2_slides.html#porqué-datos-tidy",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Porqué datos Tidy?",
    "text": "¿Porqué datos Tidy?\n\nEstandarizanción\n\n\n\n\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#facilitar-la-colaboración",
    "href": "sesion2_slides.html#facilitar-la-colaboración",
    "title": "Sesión 2: Preparando los datos",
    "section": "Facilitar la colaboración",
    "text": "Facilitar la colaboración\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#simplifica-la-reproducibilidad",
    "href": "sesion2_slides.html#simplifica-la-reproducibilidad",
    "title": "Sesión 2: Preparando los datos",
    "section": "Simplifica la reproducibilidad",
    "text": "Simplifica la reproducibilidad\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#siempre-tidy",
    "href": "sesion2_slides.html#siempre-tidy",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Siempre Tidy?",
    "text": "¿Siempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o “desordenadas”.\nEs importante tener en cuenta que siempre existen múltiples formas de representar la misma información."
  },
  {
    "objectID": "sesion2_slides.html#otras-estructuras",
    "href": "sesion2_slides.html#otras-estructuras",
    "title": "Sesión 2: Preparando los datos",
    "section": "Otras estructuras:",
    "text": "Otras estructuras:\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2_slides.html#algunas-tareas-comunes",
    "href": "sesion2_slides.html#algunas-tareas-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Algunas tareas comunes",
    "text": "Algunas tareas comunes\n\nDescartar e imputar missing data\nRemover duplicados.\nModificar datos\n\nmapear strings, expresiones aritméticas. Ejemplos:\n\nConvertir strings de mayúsculas a minúsculas (upper/lower case)\nConvertir T en Fahrenheit a Celsius\nCrear una nueva columna basada en la columna anterior."
  },
  {
    "objectID": "sesion2_slides.html#algunas-tareas-comunes-1",
    "href": "sesion2_slides.html#algunas-tareas-comunes-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Algunas tareas comunes",
    "text": "Algunas tareas comunes\n\nReemplazar valores\n\n(e.g. -999 → NaN). Usar método df[‘column’].replace()\n\nRestringir valores:\n\nvalores por encima o por debajo de los umbrales especificados se establecen en un valor máximo/mínimo."
  },
  {
    "objectID": "sesion2_slides.html#datos-perdidos-o-missing-na-nan",
    "href": "sesion2_slides.html#datos-perdidos-o-missing-na-nan",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos (o missing, NA, NAN)",
    "text": "Datos perdidos (o missing, NA, NAN)\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\nse pueden escoger filas o columnas\n\nLlenar/ reemplazar :\n\ncon un valor por default\ncon un valor interpolados, otros"
  },
  {
    "objectID": "sesion2_slides.html#datos-perdidos",
    "href": "sesion2_slides.html#datos-perdidos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-missing",
    "href": "sesion2_slides.html#ejemplo-missing",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo missing",
    "text": "Ejemplo missing\n\nimport pandas as pd\nimport numpy as np\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', np.nan, 'Carlos'],\n        'Edad': [25, 30, np.nan, 30, 28]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar filas con valores faltantes\ndf = df.dropna()\n\n# Llenar valores faltantes con un valor específico\ndf['Edad'].fillna(0, inplace=True)\n\nprint(df)\n\n\n\n   Nombre  Edad\n0    Juan  25.0\n1     Ana  30.0\n4  Carlos  28.0"
  },
  {
    "objectID": "sesion2_slides.html#fitrando-y-limpiando",
    "href": "sesion2_slides.html#fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cuál de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key."
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-fitrando-y-limpiando",
    "href": "sesion2_slides.html#ejemplo-fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo Fitrando y limpiando",
    "text": "Ejemplo Fitrando y limpiando\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-datos-de-educación",
    "href": "sesion2_slides.html#ejemplo-datos-de-educación",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo: Datos de educación",
    "text": "Ejemplo: Datos de educación\n\n\n\n\n\n\nTaller de aplicación 1: Pregunta 4"
  },
  {
    "objectID": "sesion2_slides.html#errores-de-registro-y-textos",
    "href": "sesion2_slides.html#errores-de-registro-y-textos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) “quiebra” o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14”"
  },
  {
    "objectID": "sesion2_slides.html#errores-de-registro-y-textos-1",
    "href": "sesion2_slides.html#errores-de-registro-y-textos-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\n\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14”\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD” \"AbCd\".lower() # \"abcd\""
  },
  {
    "objectID": "sesion2_slides.html#errores-de-formato",
    "href": "sesion2_slides.html#errores-de-formato",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de formato",
    "text": "Errores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0"
  },
  {
    "objectID": "sesion2_slides.html#transformando-strings",
    "href": "sesion2_slides.html#transformando-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformando strings",
    "text": "Transformando strings\nPodemos encontrar donde estan los problemas facilmente:\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n\n\n2\n\n\n\n\n\n#s.index(':') # ValueError raised"
  },
  {
    "objectID": "sesion2_slides.html#encontrar-problemas",
    "href": "sesion2_slides.html#encontrar-problemas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Encontrar problemas",
    "text": "Encontrar problemas\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado . . .\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n\n\n2\n\n\n\n\ns.find(':') # -1\n\n\n\n-1"
  },
  {
    "objectID": "sesion2_slides.html#encontrar-problemas-1",
    "href": "sesion2_slides.html#encontrar-problemas-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Encontrar problemas",
    "text": "Encontrar problemas\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string . . .\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\n\n\nFalse"
  },
  {
    "objectID": "sesion2_slides.html#métodos-para-strings",
    "href": "sesion2_slides.html#métodos-para-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Métodos para strings",
    "text": "Métodos para strings"
  },
  {
    "objectID": "sesion2_slides.html#métodos-para-strings-1",
    "href": "sesion2_slides.html#métodos-para-strings-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Métodos para strings",
    "text": "Métodos para strings\n\nCualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido)\nSe debe usar .str. (es importante el .str)."
  },
  {
    "objectID": "sesion2_slides.html#ejemplo",
    "href": "sesion2_slides.html#ejemplo",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo:",
    "text": "Ejemplo:\n\ngmailsplit3 char\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\n\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\n\n\n\n    \nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\n\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\n\n\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2_slides.html#expresiones-regulares-para-strings",
    "href": "sesion2_slides.html#expresiones-regulares-para-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Expresiones regulares para strings",
    "text": "Expresiones regulares para strings"
  },
  {
    "objectID": "sesion2_slides.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2_slides.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets"
  },
  {
    "objectID": "sesion2_slides.html#eliminar-elegir-columnas",
    "href": "sesion2_slides.html#eliminar-elegir-columnas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Eliminar/ elegir columnas",
    "text": "Eliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis. . . .\n\nCodetabla originaltabla sin peso\n\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\ndf.head(6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\nEdad\nPeso (kg)\n\n\n\n\n0\nJuan\n25\n70\n\n\n1\nAna\n30\n65\n\n\n2\nLuis\n35\n80\n\n\n3\nCarlos\n28\n75\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\nEdad\n\n\n\n\n0\nJuan\n25\n\n\n1\nAna\n30\n\n\n2\nLuis\n35\n\n\n3\nCarlos\n28"
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato",
    "href": "sesion2_slides.html#reshapesre-formato",
    "title": "Sesión 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato\n\nEl “reshape” o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico.\nEsto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos.\nEl “reshape” es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo."
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato-1",
    "href": "sesion2_slides.html#reshapesre-formato-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato"
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato-2",
    "href": "sesion2_slides.html#reshapesre-formato-2",
    "title": "Sesión 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato\n\nCodeDF anchoDF largo\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCiudad\nA\nB\n\n\nFecha\n\n\n\n\n\n\n2023-08-01\n25\n28\n\n\n2023-08-02\n26\n29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFecha\nCiudad\nTemperatura\n\n\n\n\n0\n2023-08-01\nA\n25\n\n\n1\n2023-08-01\nB\n28\n\n\n2\n2023-08-02\nA\n26\n\n\n3\n2023-08-02\nB\n29"
  },
  {
    "objectID": "sesion2_slides.html#crear-variables-dummies-dicotómicas",
    "href": "sesion2_slides.html#crear-variables-dummies-dicotómicas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Crear variables dummies / dicotómicas",
    "text": "Crear variables dummies / dicotómicas\n\nMuy útil para representar información cualitativa.\nMuy usando en análisis de regresión y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\nSe pueden generar mediante pd.get_dummies(df[‘key’])"
  },
  {
    "objectID": "sesion2_slides.html#one-hot-encoding-razas-de-perros",
    "href": "sesion2_slides.html#one-hot-encoding-razas-de-perros",
    "title": "Sesión 2: Preparando los datos",
    "section": "One hot encoding: Razas de perros",
    "text": "One hot encoding: Razas de perros"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-razas-de-perros",
    "href": "sesion2_slides.html#ejemplo-razas-de-perros",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo : Razas de perros",
    "text": "Ejemplo : Razas de perros\n\nCodetabla\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabrador\nPoodle\nBulldog\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n\n\n3\n0\n0\n0\n\n\n4\n0\n0\n0\n\n\n5\n0\n0\n0"
  },
  {
    "objectID": "sesion2_slides.html#unir-datasets",
    "href": "sesion2_slides.html#unir-datasets",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\nApliar/concatenar/append: Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join: Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como “inner”, “outer”, “left” y “right”"
  },
  {
    "objectID": "sesion2_slides.html#apilar-dataframes",
    "href": "sesion2_slides.html#apilar-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "1. Apilar dataframes",
    "text": "1. Apilar dataframes\nCreemos los dataframes: . . .\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\n\n\n   P  Q\n0  2  3\n1  4  5\n\n\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n\n\n   P  Q\n0  6  7\n1  8  9"
  },
  {
    "objectID": "sesion2_slides.html#apilar",
    "href": "sesion2_slides.html#apilar",
    "title": "Sesión 2: Preparando los datos",
    "section": "Apilar",
    "text": "Apilar\nHagamos el apilar. Atención a los index\n\n\ndf.append(df2)\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9"
  },
  {
    "objectID": "sesion2_slides.html#apilar-1",
    "href": "sesion2_slides.html#apilar-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Apilar",
    "text": "Apilar"
  },
  {
    "objectID": "sesion2_slides.html#ojo-al-apilar-con-los-index",
    "href": "sesion2_slides.html#ojo-al-apilar-con-los-index",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ojo al apilar con los index",
    "text": "Ojo al apilar con los index\nUsemos ignorar index . . .\n\ndf.append(df2, ignore_index=True)\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9"
  },
  {
    "objectID": "sesion2_slides.html#ojo-al-apilar-con-los-index-1",
    "href": "sesion2_slides.html#ojo-al-apilar-con-los-index-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ojo al apilar con los index",
    "text": "Ojo al apilar con los index"
  },
  {
    "objectID": "sesion2_slides.html#unir-dataframes",
    "href": "sesion2_slides.html#unir-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Unir dataframes",
    "text": "2. Unir dataframes"
  },
  {
    "objectID": "sesion2_slides.html#unir-datasets-1",
    "href": "sesion2_slides.html#unir-datasets-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nUtilizando la función pd.merge()\n\n\ncodeinner joinouter join\n\n\n\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nValue_x\nValue_y\n\n\n\n\n0\nB\n2\n4\n\n\n1\nC\n3\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nValue_x\nValue_y\n\n\n\n\n0\nA\n1.0\nNaN\n\n\n1\nB\n2.0\n4.0\n\n\n2\nC\n3.0\n5.0\n\n\n3\nD\nNaN\n6.0"
  },
  {
    "objectID": "sesion2_slides.html#transformaciones-estadísticas",
    "href": "sesion2_slides.html#transformaciones-estadísticas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones estadísticas:",
    "text": "Transformaciones estadísticas:\n1. Outliers\n\nLa identificación de valores atípicos es importante por varias razones:\n\n\nCalidad de los datos\nImpacto en estadísticas y modelos\nAnomalías y problemas reales\nToma de decisiones informada"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-outliers",
    "href": "sesion2_slides.html#ejemplo-outliers",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo outliers",
    "text": "Ejemplo outliers\nDetección\n\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n\n\n\nValores atípicos:\n   Valor\n6    200"
  },
  {
    "objectID": "sesion2_slides.html#estandarización-de-datos",
    "href": "sesion2_slides.html#estandarización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Estandarización de Datos",
    "text": "2. Estandarización de Datos\n\nLa estandarización es un proceso importante en el análisis de datos y el aprendizaje automático.\nConsiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno.\nEsta transformación se logra mediante la fórmula:\n\n\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]"
  },
  {
    "objectID": "sesion2_slides.html#estandarización",
    "href": "sesion2_slides.html#estandarización",
    "title": "Sesión 2: Preparando los datos",
    "section": "Estandarización",
    "text": "Estandarización\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\n\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]"
  },
  {
    "objectID": "sesion2_slides.html#normalización-de-datos",
    "href": "sesion2_slides.html#normalización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Normalización de Datos",
    "text": "Normalización de Datos\n\nLa normalización es un proceso esencial en el análisis de datos y el aprendizaje automático.\nConsiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1.\nLa fórmula matemática utilizada para la normalización es:\n\n\n\\[x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\]"
  },
  {
    "objectID": "sesion2_slides.html#normalización-de-datos-1",
    "href": "sesion2_slides.html#normalización-de-datos-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Normalización de Datos",
    "text": "Normalización de Datos\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\n\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]\n\n\n\n\n\n\n\n\nCurso Análisis de Datos - Sesión 2"
  },
  {
    "objectID": "sesion1_slides.html#contenidos",
    "href": "sesion1_slides.html#contenidos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Contenidos:",
    "text": "Contenidos:\n\n 1. El proceso de análisis de datos \n \n     Visión general\n      \n        Las metodologías de análisis que veremos en el curso\n        Adquisición y almacenamiento de los datos\n        Preparación de los datos\n      \n    \n     Preguntando a los datos\n      \n        Abstrayendo la realidad.\n        Planteamiento de preguntas.\n        El rol de las hipótesis.\n      \n    \n  \n\n\n\n   2. Respondiendo desde los datos: Pruebas de hipótesis  \n  \n    Conceptos Básicos de Pruebas de Hipótesis:\n      \n        Definición de hipótesis nula y alternativa.\n        Intervalos de confianza.\n        Niveles de significancia y p-values.\n        Errores tipo I y tipo II.\n      \n    \n    Tipos de Pruebas de Hipótesis:\n      \n        Pruebas t para comparación de medias.\n        Pruebas chi-cuadrado para variables categóricas.\n        Pruebas ANOVA para comparación de múltiples grupos.\n      \n    \n    Interpretación de Resultados:\n  \n\n\n\n\n   3. Buenas prácticas en análisis de datos \n  \n     Desafíos y Consideraciones:\n      \n        Privacidad y seguridad de los datos.\n        Limpieza y transformación durante la preparación de datos.\n      \n    \n     Reproducibilidad y Control de Versiones (GIT):\n      \n        Importancia de mantener un registro de los cambios en los datos.\n        Uso de sistemas de control de versiones como GIT para rastrear cambios.\n        Aplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-1",
    "href": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "El proceso de la ciencia de datos",
    "text": "El proceso de la ciencia de datos"
  },
  {
    "objectID": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-2",
    "href": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "El proceso de la ciencia de datos",
    "text": "El proceso de la ciencia de datos\nEn este curso nos enfocaremos en:\n\n\n\nLa preparación de los datos\nAnálisis mediante modelos de regresión"
  },
  {
    "objectID": "sesion1_slides.html#el-objetivo-dar-valor",
    "href": "sesion1_slides.html#el-objetivo-dar-valor",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "El objetivo: dar valor",
    "text": "El objetivo: dar valor\n\nEsto con el objetivo de responder preguntas desde los datos, que provean información valiosa."
  },
  {
    "objectID": "sesion1_slides.html#adquisición-de-datos",
    "href": "sesion1_slides.html#adquisición-de-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Adquisición de datos:",
    "text": "Adquisición de datos:\n\nEl primer paso en el proceso de análisis de datos implica la adquisición y el almacenamiento de los datos.\nEsto se refiere a la recolección de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilación de datos de fuentes diversas, como bases de datos, archivos CSV, páginas web o incluso sensores en tiempo real."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-i",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-i",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes I",
    "text": "Fuentes de datos comunes I\nExisten tantas fuentes de datos, como podríamos imaginar…\n\nEncuestas y Cuestionarios:\n\nDiseño y administración de encuestas para recopilar datos directamente de los participantes.\nPermite obtener información específica y detallada según las preguntas planteadas.\n\nExperimentos Controlados:\n\nDiseño de experimentos para recopilar datos bajo condiciones controladas.\nÚtil para establecer relaciones causales y evaluar efectos de cambios controlados."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-ii",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-ii",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes II",
    "text": "Fuentes de datos comunes II\n\nObservación y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar información.\nUtilización de sensores en dispositivos móviles y wearables para recopilar datos de ubicación, salud y actividad.\n\nRecopilación de Datos Existentes:\n\nUtilización de datos ya recopilados y disponibles en bases de datos o fuentes públicas.\nReduce el tiempo y costo de recopilación, pero puede tener limitaciones en términos de calidad y relevancia."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-iii",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-iii",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes III",
    "text": "Fuentes de datos comunes III\n\nWeb Scraping (Web Scrapping):\n\nExtracción de datos de sitios web utilizando herramientas y técnicas automatizadas.\nPermite recopilar información no estructurada de manera eficiente, pero requiere atención a la ética y términos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracción programática con sistemas y servicios para obtener datos en tiempo real.\nComún en la obtención de datos de redes sociales, información climática, finanzas, entre otros."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-iv",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-iv",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes IV",
    "text": "Fuentes de datos comunes IV\n\nColaboración y Participación Comunitaria:\n\nColaboración con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser útil para proyectos de mapeo colaborativo, ciencia ciudadana y recopilación de información local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes volúmenes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilación y posterior análisis de datos heterogéneos.\nUsualmente se accede a través de querys SQL"
  },
  {
    "objectID": "sesion1_slides.html#proyecto",
    "href": "sesion1_slides.html#proyecto",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Proyecto",
    "text": "Proyecto\n\n\n\n\n\n\nDatos disponibles\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos públicos sobre educación chilena\nDatos públicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad económica\n\nVeamos como acceder algunos de estos datos."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-encuesta-casen",
    "href": "sesion1_slides.html#ejemplo-encuesta-casen",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Encuesta Casen",
    "text": "Ejemplo: Encuesta Casen\nDatos públicos\nLa Encuesta de Caracterización Socioeconómica Nacional (CASEN), se realuza en chile:\n\nObjetivo: recopilar información detallada sobre la situación socioeconómica de los hogares y las personas en el país.\n\nEsta encuesta se lleva a cabo de manera periódica y abarca una amplia variedad de temas, como ingresos, educación, empleo, salud, vivienda y otros aspectos.\nSe utiliza para informar políticas públicas, tomar decisiones informadas y analizar la evolución de indicadores sociales a lo largo del tiempo.\n\nSitio Web oficial"
  },
  {
    "objectID": "sesion1_slides.html#encuesta-casen",
    "href": "sesion1_slides.html#encuesta-casen",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Encuesta Casen",
    "text": "Encuesta Casen\nEjemplo\n\n\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. . . .\n\n\ncodeOutput\n\n\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(4) # veamos las 4 primeros registros\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegión de Tarapacá\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegión de Tarapacá\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educación formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegión de Tarapacá\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nBásica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegión de Tarapacá\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nTécnico nivel superior incompleta\nSí\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n\n\n4 rows × 22 columns"
  },
  {
    "objectID": "sesion1_slides.html#encuesta-casen-1",
    "href": "sesion1_slides.html#encuesta-casen-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Encuesta Casen",
    "text": "Encuesta Casen\nEjemplo\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploración, para ser preparados para analizarlos.\nDe esto se tratará la siguiente sesión del curso."
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\nOtra opción es que los datos estén en una API:\n\nCodeOutput\n\n\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb \n# para instalar: conda install pandas-datareader  \n# o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. \n# En este caso revisare de PIB (GDP en ingés), \n# pero se pueden explorar muchas más opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-1",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\n\ncoderesults\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-2",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\n\ncoderesults\n\n\n\n#sabemos que queremos Chile, asi que busquemos su info\n\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n\n\n\n\nObservar que este es un data frame con dos índices: pais y año.\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-3",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del banco mundial",
    "text": "Datos desde la API del banco mundial\nEjemplo\nData frame con los datos de Chile, entre 1980 y 2020.\n\ncoderesults\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n2015\n13569.948127\n\n\n2014\n13421.538342\n\n\n2013\n13318.595215"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-4",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\nSi quisieramos, por simplicidad quedarnos solo con el indice del año y reordenar el dataframe:\n\ncoderesults\n\n\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el análisis es para un solo país\nreversed_df.head(5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-5",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del banco mundial",
    "text": "Datos desde la API del banco mundial\nEjemplo\nAhora, realicemos un grafico rápido con nuestros datos:\n\ncodeplot\n\n\n\n# Graficamos\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'Año')\n\n\n\n\n\n\n\n\nText(0.5, 0, 'Año')"
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicación-1",
    "href": "sesion1_slides.html#taller-de-aplicación-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicación 1",
    "text": "Taller de aplicación 1\n\n\n\n\n\n\nPregunta 1 - Bajando y formateando datos del Banco Mundial\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?"
  },
  {
    "objectID": "sesion1_slides.html#preguntando-a-los-datos",
    "href": "sesion1_slides.html#preguntando-a-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntando a los datos",
    "text": "Preguntando a los datos\n¿Cómo plantear preguntas y formular hipótesis en el contexto del análisis de datos?\n\nLa formulación de preguntas relevantes que se puedan responder mediante la exploración y el examen de los datos disponibles.\nPueden surgir de la necesidad de resolver un problema, entender un fenómeno o explorar patrones en los datos.\nUn buen planteamiento de preguntas es crucial, ya que guiará todo el proceso de análisis."
  },
  {
    "objectID": "sesion1_slides.html#abstrayendo-la-realidad",
    "href": "sesion1_slides.html#abstrayendo-la-realidad",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Abstrayendo la realidad",
    "text": "Abstrayendo la realidad\n\nEl proceso de abstraer la realidad"
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis",
    "href": "sesion1_slides.html#preguntas-e-hipótesis",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\n\nUna hipótesis es una afirmación, verificable con evidencia.\nEn este sentido, para toda pregunta podemos responderla mediante hipótesis.\nPara responder a las preguntas en el contexto de datos, es común formular hipótesis nulas y alternativas."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis-1",
    "href": "sesion1_slides.html#preguntas-e-hipótesis-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\n\nLa hipótesis nula es aquella que propone que algun parámetro toma cierto valor.\nEste generlamente es un punto de verdad.\nSi bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto.\nEn general, planteamos el problema de tal manera que podamos rechazar la hipótesis nula, en favor de otra que llamamos alternatiba."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis-2",
    "href": "sesion1_slides.html#preguntas-e-hipótesis-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\nPrueba de significancia\n\nQuizas, la hipotesis nula más famosa es la prueba de “significancia”.\nEn esta se propone que un parámetro (muchas veces un efecto, o correlación) es 0,\n\nes decir, plantea que no hay efecto o relación entre las variables\nmientras que la hipótesis alternativa sugiere que sí existe una relación o efecto significativo."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis-3",
    "href": "sesion1_slides.html#preguntas-e-hipótesis-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\nUna guía clave para el análisis\n\nSon fundamentales para establecer una base objetiva para el análisis y para evaluar las evidencias encontradas en los datos.\nEl proceso de plantear preguntas y formular hipótesis es el primer paso en el análisis de datos, ya que establece una guía clara para el enfoque y la dirección del trabajo.\nAl identificar preguntas y establecer hipótesis, se crea un marco sólido que orientará la exploración y el análisis de los datos disponibles."
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicación-1-1",
    "href": "sesion1_slides.html#taller-de-aplicación-1-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicación 1",
    "text": "Taller de aplicación 1\n\n\n\n\n\n\nPregunta 2 - Investigando sobre países:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna característica de dicho país en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles.\n\n¿Cómo definiria la variable aleatoria relevante?\n¿Qué hipótesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estadística\nInferencia se refiere al proceso de hacer generalizaciones de una población a partir de una muestra de esa población.\n\nPoblación y Muestra"
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos-1",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estadística\n\nSi tenemos un sub-conjunto de datos representativos de una población\n\npodemos utilizar métodos estadísticos\npara sacar conclusiones sobre las características\nY propiedades de esa población en su totalidad."
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos-2",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estadística\n\nEl proceso de inferencia estadística se basa en el principio de que una muestra bien s eleccionada puede proporcionar información valiosa sobre la población en general.\nEl uso de la inferencia estadística es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una población en particular."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos",
    "href": "sesion1_slides.html#estadígrafos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos",
    "text": "Estadígrafos\nFunciones que aproximan parámetros\n\nEstadigrafos"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-1",
    "href": "sesion1_slides.html#estadígrafos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos",
    "text": "Estadígrafos\nson variables aleatorias\n\n\n\nDado que por cada muestra que tenemos, vamos a calcular un estadígrafo este es en si mismo una variable aleatoria.\nTiene su propia distribución, media y varianza!"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-2",
    "href": "sesion1_slides.html#estadígrafos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos",
    "text": "Estadígrafos\nlos más comunes\n\nEstadigrafos más comunes"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\nLa distribución de las medias muestrales de una población se aproxima a una distribución normal\nIndependientemente de la forma de la distribución original de la población.\nEste teorema es esencial en inferencia estadística y tiene amplias aplicaciones en análisis de datos y toma de decisiones."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-1",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\nLa media muestral se distribuye normal, sin importar la distribución de la variable subyacente"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-2",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\n\nFormalmente: \\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\nSea x con media μ y desviación estándar σ finitas.\n\n\n\nSi tomamos muestras aleatorias de tamaño n de esta población y calculamos la media muestral de cada muestra\nLas medias muestrales se aproximará a una distribución normal con media μ y desviación estándar σ/√n."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-3",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\n\nFormalmente: \\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hipótesis\np-valor"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-4",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-5",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-6",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-6",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\nEste intervalo es aleatorio, porque \\(\\bar{y}\\) es diferente en cada muestra."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-7",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-7",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza\nMatemáticamente, para cada muestra podemos construir un intervalo.\n\nCon varianza conocidaCon varianza desconocida\n\n\n\\[ P\\left( \\bar{y}-\\frac{1.96\\sigma }{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{1.96\\sigma }{\\sqrt{n}}  \\right) = 0.95 \\]\n\n\n\\[ \\left(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} \\right) \\]"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-8",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-8",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza\n\nCon 20 muestras, tenemos 20 intervalos."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-9",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-9",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza - Interpretación\nPensemos en un 95% de confianza (un valor usual):\n\nEsto quiere decir, que si se repitiera este ejercicio muchas veces y construyéramos un intervalo de esta forma…\nel 95% de ellos contendría el verdadero parámetro poblacional.\nNo significa que con 95% de certeza el parámetro está exactamente en estos valores."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-10",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-10",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza - Interpretación\n\nAl 95% de confianza con 20 intervalos 19 contendrán el parámetro."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-11",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-11",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nUna forma de verificar hipotesis sobre los parámetros es mediante el contraste de hipótesis."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-12",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-12",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nEmpezamos suponiendo que hay una distribución conocida para el estadígrafo, centrada en un valor específico."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-13",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-13",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nY nos preguntamos, si esto fuea verdad ¿qué tan probable es la muestra que tengo?"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-14",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-14",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nLlamamos la hipótesis a probar Ho, y su alternativa H1."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-15",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-15",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-16",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-16",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nErrores y P-valor\n\nSe elige nivel de significancia de contraste (α) = probabilidad de cometer error Tipo I. Típicamente α = 0,01, 0,05, 0,10.\nDefinimos la prueba de hipótesis de significancia como aquella que indica si un estimador \\(\\hat{T}\\) es 0.\n\n\\[ H_0: T =0\\text{ vs }H_1: T \\neq 0 \\]"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-17",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-17",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nErrores y P-valor\nEl Valor de probabilidad (ó p-valor) es el nivel probabilidad más alto para el cual no podemos rechazar la hipótesis nula de la prueba de significancia."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nLos datos “Palmer Penguins” son un conjunto que detalla medidas morfológicas y características de tres especies de pingüinos: Adelie, Gentoo y Chinstrap.\nRecopilados por el Dr. Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-1",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\n\n\n\n\ncodetabla\n\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head(6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-2",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nEn el contexto de los pingüinos y el peso de su población:\n\npodríamos tomar una muestra de pingüinos\ny calcular un intervalo de confianza para el peso promedio.\n\nEsto nos daría una estimación del peso promedio de la población total, junto con la confianza en que este valor estimado es preciso."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-3",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nLa elección de la muestra, la interpretación de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hipótesis, sobre el peso de los pingüinos.\nPor ahora, pensemos que nuestra información es la población completa"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-4",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\nCalcularemos el promedio muestral y lo veremos en el contexto de los datos observados: . . .\n\ncodeoutput\n\n\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los pingüinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribución del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribución de Peso de Pingüinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-5",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral.\nDe que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus características.\nPor ejemplo, consideremos que de esta población de pingüinos obtenemos 1000 muestras de 40 individuos cada una.\nSi graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tamaño de muestra, más nos alejamos de la distribución normal.\nSi reducimos el número de repeticiones tambieé."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-6",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-6",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\ncodePlot\n\n\n\nimport numpy as np\n\n# Definir el tamaño de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y cálculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gráfico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribución de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#intervalo-de-confianza",
    "href": "sesion1_slides.html#intervalo-de-confianza",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Intervalo de confianza",
    "text": "Intervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n#  Obtener una muestra simple de 40 pingüinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error estándar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviación estándar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)"
  },
  {
    "objectID": "sesion1_slides.html#intervalo-de-confianza-1",
    "href": "sesion1_slides.html#intervalo-de-confianza-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Intervalo de confianza",
    "text": "Intervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nEl resultado será un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los pingüinos en la población, con un nivel de confianza del 95%.\n¿Como nos fue? ¿Contiene al verdadero valor?\n\n\n\nprint(\"Media poblacional:\", penguins['body_mass_g'].mean())\n\nprint(\"Intervalo de Confianza para el Peso:\", confidence_interval)\n\n\n\nMedia poblacional: 4201.754385964912\nIntervalo de Confianza para el Peso: (3788.9457238155824, 4341.054276184418)"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos",
    "href": "sesion1_slides.html#comparaciones-de-grupos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribución de tamaño por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n\n\ncodeTabla\n\n\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\nprint(tabla_doble_entrada)\n\n\n\n\n\n\n\n\n     species     sex     Promedio       Varianza\n0     Adelie  Female  3368.835616   72565.639269\n1     Adelie    Male  4043.493151  120278.253425\n2  Chinstrap  Female  3527.205882   81415.441176\n3  Chinstrap    Male  3938.970588  131143.605169\n4     Gentoo  Female  4679.741379   79286.335451\n5     Gentoo    Male  5484.836066   98068.306011"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-1",
    "href": "sesion1_slides.html#comparaciones-de-grupos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hipótesis: ¿Existe una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”?"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-2",
    "href": "sesion1_slides.html#comparaciones-de-grupos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nHipótesis Nula (H0):\nNo hay diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\nHipótesis Alternativa (H1):\nExiste una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”."
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-3",
    "href": "sesion1_slides.html#comparaciones-de-grupos-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nPara probar esta hipótesis, podrías utilizar una prueba de hipótesis para comparar las medias de las muestras de peso de los pingüinos machos y hembras en la especie “Adelie”.\n\n\n\ncodePlot\n\n\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribución de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribución de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-4",
    "href": "sesion1_slides.html#comparaciones-de-grupos-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nA simple vista podriamos pensar ambos grupos son diferentes.\nEs más claro si dibujamos el promedio muestral observado.\n\n\n\ncodeplot\n\n\n\n# Crear un gráfico de densidad con líneas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-5",
    "href": "sesion1_slides.html#comparaciones-de-grupos-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\ncodeTest t diferenciaplot\n\n\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estadística t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gráfico de comparación de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Machos y Hembras de Pingüinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\n\n\n\n\n\n\n\nEstadística t: 13.126285923485874\nValor p: 6.402319748031793e-26"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-6",
    "href": "sesion1_slides.html#comparaciones-de-grupos-6",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas.\nPara esto podriamos usar una prueba ANOVA.\n\n\n\ncodeANOVA ResultsPlot\n\n\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estadística F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\n\n\n\n\n\n\n\nEstadística F: 72.96098633250911\nValor p: 4.897246751596325e-16"
  },
  {
    "objectID": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab",
    "href": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Experimentos Aleatorios y pruebas A/B",
    "text": "Experimentos Aleatorios y pruebas A/B\n\nUn experimento estadístico busca establecer relaciones causales entre variables y obtener conclusiones sobre su impacto.\nSe diseñan para manipular variables independientes y observar sus efectos en una variable dependiente.\nLos experimentos controlan y manipulan variables para hacer afirmaciones sólidas sobre relaciones causales.\nLas pruebas A/B son comunes en áreas como marketing y diseño de productos.\nEn una prueba A/B, se comparan dos grupos de muestra (A y B) para evaluar si la variante B produce cambios significativos en una métrica de interés."
  },
  {
    "objectID": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab-1",
    "href": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Experimentos Aleatorios y pruebas A/B",
    "text": "Experimentos Aleatorios y pruebas A/B\nCuidados:\n\nPruebas A/B ofrecen evidencia de asociación causal, pero no aseguran causalidad total debido a factores no controlados.\nExperimentos controlados y métodos de diseño sólidos son esenciales para una comprensión completa de la causalidad.\nPruebas A/B son herramientas poderosas para analizar efectos y comparar opciones en condiciones controladas."
  },
  {
    "objectID": "sesion1_slides.html#enunciado",
    "href": "sesion1_slides.html#enunciado",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Enunciado",
    "text": "Enunciado\n\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico"
  },
  {
    "objectID": "sesion1_slides.html#enunciado-1",
    "href": "sesion1_slides.html#enunciado-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Enunciado",
    "text": "Enunciado\n\nLos clientes son asignados a uno de los siguientes grupos:\n\nControl: no les da una promoción (mala suerte, intentalo otra vez)\nTratamiento 1: 20% de descuento en el producto\n-Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento."
  },
  {
    "objectID": "sesion1_slides.html#creación-de-los-datos",
    "href": "sesion1_slides.html#creación-de-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Creación de los datos",
    "text": "Creación de los datos\n\ncodeDataframe\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n#| output: false\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n#| echo: false\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns"
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicación-1-2",
    "href": "sesion1_slides.html#taller-de-aplicación-1-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicación 1:",
    "text": "Taller de aplicación 1:\n\n\n\n\n\n\nPregunta 3 -Ejemplo AB test en Marketing:\n\n\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones",
    "href": "sesion1_slides.html#desafíos-y-consideraciones",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nImportancia de la Adquisición y Almacenamiento de Datos\n\nLa adquisición y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de análisis.\nLa calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes.\nGarantía de Calidad y Fiabilidad en la Obtención de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean sólidas."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-1",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nImportancia de la Adquisición y Almacenamiento de Datos\n\nExploración de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros.\nCada fuente tiene sus propias características y potenciales sesgos.\nComprender las diferencias entre estas fuentes y cómo pueden influir en los resultados es crucial para tomar decisiones informadas."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-2",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nPrivacidad y Seguridad de los Datos:\n\nUno de los aspectos más críticos en el análisis de datos es la privacidad y seguridad de la información.\nLos datos pueden contener información sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-3",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nPrivacidad y Seguridad de los Datos:\n\nExploraremos prácticas y regulaciones para garantizar que los datos se manejen de manera ética y legal.\nDiscutiremos cómo anonimizar los datos, utilizar técnicas de enmascaramiento y seguir las mejores prácticas para resguardar la privacidad de los individuos."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-4",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nLimpieza y Transformación durante la Preparación de Datos:\n\nLa etapa de preparación de datos es crucial para asegurarse de que los datos sean aptos para el análisis.\nSin embargo, este proceso no está exento de desafíos.\nLos datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-5",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nLimpieza y Transformación durante la Preparación de Datos:\n\nExploraremos técnicas para identificar y manejar valores atípicos y faltantes, errores de digitación, etc.\nLos invetsigadores toman muchas decisiones en este proceso, que deben ser transparentes.\nAbordar estos desafíos de manera adecuada es esencial para garantizar que nuestras conclusiones sean sólidas, confiables y éticas."
  },
  {
    "objectID": "sesion1_slides.html#reproducibilidad-y-control-de-versiones-git",
    "href": "sesion1_slides.html#reproducibilidad-y-control-de-versiones-git",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Reproducibilidad y Control de Versiones (GIT):",
    "text": "Reproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios",
    "href": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:",
    "text": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\n\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que también es una herramienta poderosa en el análisis de datos.\nPermite rastrear cada modificación realizada en el código y en los documentos, incluidos los notebooks.\nCada cambio es registrado como un “commit”, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\nLa aplicación de GIT en proyectos de preparación de datos agrega un nivel adicional de transparencia y colaboració"
  },
  {
    "objectID": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios-1",
    "href": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:",
    "text": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\n\nUn esquema de git por Allison Horst @allison_horst"
  },
  {
    "objectID": "sesion1_slides.html#actividad-de-proyecto",
    "href": "sesion1_slides.html#actividad-de-proyecto",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Actividad de proyecto",
    "text": "Actividad de proyecto\n\n\n\n\n\n\nInicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del análisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustaría trabajar.\nPropongan una o dos preguntas de investigación y las hipotesis que las responderían.\n\nLa siguiente sesión, vamos a explorar los datos y empezar los primeros pasos en su análisis.\n\n\n\n\n\n\n\nCurso Análisis de Datos - Sesión 1"
  },
  {
    "objectID": "taller1_aplicacion_educ.html",
    "href": "taller1_aplicacion_educ.html",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "Vamos a usar una situación ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de identificación revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy rápidamente y en otras, vamos a tener mayor énfasis.\n\n\nNuestro objetivo es responder la siguiente pregunta ficticia de investigación:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\n\nLa pregunta de investigación se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioeconómico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opción de asistir a un curso de verano intensivo durante el verano entre 5 y 6to básico.\nEl curso de verano se enfoca en mejorar las habilidades académicas de preparar la prueba de admisión a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participación en el curso en los resultados académicos de los estudiantes.\n\n\n\n\nLos datos estan disponibles en https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene información sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si participó en el curso de verano, sexo, ingreso del hogar (en logaritmo), educación de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el año 5 como para el año 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene información de cada individuo (con identificador id).\nEste dataset tiene la información si el individuo recibió la carta de invitación para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene información de rendimiento académico antes y después del curso de verano.\n\n\n\n\n\nLa idea de este taller es poner en práctica los primeros pasos para un análisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n\nTambién exploraremos los datos, usaremos estadísticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\n\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre lso pasos anteriores múltiples veces. También invlocura tomar desiciones por parte de los investigadores, por lo cual la documentación de esta fase es especialmente importante.\nEn nuestro caso, será bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser así.\nPasos que debe realizar:\n\nPreparación de los datos\n\n1.1. Cargue las tres bases de datos\n1.2. Unir los datasets\nTenemos 3 bases de datos con información diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta información, especialmente si estimaremos modelos en base a ésta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido debería tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de información del estudiante si recibió o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de unión person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que acá unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id única tiene un solo colegio, pero sirve de ejemplo en como usar más de una columna mediante.\nUsamos la función summary() para obtener una estadística descriptiva de las variables en el dataset unido.\nPreparar los datos\n\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera “tidy” (Según Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observación.\n\n(puedes profundizar y ver más ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observación.\nSi inspeccionamos el número de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del año 2 al 10. Este tipo de datos son de panel\n\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y análisi preferimos que esté en formato long. Especialmente cuando modelamos incluyendo efectos fijos También es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos melt()\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selección de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podríamos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y número de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn último paso que haremos antes de hacer estadística decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviación estándar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto específico de la medida y es más facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estadística descriptiva.\n\n\n\nHasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estadísticas descriptivas y también algunos graficos descriptivos.\n3.1 Tablas de etsádistocas descriptivas Incluya la media, la desviación estandar, la mediana, max y min, al menos.\n3.2 Gráficos de estadística descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#taller-1-pregunta-4---aplicación-datos-de-educación",
    "href": "taller1_aplicacion_educ.html#taller-1-pregunta-4---aplicación-datos-de-educación",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "Vamos a usar una situación ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de identificación revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy rápidamente y en otras, vamos a tener mayor énfasis.\n\n\nNuestro objetivo es responder la siguiente pregunta ficticia de investigación:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\n\nLa pregunta de investigación se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioeconómico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opción de asistir a un curso de verano intensivo durante el verano entre 5 y 6to básico.\nEl curso de verano se enfoca en mejorar las habilidades académicas de preparar la prueba de admisión a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participación en el curso en los resultados académicos de los estudiantes.\n\n\n\n\nLos datos estan disponibles en https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene información sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si participó en el curso de verano, sexo, ingreso del hogar (en logaritmo), educación de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el año 5 como para el año 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene información de cada individuo (con identificador id).\nEste dataset tiene la información si el individuo recibió la carta de invitación para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene información de rendimiento académico antes y después del curso de verano."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#objetivos",
    "href": "taller1_aplicacion_educ.html#objetivos",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "La idea de este taller es poner en práctica los primeros pasos para un análisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n\nTambién exploraremos los datos, usaremos estadísticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\n\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre lso pasos anteriores múltiples veces. También invlocura tomar desiciones por parte de los investigadores, por lo cual la documentación de esta fase es especialmente importante.\nEn nuestro caso, será bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser así.\nPasos que debe realizar:\n\nPreparación de los datos\n\n1.1. Cargue las tres bases de datos\n1.2. Unir los datasets\nTenemos 3 bases de datos con información diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta información, especialmente si estimaremos modelos en base a ésta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido debería tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de información del estudiante si recibió o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de unión person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que acá unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id única tiene un solo colegio, pero sirve de ejemplo en como usar más de una columna mediante.\nUsamos la función summary() para obtener una estadística descriptiva de las variables en el dataset unido.\nPreparar los datos\n\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera “tidy” (Según Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observación.\n\n(puedes profundizar y ver más ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observación.\nSi inspeccionamos el número de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del año 2 al 10. Este tipo de datos son de panel\n\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y análisi preferimos que esté en formato long. Especialmente cuando modelamos incluyendo efectos fijos También es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos melt()\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selección de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podríamos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y número de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn último paso que haremos antes de hacer estadística decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviación estándar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto específico de la medida y es más facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estadística descriptiva."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#estadística-descriptiva",
    "href": "taller1_aplicacion_educ.html#estadística-descriptiva",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "Hasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estadísticas descriptivas y también algunos graficos descriptivos.\n3.1 Tablas de etsádistocas descriptivas Incluya la media, la desviación estandar, la mediana, max y min, al menos.\n3.2 Gráficos de estadística descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  },
  {
    "objectID": "sesion0_notas.html",
    "href": "sesion0_notas.html",
    "title": "Presentación del curso",
    "section": "",
    "text": "Hola! Soy Melanie y seré la docente de este curso, en el que aprenderás los fundamentos del análisis de datos, tanto desde una perspectiva teórica como práctica (en Python).\nMe pueden contactar al mail melanie.oyarzun@udd.cl\n\n\n\n\n\n\n\nRevisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0_notas.html#en-la-sesión-de-hoy",
    "href": "sesion0_notas.html#en-la-sesión-de-hoy",
    "title": "Presentación del curso",
    "section": "",
    "text": "Revisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0_notas.html#contexto-en-el-programa-de-magister",
    "href": "sesion0_notas.html#contexto-en-el-programa-de-magister",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la línea de desarrollo de data science.\nEsta asignatura tributa, a través de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Magíster en Data Science:\n\nAplicar teorías, algoritmos, métodos, técnicas y herramientas básicas y avanzadas de Data Science para analizar, resolver y hacer una evaluación crítica de desafíos complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones.\nComunica efectivamente y argumenta sobre los resultados de su trabajo a públicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento ético, cumpliendo los protocolos y normas que guían su desempeño, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicación de estrategias para utilizar nuevo conocimiento en data science en su ámbito de desempeño."
  },
  {
    "objectID": "sesion0_notas.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "href": "sesion0_notas.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura (resultados de aprendizaje)",
    "text": "Objetivos de la asignatura (resultados de aprendizaje)\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el análisis de datos, utilizando lenguaje técnico afín.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso.\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer información y generar conclusiones basadas en evidencia.\nIdentificar las buenas prácticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0_notas.html#resumen",
    "href": "sesion0_notas.html#resumen",
    "title": "Presentación del curso",
    "section": "Resumen:",
    "text": "Resumen:"
  },
  {
    "objectID": "sesion0_notas.html#detallado",
    "href": "sesion0_notas.html#detallado",
    "title": "Presentación del curso",
    "section": "Detallado",
    "text": "Detallado\n\n\n\n\n\n\nSesión 1: Respondiendo Preguntas con datos\n\n\n\n\n\nFecha: 19 agosto\nObjetivos:\n\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados.\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos.\n\nContenidos:\n\nEl proceso de análisis de datos\n\nPlanteamiento de preguntas\nAdquision y almacenmiento de los datos\nPreparación de los datos\nUna visión general a las metodologías de análisis que veremos en el curso\n\nFormulación de Preguntas y Hipótesis:\n\nImportancia de definir preguntas claras y específicas.\nDiferenciación entre preguntas exploratorias y confirmatorias.\nCreación de hipótesis nulas y alternativas.\n\nHipótesis y Variables:\n\nIdentificación de variables independientes y dependientes.\nRelación entre hipótesis y variables a analizar.\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\nImportancia de la Adquisición y Almacenamiento de Datos:\n\nGarantía de calidad y fiabilidad en la obtención de datos.\nExploración de diferentes fuentes de datos y su impacto en los resultados.\nMetodologias de levantamiento y adquision\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\n\nBibliografia recomendada:\nActividades:\n\nTaller 1 (incluidas en slides 1)\nProyecto clase 1: Conformación de grupos, definición de temas, primeras hipótesis y datos.\n\n\n\n\n\n\n\n\n\n\nSesión 2:Preparando los datos\n\n\n\n\n\nFecha: 26 agosto\nObjetivos:\n\nComprender la importancia del proceso de preparación de datos para el análisis, reconociendo principios y enfoques clave junto con sus ventajas y desventajas.\nDesarrollar habilidades prácticas en la preparación de datos, identificando y abordando problemas comunes como valores faltantes, valores atípicos y formatos inconsistentes, así como enfoques de trabajo eficientes.\n\nContenidos:\n\nEl proceso de preparación de los datos\n\nSignificado y relevancia de la preparación de datos.\nEjemplos reales de cómo la falta de preparación puede afectar los resultados.\n\nPrincipios y enfoques\n\nExtract, Transform, Load (ETL): Proceso fundamental en la preparación de datos.\nData Wrangling: Técnicas para dar forma y estructura a los datos.\nDatos Tidy: Organización y reestructuración para un análisis eficaz.\n\nBuenas Prácticas en la Preparación de Datos\n\nDocumentación y Consistencia\n\nImportancia de la documentación detallada.\nMantener nomenclatura y convenciones consistentes.\n\nValidación y Verificación\n\nValidación cruzada y verificación de integridad.\nCumplir con reglas y restricciones esperadas.\n\nReproducibilidad y Versionado\n\nEntorno de trabajo reproducible (Jupyter Notebooks, R Markdown).\nUtilización de sistemas de control de versiones (GIT).\n\nComunicación y Validación Colaborativa\n\nComunicación clara de pasos y resultados.\nValidación intermedia con colaboradores para feedback.\n\nSeguridad y privacidad de los datos\n\nProblemas comunes presentes en datos\n\nValores faltantes: \n\nEstrategias para manejar valores faltantes.\nDecidir entre imputación, eliminación o conservación.\n\nValores atípicos\nNormalización y estandarización\nErrores de registro Bibliografia recomendada:\n\n\n\n“Practical Statistics for Data Scientists” (Capítulo 2).\n“Doing Data Science” (Capítulo 1).\n\nActividades de aplicación práctica:\n\nTaller 1: Limpieza y análisis descriptivo de datos en la practica con datos de educación (repasa elementos del curso anterior) (sesión 1)\nProyecto:\n\nInicie el proyecto, cree un documento notebook en el cual van a alojarsu proyecto\nExplorar los datos\nDiagnosticar problemas.\nLa hipótesis que pensamos, ¿tienen variables que pueda concretizarlas? ¿Qué variables usar?\n\n\n\n\n\n\n\n\n\n\n\nSesión 3: Introduccion al análisis de regresión\n\n\n\n\n\nFecha: 2 septiembre\nObjetivos:\n\nComprender los conceptos fundamentales del análisis de regresión lineal y su aplicación en la resolución de problemas.\nDesarrollar la habilidad de plantear modelos e interpretar los resultados obtenidos del análisis de regresión, para aplicarlos en la toma de decisiones.\n\nContenidos\n\nIntroducción al Análisis de Regresión:\n\nDefinición y concepto de regresión.\nUso y aplicabilidad en la toma de decisiones.\n\nRegresión Lineal Múltiple:\n\nExtensión del modelo de regresión a múltiples variables predictoras.\nEcuación de regresión lineal múltiple.\n\nInterpretación de Coeficientes:\n\nSignificado e interpretación de los coeficientes de regresión.\nInfluencia de las variables predictoras en la variable de respuesta.\n\nEvaluación de Modelos de Regresión:\n\nUso de medidas como el coeficiente de determinación (R²) y el error estándar de estimación.\nInterpretación de los resultados de evaluación.\n\nIncorporación de Variables Categóricas:\n\nTransformación de variables categóricas en variables numéricas.\nInterpretación de coeficientes para variables categóricas.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nPlantear modelos de regresión que implementen las hipótesis del proyecto\nEstimar e interpretar modelos\n\n\n\n\n\n\n\n\n\n\nSesión 4: Profundizando en Análisis de Regresión, Supuestos y Limitaciones\n\n\n\n\n\nFecha: 9 septiembre\nObjetivos:\n\nExplorar los supuestos y limitaciones asociados al análisis de regresión y desarrollar estrategias para manejar problemas comunes.\nAplicar estrategias prácticas para identificar y abordar problemas en el análisis de regresión.\n\nContenidos\n\nSupuestos en el Análisis de Regresión:\n\nIdentificación de supuestos clave: linealidad, independencia, homoscedasticidad y normalidad.\nSignificado de cada supuesto y su importancia en la interpretación de resultados.\n\nIdentificación de Problemas en la Regresión:\n\nIdentificación y manejo de outliers en los datos.\nReconocimiento de la heterocedasticidad y sus implicaciones.\nDetección de la no-normalidad de los residuos.\n\nEstrategias para Manejar Problemas:\n\nTransformación de variables para abordar problemas de linealidad.\nMétodos para reducir la influencia de outliers.\nUso de transformaciones para tratar la heterocedasticidad.\nPruebas y técnicas para verificar y mejorar la normalidad.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nRevisar supuestos de modelo de regresión\nDiscutir problemas de identificación y limitaciones\n\n\n\n\n\n\n\n\n\n\nSesión 5: Introduccion al análisis de series de tiempo\n\n\n\n\n\nFecha: 30 septiembre\nObjetivos:\n\nIdentificar las características y particularidades de los datos de series de tiempo, comprendiendo sus aplicaciones profesionales.\nRealizar un análisis exploratorio de una serie de tiempo, identificando características clave para su modelamiento.\n\nContenidos\n\nConceptos Básicos de Series de Tiempo:\n\nDefinición y características de una serie de tiempo.\nEjemplos de aplicaciones en distintos campos profesionales.\n\nParticularidades de los Datos Temporales:\n\nDependencia temporal y autocorrelación.\nTendencias, estacionalidad y ciclos.\n\nAplicaciones Profesionales:\n\nCasos de estudio en finanzas, economía, medicina y otros campos.\nCómo el análisis de series de tiempo puede brindar insights valiosos.\n\nBúsqueda y Reorganización de Datos Temporales:\n\nFuentes de datos para series de tiempo (bases de datos, APIs, archivos).\nImportancia de la temporalidad y el orden en los datos.\n\nVisualización y Exploración Inicial:\n\nGráficos de línea y dispersión para identificar tendencias y patrones.\nEstudio de estacionalidad y ciclos mediante gráficos.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 6: Modelando series temporales\n\n\n\n\n\nFecha: 7 occtubre\nObjetivos:\n\nComprender los conceptos y aplicaciones de los modelos ARIMA y VAR en el análisis de series temporales.\nEvaluar las ventajas y desventajas de los métodos estadísticos para el análisis de series de tiempo y seleccionar la técnica más adecuada.\n\nContenidos\n\nModelos ARIMA:\n\nDefinición y componentes de los modelos ARIMA.\nIdentificación, Estimación y Validación de un modelo ARIMA.\nUso de correlogramas y gráficos ACF/PACF para la identificación.\n\nModelos VAR (Vector Autoregressive):\n\nIntroducción a los modelos VAR y su aplicación.\nUso de matrices de coeficientes para representar relaciones entre variables.\n\nVentajas y Desventajas de los Métodos Estadísticos:\n\nUso de modelos estadísticos en comparación con otros enfoques.\nLimitaciones y supuestos asociados a los modelos ARIMA y VAR.\n\nSelección del Método Adecuado:\n\nCriterios para elegir entre modelos ARIMA y VAR.\nConsideraciones al evaluar las alternativas disponibles.\n\nOtros modelos\n\n\nGARCH\nSARIMA y SARIMAX\nAlisado exponencial\nCambio estructural\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 7: Principales lecciones para el análisis de datos y presentaciones de proyectos\n\n\n\n\n\nFecha: TBA\nObjetivos:\n\nCerrar el curso, poniendo en contexto las principales herramientas de análisis de datos.\nPresentar un proyecto de análisis de datos\nRecibir feedback y propuestas de mejoras, tanto del trabajo propio como el de sus compañeros.\n\nContenidos\nBibliografía recomendada\nActividades de aplicación práctica\n\nProyecto: Presentaciones finales"
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión",
    "href": "sesion3_slides.html#el-análisis-de-regresión",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\n\nEn las aplicaciones de la ciencia de datos, es muy común estar interesado en la relación entre dos o más variables.\nEl análisis de regresión es una técnica en la cual buscamos encontrar una función que pueda describir la relación observada en los datos entre dos o mas variables.\nPor ejemplo, podríamos querer relacionar los pesos de los individuos con sus alturas…\n\n¿son los más altos, más pesados?\ny… ¿cuánto más pesados?"
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-regresión-simple",
    "href": "sesion3_slides.html#el-análisis-de-regresión-regresión-simple",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión: Regresión Simple",
    "text": "El análisis de regresión: Regresión Simple\n\nCaso más sencillo: univariada o regresión lineal simple.\n\nUna variable que deseamos explicar o predecir (Y) como función de otra (X).\nBuscamos la pendiente e intercepto de una funciónla recta de la forma:\n\n\n\n\\[Y = \\alpha + \\beta X\\]\ndonde:\n\nY es la variale dependiente o que deseamos entender\nX es la variable independiente\n\\(\\beta\\) es la pendiente de la recta\n\\(\\alpha\\) es la constante o intersección (el valor de y cuando x=0)"
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-1",
    "href": "sesion3_slides.html#el-análisis-de-regresión-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\nBuscamos los coeficientes de la función entre Y y X: constante y pendiente"
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-2",
    "href": "sesion3_slides.html#el-análisis-de-regresión-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\nPara esto, pensamos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes:\n\nuna que es sistemática o que se puede explicar directamente con una o más variables independientes (Xs o regresores)\ny otra que es no sistemática o error (\\(\\mu\\) o \\(epsilon\\)) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fenómeno."
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-3",
    "href": "sesion3_slides.html#el-análisis-de-regresión-3",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\n\n\n\nLa parte sistemática entonces la describimos con una forma funcional, que depende de otras variables o regresores.\n\n\nEsta forma funcional puede:\n\nser lineal univariada,\nlineal múltiple o\nno lineal.\n\n. . . El tipo de forma funcional, definirá el tipo de regresión de la que estemos hablando."
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-4",
    "href": "sesion3_slides.html#el-análisis-de-regresión-4",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\nVentajas del análisis de regersión: es facil decsribir cuantitaivamente una rlación.\nEsquemáticamente, los elementos son:"
  },
  {
    "objectID": "sesion3_slides.html#para-qué-hacer-regresiones",
    "href": "sesion3_slides.html#para-qué-hacer-regresiones",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "¿Para qué hacer regresiones?",
    "text": "¿Para qué hacer regresiones?\nPodemos pensar en tres uso, al menos, del análisis d eregresión:\n\nDescribir cuantitativamente una relación empírica\nProbar hipótesis sobre ciertas teorías\nRealizar predicciones"
  },
  {
    "objectID": "sesion3_slides.html#regresión-simple-y-scatterplot",
    "href": "sesion3_slides.html#regresión-simple-y-scatterplot",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Regresión simple y scatterplot",
    "text": "Regresión simple y scatterplot\n\nPor ejemplo, pensemos en la relación entre los años de educación y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente economía.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\n\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 años\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nBásica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegión de Ñuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nSí\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nBásica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegión de Ñuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nTécnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegión de Ñuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserción\nMedia humanista completa\nNaN"
  },
  {
    "objectID": "sesion3_slides.html#regresión-simple-y-scatterplot-1",
    "href": "sesion3_slides.html#regresión-simple-y-scatterplot-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Regresión simple y scatterplot",
    "text": "Regresión simple y scatterplot\nY lo agruparemos por región, para facilitar el ejemplo:\n\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregación\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por región\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegión de Tarapacá\n658026.6250\n11.679582\n\n\n1\nRegión de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegión de Atacama\n666128.3125\n11.126735\n\n\n3\nRegión de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegión de Valparaíso\n611298.1250\n11.559877"
  },
  {
    "objectID": "sesion3_slides.html#regresión-simple-y-scatterplot-2",
    "href": "sesion3_slides.html#regresión-simple-y-scatterplot-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Regresión simple y scatterplot",
    "text": "Regresión simple y scatterplot\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por región)')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de región a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la última palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con Línea de Regresión y Etiquetas de Región (Última Palabra)')\nplt.show()\n\n\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relación positiva: a mayor escolaridad promedio, mayor salario promedio por región."
  },
  {
    "objectID": "sesion3_slides.html#especificación",
    "href": "sesion3_slides.html#especificación",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Especificación",
    "text": "Especificación\nLlamamos especifiación al precisar la relación entre las variables que deseamos estimar. . . .\nEn nuestro caso, la función base que queremos entender es entre salario y educación: . . .\n\\[ \\text{Salario} = f(Educacion))\\]\n\nEste es una relación teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales:\n\nagregar el error aleatorio\nespecificar una forma funcional\ndefinir una forma de medir las variables en los datos\n\n\n\nEn nuestro caso, entonces el modelo especificado sería:\n. . . \\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\\]"
  },
  {
    "objectID": "sesion3_slides.html#interpretación",
    "href": "sesion3_slides.html#interpretación",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Interpretación",
    "text": "Interpretación\nCon nuestro modelo especificado:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\\]\nPodemos interpretar \\(\\beta\\) y \\(alpha\\):\n\n\\(\\beta = \\frac{\\partial ingr}{\\partial educ}\\): un año adiciónal de educación, en cuanto incrementa el salario (si nada más cambia)\n\\(\\alpha\\) valor esperado de y, si x=0…"
  },
  {
    "objectID": "sesion3_slides.html#modelo-poblaciónal-y-estimación",
    "href": "sesion3_slides.html#modelo-poblaciónal-y-estimación",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Modelo poblaciónal y estimación",
    "text": "Modelo poblaciónal y estimación\nEste modelo especificado esta definido en la población:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{años educación}_i + \\mu_i\\]\npero necesitamos calcularlo con la muestra…. por lo cual tenemos estimadores para los coeficientes poblacionales!\n\\[\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{años educación}_i \\]"
  },
  {
    "objectID": "sesion3_slides.html#modelo-poblaciónal-y-estimación-1",
    "href": "sesion3_slides.html#modelo-poblaciónal-y-estimación-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Modelo poblaciónal y estimación",
    "text": "Modelo poblaciónal y estimación\nEl método más comun de estimación es el de los mínimos cuadrados ordinarios. Veremos detalles sobre la estimación, supuestos, propiedades estadísticas la proxima sesión.\nPor ahora, pensaremos que es el método que busca la línea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresión).\n\\[ \\hat{\\mu}_i= y_i-\\hat{y}_i\\]\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuración del estilo del gráfico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gráfico de dispersión con la línea de regresión\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresión lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el término constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar líneas que conecten cada punto a la línea de regresión\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # Línea que conecta el punto a la línea de regresión\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresión y residuos')\nplt.show()\n\n\n\n\n\n\nEs decir, minimiza \\[\\sum_{i}^{n} \\hat{\\mu}_i \\]"
  },
  {
    "objectID": "sesion3_slides.html#modelo-estimado",
    "href": "sesion3_slides.html#modelo-estimado",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Modelo estimado",
    "text": "Modelo estimado\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regiónTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:26:49   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nPodemos ver que un año adicional de educación ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¿y la constante, como la podemos interpretar?"
  },
  {
    "objectID": "sesion3_slides.html#modelos-simples-y-múltiples",
    "href": "sesion3_slides.html#modelos-simples-y-múltiples",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Modelos simples y múltiples",
    "text": "Modelos simples y múltiples\nMuchas veces una sola variable no es suficiente para describir bien un fenómeno. Necesitamos incluir más variables.\nEsto puede ser:\n\nUna nueva variable\nUna forma funcional no lineal de la variable ya incluida\n\nNuestra interpretación del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores."
  },
  {
    "objectID": "sesion3_slides.html#modelos-simples-y-múltiples-1",
    "href": "sesion3_slides.html#modelos-simples-y-múltiples-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Modelos simples y múltiples",
    "text": "Modelos simples y múltiples\nProbemos, agregar edad al modelo:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta_1 \\text{años educación}_i + \\beta_2 \\text{edad}_i + \\mu_i\\]\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como término constante\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "sesion3_slides.html#modelos-simples-y-múltiples-2",
    "href": "sesion3_slides.html#modelos-simples-y-múltiples-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Modelos simples y múltiples",
    "text": "Modelos simples y múltiples\nEs muy usual, agregar edad al cuadrado…. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer…\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta_1 \\text{años educación}_i + \\beta_2 \\text{edad}_i  + \\beta_3 \\text{edad}^2_i + \\mu_i\\]\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el término constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresión lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "sesion3_slides.html#ya-no-es-lineal-el-modelo",
    "href": "sesion3_slides.html#ya-no-es-lineal-el-modelo",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Ya no es lineal el modelo?",
    "text": "Ya no es lineal el modelo?\nOjo! La linealidad es en los parámetros, no en las variables.\nLa siguiente ecuación muestra un modelo lineal en el que el predictor 𝑥1 no es lineal respecto a y:\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2log(x_1) + \\epsilon\\)\n\nEn contraposición, el siguiente no es un modelo lineal:\n\\(y = \\beta_0 + \\beta_1x_1^{\\beta_2} + \\epsilon\\)"
  },
  {
    "objectID": "sesion3_slides.html#ya-no-es-lineal-el-modelo-1",
    "href": "sesion3_slides.html#ya-no-es-lineal-el-modelo-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Ya no es lineal el modelo?",
    "text": "Ya no es lineal el modelo?\nEn ocasiones, algunas relaciones no-lineales pueden transformarse de forma que se pueden expresar de manera lineal:\n\nModelo no-lineal a estimar: \\(y = \\beta_0x_1^{\\beta_1}\\epsilon\\)\nSolucion: pasamos todo a logaritmos:\n\n\\[log(y)=log(\\beta_0) + \\beta_1log(x_1) + log(\\epsilon)\\]\n\\[y^{'}=\\beta_0^{'}+\\beta_1x_1^{'} + \\epsilon^{'}\\]\n\nEstimar el modelo y extraer los coeficientes.\nVolvera a la forma funcional incial exponenciando los logaritmos.\n\n\\(\\beta_1\\) es explicito.\n\\(\\beta_0^{'}=log(\\beta_0)=&gt; exp(log(\\beta_0))\\)"
  },
  {
    "objectID": "sesion3_slides.html#un-poco-más-sobre-interpretación",
    "href": "sesion3_slides.html#un-poco-más-sobre-interpretación",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Un poco más sobre interpretación",
    "text": "Un poco más sobre interpretación\nElementos clave en la interpretación de un modelo de regresión lineal:\n\n\\(\\beta_0\\): Ordenada en el origen, valor esperado de \\(y\\) cuando todos los predictores son cero.\n\\(\\beta_j\\): Coeficientes de regresión parcial de cada predictor, representan el cambio promedio esperado en \\(y\\) al aumentar en una unidad \\(x_j\\), manteniendo otros predictores constantes (“ceteris paribus”)."
  },
  {
    "objectID": "sesion3_slides.html#un-poco-más-sobre-interpretación-magnitud",
    "href": "sesion3_slides.html#un-poco-más-sobre-interpretación-magnitud",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Un poco más sobre interpretación: Magnitud",
    "text": "Un poco más sobre interpretación: Magnitud\nLos coeficientes están medidos en las unidades que se está trabajando.\n\nImportancia de coeficientes parciales estandarizados: - Se obtienen al estandarizar las variables predictoras antes del ajuste del modelo. - \\(\\beta_0\\) refleja el valor esperado de \\(y\\) cuando los predictores están en su promedio. - \\(\\beta_j\\) indica el cambio promedio esperado en \\(y\\) al aumentar en una desviación estándar \\(x_j\\), manteniendo otros predictores constantes."
  },
  {
    "objectID": "sesion3_slides.html#causalidad-regresión-y-correlación",
    "href": "sesion3_slides.html#causalidad-regresión-y-correlación",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Causalidad, regresión y correlación",
    "text": "Causalidad, regresión y correlación\nImportante tener en cuenta:\n\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relación entre las variables de interés.\nEsto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes más altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociación significativa entre las dos variables.\nUn diagrama de dispersión puede ser una herramienta útil para determinar la fuerza de la relación entre dos variables."
  },
  {
    "objectID": "sesion3_slides.html#causalidad-regresión-y-correlación-1",
    "href": "sesion3_slides.html#causalidad-regresión-y-correlación-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Causalidad, regresión y correlación",
    "text": "Causalidad, regresión y correlación\nImportante tener en cuenta:\n\nSi parece no haber asociación entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersión no indica ninguna tendencia creciente o decreciente),\n\nentonces ajustar un modelo de regresión lineal a los datos probablemente no proporcionará un modelo útil.\nUna valiosa medida numérica de asociación entre dos variables es el coeficiente de correlación, que es un valor entre -1 y 1 que indica la fuerza de la asociación de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3_slides.html#una-perspectiva-histórica",
    "href": "sesion3_slides.html#una-perspectiva-histórica",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Una perspectiva histórica:",
    "text": "Una perspectiva histórica:\n\nEl origen de la técnica, podemos remontarlo a la genética.\nFrancis Galton estudió la variación y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolectó y estudió datos de altura de familias para tratar de entender la herencia. Mientras hacía esto, desarrolló los conceptos de correlación y regresión.\nPregunta: ¿qué tan bien podemos predecir la estatura de un niño basado en la estatura de los padres?\nLa técnica que desarrolló para responder a esta pregunta, la regresión, también puede aplicarse en muchas otras circunstancias."
  },
  {
    "objectID": "sesion3_slides.html#una-perspectiva-histórica-1",
    "href": "sesion3_slides.html#una-perspectiva-histórica-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Una perspectiva histórica:",
    "text": "Una perspectiva histórica:\n\n\n\n\nNota histórica:\n\nGalton hizo importantes contribuciones a la estadística y la genética…\npero también fue uno de los primeros defensores de la eugenesia…\nun movimiento filosófico científicamente defectuoso favorecido por muchos biólogos de la época de Galton, pero con terribles consecuencias históricas."
  },
  {
    "objectID": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura",
    "href": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Estudio de caso: ¿es hereditaria la altura?",
    "text": "Estudio de caso: ¿es hereditaria la altura?\n\nTenemos acceso a los datos de altura de familias recolectado por Galton, a través del paquete HistData.\nEstos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n\n\nCargar datosTabla de datos\n\n\n\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\nprint(galton_data.head(4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\nmidparentHeight\nchildren\nchildNum\ngender\nchildHeight\n\n\n\n\n0\n001\n78.5\n67.0\n75.43\n4\n1\nmale\n73.2\n\n\n1\n001\n78.5\n67.0\n75.43\n4\n2\nfemale\n69.2\n\n\n2\n001\n78.5\n67.0\n75.43\n4\n3\nfemale\n69.0\n\n\n3\n001\n78.5\n67.0\n75.43\n4\n4\nfemale\n69.0"
  },
  {
    "objectID": "sesion3_slides.html#análisis-de-caso-es-hereditaria-la-altura",
    "href": "sesion3_slides.html#análisis-de-caso-es-hereditaria-la-altura",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Análisis de caso: ¿es hereditaria la altura?",
    "text": "Análisis de caso: ¿es hereditaria la altura?\nPara imitar el análisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n\nCargar datosTabla de datos\n\n\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfather\nson\n\n\n\n\n0\n78.5\n73.2\n\n\n1\n75.5\n73.5\n\n\n2\n75.0\n71.0\n\n\n3\n75.0\n68.5\n\n\n4\n75.0\n68.0"
  },
  {
    "objectID": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-1",
    "href": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Estudio de caso: ¿es hereditaria la altura?",
    "text": "Estudio de caso: ¿es hereditaria la altura?\n\nSupongamos que se nos pidiera que resumiéramos (describieramos) los datos de padres e hijos.\nComo ambas distribuciones están aproximadas por la distribución normal, podríamos usar los dos promedios y dos desviaciones estándar como resúmenes:\n\n\n\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nprint(resumen_estadistico)\n\n\n\n   promedio_padre  sd_padre  promedio_hijo   sd_hijo\n0       69.098883  2.546555      69.248045  2.680733\n\n\nSin embargo, este resumen no describe una característica importante de los datos:\nla tendencia de que cuanto más alto es el padre, más alto es el hijo."
  },
  {
    "objectID": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-2",
    "href": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Estudio de caso: ¿es hereditaria la altura?",
    "text": "Estudio de caso: ¿es hereditaria la altura?\n\nCodePlot\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gráfico de dispersión con línea de regresión\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relación entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gráfico\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#regresión-y-la-correlación",
    "href": "sesion3_slides.html#regresión-y-la-correlación",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "¿Regresión? ¿Y la correlación?",
    "text": "¿Regresión? ¿Y la correlación?\n\n\n\nAmbos están muy relacionados.\nAprenderemos que el coeficiente de correlación es un resumen informativo de cómo dos variables se mueven juntas…\ny luego veremos cómo esto puede ser usado para predecir una variable usando la otra y modelado en una regresión"
  },
  {
    "objectID": "sesion3_slides.html#taller-de-aplicación-2",
    "href": "sesion3_slides.html#taller-de-aplicación-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Taller de aplicación 2:",
    "text": "Taller de aplicación 2:\nCaso aplicación: Cursos de Verano\n\n\n\n\n\n\nTaller de aplicación 2: Pregunta 1\n\n\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que queríamos responder:\nAsistir a cursos de verano mejora los resultados académicos?\n\n\nPlantee un modelo de regresión con los datos disponibles que deseamos estimar.\nGrafique la dispersión y la recta de regresión estimada."
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlación",
    "href": "sesion3_slides.html#el-coeficiente-de-correlación",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El coeficiente de correlación",
    "text": "El coeficiente de correlación\nEl coeficiente de correlación se define para una lista de pares \\((x_1,y_1),...(x_n,y_n)\\) como la media de los productos de los valores normalizados:\n\\[\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n\\]\nDónde \\(\\mu\\) son promedios y \\(\\sigma\\) son desviaciones estándar. La letra griega para r, \\(\\rho\\) se utiliza comúnmente en los libros de estadística para denotar la correlación, porque es la primera letra de regresión. Pronto aprenderemos sobre la conexión entre correlación y regresión.\nPodemos representar la fórmula anterior con el código R usando:\nrho &lt;- mean(scale(x) * scale(y))\nLa correlación entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\):"
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlación-1",
    "href": "sesion3_slides.html#el-coeficiente-de-correlación-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El coeficiente de correlación",
    "text": "El coeficiente de correlación\nPodemos representar la fórmula anterior con el siguiente código usando:\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aquí\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aquí\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n\n\n\n0.9999999999999998"
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlación-2",
    "href": "sesion3_slides.html#el-coeficiente-de-correlación-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El coeficiente de correlación",
    "text": "El coeficiente de correlación\nLa correlación entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\).\n\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlación:\", correlation_coefficient)\n\n\n\nCoeficiente de Correlación: 0.4501189204142688\n\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por género masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviación estándar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviación Estándar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviación Estándar de la Altura del Padre:\", sd_father)\n\n\n\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviación Estándar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviación Estándar de la Altura del Padre: 2.546555038637639"
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlación-3",
    "href": "sesion3_slides.html#el-coeficiente-de-correlación-3",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El coeficiente de correlación",
    "text": "El coeficiente de correlación\n\n# Calcular la correlación entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n\n\n0.5104684885147245\n\n\nPara ver cómo se ven los datos para los diferentes valores de \\(\\rho\\) aquí hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\nimage"
  },
  {
    "objectID": "sesion3_slides.html#la-correlación-es-variable-aleatoria",
    "href": "sesion3_slides.html#la-correlación-es-variable-aleatoria",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La correlación es variable aleatoria",
    "text": "La correlación es variable aleatoria\nAntes de continuar conectando la correlación con la regresión, recordemos la variabilidad aleatoria.\nEn la mayoría de las aplicaciones de la ciencia de datos, observamos datos que incluyen variación aleatoria.\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra población. Un genetista menos afortunado sólo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlación de la muestra se puede calcular con:\n\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tamaño 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlación entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlación en la Muestra:\", correlation_coefficient)\n\n\n\nCoeficiente de Correlación en la Muestra: 0.5072834085504967"
  },
  {
    "objectID": "sesion3_slides.html#section",
    "href": "sesion3_slides.html#section",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "",
    "text": "R es una variable aleatoria. Podemos ejecutar una simulación de Monte Carlo para ver su distribución:\n\nNota: el objetivo principal de la simulación de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir cómo van a evolucionar.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlación\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlación\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlación\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#section-1",
    "href": "sesion3_slides.html#section-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "",
    "text": "Vemos que el valor esperado de R es la correlación de la población:\n\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlación:\", mean_R)\n\n\n\nMedia de Coeficientes de Correlación: 0.45842882440885\n\n\ny que tiene un error estándar relativamente alto en relación con el rango de valores que puede tomar R:\n\nsd_R = np.std(R)\nprint(\"Desviación Estándar de Coeficientes de Correlación:\", sd_R)\n\n\n\nDesviación Estándar de Coeficientes de Correlación: 0.047460810230179874"
  },
  {
    "objectID": "sesion3_slides.html#section-2",
    "href": "sesion3_slides.html#section-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "",
    "text": "Por lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\nAdemás, tenga en cuenta que debido a que la correlación de la muestra es un promedio de extracciones independientes, el teorema del límite central realmente funciona.\nPor lo tanto, para \\(N\\) lo suficientemente grande la distribución de \\(R\\) es aproximadamente normal con el valor esperado \\(\\rho\\).\nLa desviación estándar, que es algo compleja de derivar, es: \\(\\sqrt{\\frac{1-r^2}{N-2}}\\)."
  },
  {
    "objectID": "sesion3_slides.html#section-3",
    "href": "sesion3_slides.html#section-3",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "",
    "text": "En nuestro ejemplo, \\(N=25\\) no parece ser lo suficientemente grande para que la aproximación sea buena\n\n-Si N aumenta verás que la distribución converge a una normal.\n\nNota: El gráfico Q-Q, o gráfico cuantitativo, es una herramienta gráfica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribución teórica como una Normal o exponencial.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlación\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tamaño de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gráfico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Teóricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gráfico QQ-plot para los Coeficientes de Correlación\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # Línea de referencia\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#la-correlación-no-siempre-es-un-resumen-útil",
    "href": "sesion3_slides.html#la-correlación-no-siempre-es-un-resumen-útil",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La correlación no siempre es un resumen útil",
    "text": "La correlación no siempre es un resumen útil\nLa correlación no siempre es un buen resumen de la relación entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlación de 0,82:\n\nimageLa correlación sólo tiene sentido en un contexto particular. Para ayudarnos a entender cuándo es que la correlación es significativa como estadística de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudará a motivar y definir la regresión lineal. Comenzamos demostrando cómo la correlación puede ser útil para la predicción."
  },
  {
    "objectID": "sesion3_slides.html#correlación-espuria",
    "href": "sesion3_slides.html#correlación-espuria",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Correlación espuria",
    "text": "Correlación espuria\nVemos una fuerte correlación entre las tasas de divorcio y el consumo de margarina.\n\nimage(Acá pueden encontrar más http://tylervigen.com/old-version.html)\n\n¿Significa esto que la margarina causa divorcios?\n\n¿O los divorcios hacen que la gente coma más margarina?"
  },
  {
    "objectID": "sesion3_slides.html#la-paradoja-de-simpson",
    "href": "sesion3_slides.html#la-paradoja-de-simpson",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La paradoja de Simpson",
    "text": "La paradoja de Simpson\n\nSe llama paradoja porque vemos el signo de la correlación cambiar cuando comparamos toda la data y estratos específicos.\nComo ejemplo ilustrativo, supongamos que tiene tres variables aleatorias \\(X\\), \\(Y\\) y \\(Z\\) y que observamos realizaciones de estas.\nAquí está el gráfico de observaciones simuladas para \\(X\\) y \\(Y\\) a lo largo de la correlación de la muestra:"
  },
  {
    "objectID": "sesion3_slides.html#la-paradoja-de-simpson-1",
    "href": "sesion3_slides.html#la-paradoja-de-simpson-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La paradoja de Simpson",
    "text": "La paradoja de Simpson\n\nPuedes ver que \\(X\\) e \\(Y\\) están negativamente correlacionados.\nSin embargo, una vez que estratificamos por \\(Z\\) (mostrado en diferentes colores abajo) emerge otro patrón:"
  },
  {
    "objectID": "sesion3_slides.html#la-paradoja-de-simpson-2",
    "href": "sesion3_slides.html#la-paradoja-de-simpson-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La paradoja de Simpson",
    "text": "La paradoja de Simpson\n\n\nEs realmente \\(Z\\) que está negativamente correlacionado con \\(X\\).\nSi estratificamos por \\(Z\\) las variables \\(X\\) e \\(Y\\) están en realidad correlacionados positivamente como se ha visto en el gráfico anterior."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales",
    "href": "sesion3_slides.html#expectativas-condicionales",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nSupongamos que nos piden que adivinemos la altura de un hijo seleccionado al azar y no sabemos la altura de su padre.\nDebido a que la distribución de las alturas de los hijos es aproximadamente normal, sabemos que la altura media, \\(69.2\\), es el valor con la mayor proporción y sería la predicción con mayores posibilidades de minimizar el error.\nPero, ¿y si nos dicen que el padre es más alto que el promedio, digamos que mide 72 pulgadas de alto, todavía esperaríamos que la altura más probable del hijo sea 69.2 pulgadas?"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-1",
    "href": "sesion3_slides.html#expectativas-condicionales-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "expectativas condicionales",
    "text": "expectativas condicionales\n\nResulta que si pudiéramos recolectar datos de un gran número de padres que miden 72 pulgadas…\n\nla distribución de las alturas de sus hijos sería normalmente distribuida.\nEsto implica que el promedio de la distribución calculada en este subconjunto sería nuestra mejor predicción."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-2",
    "href": "sesion3_slides.html#expectativas-condicionales-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "expectativas condicionales",
    "text": "expectativas condicionales\n\nEn general, llamamos a este enfoque condicional.\nLa idea general es que estratificamos una población en grupos y calculamos resúmenes en cada grupo.\nPor lo tanto, el condicionamiento está relacionado con el concepto de estratificación descrito.\nPorque la expectativa condicional \\(E(Y|X=x)\\) es el mejor predictor para la variable aleatoria \\(Y\\) para un individuo en los estratos definidos por \\(X=x\\) muchos de los desafíos de la ciencia de datos se reducen a la estimación de esta cantidad."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-3",
    "href": "sesion3_slides.html#expectativas-condicionales-3",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nEn el ejemplo que hemos estado considerando, estamos interesados en calcular la altura promedio del hijo condicionada a que el padre tenga 72 pulgadas de altura.\nQueremos estimar \\(E(Y|X=72)\\) usando la muestra recolectada por Galton.\n¿Cuantos padres miden 72?\n\n\n\ncount_72 = (galton_heights['father'] == 72).sum()\nprint(\"Cantidad de registros con valor 72 en la columna 'father':\", count_72)\n\n\n\nCantidad de registros con valor 72 en la columna 'father': 8\n\n\n\nSi cambiamos el número a 72.5, obtenemos aún menos puntos de datos:\n\n\n\n\ncount_725 = (galton_heights['father'] == 72.5).sum()\nprint(\"Cantidad de registros con valor 72.5 en la columna 'father':\", count_725)\n\n\n\nCantidad de registros con valor 72.5 en la columna 'father': 1"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-4",
    "href": "sesion3_slides.html#expectativas-condicionales-4",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nUna forma práctica de mejorar estas estimaciones de las expectativas condicionales, es definir estratos con valores similares de \\(x\\).\nEn nuestro ejemplo, podemos redondear las alturas paternas a la pulgada más cercana y asumir que todas son de 72 pulgadas.\nSi hacemos esto, terminamos con la siguiente predicción para el hijo de un padre que mide 72 pulgadas de alto:\n\n\nconditional_avg = galton_heights[galton_heights['father'].round() == 72]['son'].mean()\nprint(\"Promedio condicional para father == 72:\", conditional_avg)\n\n\n\nPromedio condicional para father == 72: 70.44285714285715"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-5",
    "href": "sesion3_slides.html#expectativas-condicionales-5",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nNote que un padre de 72 pulgadas es más alto que el promedio – específicamente, 72 - 69.1/2.5 = 1.1 desviaciones estándar más alto que el padre promedio.\nNuestra predicción, \\(70.5\\), es también más alta que el promedio, pero sólo \\(0.49\\) desviaciones estándar más grandes que el hijo promedio.\nLos hijos de padres de 72 pulgadas han regresado algunos a la estatura promedio.\nObservamos que la reducción en el número de SD más altas es de alrededor de \\(0.5\\), lo que resulta ser la correlación.\nComo veremos en una sección posterior, esto no es una coincidencia."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-6",
    "href": "sesion3_slides.html#expectativas-condicionales-6",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nSi queremos hacer una predicción de cualquier altura, no sólo de 72, podríamos aplicar el mismo enfoque a cada estrato.\nLa estratificación seguida de los boxplots nos permite ver la distribución de cada grupo:\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Crear una nueva columna 'father_strata' con los valores redondeados de 'father'\ngalton_heights['father_strata'] = galton_heights['father'].round().astype(int)\n\n# Crear el gráfico de boxplots\nplt.figure(figsize=(10, 6))  # Tamaño del gráfico\nsns.boxplot(data=galton_heights, x='father_strata', y='son')\n\n# Agregar puntos para mostrar las medias condicionadas\nsns.swarmplot(data=galton_heights, x='father_strata', y='son', color='black', size=4)\n\nplt.xlabel('father_strata')\nplt.ylabel('son')\nplt.title('Boxplots de son condicionado por father_strata con Medias Condicionadas')\nplt.xticks(rotation=45)  # Rotar etiquetas del eje x si es necesario\n\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-7",
    "href": "sesion3_slides.html#expectativas-condicionales-7",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\nNo es de extrañar que los centros de los grupos aumenten con la altura.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n# Crear un gráfico de puntos para mostrar el promedio condicional por \"father\"\nplt.figure(figsize=(10, 6))\nplt.scatter(conditional_avg_by_father['father'], conditional_avg_by_father['son'], color='blue')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres\")\nplt.show()\n\n\n\n\n\n\n\nAdemás, estos centros parecen seguir una relación lineal."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-8",
    "href": "sesion3_slides.html#expectativas-condicionales-8",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nA continuación se presentan los promedios de cada grupo.\nSi tenemos en cuenta que estos promedios son variables aleatorias con errores estándar, los datos son consistentes con estos puntos siguiendo una línea recta:\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n\nconditional_avg_by_father.head()\n\n\n# Crear un gráfico de puntos con ajuste de regresión lineal\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='father', y='son', data=conditional_avg_by_father, color='blue')\nsns.regplot(x='father', y='son', data=conditional_avg_by_father, scatter=False, color='orange')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres con Regresión Lineal\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-9",
    "href": "sesion3_slides.html#expectativas-condicionales-9",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nEl hecho de que estos promedios condicionales sigan una línea no es una coincidencia.\nEn la siguiente sección, explicamos que la línea que siguen estos promedios es lo que llamamos la línea de regresión, que mejora la precisión de nuestras estimaciones.\nSin embargo, no siempre es apropiado estimar las expectativas condicionales con la línea de regresión, por lo que también describimos la justificación teórica de Galton para usar la línea de regresión."
  },
  {
    "objectID": "sesion3_slides.html#la-línearecta-de-regresión",
    "href": "sesion3_slides.html#la-línearecta-de-regresión",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La línea/recta de regresión",
    "text": "La línea/recta de regresión\n\nSi estamos prediciendo una variable aleatoria \\(Y\\) conociendo el valor de otra variable \\(X=x\\) usando una línea de regresión, entonces predecimos que para cada desviación estándar, \\(\\sigma_x\\) que \\(x\\) aumenta por encima de la media \\(\\mu_x\\), \\(Y\\) incrementa \\(\\rho\\) veces la desviación estándar \\(\\sigma_Y\\) sobre el promedio \\(\\mu_Y\\), con \\(\\rho\\) la correlación entre \\(X\\) e \\(Y\\). Por lo tanto, la formula de la regresión es:\n\n\\[\n\\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right)=\\rho \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\n\\]\nLo que podemos reescribir como:\n\\[\nY=\\mu_Y + \\rho \\big(\\frac{x-\\mu_X}{\\sigma_X}\\big) \\sigma_Y\n\\]"
  },
  {
    "objectID": "sesion3_slides.html#la-línearecta-de-regresión-1",
    "href": "sesion3_slides.html#la-línearecta-de-regresión-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La línea/recta de regresión",
    "text": "La línea/recta de regresión\n\nSi existe una correlación perfecta, la línea de regresión predice un aumento que corresponde al mismo número de desviacones estándar.\nSi hay correlación 0, entonces no usamos \\(x\\) en absoluto en la predicción y simplemente predecimos el promedio \\(\\mu_Y\\).\nPara valores entre 0 y 1, la predicción se encuentra en un punto intermedio.\nSi la correlación es negativa, predecimos una reducción en lugar de un aumento."
  },
  {
    "objectID": "sesion3_slides.html#regresión-a-la-media",
    "href": "sesion3_slides.html#regresión-a-la-media",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Regresión a la media",
    "text": "Regresión a la media\n\nNótese que si la correlación es positiva e inferior a 1, nuestra predicción está más cerca (en unidades estándar) de la altura media que de lo que el valor utilizado para predecir, \\(x\\), está del promedio de los \\(x\\).\nPor eso lo llamamos regresión: el hijo regresa a la estatura media.\nDe hecho, el título del artículo de Galton era: Regresión a la mediocridad en la estatura hereditaria (Regression toward mediocrity in hereditary stature.)."
  },
  {
    "objectID": "sesion3_slides.html#la-línearecta-de-regresión-2",
    "href": "sesion3_slides.html#la-línearecta-de-regresión-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La línea/recta de regresión",
    "text": "La línea/recta de regresión\n\nPara añadir líneas de regresión a los gráficos, necesitaremos la fórmula anterior en la forma: \\(y=b+mx\\), con pendiente \\(m=\\rho \\sigma_y / \\sigma_x\\) e intercepto \\(b=\\mu_y - m \\mu_x\\)\nAquí agregamos la línea de regresión a la data original.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cálculo de las medias y desviaciones estándar\nmu_x = galton_heights['father'].mean()\nmu_y = galton_heights['son'].mean()\ns_x = galton_heights['father'].std()\ns_y = galton_heights['son'].std()\n\n# Cálculo del coeficiente de correlación\nr = galton_heights['father'].corr(galton_heights['son'])\n\n# Cálculo de la pendiente y el intercepto para la línea de regresión\nm = r * s_y / s_x\nb = mu_y - m * mu_x\n\n# Configuración del tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gráfico de dispersión con línea de regresión\nsns.scatterplot(x='father', y='son', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father', y='son', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Son Height\")\nplt.title(\"Relación entre Altura de Padres e Hijos con Línea de Regresión\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#la-línearecta-de-regresión-3",
    "href": "sesion3_slides.html#la-línearecta-de-regresión-3",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "La línea/recta de regresión",
    "text": "La línea/recta de regresión\n\nLa fórmula de regresión implica que si primero estandarizamos las variables, es decir, restamos el promedio y dividimos por la desviación estándar, entonces la línea de regresión tiene intercepto 0 y pendiente igual a la correlación \\(\\rho\\).\nAquí está la misma gráfica, pero usando unidades estándar:\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Estandarizar las variables 'father' y 'son'\ngalton_heights['father_standardized'] = (galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std()\ngalton_heights['son_standardized'] = (galton_heights['son'] - galton_heights['son'].mean()) / galton_heights['son'].std()\n\n# Calcular la correlación de las variables estandarizadas\nr = galton_heights['father_standardized'].corr(galton_heights['son_standardized'])\n\n# Configuración del tamaño de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gráfico de dispersión con línea de regresión\nsns.scatterplot(x='father_standardized', y='son_standardized', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father_standardized', y='son_standardized', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height (Standardized)\")\nplt.ylabel(\"Son Height (Standardized)\")\nplt.title(\"Relación Estandarizada entre Altura de Padres e Hijos con Línea de Regresión (Intercepto = 0, Pendiente = Correlación)\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#interpretación-1",
    "href": "sesion3_slides.html#interpretación-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Interpretación:",
    "text": "Interpretación:\nEn ambos casos, la interpretación de los elementos del modelo es la misma:\n\n\\(\\beta_0\\): es la ordenada en el origen, se corresponde con el valor promedio de la variable respuesta \\(y\\) cuando todos los predictores son cero.\n\\(\\beta_j\\): es el efecto promedio que tiene sobre la variable respuesta el incremento en una unidad de la variable predictora \\(x_j\\), manteniéndose constantes el resto de variables. Se conocen como coeficientes de regresión.\n\\(\\epsilon\\): es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo. Recoge el efecto de todas aquellas variables que influyen en \\(y\\) pero que no se incluyen en el modelo como predictores."
  },
  {
    "objectID": "sesion3_slides.html#interpretación-2",
    "href": "sesion3_slides.html#interpretación-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Interpretación:",
    "text": "Interpretación:\n\nEn la gran mayoría de casos, los valores \\(\\beta_0\\) y \\(\\beta_j\\) poblacionales se desconocen, por lo que, a partir de una muestra, se obtienen sus estimaciones \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_j}\\).\nAjustar el modelo consiste en estimar, a partir de los datos disponibles, los valores de los coeficientes de regresión que maximizan la verosimilitud (likelihood), es decir, los que dan lugar al modelo que con mayor probabilidad puede haber generado los datos observados.\nEl método empleado con más frecuencia es el ajuste por mínimos cuadrados ordinarios (OLS)\n\nque identifica como mejor modelo la recta (o plano si es regresión múltiple)\nque minimiza la suma de las desviaciones verticales entre cada dato de entrenamiento y la recta, elevadas al cuadrado."
  },
  {
    "objectID": "sesion3_slides.html#magnitud-y-significancia",
    "href": "sesion3_slides.html#magnitud-y-significancia",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Magnitud y significancia",
    "text": "Magnitud y significancia\n\nLa magnitud de cada coeficiente parcial de regresión depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no está asociada con la importancia de cada predictor.\nUna buena practica es estandarizar"
  },
  {
    "objectID": "sesion3_slides.html#cuidado-hay-dos-líneas-de-regresión",
    "href": "sesion3_slides.html#cuidado-hay-dos-líneas-de-regresión",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Cuidado: hay dos líneas de regresión",
    "text": "Cuidado: hay dos líneas de regresión\nCalculamos una línea de regresión para predecir la altura del hijo desde la altura del padre.\nUsamos estos cálculos:\n\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviación estándar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviación estándar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlación entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n\n\n\n0.4658308243792417\n2.56441709039222\n2.6076529186746025\n69.08938547486034\n69.263687150838\n\n\n\n# Calcular la pendiente de la primera línea de regresión\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera línea de regresión\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n\n\n\npendiente 0.47368468778038647\nconstante 36.53710316324001"
  },
  {
    "objectID": "sesion3_slides.html#cuidado-hay-dos-líneas-de-regresión-1",
    "href": "sesion3_slides.html#cuidado-hay-dos-líneas-de-regresión-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Cuidado: hay dos líneas de regresión",
    "text": "Cuidado: hay dos líneas de regresión\n\n¿Y si queremos predecir la estatura del padre basándonos en la del hijo?\nEs importante saber que esto no se determina calculando la función inversa!.\nNecesitamos computar \\(E(X∣Y=y)\\). Dado que los datos son aproximadamente normales bivariados, la teoría descrita anteriormente nos dice que esta expectativa condicional seguirá una línea con pendiente e intercepto:\n\n\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n\n\n\npendiente 0.4581071808731349\nconstante 37.35919301731116"
  },
  {
    "objectID": "sesion3_slides.html#cuidado-hay-dos-líneas-de-regresión-2",
    "href": "sesion3_slides.html#cuidado-hay-dos-líneas-de-regresión-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Cuidado: hay dos líneas de regresión",
    "text": "Cuidado: hay dos líneas de regresión\n\nAquí hay un gráfico que muestra las dos líneas de regresión:\ncon azul para la predicción de las alturas del hijo con las alturas del padre y rojo para la predicción de las alturas del padre con las alturas del hijo.\n\n\nCodigoplot\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gráfico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#taller-aplicación-2-altura-de-madres-padres-hijos-e-hijas",
    "href": "sesion3_slides.html#taller-aplicación-2-altura-de-madres-padres-hijos-e-hijas",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Taller aplicación 2: ALtura de madres, padres, hijos e hijas",
    "text": "Taller aplicación 2: ALtura de madres, padres, hijos e hijas\n\n\n\n\n\n\nTaller aplicacción 2: Altura de madres, padres, hijos e hijas\n\n\n\nCargue los datos de GaltonFamilies desde el HistData. Los niños de cada familia están ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado galton_heights seleccionando niños y niñas al azar. (HINT: use sample).\nHaga una gráfica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nCalcular la correlación para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nPlotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\nObtener el modelo de regresión e interpretar los coeficientes.\n\n\n\n\n\n\n\nCurso Análisis de Datos - Sesión 3"
  },
  {
    "objectID": "sesion1_notas.html",
    "href": "sesion1_notas.html",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "",
    "text": "Notas detalladas de la sesión 1, curso análisis de datos, magíster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versión 1"
  },
  {
    "objectID": "sesion1_notas.html#detalles",
    "href": "sesion1_notas.html#detalles",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "",
    "text": "Notas detalladas de la sesión 1, curso análisis de datos, magíster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versión 1"
  },
  {
    "objectID": "sesion1_notas.html#objetivos-de-aprendizaje-de-la-sesión",
    "href": "sesion1_notas.html#objetivos-de-aprendizaje-de-la-sesión",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Objetivos de aprendizaje de la sesión",
    "text": "Objetivos de aprendizaje de la sesión\n\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos, junto a buenas prácticas que promuevan la transparencia y replicabilidad.\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados."
  },
  {
    "objectID": "sesion1_notas.html#contenidos",
    "href": "sesion1_notas.html#contenidos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Contenidos:",
    "text": "Contenidos:\n\nEl proceso de análisis de datos\n\nEl proceso de análisis de datos\n\nUna visión general a las metodologías de análisis que veremos en el curso\nAdquision y almacenmiento de los datos\nPreparación de los datos\n\nPreguntando a los datos\n\nAsbtrayendo la realidad, variables aleatorias y probabilidades.\nPlanteamiento de preguntas.\nPreguntas y respuestas: el rol de las hipótesis.\n\n\n\n\nRespondiendo desde los datos: Pruebas de hipótesis\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\n\n\n\nBuenas prácticas en análisis de datos\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1_notas.html#el-proceso-de-análisis-de-datos-1",
    "href": "sesion1_notas.html#el-proceso-de-análisis-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "El proceso de análisis de datos",
    "text": "El proceso de análisis de datos\nEn el mundo actual, la generación y recopilación de datos se ha vuelto más accesible y significativa que nunca antes. Esta abundancia de información ofrece la oportunidad de extraer conocimientos valiosos que pueden influir en la toma de decisiones y el desarrollo de soluciones eficientes.\nSin embargo, el proceso de transformar estos datos crudos en información útil y significativa requiere una serie de pasos fundamentales que forman parte integral de la disciplina conocida como Ciencia de Datos.\n\nEn esta primera parte, daremos un vistazo general a las metodologías y enfoques clave que exploraremos a lo largo del curso, con énfasis en la importancia de la preparación de los datos.\nEl proceso de análisis de datos se puede dividir en varias etapas interconectadas, cada una con su propio conjunto de desafíos y consideraciones.\n\nBajo esta mirada, tenemos varias fases clave que están interconectadas. En este curso nos enfocaremos en la preparación de los datos y en su análisis mediante modelos de regresión. Esto con el objetivo de responder preguntas desde los datos, que provean información valiosa.\n\nAdquisición de datos:\nEl primer paso en el proceso de análisis de datos implica la adquisición y el almacenamiento de los datos. Esto se refiere a la recolección de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilación de datos de fuentes diversas, como bases de datos, archivos CSV, páginas web o incluso sensores en tiempo real.\nEs crucial comprender cómo recopilar y almacenar estos datos de manera adecuada, garantizando su calidad, integridad y seguridad.\nExisten tantas fuentes de datos, como podríamos imaginar. ALgunas de las más comunes son las siguientes:\n\nEncuestas y Cuestionarios:\n\nDiseño y administración de encuestas para recopilar datos directamente de los participantes.\nPermite obtener información específica y detallada según las preguntas planteadas.\n\nExperimentos Controlados:\n\nDiseño de experimentos para recopilar datos bajo condiciones controladas.\nÚtil para establecer relaciones causales y evaluar efectos de cambios controlados.\n\nObservación y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar información ambiental.\nUtilización de sensores en dispositivos móviles y wearables para recopilar datos de ubicación, salud y actividad.\n\nRecopilación de Datos Existentes:\n\nUtilización de datos ya recopilados y disponibles en bases de datos o fuentes públicas.\nReduce el tiempo y costo de recopilación, pero puede tener limitaciones en términos de calidad y relevancia.\n\nWeb Scraping (Web Scrapping):\n\nExtracción de datos de sitios web utilizando herramientas y técnicas automatizadas.\nPermite recopilar información no estructurada de manera eficiente, pero requiere atención a la ética y términos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracción programática con sistemas y servicios para obtener datos en tiempo real.\nComún en la obtención de datos de redes sociales, información climática, finanzas, entre otros.\n\nColaboración y Participación Comunitaria:\n\nColaboración con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser útil para proyectos de mapeo colaborativo, ciencia ciudadana y recopilación de información local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes volúmenes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilación y posterior análisis de datos heterogéneos.\nUsualmente se accede a través de querys SQL\n\n\n\n\n\n\n\n\nDatos disponibles para el proyecto\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos públicos sobre educación chilena\nDatos públicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad económica\n\nVeamos como acceder algunos de estos datos.\n\n\nEjemplo: Datos públicos sobre individuos en comunas chilenas (encuesta Casen)\nLa Encuesta de Caracterización Socioeconómica Nacional (CASEN) es una investigación realizada en Chile que tiene como objetivo principal recopilar información detallada sobre la situación socioeconómica de los hogares y las personas en el país. Esta encuesta se lleva a cabo de manera periódica y abarca una amplia variedad de temas, como ingresos, educación, empleo, salud, vivienda y otros aspectos relevantes para comprender la realidad socioeconómica de la población chilena. La información recopilada en la Encuesta CASEN se utiliza para informar políticas públicas, tomar decisiones informadas y analizar la evolución de indicadores sociales a lo largo del tiempo.\nSitio Web oficial\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. El formato mas comun es .csv, pero a veces estan en formatos extraños. Por ejemplo, .dta de STATA.\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(5)\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegión de Tarapacá\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegión de Tarapacá\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educación formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegión de Tarapacá\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nBásica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegión de Tarapacá\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nTécnico nivel superior incompleta\nSí\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n4\n1.101100e+11\n3\n30\nRegión de Tarapacá\nIquique\nUrbano\n67\n19\nMujer\n3\n...\nNaN\nNo sabe\nNo\n27083.0\n941583\n27083.0\n941583\n2083.0\n891583.0\n439170.0\n\n\n\n\n5 rows × 22 columns\n\n\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploración, para ser preparados para analizarlos. De esto se tratará la siguiente sesión del curso.\nEjemplo: Datos desde la API del banco mundial Primero siga este ejemplo practico de importar datos, luego será facil responder la pregunta anterior.\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb # para instalar: conda install pandas-datareader  o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. En este caso revisare de PIB (GDP en ingés), pero se pueden explorar muchas más opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n1503\nBG.GSR.NFSV.GD.ZS\nTrade in services (% of GDP)\n\nWorld Development Indicators\nTrade in services is the sum of service export...\nb'International Monetary Fund, Balance of Paym...\nEconomy & Growth ; Private Sector ; Trade\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16704\nUIS.XUNIT.GDPCAP.23.FSGOV\nInitial government funding per secondary stude...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16705\nUIS.XUNIT.GDPCAP.23.FSHH\nInitial household funding per secondary studen...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n16706\nUIS.XUNIT.GDPCAP.3.FSGOV\nInitial government funding per upper secondary...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16707\nUIS.XUNIT.GDPCAP.5T8.FSGOV\nInitial government funding per tertiary studen...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16708\nUIS.XUNIT.GDPCAP.5T8.FSHH\nInitial household funding per tertiary student...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n\n\n540 rows × 7 columns\n\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n\n\n\n\n\nObservar que este es un data frame con dos índices: pais y año. Para mayor referencia coo tratar este tipo de datos ver en https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\nObtengamos un data frame con los datos de Chile, entre 1980 y 2020.\n\n#sabemos que queremos Chile, asi que busquemos su info\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n\n\n\n\n\nSi quisieramos, por simplicidad quedarnos solo con el indice del año y reordenar el dataframe:\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el análisis es para un solo país\nreversed_df.head(5)\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068\n\n\n\n\n\n\n\nAhora, realicemos un grafico rápido con nuestros datos:\n\n# Graficamos\n\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'Año')\n\nText(0.5, 0, 'Año')\n\n\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 1 - Bajando y formateando datos del Banco Mundial**\n\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\n\nPreguntando a los datos\n¿Cómo plantear preguntas y formular hipótesis en el contexto del análisis de datos? El proceso de análisis comienza con la curiosidad y/o necesidad.cLa formulación de preguntas relevantes que se puedan responder mediante la exploración y el examen de los datos disponibles.\nInicia con la identificación de áreas de interés y la formulación de preguntas específicas relacionadas con esos temas. Estas preguntas pueden surgir de la necesidad de resolver un problema, entender un fenómeno o explorar patrones en los datos. Un buen planteamiento de preguntas es crucial, ya que guiará todo el proceso de análisis.\n\n\n\nEl proceso de abstraer la realidad\n\n\nPreguntas e hipótesis:\nUna hipótesis es una afirmación, verificable con evidencia. En este sentido, para toda pregunta podemos responderla mediante hipótesis.\nEn particular, para responder a las preguntas en el contexto de datos, es común formular hipótesis nulas y alternativas.\nLa hipótesis nula es aquella que propone que algun parámetro toma cierto valor. Este generlamente es un punto de verdad. Si bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto. En general, planteamos el problema de tal manera que podamos rechazar la hipótesis nula, en favor de otra que llamamos alternatiba.\nQuizas, la hipotesis nula más famosa es la prueba de “significancia”. En esta se propone que un parámetro (muchas veces un efecto, o correlación) es 0, es decir, plantea que no hay efecto o relación entre las variables, mientras que la hipótesis alternativa sugiere que sí existe una relación o efecto significativo.\nEstas hipótesis son fundamentales para establecer una base objetiva para el análisis y para evaluar las evidencias encontradas en los datos. El proceso de plantear preguntas y formular hipótesis es el primer paso en el análisis de datos, ya que establece una guía clara para el enfoque y la dirección del trabajo. Al identificar preguntas y establecer hipótesis, se crea un marco sólido que orientará la exploración y el análisis de los datos disponibles.\n\n\n\n\n\n\nTaller 1: Pregunta 2 - Investigando sobre países:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1_notas.html#respondiendo-desde-los-datos",
    "href": "sesion1_notas.html#respondiendo-desde-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\n\nInferencia estadística\nInferencia se refiere al proceso de hacer generalizaciones de una población a partir de una muestra de esa población. En particular, la idea es que si tenemos un conjunto de datos (muestra) obtenido de una población más grande, el cual es representativo de esta, podemos utilizar métodos estadísticos para sacar conclusiones sobre las características y propiedades de esa población en su totalidad.\n\n\n\nPoblación y Muestra\n\n\nEl proceso de inferencia estadística se basa en el principio de que una muestra bien seleccionada puede proporcionar información valiosa sobre la población en general. Mediante el análisis de la muestra, podemos estimar parámetros poblacionales, como la media, la proporción o la desviación estándar, y también podemos construir intervalos de confianza para estimar el rango dentro del cual se espera que se encuentren estos parámetros.\nEl uso de la inferencia estadística es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una población en particular. Por ejemplo, en lugar de encuestar a todos los ciudadanos de un país, es mucho más factible encuestar una muestra representativa y utilizar esa información para hacer suposiciones sobre la opinión de la población en general.\n\nEstadígrafos y el Teorema del Límite central\nEntonces, en cada muestra que tenemos podemos calcular aproximaciones a los parámetros poblacionales de interés. Estos son los llamados estadísgrafos \nDado que por cada muestra que tenemos, vamos a calcular un estadígrafo este es en si mismo una variable aleatoria. Tiene su propia distribución, media y varianza!\n\nEl estadígrafo más conocido es el promedio o media muestral.\n\n\n\nEstadigrafos más comunes\n\n\nCada estimador es una función de la muestra, por ende para cada muestra que tengamos obtendremos un valor numérico específico para el estimador. Por este motivo, cuando estamos trabajando con una única muestra específica, tenemos un único valor del estimador, o estimador puntual.\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población, por lo cual un primer camino tentador es usar el estimador puntual para tomar una decisión. Como nunca podemos conocer el verdadero parámetro, tampoco podemos saber a ciencia cierta si el estimador puntual es cercano a este.\n¿Cómo conectamos estadpigrafos y parámetros?\nEl teorema del límite central, nos dice que, bajo ciertas condiciones, la distribución de las medias muestrales de una población se aproxima a una distribución normal a medida que el tamaño de la muestra aumenta, independientemente de la forma de la distribución original de la población. Este teorema es esencial en inferencia estadística y tiene amplias aplicaciones en análisis de datos y toma de decisiones.\n\n\n\nLa media muestral se distribuye normal, sin importar la distribución de la variable subyacente\n\n\nFormalmente, el Teorema del Límite Central establece lo siguiente:\nx‾∼aN(μ,σn)\\bar{x} \\sim_a N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) \nSupongamos que tenemos una población con media μ y desviación estándar σ finitas. Si tomamos muestras aleatorias de tamaño n de esta población y calculamos la media muestral de cada muestra, entonces, a medida que n tiende a infinito, la distribución de estas medias muestrales se aproximará a una distribución normal con media μ y desviación estándar σ/√n.\nEn otras palabras, sin importar la distribución original de la población, cuando el tamaño de la muestra es suficientemente grande, la distribución de las medias muestrales seguirá una forma de campana similar a la distribución normal. Este resultado es fundamental para realizar inferencias sobre la población a partir de muestras, ya que nos permite aplicar métodos basados en la distribución normal incluso cuando la población original no sigue una distribución normal.\n\n\nError estándar\n\nCorresponde a un estimador de la desviación estándar del estimador.\nIdentifica que tan lejos estamos del verdadero valor poblacional.\nPara la media muestral:\n\nSE=Syn SE = \\frac{S_y}{\\sqrt{n}}\nSe utiliza para evaluar a los estimadores, mediante pruebas de hipotesis y construir intervalos de confianza\n\nSi se conoce un estimador y su desviación estándar, podemos saber qué tan precisa es la estimación (mucha o poca varianza), pero no podemos saber si el estimador está cercano o no a su valor verdadero en la población (el cual no conocemos).\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población.\nSí se puede construir un conjunto de valores que contienen el parámetro poblacional con alguna probabilidad (llamada el nivel de confianza).\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n\n\nInferencia sobre Estadígrafos y parámetros - Conectados por el Teorema del Límite central\nx‾∼aN(μ,σn) \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hipótesis\np-valor\n\n\n\n\n\nIntervalos de confianza\nUna primera manera de aproximarnos a los parámetros poblacionales (particularmente a la esperanza) es mediante la construcción de intervalos de confianza.\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n¿De dónde saco los valores críticos?\nLos valores críticos de una distribución los obtenemos de una tabla de distribución o para calcular podemos usar excel, R o en python:\nscipy.stats.t.isf(alpha, n-p)\nSi estamos trabajando con dos colas, usar alpha/2 porque la probabilidad de error la estamos repartiendo a ambas colas.\n\n\n\n\n\n\n[Matemáticamente] Caso 1: Varianza conocida\n\n\n\nSumpongamos que tenemos una muestra aleatoria: y1,y2,…,yny_1, y_2, \\dots, y_n de una población Y∼N(μ,σ2)Y\\sim N(\\mu, \\sigma^2)\n\nLa media muestral y‾=1n∑i=1nyi\\bar{y}= \\frac{1}{n}\\sum_{i=1}{n}y_i\n\nSu esperanza es: E(y‾)=μE(\\bar{y}) =\\mu\nSu varianza es: var(y‾)=σ2nvar(\\bar{y}) = \\frac{\\sigma^2}{n}\nse distribuye normal, tal que podemos estandarrizar: $ N(0,1) $\n\n\nEntonces, podemos describir que:\n$ P( -1.96 &lt; &lt; 1.96 ) = 0.95 $$\n$ P( {y}- &lt; &lt; {y} + ) = 0.95 $\n\neste intervalo es aleatorio, porque y‾\\bar{y} es diferente en cada muestra.\npara el 95% de las muestras elatorias, el intervalo construido de esta manera contendrá a μ\\mu\n\n\n\n\n\n\n\n\n\n[Matemáticamente] Caso 2: Varianza desconocida\n\n\n\n\nSupongamos que tenemos una muestra aleatoria y1,y2,…,yny_1, y_2, \\dots, y_n de una población y∼N(μ,σ2)y\\sim N(\\mu, \\sigma^2 )\nUsamos la estimación de la desviación estándar muestral: Sy=1n−1∑i=1n(yi−y‾)2 S_y = \\sqrt{\\frac{1}{n-1} \\sum{i=1}{n}(y_i - \\bar{y})^2} \nY si estandarizamos y‾\\bar{y}: Y‾−μySn∼tn−1 \\frac{\\bar{Y} - \\mu_y }{ \\frac{S}{\\sqrt{n}}} \\sim t_{n-1} \nPodeos constrir un intervalo de la porbabilidad de estar al 95 con el valor critico c adecuado a los grados de libertad n-1:\n\nP(−c&lt;Y‾−μySn&lt;c)=0.95 P( -c &lt;   \\frac{\\bar{Y} - \\mu_y }{ \\frac{ S}{\\sqrt{n}}}  &lt; c  ) =0.95  \nP(y‾−c×Sn&lt;μ&lt;y‾+c×Sn)0.95 P(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) 0.95\n\nSi llamamos al error estandar SE: SE(y‾)=SnSE(\\bar{y})=\\frac{S}{\\sqrt{n}}\nEl IC es: (y‾−c×Sn,y‾+c×Sn) (\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) \n\n\n\n\n\nEl intervalo es una muestra aleatoria\nEsto quiere decir que para cada muestra podemos construir un intervalo.\nAsí como el estimador es una variable aleatoria, esto también es cierto para los intervalos de confianza. Por eso también se les llama intervalos aleatorios, ya que con diferentes muestras obtendremos un diferente estimador e intervalo.\nPor ende, supongamos que contamos con 20 muestras, entonces construiremos 20 intervalos de confanza diferentes para los 20 estimadores puntuales.\n\nInterpretación de intervalo de confianza\nPensemos en un 95% de confianza (un valor usual). Esto quiere decir, que si se repitiera este ejercicio muchas veces y construyéramos un intervalo de esta forma el 95% de ellos contendría el verdadero parámetro poblacional.\nUn elemento importante a considerar es que esto no significa que con 95% de certeza el parámetro está exactamente en estos valores. Por ejemplo, al 95% de confianza con 20 intervalos 19 contendrán el parámetro.\n\n\n\nPruebas de hipótesis\n\nUna forma de verificar hipotesis sobre los parámetros es mediante el contraste de hipótesis.\n\nEmpezamos suponiendo que hay una distribución conocida para el estadígrafo, centrada en un valor específico. Y nos preguntamos, si esto fuea verdad ¿qué tan probable es la muestra que tengo?\n\nLlamamos la hipótesis a probar Ho, y su alternativa H1.\n\n\n\n\n\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:\n\n\nTipo I: Rechazar Ho cuando es cierta\nTipo II: No rechazar Ho cuando es falsa.\n\n\n\n\nSe elige nivel de significancia de contraste (α) = probabilidad de cometer error Tipo I. Típicamente α = 0,01, 0,05, 0,10.\nPara contrastar una hipótesis con su alternativa, debemos elegir:\n\nUn estadístico de contraste\nUna regla de rechazo, la cual depende de un valor crítico.\n\n\n\n\n\nPrueba de significancia\nDefinimos la prueba de hipótesis de significancia como aquella que indica si un estimador T̂\\hat{T} es 0.\nH0:T=0 vs H1:T≠0 H_0: T =0\\text{ vs }H_1: T \\neq 0 \nEl Valor de probabilidad (ó p-valor) es el nivel probabilidad más alto para el cual no podemos rechazar la hipótesis nula de la prueba de significancia.\nEjemplo: $H_0: $ y en la muestra especifica t= 1.52:\nP−valor=P(T&gt;1.52|h0)=1−ϕ(1.52)=0.0065 P-valor = P(T&gt;1.52 \\vert h_0)= 1- \\phi(1.52)=0.0065\n\nel mayor nivel de significancia estadistica al cual no rechazamos H0H_0 es 6.5%\nla probabilidad de observar un velor T≥1.52T\\geq 1.52 cuando H0H_0 es cierta es en un 6.5 de las muestras.\nP-valores bajos dan evidencia en contra de H0H_0, ya que la probabilidad de observarlo si H0H_0 es cierta es bajo.\n\n\n\nEjemplo de aplicación: Peso de los Pingüinos Palmer\nConsideremos los datos de los pingüinos Palmer. Los datos “Palmer Penguins” son un conjunto que detalla medidas morfológicas y características de tres especies de pingüinos: Adelie, Gentoo y Chinstrap. Recopilados por el Dr. Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nEn el contexto de los pingüinos y el peso de su población, podríamos tomar una muestra de pingüinos y calcular un intervalo de confianza para el peso promedio. Esto nos daría una estimación del peso promedio de la población total, junto con la confianza en que este valor estimado es preciso.\nEs importante tener en cuenta que el proceso de inferencia estadística se basa en suposiciones y en el uso adecuado de técnicas estadísticas.\nLa elección de la muestra, la interpretación de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hipótesis, sobre el peso de los pingüinos.\nPrimero calcularemos el promedio muestral y lo veremos en el contexto de los datos observados:\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los pingüinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribución del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribución de Peso de Pingüinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral. De que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus características.\nPor ejemplo, consideremos que de esta población de pingüinos obtenemos 1000 muestras de 50 individuos cada una. Si graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tamaño de muestra, más nos alejamos de la distribución normal.\nSi reducimos el número de repeticiones tambieé.\n\n\nimport numpy as np\n\n# Definir el tamaño de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y cálculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gráfico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribución de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\n\nIntervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n# Obtener una muestra simple de 40 pingüinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error estándar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviación estándar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)\n\nIntervalo de Confianza para el Peso:\n(4091.059218311082, 4515.190781688918)\n\n\nEl resultado será un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los pingüinos en la población, con un nivel de confianza del 95%.\n¿Como nos fue? ¿Contiene al verdadero valor?\n\n\n\nComparaciones de grupos\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribución de tamaño por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\ntabla_doble_entrada\n\n\n\n\n\n\n\n\nspecies\nsex\nPromedio\nVarianza\n\n\n\n\n0\nAdelie\nFemale\n3368.835616\n72565.639269\n\n\n1\nAdelie\nMale\n4043.493151\n120278.253425\n\n\n2\nChinstrap\nFemale\n3527.205882\n81415.441176\n\n\n3\nChinstrap\nMale\n3938.970588\n131143.605169\n\n\n4\nGentoo\nFemale\n4679.741379\n79286.335451\n\n\n5\nGentoo\nMale\n5484.836066\n98068.306011\n\n\n\n\n\n\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hipótesis:\n¿Existe una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”?\n\nHipótesis Nula (H0):\n\nNo hay diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\n\nHipótesis Alternativa (H1):\n\nExiste una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\nPara probar esta hipótesis, podrías utilizar una prueba de hipótesis para comparar las medias de las muestras de peso de los pingüinos machos y hembras en la especie “Adelie”. Esto te permitiría determinar si la diferencia observada en el peso promedio es lo suficientemente grande como para considerarse estadísticamente significativa.\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribución de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribución de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nA simple vista podriamos pensar ambos grupos son diferentes. Es más claro si dibujamos el promedio muestral observado.\n\n# Crear un gráfico de densidad con líneas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()\n\n\n\n\nSi construimos una prueba t de diferencia de medias:\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estadística t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gráfico de comparación de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Machos y Hembras de Pingüinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\nEstadística t: 13.126285923485874\nValor p: 6.402319748031793e-26\n\n\n\n\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas. Para esto podriamos usar una prueba ANOVA.\nEn este código, primero cargamos el conjunto de datos “Penguins” y luego creamos dos subconjuntos separados para machos y hembras. Después, utilizamos la función stats.f_oneway() para realizar una prueba ANOVA para comparar los pesos entre hembras y machos. El resultado incluye la estadística F y el valor p.\nEl valor p nos indica si hay una diferencia significativa entre los grupos. Si el valor p es menor que un umbral de significancia (por ejemplo, 0.05), podríamos rechazar la hipótesis nula y concluir que hay una diferencia significativa en el peso entre hembras y machos de diferentes islas.\nRecuerda que, antes de realizar una prueba ANOVA, es importante verificar las suposiciones necesarias, como la normalidad y la homogeneidad de varianzas en los grupos. Si estas suposiciones no se cumplen, podría ser necesario considerar otras pruebas estadísticas o transformaciones de los datos.\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estadística F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\nEstadística F: 72.96098633250911\nValor p: 4.897246751596325e-16\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Calcular las medias de peso por género e isla\nmedias_peso = penguins.groupby(['species', 'island', 'sex'])['body_mass_g'].mean().reset_index()\n\n# Crear un gráfico de barras con puntos y intervalos de confianza\nplt.figure(figsize=(10, 6))\nsns.barplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', errorbar='sd', palette=['pink', 'blue'])\n#sns.boxplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Hembras y Machos por Isla')\nplt.xlabel('Especie e Isla')\nplt.ylabel('Media de Peso (g)')\nplt.legend(title='Sexo')\nplt.show()\n\n\n\n\n\n\nExperimentos Aleatorios y pruebas A/B\nUn experimento estadístico es un enfoque científico que busca establecer relaciones de causalidad y obtener conclusiones sobre cómo ciertas variables afectan a otras. Los experimentos estadísticos se diseñan para manipular deliberadamente una o más variables independientes y observar los efectos que tienen sobre una variable dependiente. Al controlar y manipular las variables de interés, los experimentos permiten a los investigadores hacer afirmaciones más sólidas sobre las relaciones causales.\nUna prueba A/B, también conocida como prueba de división, es una técnica utilizada en la investigación y el análisis para comparar dos variantes o grupos con el fin de determinar cuál de ellos produce un mejor resultado en términos de rendimiento, efectividad o preferencia. En una prueba A/B, se selecciona un grupo de muestra y se divide en dos grupos, uno que experimenta la variante “A” (por ejemplo, una versión actual) y otro que experimenta la variante “B” (por ejemplo, una versión modificada). Luego, se recopilan datos y se comparan los resultados de ambos grupos para determinar cuál variante es más efectiva. Las pruebas A/B son comunes en marketing, diseño de productos y desarrollo web para tomar decisiones informadas sobre mejoras y optimizaciones.\nLas pruebas A/B es son ampliamente utilizado en diversas áreas, como el marketing, la investigación de usuarios y el diseño de productos. En una prueba A/B, se seleccionan dos grupos de muestra: uno experimenta la versión original (A) y el otro experimenta una variante modificada (B). La idea detrás de una prueba A/B es evaluar si la variante B produce un efecto significativamente diferente en una métrica de interés en comparación con la variante A.\nMediante la asignación aleatoria de los participantes a los grupos A y B, y al controlar las condiciones en las que se les presenta cada variante, se reduce la posibilidad de sesgos y se permite un análisis causal más confiable. Al comparar las diferencias observadas en los resultados entre los grupos A y B, es posible inferir si la variante B tiene un impacto significativo en la variable de interés.\nSin embargo, es importante tener en cuenta que aunque las pruebas A/B proporcionan evidencia de asociación causal, no garantizan que la causalidad sea absoluta. Otros factores no controlados pueden influir en los resultados. Para obtener una comprensión más completa de la causalidad, los experimentos controlados aleatorizados y el uso de métodos de diseño experimental sólidos son esenciales. Las pruebas A/B son una herramienta poderosa para explorar causas y efectos en condiciones controladas y analizar el rendimiento relativo de diferentes opciones.\nVeamos un ejemplo en la práctica. Este es parte del ejercicio de aplicación."
  },
  {
    "objectID": "sesion1_notas.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "href": "sesion1_notas.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Caso: Aplicación de A/B testing para promoción de Marketing",
    "text": "Caso: Aplicación de A/B testing para promoción de Marketing\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreación de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 3 -Ejemplo AB test en Marketing:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "sesion1_notas.html#buenas-prácticas-en-análisis-de-datos-1",
    "href": "sesion1_notas.html#buenas-prácticas-en-análisis-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Buenas prácticas en análisis de datos",
    "text": "Buenas prácticas en análisis de datos\n\nImportancia de la Adquisición y Almacenamiento de Datos\nLa adquisición y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de análisis. La calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes. En esta sección, exploraremos la importancia de esta etapa y cómo afecta todo el flujo de trabajo de la ciencia de datos.\nGarantía de Calidad y Fiabilidad en la Obtención de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean sólidas. La calidad de los datos está relacionada con la precisión, integridad y consistencia de la información que recopilamos. Asegurarnos de que los datos sean precisos desde el principio minimiza la posibilidad de errores en análisis posteriores. Exploraremos técnicas y prácticas para verificar la calidad de los datos y cómo mitigar posibles fuentes de error.\nExploración de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros. Cada fuente tiene sus propias características y potenciales sesgos. Comprender las diferencias entre estas fuentes y cómo pueden influir en los resultados es crucial para tomar decisiones informadas. Analizaremos ejemplos de cómo la elección de la fuente de datos puede afectar las conclusiones y cómo evaluar la confiabilidad de las fuentes.\n\n\nMetodologías de Levantamiento y Adquisición de Datos:\nEl proceso de obtención de datos implica una planificación cuidadosa. Exploraremos diversas metodologías utilizadas para recopilar datos, desde encuestas y experimentos hasta scraping de datos en línea. Cada metodología tiene sus propias ventajas y desventajas, y es importante seleccionar la más adecuada para los objetivos del análisis. Discutiremos cómo diseñar encuestas efectivas, cómo considerar la ética en la recopilación de datos y cómo aprovechar las fuentes de datos existentes.\nEsta sección nos proporcionará una base sólida para comprender cómo adquirir y almacenar datos de manera efectiva y confiable. Una vez que comprendamos cómo obtener datos de calidad, podremos avanzar con confianza en las etapas posteriores del proceso de análisis, sabiendo que estamos trabajando con una base sólida y confiable.\n\n\nDesafíos y Consideraciones:\nA medida que ingresamos al emocionante mundo del análisis de datos, nos encontramos con una serie de desafíos y consideraciones que debemos abordar de manera efectiva para garantizar el éxito de nuestro proyecto. Estos desafíos abarcan desde la protección de la privacidad de los datos hasta las complejidades de la limpieza y transformación durante la etapa de preparación.\n\n\nPrivacidad y Seguridad de los Datos:\nUno de los aspectos más críticos en el análisis de datos es la privacidad y seguridad de la información. Los datos pueden contener información sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas. Exploraremos prácticas y regulaciones para garantizar que los datos se manejen de manera ética y legal. Discutiremos cómo anonimizar los datos, utilizar técnicas de enmascaramiento y seguir las mejores prácticas para resguardar la privacidad de los individuos.\n\n\nLimpieza y Transformación durante la Preparación de Datos:\nLa etapa de preparación de datos es crucial para asegurarse de que los datos sean aptos para el análisis. Sin embargo, este proceso no está exento de desafíos. Los datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada. Exploraremos técnicas para identificar y manejar valores atípicos y faltantes, así como la importancia de la normalización y estandarización de los datos. Aprenderemos cómo transformar los datos en un formato adecuado para el análisis, incluida la reorganización de variables y la creación de nuevas características. En resumen, enfrentamos una serie de desafíos y consideraciones clave en nuestro viaje hacia el análisis de datos significativo. Desde la protección de la privacidad hasta la preparación efectiva de los datos, abordar estos desafíos de manera adecuada es esencial para garantizar que nuestras conclusiones sean sólidas, confiables y éticas.\n\n\nReproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\nLa reproducibilidad y el control de versiones son componentes fundamentales para garantizar la integridad y la transparencia en el análisis de datos. Además de mantener un registro detallado de las decisiones tomadas durante el proceso, el uso de sistemas de control de versiones como GIT se vuelve esencial para mantener la trazabilidad y la colaboración efectiva en proyectos de preparación y análisis de datos.\nDocumentación Detallada del Análisis y Uso de Notebooks: Una documentación exhaustiva del análisis es esencial para comprender el flujo de trabajo, las decisiones tomadas y las transformaciones aplicadas a los datos. Los notebooks, como Jupyter Notebooks, ofrecen una herramienta excepcional para lograr esto. En cada celda de un notebook, es posible combinar explicaciones en lenguaje natural con código ejecutable y visualizaciones. Esto permite registrar no solo el qué y el cómo, sino también el porqué detrás de cada paso.\nImportancia de Mantener un Registro de los Cambios en los Datos: Cada decisión tomada durante la preparación y el análisis de datos puede tener un impacto significativo en los resultados finales. Mantener un registro detallado de estas decisiones, desde la limpieza de datos hasta la creación de variables derivadas, es crucial para comprender cómo se obtuvieron ciertos resultados. Una documentación precisa y detallada permite a otros analistas validar y replicar el análisis en el futuro.\nUso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que también es una herramienta poderosa en el análisis de datos. Permite rastrear cada modificación realizada en el código y en los documentos, incluidos los notebooks. Cada cambio es registrado como un “commit”, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\n\n\n\nUn esquema de git por Allison Horst @allison_horst\n\n\nAplicación de Control de Versiones en Proyectos de Preparación de Datos:* La aplicación de GIT en proyectos de preparación de datos agrega un nivel adicional de transparencia y colaboración. Los repositorios de GIT almacenan no solo los datos originales, sino también los notebooks y scripts utilizados en el proceso. Esto permite a los analistas colaborar en un entorno controlado y mantener un historial de cambios. En caso de que surjan problemas o se necesite retroceder en el tiempo, GIT ofrece la capacidad de volver a versiones anteriores de manera segura.\nLa combinación de documentación detallada a través de notebooks y el uso de sistemas de control de versiones como GIT proporciona una base sólida para el análisis de datos reproducible y transparente. Esto no solo facilita la comprensión y validación de los resultados, sino que también fomenta la colaboración y la mejora continua en proyectos de preparación y análisis de datos.\n\n\n\n\n\n\nActividad de proyecto - Inicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del análisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustaría trabajar.\nPropongan una o dos preguntas de investigación y las hipotesis que las responderían.\n\nLa siguiente sesión, vamos a explorar los datos y empezar los primeros pasos en su análisis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de datos",
    "section": "",
    "text": "Sitio web complementario con material curso Análisis de Datos\n\n\n\nSesión\nFecha\nTema\nSlides\nNotas\n\n\n\n\n0\n17 ago\nIntroducción al Curso\nslides sesión introductoria\nNotas sesión introductoria\n\n\n1\n17 ago\nRespondiendo Preguntas con datos\nslides sesión 1\nNotas sesión 1\n\n\n2\n25 ago\nPreparando los datos\nslides sesión 2\nNotas sesión 2\n\n\n3\n2 sep\nIntroducción al análisis de regresión\nslides sesión 3\nNotas sesión 3\n\n\n4\n9 sep\nProfundizando en análisis de regresión, supuestos y limitaciones\n\n\n\n\n5\n30 sep\nIntroduccion al análisis de series de tiempo\n\n\n\n\n6\n7 oct\nModelando series temporales"
  },
  {
    "objectID": "index.html#calendario-clases",
    "href": "index.html#calendario-clases",
    "title": "Análisis de datos",
    "section": "",
    "text": "Sitio web complementario con material curso Análisis de Datos\n\n\n\nSesión\nFecha\nTema\nSlides\nNotas\n\n\n\n\n0\n17 ago\nIntroducción al Curso\nslides sesión introductoria\nNotas sesión introductoria\n\n\n1\n17 ago\nRespondiendo Preguntas con datos\nslides sesión 1\nNotas sesión 1\n\n\n2\n25 ago\nPreparando los datos\nslides sesión 2\nNotas sesión 2\n\n\n3\n2 sep\nIntroducción al análisis de regresión\nslides sesión 3\nNotas sesión 3\n\n\n4\n9 sep\nProfundizando en análisis de regresión, supuestos y limitaciones\n\n\n\n\n5\n30 sep\nIntroduccion al análisis de series de tiempo\n\n\n\n\n6\n7 oct\nModelando series temporales"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n2"
  },
  {
    "objectID": "sesion2_notas.html",
    "href": "sesion2_notas.html",
    "title": "Sesión 2: Preparando los datos",
    "section": "",
    "text": "La preparación de datos es una fase esencial en el proceso de análisis de datos que involucra una serie de actividades destinadas a garantizar que los datos estén en condiciones óptimas para su posterior análisis.\n\nExtraccion de los datos\nLimpieza\nTransformación\nOrganización\n\n\nEsta fase implica la extracción de los datos, su limpieza, transformación y organización de manera que sean coherentes, completos y adecuados para el análisis que se va a realizar.\nTrash in , trash out\n\n\nLa preparación de datos es crucial porque afecta directamente la calidad y confiabilidad de los resultados obtenidos en cualquier análisis posterior.\n\n\n\n\n\n\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje común.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relación a:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis.\n\n\n\n\nLa estructura de un dato se refiere a su formato y codificación.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\n\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\n\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\n\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n\n\n\n\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n\n\n\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\n\n\n\n\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_notas.html#tipos-de-datos",
    "href": "sesion2_notas.html#tipos-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "",
    "text": "Antes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje común.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relación a:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis.\n\n\n\n\nLa estructura de un dato se refiere a su formato y codificación.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\n\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\n\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\n\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n\n\n\n\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n\n\n\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\n\n\n\n\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_notas.html#leyendo-datos-en-pandas",
    "href": "sesion2_notas.html#leyendo-datos-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo\n\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read_*\n\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: automático o definido por el usuario\nDatetime parsing: puede combinar información de múltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con números con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2_notas.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2_notas.html#formato-csv-valores-separados-por-comas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que parece ser\n\n¿Y qué pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2_notas.html#csv-en-pandas",
    "href": "sesion2_notas.html#csv-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nBásica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nBásica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representación de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2_notas.html#json-java-script-object-notation",
    "href": "sesion2_notas.html#json-java-script-object-notation",
    "title": "Sesión 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, números, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores numéricos"
  },
  {
    "objectID": "sesion2_notas.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2_notas.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2_notas.html#orientaciones-posibles-de-json",
    "href": "sesion2_notas.html#orientaciones-posibles-de-json",
    "title": "Sesión 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2_notas.html#extensible-markup-language-xml",
    "href": "sesion2_notas.html#extensible-markup-language-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato más antiguo y auto descriptivo, con estructura jerárquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un método incorporado en Python.\nSe puede usar la librería lxml (también ElementTree)\n\n\nEjemplo XML"
  },
  {
    "objectID": "sesion2_notas.html#formatos-binarios",
    "href": "sesion2_notas.html#formatos-binarios",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¿Qué es un formato binario?\nPickle: Python’s built-in serialization\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n\n\nHDF5: Librería para almacenar gran cantidad de datos científicos\n\nFormato jerárquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresión\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2_notas.html#bases-de-datos-relacionales",
    "href": "sesion2_notas.html#bases-de-datos-relacionales",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\n\nLas bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\nvínculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos \n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayoría de los sistemas de bases de datos a través de un API común\nSintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.) \nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n`import sqlalchemy as sqla\ndb = sqla.create_engine(‘sqlite:///mydata.sqlite’)\npd.read_sql(‘select * from test’, db)`"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-estadístico",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-estadístico",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista estadístico",
    "text": "Dirty data: punto de vista estadístico\n\nLos datos son generados desde algún proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsión: algunas muestras se corrompieron por un proceso\nSesgo de selección: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: Left and right censorship: los usuarios van y vienen del escrutinio\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisión y simplicidad"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores están missing, corrompidos, erróneos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un área",
    "text": "Dirty Data: Punto de vista del experto en un área\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¿Qué ocurrió?\nLos expertos en un área llevan un modelo implícito de los datos que están testeando\nNo siempre necesitas ser un experto en un área para hacer esto\n\n¿Puede una persona correr 500 km por hora?\n¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido común"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinación de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregación es menos susceptible a los errores numéricos\nSer cuidadoso, puede que los datos estén bien…"
  },
  {
    "objectID": "sesion2_notas.html#dónde-se-origina-el-dirty-data",
    "href": "sesion2_notas.html#dónde-se-origina-el-dirty-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Dónde se origina el dirty data?",
    "text": "¿Dónde se origina el dirty data?\n\nLa fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integración de diferentes sets de datos causa problemas\nPropagación del error: se magnifica un error"
  },
  {
    "objectID": "sesion2_notas.html#problemas-dirty-comunes",
    "href": "sesion2_notas.html#problemas-dirty-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs. New York\nPérdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs. dos\nDatos truncados: “Janice Keihanaikukauakahihuliheekahaunaele” se vuelve “Janice - Keihanaikukauakahihuliheek” en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposición\nProblemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017"
  },
  {
    "objectID": "sesion2_notas.html#data-wrangling",
    "href": "sesion2_notas.html#data-wrangling",
    "title": "Sesión 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato más significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representación a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2_notas.html#tidy-data",
    "href": "sesion2_notas.html#tidy-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para análisis de corte transversal nuestro ideal es trabajar con Tidy Data\n“Tidy Data is a standar way of mapping the meaning of a dataset to its structure¨\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un estándar deseable para analizar datos.\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observación (registro) forma una fila.\nCada dato (valor) está en una celda de la tabla.\n\n\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:\n\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n\n¿Porqué datos Tidy?\n\nEstandarizanción\n\nLos datos organizados te permiten ser más eficiente al utilizar herramientas existentes diseñadas específicamente para realizar las tareas que necesitas hacer, desde la selección de porciones de tus datos hasta la creación de mapas de tu área de estudio.\nUtilizar herramientas existentes te ahorra tener que construir todo desde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual puede ser consume tiempo y desmotivante).\nAfortunadamente, existen muchas herramientas diseñadas específicamente para transformar datos desorganizados en datos organizados (por ejemplo, en el paquete tidyr). Al estar mejor preparado para transformar tus datos en un formato organizado, podrás llegar más rápido a tus análisis y comenzar a responder las preguntas que estás planteando.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nFacilitar la colaboración\n\nDado que nuestros colegas pueden emplear las mismas herramientas de manera familiar. Tanto si consideramos a los colaboradores como compañeros actuales, su futuro propio o futuros colegas, la organización y compartición de datos de manera coherente y predecible implica menos ajustes, tiempo y esfuerzo para todos.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nSimplifica la reproducibilidad\n\nLos datos organizados también facilitan la reproducción de análisis, ya que son más fáciles de comprender, actualizar y reutilizar. Al utilizar herramientas que esperan datos organizados como entrada, puedes construir e iterar flujos de trabajo realmente potentes. Y cuando tienes entradas de datos adicionales, ¡no hay problema en volver a ejecutar tu código!\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\n\n¿Siempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o “desordenadas”.\nEs importante tener en cuenta que siempre existen múltiples formas de representar la misma información.\n\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2_notas.html#datos-perdidos",
    "href": "sesion2_notas.html#datos-perdidos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\n\nse pueden escoger filas o columnas\n\n\nLlenar/ reemplazar :\n\n\ncon un valor por default\ncon un valor interpolados, otros\n\n\n\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2_notas.html#fitrando-y-limpiando",
    "href": "sesion2_notas.html#fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cuál de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key.\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2_notas.html#problemas-de-registro-relacionados-con-textos",
    "href": "sesion2_notas.html#problemas-de-registro-relacionados-con-textos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas de registro relacionados con textos",
    "text": "Problemas de registro relacionados con textos\nTextos o strings, es muy común tener problemas de codificación, errores de tipeo, problemas de formato, etc.\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) “quiebra” o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14”\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14”\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD” \"AbCd\".lower() # \"abcd\"\n\n\nErrores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_18558/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\n\n\n\n\nTransformando strings\n\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n2\n\n\n\n#s.index(':') # ValueError raised\n\n\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado\n\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n2\n\n\n\ns.find(':') # -1\n\n-1\n\n\n\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string\n\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\nFalse\n\n\n\n\nMétodos para strings\n\n\nCualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido) Se debe usar .str. (es importante el .str).\nEjemplo:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2_notas.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2_notas.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets\n\n\nEliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis.\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n\n\n\n\nReshapes/re formato\n\nEl “reshape” o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico.\nEsto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos.\nEl “reshape” es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo.\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n\n\n\n\nCrear variables dummies / dicotómicas\n\nMuy útil para representar información cualitativa.\nMuy usando en análisis de regresión y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\nSe pueden generar mediante pd.get_dummies(df[‘key’])\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0"
  },
  {
    "objectID": "sesion2_notas.html#unir-datasets",
    "href": "sesion2_notas.html#unir-datasets",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\nApliar/concatenar/append Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como “inner”, “outer”, “left” y “right”\n\n\n\n1. Apilar dataframes\nCreemos los dataframes:\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n\n\nHagamos el apilar. Atención a los index\n\ndf.append(df2)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_18558/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9\n\n\n\n\n\n\n\n\nUsemos ignorar index\n\ndf.append(df2, ignore_index=True)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_18558/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9\n\n\n\n\n\n\n\n\n\n\n2. Unir dataframes\nUtilizando la función pd.merge().\n\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n\nUnión interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUnión externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0"
  },
  {
    "objectID": "sesion2_notas.html#transformaciones-estadísticas",
    "href": "sesion2_notas.html#transformaciones-estadísticas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones estadísticas:",
    "text": "Transformaciones estadísticas:\n\n1. Outliers\nUn valor atípico, también conocido como valor outlier en inglés, es un punto de datos que difiere significativamente del patrón general de los demás datos en un conjunto. Estos valores son inusuales en relación con el resto de la distribución de los datos y pueden ser considerablemente más altos o más bajos que los valores típicos del conjunto.\nLa identificación de valores atípicos es importante por varias razones: 1. Calidad de los datos 2. Impacto en estadísticas y modelos 3. Anomalías y problemas reales 4. Toma de decisiones informada\nVeamos un ejemplo:\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n\nValores atípicos:\n   Valor\n6    200\n\n\n\n\n2. Estandarización de Datos\nLa estandarización es un proceso importante en el análisis de datos y el aprendizaje automático. Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno. Esta transformación se logra mediante la fórmula:\nz=x−μσ z = \\frac{x - \\mu}{\\sigma} \nDonde $ x $ es el valor original, $ $ es la media de los valores y $ $ es la desviación estándar.\nLa estandarización es especialmente útil cuando se tienen variables con diferentes escalas y distribuciones, ya que permite compararlas de manera equitativa y mejorar la eficacia de los algoritmos de aprendizaje automático al mitigar el impacto de las diferencias en las magnitudes de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]\n\n\n\n\n\n\n\n3. Normalización de Datos\nLa normalización es un proceso esencial en el análisis de datos y el aprendizaje automático. Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1. La fórmula matemática utilizada para la normalización es:\nxnorm=x−min(x)max(x)−min(x) x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \nDonde $ x $ es el valor original y $x_{} $ es el valor normalizado.\nLa normalización es beneficiosa para igualar la escala de las variables y mejorar el rendimiento de los modelos que son sensibles a la magnitud de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]"
  },
  {
    "objectID": "sesion0_slides.html#un-poco-sobre-mi",
    "href": "sesion0_slides.html#un-poco-sobre-mi",
    "title": "Presentación del curso",
    "section": "Un poco sobre mi:",
    "text": "Un poco sobre mi:"
  },
  {
    "objectID": "sesion0_slides.html#en-la-sesión-de-hoy",
    "href": "sesion0_slides.html#en-la-sesión-de-hoy",
    "title": "Presentación del curso",
    "section": "En la sesión de hoy:",
    "text": "En la sesión de hoy:\n\nRevisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0_slides.html#descripción-del-curso",
    "href": "sesion0_slides.html#descripción-del-curso",
    "title": "Presentación del curso",
    "section": "Descripción del curso",
    "text": "Descripción del curso\n\nEsta asignatura presentará los conceptos básicos de pre-procesamiento y análisis descriptivo de datos.\nEl objetivo principal es poder determinar cuáles datos son susceptibles de ser convertidos en información para apoyar la toma de decisiones, y separar el ruido de la señal."
  },
  {
    "objectID": "sesion0_slides.html#descripción-del-curso-1",
    "href": "sesion0_slides.html#descripción-del-curso-1",
    "title": "Presentación del curso",
    "section": "Descripción del curso",
    "text": "Descripción del curso\n\nEs el primer paso en un proyecto de ciencia de datos.\nLos estudiantes aprenderán a identificar\n\nlas problemáticas que presentan los datos desde el momento de su registro (por ej., error muestral, outliers),\nasí como usar las herramientas necesarias para describirlos (por ej., distribuciones e histogramas),"
  },
  {
    "objectID": "sesion0_slides.html#descripción-del-curso-2",
    "href": "sesion0_slides.html#descripción-del-curso-2",
    "title": "Presentación del curso",
    "section": "Descripción del curso",
    "text": "Descripción del curso\n\nexplorarlos (por ej., agrupar o filtrar bajo un criterio específico),\ny cruzarlos (por ej., utilizando otras fuentes).\nAsimismo, los estudiantes comprenderán que las etapas de este proceso no son lineales, sino que se benefician del diseño iterativo."
  },
  {
    "objectID": "sesion0_slides.html#contexto-en-el-programa-de-magister",
    "href": "sesion0_slides.html#contexto-en-el-programa-de-magister",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la línea de desarrollo de data science.\nEsta asignatura tributa, a través de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Magíster en Data Science:\n\nAplicar teorías, algoritmos, métodos, técnicas y herramientas básicas y avanzadas de Data Science para analizar, resolver y hacer una evaluación crítica de desafíos complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones."
  },
  {
    "objectID": "sesion0_slides.html#contexto-en-el-programa-de-magister-1",
    "href": "sesion0_slides.html#contexto-en-el-programa-de-magister-1",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nComunica efectivamente y argumenta sobre los resultados de su trabajo a públicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento ético, cumpliendo los protocolos y normas que guían su desempeño, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicación de estrategias para utilizar nuevo conocimiento en data science en su ámbito de desempeño."
  },
  {
    "objectID": "sesion0_slides.html#objetivos-de-la-asignatura",
    "href": "sesion0_slides.html#objetivos-de-la-asignatura",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura",
    "text": "Objetivos de la asignatura\nResultados de aprendizaje\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el análisis de datos, utilizando lenguaje técnico afín.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso."
  },
  {
    "objectID": "sesion0_slides.html#objetivos-de-la-asignatura-1",
    "href": "sesion0_slides.html#objetivos-de-la-asignatura-1",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura",
    "text": "Objetivos de la asignatura\nResultados de aprendizaje\n\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer información y generar conclusiones basadas en evidencia.\nIdentificar las buenas prácticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0_slides.html#contenidos",
    "href": "sesion0_slides.html#contenidos",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n\nLimpieza y estructura de datos.\nRegresión y predicción.\nSeries de tiempo"
  },
  {
    "objectID": "sesion0_slides.html#contenidos-1",
    "href": "sesion0_slides.html#contenidos-1",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n1. Limpieza y estructura de datos.\na. Formateo de datos\nb. Transformación de datos\nc. ETL"
  },
  {
    "objectID": "sesion0_slides.html#contenidos-2",
    "href": "sesion0_slides.html#contenidos-2",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n2. Regresión y predicción.\na. Regresión lineal múltiple.\nb. Predicción usando regresión y los peligros de la extrapolación.\nc. Factores y variables categóricas en una regresión.\nd. Multicolinealidad, variables de confusión e interacciones.\ne. Diagnóstico de una regresión y supuestos (outliers, heterocedasticidad, no-normalidad, errores correlacionados y no-linealidad)\nf. Sesgos en los análisis: Paradoja de Simpson, Paradoja de Berkson y Collider Bias."
  },
  {
    "objectID": "sesion0_slides.html#contenidos-3",
    "href": "sesion0_slides.html#contenidos-3",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n3. Series de tiempo\na. Búsqueda y reorganización de datos de series de tiempo\nb. Análisis de datos exploratorios para series temporales\n    i. Histogramas, gráfico de dispersión y métodos exploratorios para series de tiempo\n    ii. Estacionariedad y raíz unitaria\n    iii. Autocorrelación y correlaciones espurias en series de tiempo\nc. Modelos estadísticos para series de tiempo\n    i. ¿Por qué no utilizar una regresión lineal?\n    ii. Modelos autorregresivos (AR), ARIMA y Autorregresión vectorial (VAR)\n    iii. Ventajas y desventajas de los métodos estadísticos para series de tiempo"
  },
  {
    "objectID": "sesion0_slides.html#evaluación",
    "href": "sesion0_slides.html#evaluación",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\n\nEl curso tendrá dos evaluaciones basadas en el trabajo en clase y refuerzo de los contenidos fuera del horario lectivo.\n\nTalleres de aplicación (30%)\nProyecto final (70%)\n\nTodas las evaluaciones se realizarán mediante un set de rubricas, publicadas en Canva."
  },
  {
    "objectID": "sesion0_slides.html#evaluación-1",
    "href": "sesion0_slides.html#evaluación-1",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nTalleres de aplicacion (30%)\n\nDurante las clases se desarrollará un taller que aplique los contenidos desarrollados en cada una de las tres principales unidades. Se pueden trabajar de manera individual, o en grupo de hasta 3 personas.\n\nTaller 1: Limpieza, análisis descriptivo de datosy pruebas de hipótesis (repasa elementos del curso anterior) (sesión 1 - 2)\nTaller 2: Análisis de regresión. (Sesiones  3-4)\nTaller 3: Análisis de serie de tiempo (sesiones 5-6)"
  },
  {
    "objectID": "sesion0_slides.html#evaluación-2",
    "href": "sesion0_slides.html#evaluación-2",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nTalleres de aplicacion (30%) (cont.)\n\nLa mayoría del taller se espera lo puedan responder durante la clase, sin embargo tendrán una semana de margen para su entrega. Estos se deben entregar en la plataforma canvas, en PDF (para su evaluación) y en .ipynb/.qmd, que será corroborado que se pueda ejecutar y sea consistente con el pdf. \nstos se evaluarán de acuerdo a la rubrica talleres de aplicacion\nUna vez sean entregados los talleres, se hará publica una pauta de desarrollo de cada taller."
  },
  {
    "objectID": "sesion0_slides.html#evaluación-3",
    "href": "sesion0_slides.html#evaluación-3",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nProyecto de análisis de datos (70%)\n\nVamos a desarrollar durante el curso un proyecto.\nEn este deben elegir un conjunto de datos, a proponer una pregunta e hipótesis a testear, desarrollar análisis (de regresión o serie de tiempo) y concluir en base a sus resultados obtenidos, mencionando las limitaciones de su análisis.\nEl proyecto se debe realizar en grupos entre 3 a 5 personas."
  },
  {
    "objectID": "sesion0_slides.html#evaluación-4",
    "href": "sesion0_slides.html#evaluación-4",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nProyecto de análisis de datos (70%)\nEste proyecto se evaluará entonces en base a tres elementos:\n\nAvance durante la clase (20%)\nReporte de análisis y resultados (20%)\nPresentación oral final (30%)"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-análisis-de-datos",
    "href": "sesion0_slides.html#proyecto-de-análisis-de-datos",
    "title": "Presentación del curso",
    "section": "Proyecto de análisis de datos",
    "text": "Proyecto de análisis de datos\nAvance en clase ( 20% )\n\nAlgunos elementos del proyecto serán desarrollados durante tiempo de clase, pero se espera que la profundización sea llevada en el tiempo lectivo dedicado al curso.\nNO ES SUFICIENTE PARA TERMINAR EL TRABAJO COMPLETO.\nRubrica de trabajo en clase (se evalúa al final de la clase)\n\nAsistencia y participación\nPlanteamiento de problemas \nDesarrollo\nResultados, interpretación y conclusiones"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-análisis-de-datos-1",
    "href": "sesion0_slides.html#proyecto-de-análisis-de-datos-1",
    "title": "Presentación del curso",
    "section": "Proyecto de análisis de datos",
    "text": "Proyecto de análisis de datos\nReporte de análisis y resultados (20%)\n\nDeben documentar su análisis de datos mediante un notebook. Este se revisará en si mismo, para fomentar las buenas prácticas y reproducibilidad de su análisis.\nRubrica de notebook reporte de análisis\n\nEntrega a tiempo\nUso correcto del lenguaje y redacción a nivel profesional\nOrden\nCalidad de código\nConsistencia con presentación"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-análisis-de-datos-2",
    "href": "sesion0_slides.html#proyecto-de-análisis-de-datos-2",
    "title": "Presentación del curso",
    "section": "Proyecto de análisis de datos",
    "text": "Proyecto de análisis de datos\nPresentación oral final (30%)\n\nEn la última sesión del curso, cada grupo debe presentar su análisis y resultados.\nEsta presentación será de 10 minutos por grupo y 5 minutos para preguntas, las cuales serán dirigidas a cada estudiante del grupo."
  },
  {
    "objectID": "sesion0_slides.html#calendario-por-sesión",
    "href": "sesion0_slides.html#calendario-por-sesión",
    "title": "Presentación del curso",
    "section": "Calendario por sesión:",
    "text": "Calendario por sesión:\n\n\n\n\nCurso Análisis de Datos - Sesión 1"
  },
  {
    "objectID": "sesion2.html",
    "href": "sesion2.html",
    "title": "Sesión 2: Preparando los datos",
    "section": "",
    "text": "Slides Sesión 1"
  },
  {
    "objectID": "sesion2.html#tipos-de-datos",
    "href": "sesion2.html#tipos-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tipos de datos",
    "text": "Tipos de datos\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje común.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relación a:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis.\n\n\nSegún estructura de los datos\n\nLa estructura de un dato se refiere a su formato y codificación.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\nDatos estructurados\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\nDatos No estructurados\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\nEstructurados vs no estructuraods\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n\n\n\nDatos según tamaño\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n\n\nDatos según tipo y su operacionalización\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\n\n\n\nDatos según temporalidad y unidad de análisis.\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales\n\n\nCorte transversal\n\n\n\nSerie temproal\n\n\n\nPanel"
  },
  {
    "objectID": "sesion2.html#leyendo-datos-en-pandas",
    "href": "sesion2.html#leyendo-datos-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo\n\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read_*\n\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: automático o definido por el usuario\nDatetime parsing: puede combinar información de múltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con números con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2.html#formato-csv-valores-separados-por-comas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que parece ser\n\n¿Y qué pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2.html#csv-en-pandas",
    "href": "sesion2.html#csv-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nBásica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nBásica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representación de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2.html#json-java-script-object-notation",
    "href": "sesion2.html#json-java-script-object-notation",
    "title": "Sesión 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, números, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores numéricos"
  },
  {
    "objectID": "sesion2.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2.html#orientaciones-posibles-de-json",
    "href": "sesion2.html#orientaciones-posibles-de-json",
    "title": "Sesión 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2.html#extensible-markup-language-xml",
    "href": "sesion2.html#extensible-markup-language-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato más antiguo y auto descriptivo, con estructura jerárquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un método incorporado en Python.\nSe puede usar la librería lxml (también ElementTree)\n\n\nEjemplo XML"
  },
  {
    "objectID": "sesion2.html#formatos-binarios",
    "href": "sesion2.html#formatos-binarios",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¿Qué es un formato binario?\nPickle: Python’s built-in serialization\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n\n\nHDF5: Librería para almacenar gran cantidad de datos científicos\n\nFormato jerárquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresión\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2.html#bases-de-datos-relacionales",
    "href": "sesion2.html#bases-de-datos-relacionales",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\n\nLas bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\nvínculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos \n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayoría de los sistemas de bases de datos a través de un API común\nSintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.) \nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n`import sqlalchemy as sqla\ndb = sqla.create_engine(‘sqlite:///mydata.sqlite’)\npd.read_sql(‘select * from test’, db)`"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-estadístico",
    "href": "sesion2.html#dirty-data-punto-de-vista-estadístico",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista estadístico",
    "text": "Dirty data: punto de vista estadístico\n\nLos datos son generados desde algún proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsión: algunas muestras se corrompieron por un proceso\nSesgo de selección: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: Left and right censorship: los usuarios van y vienen del escrutinio\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisión y simplicidad"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores están missing, corrompidos, erróneos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "href": "sesion2.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un área",
    "text": "Dirty Data: Punto de vista del experto en un área\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¿Qué ocurrió?\nLos expertos en un área llevan un modelo implícito de los datos que están testeando\nNo siempre necesitas ser un experto en un área para hacer esto\n\n¿Puede una persona correr 500 km por hora?\n¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido común"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinación de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregación es menos susceptible a los errores numéricos\nSer cuidadoso, puede que los datos estén bien…"
  },
  {
    "objectID": "sesion2.html#dónde-se-origina-el-dirty-data",
    "href": "sesion2.html#dónde-se-origina-el-dirty-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Dónde se origina el dirty data?",
    "text": "¿Dónde se origina el dirty data?\n\nLa fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integración de diferentes sets de datos causa problemas\nPropagación del error: se magnifica un error"
  },
  {
    "objectID": "sesion2.html#problemas-dirty-comunes",
    "href": "sesion2.html#problemas-dirty-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs. New York\nPérdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs. dos\nDatos truncados: “Janice Keihanaikukauakahihuliheekahaunaele” se vuelve “Janice - Keihanaikukauakahihuliheek” en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposición\nProblemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017"
  },
  {
    "objectID": "sesion2.html#data-wrangling",
    "href": "sesion2.html#data-wrangling",
    "title": "Sesión 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato más significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representación a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2.html#tidy-data",
    "href": "sesion2.html#tidy-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para análisis de corte transversal nuestro ideal es trabajar con Tidy Data\n“Tidy Data is a standar way of mapping the meaning of a dataset to its structure¨\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un estándar deseable para analizar datos.\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observación (registro) forma una fila.\nCada dato (valor) está en una celda de la tabla.\n\n\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:\n\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n\n¿Porqué datos Tidy?\n\nEstandarizanción\n\nLos datos organizados te permiten ser más eficiente al utilizar herramientas existentes diseñadas específicamente para realizar las tareas que necesitas hacer, desde la selección de porciones de tus datos hasta la creación de mapas de tu área de estudio.\nUtilizar herramientas existentes te ahorra tener que construir todo desde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual puede ser consume tiempo y desmotivante).\nAfortunadamente, existen muchas herramientas diseñadas específicamente para transformar datos desorganizados en datos organizados (por ejemplo, en el paquete tidyr). Al estar mejor preparado para transformar tus datos en un formato organizado, podrás llegar más rápido a tus análisis y comenzar a responder las preguntas que estás planteando.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nFacilitar la colaboración\n\nDado que nuestros colegas pueden emplear las mismas herramientas de manera familiar. Tanto si consideramos a los colaboradores como compañeros actuales, su futuro propio o futuros colegas, la organización y compartición de datos de manera coherente y predecible implica menos ajustes, tiempo y esfuerzo para todos.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nSimplifica la reproducibilidad\n\nLos datos organizados también facilitan la reproducción de análisis, ya que son más fáciles de comprender, actualizar y reutilizar. Al utilizar herramientas que esperan datos organizados como entrada, puedes construir e iterar flujos de trabajo realmente potentes. Y cuando tienes entradas de datos adicionales, ¡no hay problema en volver a ejecutar tu código!\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\n\n¿Siempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o “desordenadas”.\nEs importante tener en cuenta que siempre existen múltiples formas de representar la misma información.\n\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2.html#datos-perdidos",
    "href": "sesion2.html#datos-perdidos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\n\nse pueden escoger filas o columnas\n\n\nLlenar/ reemplazar :\n\n\ncon un valor por default\ncon un valor interpolados, otros\n\n\n\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2.html#fitrando-y-limpiando",
    "href": "sesion2.html#fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cuál de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key.\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2.html#problemas-de-registro-relacionados-con-textos",
    "href": "sesion2.html#problemas-de-registro-relacionados-con-textos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas de registro relacionados con textos",
    "text": "Problemas de registro relacionados con textos\nTextos o strings, es muy común tener problemas de codificación, errores de tipeo, problemas de formato, etc.\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) “quiebra” o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14”\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14”\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD” \"AbCd\".lower() # \"abcd\"\n\n\nErrores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46417/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\n\n\n\n\nTransformando strings\n\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n2\n\n\n\n#s.index(':') # ValueError raised\n\n\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado\n\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n2\n\n\n\ns.find(':') # -1\n\n-1\n\n\n\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string\n\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\nFalse\n\n\n\n\nMétodos para strings\n\n\nCualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido) Se debe usar .str. (es importante el .str).\nEjemplo:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets\n\n\nEliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis.\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n\n\n\n\nReshapes/re formato\n\nEl “reshape” o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico.\nEsto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos.\nEl “reshape” es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo.\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n\n\n\n\nCrear variables dummies / dicotómicas\n\nMuy útil para representar información cualitativa.\nMuy usando en análisis de regresión y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\nSe pueden generar mediante pd.get_dummies(df[‘key’])\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0"
  },
  {
    "objectID": "sesion2.html#unir-datasets",
    "href": "sesion2.html#unir-datasets",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\nApliar/concatenar/append Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como “inner”, “outer”, “left” y “right”\n\n\n\n1. Apilar dataframes\nCreemos los dataframes:\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n\n\nHagamos el apilar. Atención a los index\n\ndf.append(df2)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46417/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9\n\n\n\n\n\n\n\n\nUsemos ignorar index\n\ndf.append(df2, ignore_index=True)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46417/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9\n\n\n\n\n\n\n\n\n\n\n2. Unir dataframes\nUtilizando la función pd.merge().\n\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n\nUnión interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUnión externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0"
  },
  {
    "objectID": "sesion2.html#transformaciones-estadísticas",
    "href": "sesion2.html#transformaciones-estadísticas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones estadísticas:",
    "text": "Transformaciones estadísticas:\n\n1. Outliers\nUn valor atípico, también conocido como valor outlier en inglés, es un punto de datos que difiere significativamente del patrón general de los demás datos en un conjunto. Estos valores son inusuales en relación con el resto de la distribución de los datos y pueden ser considerablemente más altos o más bajos que los valores típicos del conjunto.\nLa identificación de valores atípicos es importante por varias razones: 1. Calidad de los datos 2. Impacto en estadísticas y modelos 3. Anomalías y problemas reales 4. Toma de decisiones informada\nVeamos un ejemplo:\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n\nValores atípicos:\n   Valor\n6    200\n\n\n\n\n2. Estandarización de Datos\nLa estandarización es un proceso importante en el análisis de datos y el aprendizaje automático. Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno. Esta transformación se logra mediante la fórmula:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nDonde $ x $ es el valor original, $ $ es la media de los valores y $ $ es la desviación estándar.\nLa estandarización es especialmente útil cuando se tienen variables con diferentes escalas y distribuciones, ya que permite compararlas de manera equitativa y mejorar la eficacia de los algoritmos de aprendizaje automático al mitigar el impacto de las diferencias en las magnitudes de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]\n\n\n\n\n\n\n\n3. Normalización de Datos\nLa normalización es un proceso esencial en el análisis de datos y el aprendizaje automático. Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1. La fórmula matemática utilizada para la normalización es:\n\\[ x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\]\nDonde $ x $ es el valor original y $x_{} $ es el valor normalizado.\nLa normalización es beneficiosa para igualar la escala de las variables y mejorar el rendimiento de los modelos que son sensibles a la magnitud de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]"
  },
  {
    "objectID": "sesion1.html",
    "href": "sesion1.html",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "",
    "text": "Slides Sesión 1"
  },
  {
    "objectID": "sesion1.html#detalles",
    "href": "sesion1.html#detalles",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Detalles",
    "text": "Detalles\nNotas detalladas de la sesión 1, curso análisis de datos, magíster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versión 1"
  },
  {
    "objectID": "sesion1.html#objetivos-de-aprendizaje-de-la-sesión",
    "href": "sesion1.html#objetivos-de-aprendizaje-de-la-sesión",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Objetivos de aprendizaje de la sesión",
    "text": "Objetivos de aprendizaje de la sesión\n\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos, junto a buenas prácticas que promuevan la transparencia y replicabilidad.\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados."
  },
  {
    "objectID": "sesion1.html#contenidos",
    "href": "sesion1.html#contenidos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Contenidos:",
    "text": "Contenidos:\n\nEl proceso de análisis de datos\n\nEl proceso de análisis de datos\n\nUna visión general a las metodologías de análisis que veremos en el curso\nAdquision y almacenmiento de los datos\nPreparación de los datos\n\nPreguntando a los datos\n\nAsbtrayendo la realidad, variables aleatorias y probabilidades.\nPlanteamiento de preguntas.\nPreguntas y respuestas: el rol de las hipótesis.\n\n\n\n\nRespondiendo desde los datos: Pruebas de hipótesis\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\n\n\n\nBuenas prácticas en análisis de datos\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1.html#el-proceso-de-análisis-de-datos-1",
    "href": "sesion1.html#el-proceso-de-análisis-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "El proceso de análisis de datos",
    "text": "El proceso de análisis de datos\nEn el mundo actual, la generación y recopilación de datos se ha vuelto más accesible y significativa que nunca antes. Esta abundancia de información ofrece la oportunidad de extraer conocimientos valiosos que pueden influir en la toma de decisiones y el desarrollo de soluciones eficientes.\nSin embargo, el proceso de transformar estos datos crudos en información útil y significativa requiere una serie de pasos fundamentales que forman parte integral de la disciplina conocida como Ciencia de Datos.\n\nEn esta primera parte, daremos un vistazo general a las metodologías y enfoques clave que exploraremos a lo largo del curso, con énfasis en la importancia de la preparación de los datos.\nEl proceso de análisis de datos se puede dividir en varias etapas interconectadas, cada una con su propio conjunto de desafíos y consideraciones.\n\nBajo esta mirada, tenemos varias fases clave que están interconectadas. En este curso nos enfocaremos en la preparación de los datos y en su análisis mediante modelos de regresión. Esto con el objetivo de responder preguntas desde los datos, que provean información valiosa.\n\nAdquisición de datos:\nEl primer paso en el proceso de análisis de datos implica la adquisición y el almacenamiento de los datos. Esto se refiere a la recolección de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilación de datos de fuentes diversas, como bases de datos, archivos CSV, páginas web o incluso sensores en tiempo real.\nEs crucial comprender cómo recopilar y almacenar estos datos de manera adecuada, garantizando su calidad, integridad y seguridad.\nExisten tantas fuentes de datos, como podríamos imaginar. ALgunas de las más comunes son las siguientes:\n\nEncuestas y Cuestionarios:\n\nDiseño y administración de encuestas para recopilar datos directamente de los participantes.\nPermite obtener información específica y detallada según las preguntas planteadas.\n\nExperimentos Controlados:\n\nDiseño de experimentos para recopilar datos bajo condiciones controladas.\nÚtil para establecer relaciones causales y evaluar efectos de cambios controlados.\n\nObservación y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar información ambiental.\nUtilización de sensores en dispositivos móviles y wearables para recopilar datos de ubicación, salud y actividad.\n\nRecopilación de Datos Existentes:\n\nUtilización de datos ya recopilados y disponibles en bases de datos o fuentes públicas.\nReduce el tiempo y costo de recopilación, pero puede tener limitaciones en términos de calidad y relevancia.\n\nWeb Scraping (Web Scrapping):\n\nExtracción de datos de sitios web utilizando herramientas y técnicas automatizadas.\nPermite recopilar información no estructurada de manera eficiente, pero requiere atención a la ética y términos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracción programática con sistemas y servicios para obtener datos en tiempo real.\nComún en la obtención de datos de redes sociales, información climática, finanzas, entre otros.\n\nColaboración y Participación Comunitaria:\n\nColaboración con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser útil para proyectos de mapeo colaborativo, ciencia ciudadana y recopilación de información local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes volúmenes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilación y posterior análisis de datos heterogéneos.\nUsualmente se accede a través de querys SQL\n\n\n\n\n\n\n\n\nDatos disponibles para el proyecto\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos públicos sobre educación chilena\nDatos públicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad económica\n\nVeamos como acceder algunos de estos datos.\n\n\nEjemplo: Datos públicos sobre individuos en comunas chilenas (encuesta Casen)\nLa Encuesta de Caracterización Socioeconómica Nacional (CASEN) es una investigación realizada en Chile que tiene como objetivo principal recopilar información detallada sobre la situación socioeconómica de los hogares y las personas en el país. Esta encuesta se lleva a cabo de manera periódica y abarca una amplia variedad de temas, como ingresos, educación, empleo, salud, vivienda y otros aspectos relevantes para comprender la realidad socioeconómica de la población chilena. La información recopilada en la Encuesta CASEN se utiliza para informar políticas públicas, tomar decisiones informadas y analizar la evolución de indicadores sociales a lo largo del tiempo.\nSitio Web oficial\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. El formato mas comun es .csv, pero a veces estan en formatos extraños. Por ejemplo, .dta de STATA.\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(5)\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegión de Tarapacá\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegión de Tarapacá\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educación formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegión de Tarapacá\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nBásica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegión de Tarapacá\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nTécnico nivel superior incompleta\nSí\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n4\n1.101100e+11\n3\n30\nRegión de Tarapacá\nIquique\nUrbano\n67\n19\nMujer\n3\n...\nNaN\nNo sabe\nNo\n27083.0\n941583\n27083.0\n941583\n2083.0\n891583.0\n439170.0\n\n\n\n\n5 rows × 22 columns\n\n\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploración, para ser preparados para analizarlos. De esto se tratará la siguiente sesión del curso.\nEjemplo: Datos desde la API del banco mundial Primero siga este ejemplo practico de importar datos, luego será facil responder la pregunta anterior.\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb # para instalar: conda install pandas-datareader  o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. En este caso revisare de PIB (GDP en ingés), pero se pueden explorar muchas más opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n1503\nBG.GSR.NFSV.GD.ZS\nTrade in services (% of GDP)\n\nWorld Development Indicators\nTrade in services is the sum of service export...\nb'International Monetary Fund, Balance of Paym...\nEconomy & Growth ; Private Sector ; Trade\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16704\nUIS.XUNIT.GDPCAP.23.FSGOV\nInitial government funding per secondary stude...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16705\nUIS.XUNIT.GDPCAP.23.FSHH\nInitial household funding per secondary studen...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n16706\nUIS.XUNIT.GDPCAP.3.FSGOV\nInitial government funding per upper secondary...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16707\nUIS.XUNIT.GDPCAP.5T8.FSGOV\nInitial government funding per tertiary studen...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16708\nUIS.XUNIT.GDPCAP.5T8.FSHH\nInitial household funding per tertiary student...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n\n\n540 rows × 7 columns\n\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n\n\n\n\n\nObservar que este es un data frame con dos índices: pais y año. Para mayor referencia coo tratar este tipo de datos ver en https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\nObtengamos un data frame con los datos de Chile, entre 1980 y 2020.\n\n#sabemos que queremos Chile, asi que busquemos su info\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n\n\n\n\n\nSi quisieramos, por simplicidad quedarnos solo con el indice del año y reordenar el dataframe:\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el análisis es para un solo país\nreversed_df.head(5)\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068\n\n\n\n\n\n\n\nAhora, realicemos un grafico rápido con nuestros datos:\n\n# Graficamos\n\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'Año')\n\nText(0.5, 0, 'Año')\n\n\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 1 - Bajando y formateando datos del Banco Mundial**\n\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\n\nPreguntando a los datos\n¿Cómo plantear preguntas y formular hipótesis en el contexto del análisis de datos? El proceso de análisis comienza con la curiosidad y/o necesidad.cLa formulación de preguntas relevantes que se puedan responder mediante la exploración y el examen de los datos disponibles.\nInicia con la identificación de áreas de interés y la formulación de preguntas específicas relacionadas con esos temas. Estas preguntas pueden surgir de la necesidad de resolver un problema, entender un fenómeno o explorar patrones en los datos. Un buen planteamiento de preguntas es crucial, ya que guiará todo el proceso de análisis.\n\n\n\nEl proceso de abstraer la realidad\n\n\nPreguntas e hipótesis:\nUna hipótesis es una afirmación, verificable con evidencia. En este sentido, para toda pregunta podemos responderla mediante hipótesis.\nEn particular, para responder a las preguntas en el contexto de datos, es común formular hipótesis nulas y alternativas.\nLa hipótesis nula es aquella que propone que algun parámetro toma cierto valor. Este generlamente es un punto de verdad. Si bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto. En general, planteamos el problema de tal manera que podamos rechazar la hipótesis nula, en favor de otra que llamamos alternatiba.\nQuizas, la hipotesis nula más famosa es la prueba de “significancia”. En esta se propone que un parámetro (muchas veces un efecto, o correlación) es 0, es decir, plantea que no hay efecto o relación entre las variables, mientras que la hipótesis alternativa sugiere que sí existe una relación o efecto significativo.\nEstas hipótesis son fundamentales para establecer una base objetiva para el análisis y para evaluar las evidencias encontradas en los datos. El proceso de plantear preguntas y formular hipótesis es el primer paso en el análisis de datos, ya que establece una guía clara para el enfoque y la dirección del trabajo. Al identificar preguntas y establecer hipótesis, se crea un marco sólido que orientará la exploración y el análisis de los datos disponibles.\n\n\n\n\n\n\nTaller 1: Pregunta 2 - Investigando sobre países:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1.html#respondiendo-desde-los-datos",
    "href": "sesion1.html#respondiendo-desde-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\n\nInferencia estadística\nInferencia se refiere al proceso de hacer generalizaciones de una población a partir de una muestra de esa población. En particular, la idea es que si tenemos un conjunto de datos (muestra) obtenido de una población más grande, el cual es representativo de esta, podemos utilizar métodos estadísticos para sacar conclusiones sobre las características y propiedades de esa población en su totalidad.\n\n\n\nPoblación y Muestra\n\n\nEl proceso de inferencia estadística se basa en el principio de que una muestra bien seleccionada puede proporcionar información valiosa sobre la población en general. Mediante el análisis de la muestra, podemos estimar parámetros poblacionales, como la media, la proporción o la desviación estándar, y también podemos construir intervalos de confianza para estimar el rango dentro del cual se espera que se encuentren estos parámetros.\nEl uso de la inferencia estadística es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una población en particular. Por ejemplo, en lugar de encuestar a todos los ciudadanos de un país, es mucho más factible encuestar una muestra representativa y utilizar esa información para hacer suposiciones sobre la opinión de la población en general.\n\nEstadígrafos y el Teorema del Límite central\nEntonces, en cada muestra que tenemos podemos calcular aproximaciones a los parámetros poblacionales de interés. Estos son los llamados estadísgrafos \nDado que por cada muestra que tenemos, vamos a calcular un estadígrafo este es en si mismo una variable aleatoria. Tiene su propia distribución, media y varianza!\n\nEl estadígrafo más conocido es el promedio o media muestral.\n\n\n\nEstadigrafos más comunes\n\n\nCada estimador es una función de la muestra, por ende para cada muestra que tengamos obtendremos un valor numérico específico para el estimador. Por este motivo, cuando estamos trabajando con una única muestra específica, tenemos un único valor del estimador, o estimador puntual.\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población, por lo cual un primer camino tentador es usar el estimador puntual para tomar una decisión. Como nunca podemos conocer el verdadero parámetro, tampoco podemos saber a ciencia cierta si el estimador puntual es cercano a este.\n¿Cómo conectamos estadpigrafos y parámetros?\nEl teorema del límite central, nos dice que, bajo ciertas condiciones, la distribución de las medias muestrales de una población se aproxima a una distribución normal a medida que el tamaño de la muestra aumenta, independientemente de la forma de la distribución original de la población. Este teorema es esencial en inferencia estadística y tiene amplias aplicaciones en análisis de datos y toma de decisiones.\n\n\n\nLa media muestral se distribuye normal, sin importar la distribución de la variable subyacente\n\n\nFormalmente, el Teorema del Límite Central establece lo siguiente:\n\\[\\bar{x} \\sim_a N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) \\]\nSupongamos que tenemos una población con media μ y desviación estándar σ finitas. Si tomamos muestras aleatorias de tamaño n de esta población y calculamos la media muestral de cada muestra, entonces, a medida que n tiende a infinito, la distribución de estas medias muestrales se aproximará a una distribución normal con media μ y desviación estándar σ/√n.\nEn otras palabras, sin importar la distribución original de la población, cuando el tamaño de la muestra es suficientemente grande, la distribución de las medias muestrales seguirá una forma de campana similar a la distribución normal. Este resultado es fundamental para realizar inferencias sobre la población a partir de muestras, ya que nos permite aplicar métodos basados en la distribución normal incluso cuando la población original no sigue una distribución normal.\n\n\nError estándar\n\nCorresponde a un estimador de la desviación estándar del estimador.\nIdentifica que tan lejos estamos del verdadero valor poblacional.\nPara la media muestral:\n\n\\[ SE = \\frac{S_y}{\\sqrt{n}}\\]\nSe utiliza para evaluar a los estimadores, mediante pruebas de hipotesis y construir intervalos de confianza\n\nSi se conoce un estimador y su desviación estándar, podemos saber qué tan precisa es la estimación (mucha o poca varianza), pero no podemos saber si el estimador está cercano o no a su valor verdadero en la población (el cual no conocemos).\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población.\nSí se puede construir un conjunto de valores que contienen el parámetro poblacional con alguna probabilidad (llamada el nivel de confianza).\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n\n\nInferencia sobre Estadígrafos y parámetros - Conectados por el Teorema del Límite central\n\\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hipótesis\np-valor\n\n\n\n\n\nIntervalos de confianza\nUna primera manera de aproximarnos a los parámetros poblacionales (particularmente a la esperanza) es mediante la construcción de intervalos de confianza.\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n¿De dónde saco los valores críticos?\nLos valores críticos de una distribución los obtenemos de una tabla de distribución o para calcular podemos usar excel, R o en python:\nscipy.stats.t.isf(alpha, n-p)\nSi estamos trabajando con dos colas, usar alpha/2 porque la probabilidad de error la estamos repartiendo a ambas colas.\n\n\n\n\n\n\n[Matemáticamente] Caso 1: Varianza conocida\n\n\n\nSumpongamos que tenemos una muestra aleatoria: \\(y_1, y_2, \\dots, y_n\\) de una población \\(Y\\sim N(\\mu, \\sigma^2)\\)\n\nLa media muestral \\(\\bar{y}= \\frac{1}{n}\\sum_{i=1}{n}y_i\\)\n\nSu esperanza es: \\(E(\\bar{y}) =\\mu\\)\nSu varianza es: \\(var(\\bar{y}) = \\frac{\\sigma^2}{n}\\)\nse distribuye normal, tal que podemos estandarrizar: $ N(0,1) $\n\n\nEntonces, podemos describir que:\n$ P( -1.96 &lt; &lt; 1.96 ) = 0.95 $$\n$ P( {y}- &lt; &lt; {y} + ) = 0.95 $\n\neste intervalo es aleatorio, porque \\(\\bar{y}\\) es diferente en cada muestra.\npara el 95% de las muestras elatorias, el intervalo construido de esta manera contendrá a \\(\\mu\\)\n\n\n\n\n\n\n\n\n\n[Matemáticamente] Caso 2: Varianza desconocida\n\n\n\n\nSupongamos que tenemos una muestra aleatoria \\(y_1, y_2, \\dots, y_n\\) de una población \\(y\\sim N(\\mu, \\sigma^2 )\\)\nUsamos la estimación de la desviación estándar muestral: \\[ S_y = \\sqrt{\\frac{1}{n-1} \\sum{i=1}{n}(y_i - \\bar{y})^2} \\]\nY si estandarizamos \\(\\bar{y}\\): \\[ \\frac{\\bar{Y} - \\mu_y }{ \\frac{S}{\\sqrt{n}}} \\sim t_{n-1} \\]\nPodeos constrir un intervalo de la porbabilidad de estar al 95 con el valor critico c adecuado a los grados de libertad n-1:\n\n\\[ P( -c &lt;   \\frac{\\bar{Y} - \\mu_y }{ \\frac{ S}{\\sqrt{n}}}  &lt; c  ) =0.95  \\]\n\\[ P(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) 0.95\\]\n\nSi llamamos al error estandar SE: \\(SE(\\bar{y})=\\frac{S}{\\sqrt{n}}\\)\nEl IC es: \\[ (\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) \\]\n\n\n\n\n\nEl intervalo es una muestra aleatoria\nEsto quiere decir que para cada muestra podemos construir un intervalo.\nAsí como el estimador es una variable aleatoria, esto también es cierto para los intervalos de confianza. Por eso también se les llama intervalos aleatorios, ya que con diferentes muestras obtendremos un diferente estimador e intervalo.\nPor ende, supongamos que contamos con 20 muestras, entonces construiremos 20 intervalos de confanza diferentes para los 20 estimadores puntuales.\n\nInterpretación de intervalo de confianza\nPensemos en un 95% de confianza (un valor usual). Esto quiere decir, que si se repitiera este ejercicio muchas veces y construyéramos un intervalo de esta forma el 95% de ellos contendría el verdadero parámetro poblacional.\nUn elemento importante a considerar es que esto no significa que con 95% de certeza el parámetro está exactamente en estos valores. Por ejemplo, al 95% de confianza con 20 intervalos 19 contendrán el parámetro.\n\n\n\nPruebas de hipótesis\n\nUna forma de verificar hipotesis sobre los parámetros es mediante el contraste de hipótesis.\n\nEmpezamos suponiendo que hay una distribución conocida para el estadígrafo, centrada en un valor específico. Y nos preguntamos, si esto fuea verdad ¿qué tan probable es la muestra que tengo?\n\nLlamamos la hipótesis a probar Ho, y su alternativa H1.\n\n\n\n\n\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:\n\n\nTipo I: Rechazar Ho cuando es cierta\nTipo II: No rechazar Ho cuando es falsa.\n\n\n\n\nSe elige nivel de significancia de contraste (α) = probabilidad de cometer error Tipo I. Típicamente α = 0,01, 0,05, 0,10.\nPara contrastar una hipótesis con su alternativa, debemos elegir:\n\nUn estadístico de contraste\nUna regla de rechazo, la cual depende de un valor crítico.\n\n\n\n\n\nPrueba de significancia\nDefinimos la prueba de hipótesis de significancia como aquella que indica si un estimador \\(\\hat{T}\\) es 0.\n\\[ H_0: T =0\\text{ vs }H_1: T \\neq 0 \\]\nEl Valor de probabilidad (ó p-valor) es el nivel probabilidad más alto para el cual no podemos rechazar la hipótesis nula de la prueba de significancia.\nEjemplo: $H_0: $ y en la muestra especifica t= 1.52:\n\\[ P-valor = P(T&gt;1.52 \\vert h_0)= 1- \\phi(1.52)=0.0065\\]\n\nel mayor nivel de significancia estadistica al cual no rechazamos \\(H_0\\) es 6.5%\nla probabilidad de observar un velor \\(T\\geq 1.52\\) cuando \\(H_0\\) es cierta es en un 6.5 de las muestras.\nP-valores bajos dan evidencia en contra de \\(H_0\\), ya que la probabilidad de observarlo si \\(H_0\\) es cierta es bajo.\n\n\n\nEjemplo de aplicación: Peso de los Pingüinos Palmer\nConsideremos los datos de los pingüinos Palmer. Los datos “Palmer Penguins” son un conjunto que detalla medidas morfológicas y características de tres especies de pingüinos: Adelie, Gentoo y Chinstrap. Recopilados por el Dr. Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nEn el contexto de los pingüinos y el peso de su población, podríamos tomar una muestra de pingüinos y calcular un intervalo de confianza para el peso promedio. Esto nos daría una estimación del peso promedio de la población total, junto con la confianza en que este valor estimado es preciso.\nEs importante tener en cuenta que el proceso de inferencia estadística se basa en suposiciones y en el uso adecuado de técnicas estadísticas.\nLa elección de la muestra, la interpretación de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hipótesis, sobre el peso de los pingüinos.\nPrimero calcularemos el promedio muestral y lo veremos en el contexto de los datos observados:\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los pingüinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribución del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribución de Peso de Pingüinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral. De que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus características.\nPor ejemplo, consideremos que de esta población de pingüinos obtenemos 1000 muestras de 50 individuos cada una. Si graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tamaño de muestra, más nos alejamos de la distribución normal.\nSi reducimos el número de repeticiones tambieé.\n\n\nimport numpy as np\n\n# Definir el tamaño de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y cálculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gráfico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribución de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\n\nIntervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n# Obtener una muestra simple de 40 pingüinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error estándar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviación estándar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)\n\nIntervalo de Confianza para el Peso:\n(3937.394990022367, 4466.355009977633)\n\n\nEl resultado será un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los pingüinos en la población, con un nivel de confianza del 95%.\n¿Como nos fue? ¿Contiene al verdadero valor?\n\n\n\nComparaciones de grupos\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribución de tamaño por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\ntabla_doble_entrada\n\n\n\n\n\n\n\n\nspecies\nsex\nPromedio\nVarianza\n\n\n\n\n0\nAdelie\nFemale\n3368.835616\n72565.639269\n\n\n1\nAdelie\nMale\n4043.493151\n120278.253425\n\n\n2\nChinstrap\nFemale\n3527.205882\n81415.441176\n\n\n3\nChinstrap\nMale\n3938.970588\n131143.605169\n\n\n4\nGentoo\nFemale\n4679.741379\n79286.335451\n\n\n5\nGentoo\nMale\n5484.836066\n98068.306011\n\n\n\n\n\n\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hipótesis:\n¿Existe una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”?\n\nHipótesis Nula (H0):\n\nNo hay diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\n\nHipótesis Alternativa (H1):\n\nExiste una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\nPara probar esta hipótesis, podrías utilizar una prueba de hipótesis para comparar las medias de las muestras de peso de los pingüinos machos y hembras en la especie “Adelie”. Esto te permitiría determinar si la diferencia observada en el peso promedio es lo suficientemente grande como para considerarse estadísticamente significativa.\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribución de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribución de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nA simple vista podriamos pensar ambos grupos son diferentes. Es más claro si dibujamos el promedio muestral observado.\n\n# Crear un gráfico de densidad con líneas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()\n\n\n\n\nSi construimos una prueba t de diferencia de medias:\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estadística t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gráfico de comparación de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Machos y Hembras de Pingüinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\nEstadística t: 13.126285923485874\nValor p: 6.402319748031793e-26\n\n\n\n\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas. Para esto podriamos usar una prueba ANOVA.\nEn este código, primero cargamos el conjunto de datos “Penguins” y luego creamos dos subconjuntos separados para machos y hembras. Después, utilizamos la función stats.f_oneway() para realizar una prueba ANOVA para comparar los pesos entre hembras y machos. El resultado incluye la estadística F y el valor p.\nEl valor p nos indica si hay una diferencia significativa entre los grupos. Si el valor p es menor que un umbral de significancia (por ejemplo, 0.05), podríamos rechazar la hipótesis nula y concluir que hay una diferencia significativa en el peso entre hembras y machos de diferentes islas.\nRecuerda que, antes de realizar una prueba ANOVA, es importante verificar las suposiciones necesarias, como la normalidad y la homogeneidad de varianzas en los grupos. Si estas suposiciones no se cumplen, podría ser necesario considerar otras pruebas estadísticas o transformaciones de los datos.\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estadística F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\nEstadística F: 72.96098633250911\nValor p: 4.897246751596325e-16\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Calcular las medias de peso por género e isla\nmedias_peso = penguins.groupby(['species', 'island', 'sex'])['body_mass_g'].mean().reset_index()\n\n# Crear un gráfico de barras con puntos y intervalos de confianza\nplt.figure(figsize=(10, 6))\nsns.barplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', errorbar='sd', palette=['pink', 'blue'])\n#sns.boxplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Hembras y Machos por Isla')\nplt.xlabel('Especie e Isla')\nplt.ylabel('Media de Peso (g)')\nplt.legend(title='Sexo')\nplt.show()\n\n\n\n\n\n\nExperimentos Aleatorios y pruebas A/B\nUn experimento estadístico es un enfoque científico que busca establecer relaciones de causalidad y obtener conclusiones sobre cómo ciertas variables afectan a otras. Los experimentos estadísticos se diseñan para manipular deliberadamente una o más variables independientes y observar los efectos que tienen sobre una variable dependiente. Al controlar y manipular las variables de interés, los experimentos permiten a los investigadores hacer afirmaciones más sólidas sobre las relaciones causales.\nUna prueba A/B, también conocida como prueba de división, es una técnica utilizada en la investigación y el análisis para comparar dos variantes o grupos con el fin de determinar cuál de ellos produce un mejor resultado en términos de rendimiento, efectividad o preferencia. En una prueba A/B, se selecciona un grupo de muestra y se divide en dos grupos, uno que experimenta la variante “A” (por ejemplo, una versión actual) y otro que experimenta la variante “B” (por ejemplo, una versión modificada). Luego, se recopilan datos y se comparan los resultados de ambos grupos para determinar cuál variante es más efectiva. Las pruebas A/B son comunes en marketing, diseño de productos y desarrollo web para tomar decisiones informadas sobre mejoras y optimizaciones.\nLas pruebas A/B es son ampliamente utilizado en diversas áreas, como el marketing, la investigación de usuarios y el diseño de productos. En una prueba A/B, se seleccionan dos grupos de muestra: uno experimenta la versión original (A) y el otro experimenta una variante modificada (B). La idea detrás de una prueba A/B es evaluar si la variante B produce un efecto significativamente diferente en una métrica de interés en comparación con la variante A.\nMediante la asignación aleatoria de los participantes a los grupos A y B, y al controlar las condiciones en las que se les presenta cada variante, se reduce la posibilidad de sesgos y se permite un análisis causal más confiable. Al comparar las diferencias observadas en los resultados entre los grupos A y B, es posible inferir si la variante B tiene un impacto significativo en la variable de interés.\nSin embargo, es importante tener en cuenta que aunque las pruebas A/B proporcionan evidencia de asociación causal, no garantizan que la causalidad sea absoluta. Otros factores no controlados pueden influir en los resultados. Para obtener una comprensión más completa de la causalidad, los experimentos controlados aleatorizados y el uso de métodos de diseño experimental sólidos son esenciales. Las pruebas A/B son una herramienta poderosa para explorar causas y efectos en condiciones controladas y analizar el rendimiento relativo de diferentes opciones.\nVeamos un ejemplo en la práctica. Este es parte del ejercicio de aplicación."
  },
  {
    "objectID": "sesion1.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "href": "sesion1.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Caso: Aplicación de A/B testing para promoción de Marketing",
    "text": "Caso: Aplicación de A/B testing para promoción de Marketing\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreación de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 3 -Ejemplo AB test en Marketing:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "sesion1.html#buenas-prácticas-en-análisis-de-datos-1",
    "href": "sesion1.html#buenas-prácticas-en-análisis-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Buenas prácticas en análisis de datos",
    "text": "Buenas prácticas en análisis de datos\n\nImportancia de la Adquisición y Almacenamiento de Datos\nLa adquisición y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de análisis. La calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes. En esta sección, exploraremos la importancia de esta etapa y cómo afecta todo el flujo de trabajo de la ciencia de datos.\nGarantía de Calidad y Fiabilidad en la Obtención de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean sólidas. La calidad de los datos está relacionada con la precisión, integridad y consistencia de la información que recopilamos. Asegurarnos de que los datos sean precisos desde el principio minimiza la posibilidad de errores en análisis posteriores. Exploraremos técnicas y prácticas para verificar la calidad de los datos y cómo mitigar posibles fuentes de error.\nExploración de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros. Cada fuente tiene sus propias características y potenciales sesgos. Comprender las diferencias entre estas fuentes y cómo pueden influir en los resultados es crucial para tomar decisiones informadas. Analizaremos ejemplos de cómo la elección de la fuente de datos puede afectar las conclusiones y cómo evaluar la confiabilidad de las fuentes.\n\nMetodologías de Levantamiento y Adquisición de Datos:\nEl proceso de obtención de datos implica una planificación cuidadosa. Exploraremos diversas metodologías utilizadas para recopilar datos, desde encuestas y experimentos hasta scraping de datos en línea. Cada metodología tiene sus propias ventajas y desventajas, y es importante seleccionar la más adecuada para los objetivos del análisis. Discutiremos cómo diseñar encuestas efectivas, cómo considerar la ética en la recopilación de datos y cómo aprovechar las fuentes de datos existentes.\nEsta sección nos proporcionará una base sólida para comprender cómo adquirir y almacenar datos de manera efectiva y confiable. Una vez que comprendamos cómo obtener datos de calidad, podremos avanzar con confianza en las etapas posteriores del proceso de análisis, sabiendo que estamos trabajando con una base sólida y confiable.\n\n\nDesafíos y Consideraciones:\nA medida que ingresamos al emocionante mundo del análisis de datos, nos encontramos con una serie de desafíos y consideraciones que debemos abordar de manera efectiva para garantizar el éxito de nuestro proyecto. Estos desafíos abarcan desde la protección de la privacidad de los datos hasta las complejidades de la limpieza y transformación durante la etapa de preparación.\n\n\n\nPrivacidad y Seguridad de los Datos:\nUno de los aspectos más críticos en el análisis de datos es la privacidad y seguridad de la información. Los datos pueden contener información sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas. Exploraremos prácticas y regulaciones para garantizar que los datos se manejen de manera ética y legal. Discutiremos cómo anonimizar los datos, utilizar técnicas de enmascaramiento y seguir las mejores prácticas para resguardar la privacidad de los individuos.\n\n\nLimpieza y Transformación durante la Preparación de Datos:\nLa etapa de preparación de datos es crucial para asegurarse de que los datos sean aptos para el análisis. Sin embargo, este proceso no está exento de desafíos. Los datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada. Exploraremos técnicas para identificar y manejar valores atípicos y faltantes, así como la importancia de la normalización y estandarización de los datos. Aprenderemos cómo transformar los datos en un formato adecuado para el análisis, incluida la reorganización de variables y la creación de nuevas características. En resumen, enfrentamos una serie de desafíos y consideraciones clave en nuestro viaje hacia el análisis de datos significativo. Desde la protección de la privacidad hasta la preparación efectiva de los datos, abordar estos desafíos de manera adecuada es esencial para garantizar que nuestras conclusiones sean sólidas, confiables y éticas.\n\n\nReproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\nLa reproducibilidad y el control de versiones son componentes fundamentales para garantizar la integridad y la transparencia en el análisis de datos. Además de mantener un registro detallado de las decisiones tomadas durante el proceso, el uso de sistemas de control de versiones como GIT se vuelve esencial para mantener la trazabilidad y la colaboración efectiva en proyectos de preparación y análisis de datos.\nDocumentación Detallada del Análisis y Uso de Notebooks: Una documentación exhaustiva del análisis es esencial para comprender el flujo de trabajo, las decisiones tomadas y las transformaciones aplicadas a los datos. Los notebooks, como Jupyter Notebooks, ofrecen una herramienta excepcional para lograr esto. En cada celda de un notebook, es posible combinar explicaciones en lenguaje natural con código ejecutable y visualizaciones. Esto permite registrar no solo el qué y el cómo, sino también el porqué detrás de cada paso.\nImportancia de Mantener un Registro de los Cambios en los Datos: Cada decisión tomada durante la preparación y el análisis de datos puede tener un impacto significativo en los resultados finales. Mantener un registro detallado de estas decisiones, desde la limpieza de datos hasta la creación de variables derivadas, es crucial para comprender cómo se obtuvieron ciertos resultados. Una documentación precisa y detallada permite a otros analistas validar y replicar el análisis en el futuro.\nUso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que también es una herramienta poderosa en el análisis de datos. Permite rastrear cada modificación realizada en el código y en los documentos, incluidos los notebooks. Cada cambio es registrado como un “commit”, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\n\n\n\nUn esquema de git por Allison Horst @allison_horst\n\n\nAplicación de Control de Versiones en Proyectos de Preparación de Datos:* La aplicación de GIT en proyectos de preparación de datos agrega un nivel adicional de transparencia y colaboración. Los repositorios de GIT almacenan no solo los datos originales, sino también los notebooks y scripts utilizados en el proceso. Esto permite a los analistas colaborar en un entorno controlado y mantener un historial de cambios. En caso de que surjan problemas o se necesite retroceder en el tiempo, GIT ofrece la capacidad de volver a versiones anteriores de manera segura.\nLa combinación de documentación detallada a través de notebooks y el uso de sistemas de control de versiones como GIT proporciona una base sólida para el análisis de datos reproducible y transparente. Esto no solo facilita la comprensión y validación de los resultados, sino que también fomenta la colaboración y la mejora continua en proyectos de preparación y análisis de datos.\n\n\n\n\n\n\nActividad de proyecto - Inicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del análisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustaría trabajar.\nPropongan una o dos preguntas de investigación y las hipotesis que las responderían.\n\nLa siguiente sesión, vamos a explorar los datos y empezar los primeros pasos en su análisis."
  }
]