[
  {
    "objectID": "sesion0.html",
    "href": "sesion0.html",
    "title": "Presentación del curso",
    "section": "",
    "text": "Hola! Soy Melanie y seré la docente de este curso, en el que aprenderás los fundamentos del análisis de datos, tanto desde una perspectiva teórica como práctica (en Python).\nMe pueden contactar al mail melanie.oyarzun@udd.cl\n\n\nPuedes ver las slides de este documento en slides seseion introductoria\n\n\n\n\n\n\n\n\nRevisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0.html#links",
    "href": "sesion0.html#links",
    "title": "Presentación del curso",
    "section": "",
    "text": "Puedes ver las slides de este documento en slides seseion introductoria"
  },
  {
    "objectID": "sesion0.html#en-la-sesión-de-hoy",
    "href": "sesion0.html#en-la-sesión-de-hoy",
    "title": "Presentación del curso",
    "section": "",
    "text": "Revisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0.html#contexto-en-el-programa-de-magister",
    "href": "sesion0.html#contexto-en-el-programa-de-magister",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la línea de desarrollo de data science.\nEsta asignatura tributa, a través de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Magíster en Data Science:\n\nAplicar teorías, algoritmos, métodos, técnicas y herramientas básicas y avanzadas de Data Science para analizar, resolver y hacer una evaluación crítica de desafíos complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones.\nComunica efectivamente y argumenta sobre los resultados de su trabajo a públicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento ético, cumpliendo los protocolos y normas que guían su desempeño, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicación de estrategias para utilizar nuevo conocimiento en data science en su ámbito de desempeño."
  },
  {
    "objectID": "sesion0.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "href": "sesion0.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura (resultados de aprendizaje)",
    "text": "Objetivos de la asignatura (resultados de aprendizaje)\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el análisis de datos, utilizando lenguaje técnico afín.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso.\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer información y generar conclusiones basadas en evidencia.\nIdentificar las buenas prácticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0.html#resumen",
    "href": "sesion0.html#resumen",
    "title": "Presentación del curso",
    "section": "Resumen:",
    "text": "Resumen:"
  },
  {
    "objectID": "sesion0.html#detallado",
    "href": "sesion0.html#detallado",
    "title": "Presentación del curso",
    "section": "Detallado",
    "text": "Detallado\n\n\n\n\n\n\nSesión 1: Respondiendo Preguntas con datos\n\n\n\n\n\nFecha: 19 agosto\nObjetivos:\n\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados.\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos.\n\nContenidos:\n\nEl proceso de análisis de datos\n\nPlanteamiento de preguntas\nAdquision y almacenmiento de los datos\nPreparación de los datos\nUna visión general a las metodologías de análisis que veremos en el curso\n\nFormulación de Preguntas y Hipótesis:\n\nImportancia de definir preguntas claras y específicas.\nDiferenciación entre preguntas exploratorias y confirmatorias.\nCreación de hipótesis nulas y alternativas.\n\nHipótesis y Variables:\n\nIdentificación de variables independientes y dependientes.\nRelación entre hipótesis y variables a analizar.\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\nImportancia de la Adquisición y Almacenamiento de Datos:\n\nGarantía de calidad y fiabilidad en la obtención de datos.\nExploración de diferentes fuentes de datos y su impacto en los resultados.\nMetodologias de levantamiento y adquision\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\n\nBibliografia recomendada:\nActividades:\n\nTaller 1 (incluidas en slides 1)\nProyecto clase 1: Conformación de grupos, definición de temas, primeras hipótesis y datos.\n\n\n\n\n\n\n\n\n\n\nSesión 2:Preparando los datos\n\n\n\n\n\nFecha: 26 agosto\nObjetivos:\n\nComprender la importancia del proceso de preparación de datos para el análisis, reconociendo principios y enfoques clave junto con sus ventajas y desventajas.\nDesarrollar habilidades prácticas en la preparación de datos, identificando y abordando problemas comunes como valores faltantes, valores atípicos y formatos inconsistentes, así como enfoques de trabajo eficientes.\n\nContenidos:\n\nEl proceso de preparación de los datos\n\nSignificado y relevancia de la preparación de datos.\nEjemplos reales de cómo la falta de preparación puede afectar los resultados.\n\nPrincipios y enfoques\n\nExtract, Transform, Load (ETL): Proceso fundamental en la preparación de datos.\nData Wrangling: Técnicas para dar forma y estructura a los datos.\nDatos Tidy: Organización y reestructuración para un análisis eficaz.\n\nBuenas Prácticas en la Preparación de Datos\n\nDocumentación y Consistencia\n\nImportancia de la documentación detallada.\nMantener nomenclatura y convenciones consistentes.\n\nValidación y Verificación\n\nValidación cruzada y verificación de integridad.\nCumplir con reglas y restricciones esperadas.\n\nReproducibilidad y Versionado\n\nEntorno de trabajo reproducible (Jupyter Notebooks, R Markdown).\nUtilización de sistemas de control de versiones (GIT).\n\nComunicación y Validación Colaborativa\n\nComunicación clara de pasos y resultados.\nValidación intermedia con colaboradores para feedback.\n\nSeguridad y privacidad de los datos\n\nProblemas comunes presentes en datos\n\nValores faltantes: \n\nEstrategias para manejar valores faltantes.\nDecidir entre imputación, eliminación o conservación.\n\nValores atípicos\nNormalización y estandarización\nErrores de registro Bibliografia recomendada:\n\n\n\n“Practical Statistics for Data Scientists” (Capítulo 2).\n“Doing Data Science” (Capítulo 1).\n\nActividades de aplicación práctica:\n\nTaller 1: Limpieza y análisis descriptivo de datos en la practica con datos de educación (repasa elementos del curso anterior) (sesión 1)\nProyecto:\n\nInicie el proyecto, cree un documento notebook en el cual van a alojarsu proyecto\nExplorar los datos\nDiagnosticar problemas.\nLa hipótesis que pensamos, ¿tienen variables que pueda concretizarlas? ¿Qué variables usar?\n\n\n\n\n\n\n\n\n\n\n\nSesión 3: Introduccion al análisis de regresión\n\n\n\n\n\nFecha: 2 septiembre\nObjetivos:\n\nComprender los conceptos fundamentales del análisis de regresión lineal y su aplicación en la resolución de problemas.\nDesarrollar la habilidad de plantear modelos e interpretar los resultados obtenidos del análisis de regresión, para aplicarlos en la toma de decisiones.\n\nContenidos\n\nIntroducción al Análisis de Regresión:\n\nDefinición y concepto de regresión.\nUso y aplicabilidad en la toma de decisiones.\n\nRegresión Lineal Múltiple:\n\nExtensión del modelo de regresión a múltiples variables predictoras.\nEcuación de regresión lineal múltiple.\n\nInterpretación de Coeficientes:\n\nSignificado e interpretación de los coeficientes de regresión.\nInfluencia de las variables predictoras en la variable de respuesta.\n\nEvaluación de Modelos de Regresión:\n\nUso de medidas como el coeficiente de determinación (R²) y el error estándar de estimación.\nInterpretación de los resultados de evaluación.\n\nIncorporación de Variables Categóricas:\n\nTransformación de variables categóricas en variables numéricas.\nInterpretación de coeficientes para variables categóricas.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nPlantear modelos de regresión que implementen las hipótesis del proyecto\nEstimar e interpretar modelos\n\n\n\n\n\n\n\n\n\n\nSesión 4: Profundizando en Análisis de Regresión, Supuestos y Limitaciones\n\n\n\n\n\nFecha: 9 septiembre\nObjetivos:\n\nExplorar los supuestos y limitaciones asociados al análisis de regresión y desarrollar estrategias para manejar problemas comunes.\nAplicar estrategias prácticas para identificar y abordar problemas en el análisis de regresión.\n\nContenidos\n\nSupuestos en el Análisis de Regresión:\n\nIdentificación de supuestos clave: linealidad, independencia, homoscedasticidad y normalidad.\nSignificado de cada supuesto y su importancia en la interpretación de resultados.\n\nIdentificación de Problemas en la Regresión:\n\nIdentificación y manejo de outliers en los datos.\nReconocimiento de la heterocedasticidad y sus implicaciones.\nDetección de la no-normalidad de los residuos.\n\nEstrategias para Manejar Problemas:\n\nTransformación de variables para abordar problemas de linealidad.\nMétodos para reducir la influencia de outliers.\nUso de transformaciones para tratar la heterocedasticidad.\nPruebas y técnicas para verificar y mejorar la normalidad.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nRevisar supuestos de modelo de regresión\nDiscutir problemas de identificación y limitaciones\n\n\n\n\n\n\n\n\n\n\nSesión 5: Introduccion al análisis de series de tiempo\n\n\n\n\n\nFecha: 30 septiembre\nObjetivos:\n\nIdentificar las características y particularidades de los datos de series de tiempo, comprendiendo sus aplicaciones profesionales.\nRealizar un análisis exploratorio de una serie de tiempo, identificando características clave para su modelamiento.\n\nContenidos\n\nConceptos Básicos de Series de Tiempo:\n\nDefinición y características de una serie de tiempo.\nEjemplos de aplicaciones en distintos campos profesionales.\n\nParticularidades de los Datos Temporales:\n\nDependencia temporal y autocorrelación.\nTendencias, estacionalidad y ciclos.\n\nAplicaciones Profesionales:\n\nCasos de estudio en finanzas, economía, medicina y otros campos.\nCómo el análisis de series de tiempo puede brindar insights valiosos.\n\nBúsqueda y Reorganización de Datos Temporales:\n\nFuentes de datos para series de tiempo (bases de datos, APIs, archivos).\nImportancia de la temporalidad y el orden en los datos.\n\nVisualización y Exploración Inicial:\n\nGráficos de línea y dispersión para identificar tendencias y patrones.\nEstudio de estacionalidad y ciclos mediante gráficos.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 6: Modelando series temporales\n\n\n\n\n\nFecha: 7 occtubre\nObjetivos:\n\nComprender los conceptos y aplicaciones de los modelos ARIMA y VAR en el análisis de series temporales.\nEvaluar las ventajas y desventajas de los métodos estadísticos para el análisis de series de tiempo y seleccionar la técnica más adecuada.\n\nContenidos\n\nModelos ARIMA:\n\nDefinición y componentes de los modelos ARIMA.\nIdentificación, Estimación y Validación de un modelo ARIMA.\nUso de correlogramas y gráficos ACF/PACF para la identificación.\n\nModelos VAR (Vector Autoregressive):\n\nIntroducción a los modelos VAR y su aplicación.\nUso de matrices de coeficientes para representar relaciones entre variables.\n\nVentajas y Desventajas de los Métodos Estadísticos:\n\nUso de modelos estadísticos en comparación con otros enfoques.\nLimitaciones y supuestos asociados a los modelos ARIMA y VAR.\n\nSelección del Método Adecuado:\n\nCriterios para elegir entre modelos ARIMA y VAR.\nConsideraciones al evaluar las alternativas disponibles.\n\nOtros modelos\n\n\nGARCH\nSARIMA y SARIMAX\nAlisado exponencial\nCambio estructural\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 7: Principales lecciones para el análisis de datos y presentaciones de proyectos\n\n\n\n\n\nFecha: TBA\nObjetivos:\n\nCerrar el curso, poniendo en contexto las principales herramientas de análisis de datos.\nPresentar un proyecto de análisis de datos\nRecibir feedback y propuestas de mejoras, tanto del trabajo propio como el de sus compañeros.\n\nContenidos\nBibliografía recomendada\nActividades de aplicación práctica\n\nProyecto: Presentaciones finales"
  },
  {
    "objectID": "sesion3.html",
    "href": "sesion3.html",
    "title": "Sesión 2: Preparando los datos",
    "section": "",
    "text": "Slides Sesión 3"
  },
  {
    "objectID": "sesion0_slides.html#un-poco-sobre-mi",
    "href": "sesion0_slides.html#un-poco-sobre-mi",
    "title": "Presentación del curso",
    "section": "Un poco sobre mi:",
    "text": "Un poco sobre mi:"
  },
  {
    "objectID": "sesion0_slides.html#en-la-sesión-de-hoy",
    "href": "sesion0_slides.html#en-la-sesión-de-hoy",
    "title": "Presentación del curso",
    "section": "En la sesión de hoy:",
    "text": "En la sesión de hoy:\n\nRevisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0_slides.html#descripción-del-curso",
    "href": "sesion0_slides.html#descripción-del-curso",
    "title": "Presentación del curso",
    "section": "Descripción del curso",
    "text": "Descripción del curso\n\nEsta asignatura presentará los conceptos básicos de pre-procesamiento y análisis descriptivo de datos.\nEl objetivo principal es poder determinar cuáles datos son susceptibles de ser convertidos en información para apoyar la toma de decisiones, y separar el ruido de la señal."
  },
  {
    "objectID": "sesion0_slides.html#descripción-del-curso-1",
    "href": "sesion0_slides.html#descripción-del-curso-1",
    "title": "Presentación del curso",
    "section": "Descripción del curso",
    "text": "Descripción del curso\n\nEs el primer paso en un proyecto de ciencia de datos.\nLos estudiantes aprenderán a identificar\n\nlas problemáticas que presentan los datos desde el momento de su registro (por ej., error muestral, outliers),\nasí como usar las herramientas necesarias para describirlos (por ej., distribuciones e histogramas),"
  },
  {
    "objectID": "sesion0_slides.html#descripción-del-curso-2",
    "href": "sesion0_slides.html#descripción-del-curso-2",
    "title": "Presentación del curso",
    "section": "Descripción del curso",
    "text": "Descripción del curso\n\nexplorarlos (por ej., agrupar o filtrar bajo un criterio específico),\ny cruzarlos (por ej., utilizando otras fuentes).\nAsimismo, los estudiantes comprenderán que las etapas de este proceso no son lineales, sino que se benefician del diseño iterativo."
  },
  {
    "objectID": "sesion0_slides.html#contexto-en-el-programa-de-magister",
    "href": "sesion0_slides.html#contexto-en-el-programa-de-magister",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la línea de desarrollo de data science.\nEsta asignatura tributa, a través de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Magíster en Data Science:\n\nAplicar teorías, algoritmos, métodos, técnicas y herramientas básicas y avanzadas de Data Science para analizar, resolver y hacer una evaluación crítica de desafíos complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones."
  },
  {
    "objectID": "sesion0_slides.html#contexto-en-el-programa-de-magister-1",
    "href": "sesion0_slides.html#contexto-en-el-programa-de-magister-1",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nComunica efectivamente y argumenta sobre los resultados de su trabajo a públicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento ético, cumpliendo los protocolos y normas que guían su desempeño, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicación de estrategias para utilizar nuevo conocimiento en data science en su ámbito de desempeño."
  },
  {
    "objectID": "sesion0_slides.html#objetivos-de-la-asignatura",
    "href": "sesion0_slides.html#objetivos-de-la-asignatura",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura",
    "text": "Objetivos de la asignatura\nResultados de aprendizaje\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el análisis de datos, utilizando lenguaje técnico afín.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso."
  },
  {
    "objectID": "sesion0_slides.html#objetivos-de-la-asignatura-1",
    "href": "sesion0_slides.html#objetivos-de-la-asignatura-1",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura",
    "text": "Objetivos de la asignatura\nResultados de aprendizaje\n\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer información y generar conclusiones basadas en evidencia.\nIdentificar las buenas prácticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0_slides.html#contenidos",
    "href": "sesion0_slides.html#contenidos",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n\nLimpieza y estructura de datos.\nRegresión y predicción.\nSeries de tiempo"
  },
  {
    "objectID": "sesion0_slides.html#contenidos-1",
    "href": "sesion0_slides.html#contenidos-1",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n1. Limpieza y estructura de datos.\na. Formateo de datos\nb. Transformación de datos\nc. ETL"
  },
  {
    "objectID": "sesion0_slides.html#contenidos-2",
    "href": "sesion0_slides.html#contenidos-2",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n2. Regresión y predicción.\na. Regresión lineal múltiple.\nb. Predicción usando regresión y los peligros de la extrapolación.\nc. Factores y variables categóricas en una regresión.\nd. Multicolinealidad, variables de confusión e interacciones.\ne. Diagnóstico de una regresión y supuestos (outliers, heterocedasticidad, no-normalidad, errores correlacionados y no-linealidad)\nf. Sesgos en los análisis: Paradoja de Simpson, Paradoja de Berkson y Collider Bias."
  },
  {
    "objectID": "sesion0_slides.html#contenidos-3",
    "href": "sesion0_slides.html#contenidos-3",
    "title": "Presentación del curso",
    "section": "Contenidos",
    "text": "Contenidos\n3. Series de tiempo\na. Búsqueda y reorganización de datos de series de tiempo\nb. Análisis de datos exploratorios para series temporales\n    i. Histogramas, gráfico de dispersión y métodos exploratorios para series de tiempo\n    ii. Estacionariedad y raíz unitaria\n    iii. Autocorrelación y correlaciones espurias en series de tiempo\nc. Modelos estadísticos para series de tiempo\n    i. ¿Por qué no utilizar una regresión lineal?\n    ii. Modelos autorregresivos (AR), ARIMA y Autorregresión vectorial (VAR)\n    iii. Ventajas y desventajas de los métodos estadísticos para series de tiempo"
  },
  {
    "objectID": "sesion0_slides.html#evaluación",
    "href": "sesion0_slides.html#evaluación",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\n\nEl curso tendrá dos evaluaciones basadas en el trabajo en clase y refuerzo de los contenidos fuera del horario lectivo.\n\nTalleres de aplicación (30%)\nProyecto final (70%)\n\nTodas las evaluaciones se realizarán mediante un set de rubricas, publicadas en Canva."
  },
  {
    "objectID": "sesion0_slides.html#evaluación-1",
    "href": "sesion0_slides.html#evaluación-1",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nTalleres de aplicacion (30%)\n\nDurante las clases se desarrollará un taller que aplique los contenidos desarrollados en cada una de las tres principales unidades. Se pueden trabajar de manera individual, o en grupo de hasta 3 personas.\n\nTaller 1: Limpieza, análisis descriptivo de datosy pruebas de hipótesis (repasa elementos del curso anterior) (sesión 1 - 2)\nTaller 2: Análisis de regresión. (Sesiones  3-4)\nTaller 3: Análisis de serie de tiempo (sesiones 5-6)"
  },
  {
    "objectID": "sesion0_slides.html#evaluación-2",
    "href": "sesion0_slides.html#evaluación-2",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nTalleres de aplicacion (30%) (cont.)\n\nLa mayoría del taller se espera lo puedan responder durante la clase, sin embargo tendrán una semana de margen para su entrega. Estos se deben entregar en la plataforma canvas, en PDF (para su evaluación) y en .ipynb/.qmd, que será corroborado que se pueda ejecutar y sea consistente con el pdf. \nstos se evaluarán de acuerdo a la rubrica talleres de aplicacion\nUna vez sean entregados los talleres, se hará publica una pauta de desarrollo de cada taller."
  },
  {
    "objectID": "sesion0_slides.html#evaluación-3",
    "href": "sesion0_slides.html#evaluación-3",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nProyecto de análisis de datos (70%)\n\nVamos a desarrollar durante el curso un proyecto.\nEn este deben elegir un conjunto de datos, a proponer una pregunta e hipótesis a testear, desarrollar análisis (de regresión o serie de tiempo) y concluir en base a sus resultados obtenidos, mencionando las limitaciones de su análisis.\nEl proyecto se debe realizar en grupos entre 3 a 5 personas."
  },
  {
    "objectID": "sesion0_slides.html#evaluación-4",
    "href": "sesion0_slides.html#evaluación-4",
    "title": "Presentación del curso",
    "section": "Evaluación",
    "text": "Evaluación\nProyecto de análisis de datos (70%)\nEste proyecto se evaluará entonces en base a tres elementos:\n\nAvance durante la clase (20%)\nReporte de análisis y resultados (20%)\nPresentación oral final (30%)"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-análisis-de-datos",
    "href": "sesion0_slides.html#proyecto-de-análisis-de-datos",
    "title": "Presentación del curso",
    "section": "Proyecto de análisis de datos",
    "text": "Proyecto de análisis de datos\nAvance en clase ( 20% )\n\nAlgunos elementos del proyecto serán desarrollados durante tiempo de clase, pero se espera que la profundización sea llevada en el tiempo lectivo dedicado al curso.\nNO ES SUFICIENTE PARA TERMINAR EL TRABAJO COMPLETO.\nRubrica de trabajo en clase (se evalúa al final de la clase)\n\nAsistencia y participación\nPlanteamiento de problemas \nDesarrollo\nResultados, interpretación y conclusiones"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-análisis-de-datos-1",
    "href": "sesion0_slides.html#proyecto-de-análisis-de-datos-1",
    "title": "Presentación del curso",
    "section": "Proyecto de análisis de datos",
    "text": "Proyecto de análisis de datos\nReporte de análisis y resultados (20%)\n\nDeben documentar su análisis de datos mediante un notebook. Este se revisará en si mismo, para fomentar las buenas prácticas y reproducibilidad de su análisis.\nRubrica de notebook reporte de análisis\n\nEntrega a tiempo\nUso correcto del lenguaje y redacción a nivel profesional\nOrden\nCalidad de código\nConsistencia con presentación"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-análisis-de-datos-2",
    "href": "sesion0_slides.html#proyecto-de-análisis-de-datos-2",
    "title": "Presentación del curso",
    "section": "Proyecto de análisis de datos",
    "text": "Proyecto de análisis de datos\nPresentación oral final (30%)\n\nEn la última sesión del curso, cada grupo debe presentar su análisis y resultados.\nEsta presentación será de 10 minutos por grupo y 5 minutos para preguntas, las cuales serán dirigidas a cada estudiante del grupo."
  },
  {
    "objectID": "sesion0_slides.html#calendario-por-sesión",
    "href": "sesion0_slides.html#calendario-por-sesión",
    "title": "Presentación del curso",
    "section": "Calendario por sesión:",
    "text": "Calendario por sesión:\n\n\n\n\nCurso Análisis de Datos - Sesión 1"
  },
  {
    "objectID": "sesion2_notas.html",
    "href": "sesion2_notas.html",
    "title": "Sesión 2: Preparando los datos",
    "section": "",
    "text": "La preparación de datos es una fase esencial en el proceso de análisis de datos que involucra una serie de actividades destinadas a garantizar que los datos estén en condiciones óptimas para su posterior análisis.\n\nExtraccion de los datos\nLimpieza\nTransformación\nOrganización\n\n\nEsta fase implica la extracción de los datos, su limpieza, transformación y organización de manera que sean coherentes, completos y adecuados para el análisis que se va a realizar.\nTrash in , trash out\n\n\nLa preparación de datos es crucial porque afecta directamente la calidad y confiabilidad de los resultados obtenidos en cualquier análisis posterior.\n\n\n\n\n\n\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje común.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relación a:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis.\n\n\n\n\nLa estructura de un dato se refiere a su formato y codificación.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\n\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\n\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\n\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n\n\n\n\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n\n\n\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\n\n\n\n\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_notas.html#tipos-de-datos",
    "href": "sesion2_notas.html#tipos-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "",
    "text": "Antes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje común.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relación a:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis.\n\n\n\n\nLa estructura de un dato se refiere a su formato y codificación.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\n\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\n\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\n\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n\n\n\n\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n\n\n\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\n\n\n\n\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_notas.html#leyendo-datos-en-pandas",
    "href": "sesion2_notas.html#leyendo-datos-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo\n\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read_*\n\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: automático o definido por el usuario\nDatetime parsing: puede combinar información de múltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con números con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2_notas.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2_notas.html#formato-csv-valores-separados-por-comas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que parece ser\n\n¿Y qué pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2_notas.html#csv-en-pandas",
    "href": "sesion2_notas.html#csv-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nBásica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nBásica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representación de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2_notas.html#json-java-script-object-notation",
    "href": "sesion2_notas.html#json-java-script-object-notation",
    "title": "Sesión 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, números, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores numéricos"
  },
  {
    "objectID": "sesion2_notas.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2_notas.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2_notas.html#orientaciones-posibles-de-json",
    "href": "sesion2_notas.html#orientaciones-posibles-de-json",
    "title": "Sesión 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2_notas.html#extensible-markup-language-xml",
    "href": "sesion2_notas.html#extensible-markup-language-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato más antiguo y auto descriptivo, con estructura jerárquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un método incorporado en Python.\nSe puede usar la librería lxml (también ElementTree)\n\n\nEjemplo XML"
  },
  {
    "objectID": "sesion2_notas.html#formatos-binarios",
    "href": "sesion2_notas.html#formatos-binarios",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¿Qué es un formato binario?\nPickle: Python’s built-in serialization\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n\n\nHDF5: Librería para almacenar gran cantidad de datos científicos\n\nFormato jerárquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresión\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2_notas.html#bases-de-datos-relacionales",
    "href": "sesion2_notas.html#bases-de-datos-relacionales",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\n\nLas bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\nvínculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos \n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayoría de los sistemas de bases de datos a través de un API común\nSintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.) \nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n`import sqlalchemy as sqla\ndb = sqla.create_engine(‘sqlite:///mydata.sqlite’)\npd.read_sql(‘select * from test’, db)`"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-estadístico",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-estadístico",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista estadístico",
    "text": "Dirty data: punto de vista estadístico\n\nLos datos son generados desde algún proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsión: algunas muestras se corrompieron por un proceso\nSesgo de selección: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: Left and right censorship: los usuarios van y vienen del escrutinio\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisión y simplicidad"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores están missing, corrompidos, erróneos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un área",
    "text": "Dirty Data: Punto de vista del experto en un área\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¿Qué ocurrió?\nLos expertos en un área llevan un modelo implícito de los datos que están testeando\nNo siempre necesitas ser un experto en un área para hacer esto\n\n¿Puede una persona correr 500 km por hora?\n¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido común"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinación de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregación es menos susceptible a los errores numéricos\nSer cuidadoso, puede que los datos estén bien…"
  },
  {
    "objectID": "sesion2_notas.html#dónde-se-origina-el-dirty-data",
    "href": "sesion2_notas.html#dónde-se-origina-el-dirty-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Dónde se origina el dirty data?",
    "text": "¿Dónde se origina el dirty data?\n\nLa fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integración de diferentes sets de datos causa problemas\nPropagación del error: se magnifica un error"
  },
  {
    "objectID": "sesion2_notas.html#problemas-dirty-comunes",
    "href": "sesion2_notas.html#problemas-dirty-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs. New York\nPérdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs. dos\nDatos truncados: “Janice Keihanaikukauakahihuliheekahaunaele” se vuelve “Janice - Keihanaikukauakahihuliheek” en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposición\nProblemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017"
  },
  {
    "objectID": "sesion2_notas.html#data-wrangling",
    "href": "sesion2_notas.html#data-wrangling",
    "title": "Sesión 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato más significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representación a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2_notas.html#tidy-data",
    "href": "sesion2_notas.html#tidy-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para análisis de corte transversal nuestro ideal es trabajar con Tidy Data\n“Tidy Data is a standar way of mapping the meaning of a dataset to its structure¨\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un estándar deseable para analizar datos.\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observación (registro) forma una fila.\nCada dato (valor) está en una celda de la tabla.\n\n\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:\n\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n\n¿Porqué datos Tidy?\n\nEstandarizanción\n\nLos datos organizados te permiten ser más eficiente al utilizar herramientas existentes diseñadas específicamente para realizar las tareas que necesitas hacer, desde la selección de porciones de tus datos hasta la creación de mapas de tu área de estudio.\nUtilizar herramientas existentes te ahorra tener que construir todo desde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual puede ser consume tiempo y desmotivante).\nAfortunadamente, existen muchas herramientas diseñadas específicamente para transformar datos desorganizados en datos organizados (por ejemplo, en el paquete tidyr). Al estar mejor preparado para transformar tus datos en un formato organizado, podrás llegar más rápido a tus análisis y comenzar a responder las preguntas que estás planteando.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nFacilitar la colaboración\n\nDado que nuestros colegas pueden emplear las mismas herramientas de manera familiar. Tanto si consideramos a los colaboradores como compañeros actuales, su futuro propio o futuros colegas, la organización y compartición de datos de manera coherente y predecible implica menos ajustes, tiempo y esfuerzo para todos.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nSimplifica la reproducibilidad\n\nLos datos organizados también facilitan la reproducción de análisis, ya que son más fáciles de comprender, actualizar y reutilizar. Al utilizar herramientas que esperan datos organizados como entrada, puedes construir e iterar flujos de trabajo realmente potentes. Y cuando tienes entradas de datos adicionales, ¡no hay problema en volver a ejecutar tu código!\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\n\n¿Siempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o “desordenadas”.\nEs importante tener en cuenta que siempre existen múltiples formas de representar la misma información.\n\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2_notas.html#datos-perdidos",
    "href": "sesion2_notas.html#datos-perdidos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\n\nse pueden escoger filas o columnas\n\n\nLlenar/ reemplazar :\n\n\ncon un valor por default\ncon un valor interpolados, otros\n\n\n\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2_notas.html#fitrando-y-limpiando",
    "href": "sesion2_notas.html#fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cuál de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key.\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2_notas.html#errores-de-registro-y-textos",
    "href": "sesion2_notas.html#errores-de-registro-y-textos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) “quiebra” o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14”\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14”\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD” \"AbCd\".lower() # \"abcd\""
  },
  {
    "objectID": "sesion2_notas.html#errores-de-formato",
    "href": "sesion2_notas.html#errores-de-formato",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de formato",
    "text": "Errores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_83086/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True."
  },
  {
    "objectID": "sesion2_notas.html#transformando-strings",
    "href": "sesion2_notas.html#transformando-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformando strings",
    "text": "Transformando strings\n\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n2\n\n\n\n#s.index(':') # ValueError raised\n\n\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado\n\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n2\n\n\n\ns.find(':') # -1\n\n-1\n\n\n\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string\n\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\nFalse"
  },
  {
    "objectID": "sesion2_notas.html#métodos-para-strings",
    "href": "sesion2_notas.html#métodos-para-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Métodos para strings",
    "text": "Métodos para strings\n\n\nCualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido) Se debe usar .str. (es importante el .str).\nEjemplo:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2_notas.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2_notas.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets\n\n\nEliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis.\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n\n\n\n\nReshapes/re formato\n\nEl “reshape” o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico.\nEsto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos.\nEl “reshape” es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo.\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n\n\n\n\nCrear variables dummies / dicotómicas\n\nMuy útil para representar información cualitativa.\nMuy usando en análisis de regresión y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\nSe pueden generar mediante pd.get_dummies(df[‘key’])\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0"
  },
  {
    "objectID": "sesion2_notas.html#unir-datasets",
    "href": "sesion2_notas.html#unir-datasets",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\nApliar/concatenar/append Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como “inner”, “outer”, “left” y “right”"
  },
  {
    "objectID": "sesion2_notas.html#apilar-dataframes",
    "href": "sesion2_notas.html#apilar-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "1. Apilar dataframes",
    "text": "1. Apilar dataframes\nCreemos los dataframes:\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n\n\nHagamos el apilar. Atención a los index\n\ndf.append(df2)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_83086/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9\n\n\n\n\n\n\n\n\nUsemos ignorar index\n\ndf.append(df2, ignore_index=True)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_83086/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9"
  },
  {
    "objectID": "sesion2_notas.html#unir-dataframes",
    "href": "sesion2_notas.html#unir-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Unir dataframes",
    "text": "2. Unir dataframes\nUtilizando la función pd.merge().\n\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n\nUnión interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUnión externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0"
  },
  {
    "objectID": "sesion2_notas.html#transformaciones-estadístocas",
    "href": "sesion2_notas.html#transformaciones-estadístocas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones estadístocas:",
    "text": "Transformaciones estadístocas:\n\nOutliers\n\nUn valor atípico, también conocido como valor outlier en inglés, es un punto de datos que difiere significativamente del patrón general de los demás datos en un conjunto. Estos valores son inusuales en relación con el resto de la distribución de los datos y pueden ser considerablemente más altos o más bajos que los valores típicos del conjunto.\nLa identificación de valores atípicos es importante por varias razones: 1. Calidad de los datos 2. Impacto en estadísticas y modelos 3. Anomalías y problemas reales 4. Toma de decisiones informada\nVeamos un ejemplo:\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n\nValores atípicos:\n   Valor\n6    200"
  },
  {
    "objectID": "sesion2_notas.html#estandarización-de-datos",
    "href": "sesion2_notas.html#estandarización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Estandarización de Datos",
    "text": "2. Estandarización de Datos\nLa estandarización es un proceso importante en el análisis de datos y el aprendizaje automático. Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno. Esta transformación se logra mediante la fórmula:\nz=x−μσ z = \\frac{x - \\mu}{\\sigma} \nDonde $ x $ es el valor original, $ $ es la media de los valores y $ $ es la desviación estándar.\nLa estandarización es especialmente útil cuando se tienen variables con diferentes escalas y distribuciones, ya que permite compararlas de manera equitativa y mejorar la eficacia de los algoritmos de aprendizaje automático al mitigar el impacto de las diferencias en las magnitudes de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]"
  },
  {
    "objectID": "sesion2_notas.html#normalización-de-datos",
    "href": "sesion2_notas.html#normalización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Normalización de Datos",
    "text": "Normalización de Datos\nLa normalización es un proceso esencial en el análisis de datos y el aprendizaje automático. Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1. La fórmula matemática utilizada para la normalización es:\nxnorm=x−min(x)max(x)−min(x) x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \nDonde $ x $ es el valor original y $x_{} $ es el valor normalizado.\nLa normalización es beneficiosa para igualar la escala de las variables y mejorar el rendimiento de los modelos que son sensibles a la magnitud de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de datos",
    "section": "",
    "text": "Sitio web complementario con material curso Análisis de Datos\n\n\n\nSesión\nFecha\nTema\nSlides\nNotas\n\n\n\n\n0\n17 ago\nIntroducción al Curso\nslides sesión introductoria\nNotas sesión introductoria\n\n\n1\n17 ago\nRespondiendo Preguntas con datos\nslides sesión 1\nNotas sesión 1\n\n\n2\n25 ago\nPreparando los datos\nslides sesión 2\nNotas sesión 2\n\n\n3\n2 sep\nIntroducción al análisis de regresión\n\n\n\n\n4\n9 sep\nProfundizando en análisis de regresión, supuestos y limitaciones\n\n\n\n\n5\n30 sep\nIntroduccion al análisis de series de tiempo\n\n\n\n\n6\n7 oct\nModelando series temporales"
  },
  {
    "objectID": "index.html#calendario-clases",
    "href": "index.html#calendario-clases",
    "title": "Análisis de datos",
    "section": "",
    "text": "Sitio web complementario con material curso Análisis de Datos\n\n\n\nSesión\nFecha\nTema\nSlides\nNotas\n\n\n\n\n0\n17 ago\nIntroducción al Curso\nslides sesión introductoria\nNotas sesión introductoria\n\n\n1\n17 ago\nRespondiendo Preguntas con datos\nslides sesión 1\nNotas sesión 1\n\n\n2\n25 ago\nPreparando los datos\nslides sesión 2\nNotas sesión 2\n\n\n3\n2 sep\nIntroducción al análisis de regresión\n\n\n\n\n4\n9 sep\nProfundizando en análisis de regresión, supuestos y limitaciones\n\n\n\n\n5\n30 sep\nIntroduccion al análisis de series de tiempo\n\n\n\n\n6\n7 oct\nModelando series temporales"
  },
  {
    "objectID": "sesion1_notas.html",
    "href": "sesion1_notas.html",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "",
    "text": "Notas detalladas de la sesión 1, curso análisis de datos, magíster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versión 1"
  },
  {
    "objectID": "sesion1_notas.html#detalles",
    "href": "sesion1_notas.html#detalles",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "",
    "text": "Notas detalladas de la sesión 1, curso análisis de datos, magíster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versión 1"
  },
  {
    "objectID": "sesion1_notas.html#objetivos-de-aprendizaje-de-la-sesión",
    "href": "sesion1_notas.html#objetivos-de-aprendizaje-de-la-sesión",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Objetivos de aprendizaje de la sesión",
    "text": "Objetivos de aprendizaje de la sesión\n\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos, junto a buenas prácticas que promuevan la transparencia y replicabilidad.\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados."
  },
  {
    "objectID": "sesion1_notas.html#contenidos",
    "href": "sesion1_notas.html#contenidos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Contenidos:",
    "text": "Contenidos:\n\nEl proceso de análisis de datos\n\nEl proceso de análisis de datos\n\nUna visión general a las metodologías de análisis que veremos en el curso\nAdquision y almacenmiento de los datos\nPreparación de los datos\n\nPreguntando a los datos\n\nAsbtrayendo la realidad, variables aleatorias y probabilidades.\nPlanteamiento de preguntas.\nPreguntas y respuestas: el rol de las hipótesis.\n\n\n\n\nRespondiendo desde los datos: Pruebas de hipótesis\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\n\n\n\nBuenas prácticas en análisis de datos\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1_notas.html#el-proceso-de-análisis-de-datos-1",
    "href": "sesion1_notas.html#el-proceso-de-análisis-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "El proceso de análisis de datos",
    "text": "El proceso de análisis de datos\nEn el mundo actual, la generación y recopilación de datos se ha vuelto más accesible y significativa que nunca antes. Esta abundancia de información ofrece la oportunidad de extraer conocimientos valiosos que pueden influir en la toma de decisiones y el desarrollo de soluciones eficientes.\nSin embargo, el proceso de transformar estos datos crudos en información útil y significativa requiere una serie de pasos fundamentales que forman parte integral de la disciplina conocida como Ciencia de Datos.\n\nEn esta primera parte, daremos un vistazo general a las metodologías y enfoques clave que exploraremos a lo largo del curso, con énfasis en la importancia de la preparación de los datos.\nEl proceso de análisis de datos se puede dividir en varias etapas interconectadas, cada una con su propio conjunto de desafíos y consideraciones.\n\nBajo esta mirada, tenemos varias fases clave que están interconectadas. En este curso nos enfocaremos en la preparación de los datos y en su análisis mediante modelos de regresión. Esto con el objetivo de responder preguntas desde los datos, que provean información valiosa.\n\nAdquisición de datos:\nEl primer paso en el proceso de análisis de datos implica la adquisición y el almacenamiento de los datos. Esto se refiere a la recolección de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilación de datos de fuentes diversas, como bases de datos, archivos CSV, páginas web o incluso sensores en tiempo real.\nEs crucial comprender cómo recopilar y almacenar estos datos de manera adecuada, garantizando su calidad, integridad y seguridad.\nExisten tantas fuentes de datos, como podríamos imaginar. ALgunas de las más comunes son las siguientes:\n\nEncuestas y Cuestionarios:\n\nDiseño y administración de encuestas para recopilar datos directamente de los participantes.\nPermite obtener información específica y detallada según las preguntas planteadas.\n\nExperimentos Controlados:\n\nDiseño de experimentos para recopilar datos bajo condiciones controladas.\nÚtil para establecer relaciones causales y evaluar efectos de cambios controlados.\n\nObservación y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar información ambiental.\nUtilización de sensores en dispositivos móviles y wearables para recopilar datos de ubicación, salud y actividad.\n\nRecopilación de Datos Existentes:\n\nUtilización de datos ya recopilados y disponibles en bases de datos o fuentes públicas.\nReduce el tiempo y costo de recopilación, pero puede tener limitaciones en términos de calidad y relevancia.\n\nWeb Scraping (Web Scrapping):\n\nExtracción de datos de sitios web utilizando herramientas y técnicas automatizadas.\nPermite recopilar información no estructurada de manera eficiente, pero requiere atención a la ética y términos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracción programática con sistemas y servicios para obtener datos en tiempo real.\nComún en la obtención de datos de redes sociales, información climática, finanzas, entre otros.\n\nColaboración y Participación Comunitaria:\n\nColaboración con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser útil para proyectos de mapeo colaborativo, ciencia ciudadana y recopilación de información local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes volúmenes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilación y posterior análisis de datos heterogéneos.\nUsualmente se accede a través de querys SQL\n\n\n\n\n\n\n\n\nDatos disponibles para el proyecto\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos públicos sobre educación chilena\nDatos públicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad económica\n\nVeamos como acceder algunos de estos datos.\n\n\nEjemplo: Datos públicos sobre individuos en comunas chilenas (encuesta Casen)\nLa Encuesta de Caracterización Socioeconómica Nacional (CASEN) es una investigación realizada en Chile que tiene como objetivo principal recopilar información detallada sobre la situación socioeconómica de los hogares y las personas en el país. Esta encuesta se lleva a cabo de manera periódica y abarca una amplia variedad de temas, como ingresos, educación, empleo, salud, vivienda y otros aspectos relevantes para comprender la realidad socioeconómica de la población chilena. La información recopilada en la Encuesta CASEN se utiliza para informar políticas públicas, tomar decisiones informadas y analizar la evolución de indicadores sociales a lo largo del tiempo.\nSitio Web oficial\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. El formato mas comun es .csv, pero a veces estan en formatos extraños. Por ejemplo, .dta de STATA.\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(5)\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegión de Tarapacá\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegión de Tarapacá\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educación formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegión de Tarapacá\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nBásica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegión de Tarapacá\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nTécnico nivel superior incompleta\nSí\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n4\n1.101100e+11\n3\n30\nRegión de Tarapacá\nIquique\nUrbano\n67\n19\nMujer\n3\n...\nNaN\nNo sabe\nNo\n27083.0\n941583\n27083.0\n941583\n2083.0\n891583.0\n439170.0\n\n\n\n\n5 rows × 22 columns\n\n\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploración, para ser preparados para analizarlos. De esto se tratará la siguiente sesión del curso.\nEjemplo: Datos desde la API del banco mundial Primero siga este ejemplo practico de importar datos, luego será facil responder la pregunta anterior.\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb # para instalar: conda install pandas-datareader  o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. En este caso revisare de PIB (GDP en ingés), pero se pueden explorar muchas más opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n1503\nBG.GSR.NFSV.GD.ZS\nTrade in services (% of GDP)\n\nWorld Development Indicators\nTrade in services is the sum of service export...\nb'International Monetary Fund, Balance of Paym...\nEconomy & Growth ; Private Sector ; Trade\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16626\nUIS.XUNIT.GDPCAP.23.FSGOV\nInitial government funding per secondary stude...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16627\nUIS.XUNIT.GDPCAP.23.FSHH\nInitial household funding per secondary studen...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n16628\nUIS.XUNIT.GDPCAP.3.FSGOV\nInitial government funding per upper secondary...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16629\nUIS.XUNIT.GDPCAP.5T8.FSGOV\nInitial government funding per tertiary studen...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16630\nUIS.XUNIT.GDPCAP.5T8.FSHH\nInitial household funding per tertiary student...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n\n\n540 rows × 7 columns\n\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n\n\n\n\n\nObservar que este es un data frame con dos índices: pais y año. Para mayor referencia coo tratar este tipo de datos ver en https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\nObtengamos un data frame con los datos de Chile, entre 1980 y 2020.\n\n#sabemos que queremos Chile, asi que busquemos su info\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n\n\n\n\n\nSi quisieramos, por simplicidad quedarnos solo con el indice del año y reordenar el dataframe:\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el análisis es para un solo país\nreversed_df.head(5)\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068\n\n\n\n\n\n\n\nAhora, realicemos un grafico rápido con nuestros datos:\n\n# Graficamos\n\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'Año')\n\nText(0.5, 0, 'Año')\n\n\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 1 - Bajando y formateando datos del Banco Mundial**\n\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\n\nPreguntando a los datos\n¿Cómo plantear preguntas y formular hipótesis en el contexto del análisis de datos? El proceso de análisis comienza con la curiosidad y/o necesidad.cLa formulación de preguntas relevantes que se puedan responder mediante la exploración y el examen de los datos disponibles.\nInicia con la identificación de áreas de interés y la formulación de preguntas específicas relacionadas con esos temas. Estas preguntas pueden surgir de la necesidad de resolver un problema, entender un fenómeno o explorar patrones en los datos. Un buen planteamiento de preguntas es crucial, ya que guiará todo el proceso de análisis.\n\n\n\nEl proceso de abstraer la realidad\n\n\nPreguntas e hipótesis:\nUna hipótesis es una afirmación, verificable con evidencia. En este sentido, para toda pregunta podemos responderla mediante hipótesis.\nEn particular, para responder a las preguntas en el contexto de datos, es común formular hipótesis nulas y alternativas.\nLa hipótesis nula es aquella que propone que algun parámetro toma cierto valor. Este generlamente es un punto de verdad. Si bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto. En general, planteamos el problema de tal manera que podamos rechazar la hipótesis nula, en favor de otra que llamamos alternatiba.\nQuizas, la hipotesis nula más famosa es la prueba de “significancia”. En esta se propone que un parámetro (muchas veces un efecto, o correlación) es 0, es decir, plantea que no hay efecto o relación entre las variables, mientras que la hipótesis alternativa sugiere que sí existe una relación o efecto significativo.\nEstas hipótesis son fundamentales para establecer una base objetiva para el análisis y para evaluar las evidencias encontradas en los datos. El proceso de plantear preguntas y formular hipótesis es el primer paso en el análisis de datos, ya que establece una guía clara para el enfoque y la dirección del trabajo. Al identificar preguntas y establecer hipótesis, se crea un marco sólido que orientará la exploración y el análisis de los datos disponibles.\n\n\n\n\n\n\nTaller 1: Pregunta 2 - Investigando sobre países:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1_notas.html#respondiendo-desde-los-datos",
    "href": "sesion1_notas.html#respondiendo-desde-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\n\nInferencia estadística\nInferencia se refiere al proceso de hacer generalizaciones de una población a partir de una muestra de esa población. En particular, la idea es que si tenemos un conjunto de datos (muestra) obtenido de una población más grande, el cual es representativo de esta, podemos utilizar métodos estadísticos para sacar conclusiones sobre las características y propiedades de esa población en su totalidad.\n\n\n\nPoblación y Muestra\n\n\nEl proceso de inferencia estadística se basa en el principio de que una muestra bien seleccionada puede proporcionar información valiosa sobre la población en general. Mediante el análisis de la muestra, podemos estimar parámetros poblacionales, como la media, la proporción o la desviación estándar, y también podemos construir intervalos de confianza para estimar el rango dentro del cual se espera que se encuentren estos parámetros.\nEl uso de la inferencia estadística es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una población en particular. Por ejemplo, en lugar de encuestar a todos los ciudadanos de un país, es mucho más factible encuestar una muestra representativa y utilizar esa información para hacer suposiciones sobre la opinión de la población en general.\n\nEstadígrafos y el Teorema del Límite central\nEntonces, en cada muestra que tenemos podemos calcular aproximaciones a los parámetros poblacionales de interés. Estos son los llamados estadísgrafos \nDado que por cada muestra que tenemos, vamos a calcular un estadígrafo este es en si mismo una variable aleatoria. Tiene su propia distribución, media y varianza!\n\nEl estadígrafo más conocido es el promedio o media muestral.\n\n\n\nEstadigrafos más comunes\n\n\nCada estimador es una función de la muestra, por ende para cada muestra que tengamos obtendremos un valor numérico específico para el estimador. Por este motivo, cuando estamos trabajando con una única muestra específica, tenemos un único valor del estimador, o estimador puntual.\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población, por lo cual un primer camino tentador es usar el estimador puntual para tomar una decisión. Como nunca podemos conocer el verdadero parámetro, tampoco podemos saber a ciencia cierta si el estimador puntual es cercano a este.\n¿Cómo conectamos estadpigrafos y parámetros?\nEl teorema del límite central, nos dice que, bajo ciertas condiciones, la distribución de las medias muestrales de una población se aproxima a una distribución normal a medida que el tamaño de la muestra aumenta, independientemente de la forma de la distribución original de la población. Este teorema es esencial en inferencia estadística y tiene amplias aplicaciones en análisis de datos y toma de decisiones.\n\n\n\nLa media muestral se distribuye normal, sin importar la distribución de la variable subyacente\n\n\nFormalmente, el Teorema del Límite Central establece lo siguiente:\nx‾∼aN(μ,σn)\\bar{x} \\sim_a N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) \nSupongamos que tenemos una población con media μ y desviación estándar σ finitas. Si tomamos muestras aleatorias de tamaño n de esta población y calculamos la media muestral de cada muestra, entonces, a medida que n tiende a infinito, la distribución de estas medias muestrales se aproximará a una distribución normal con media μ y desviación estándar σ/√n.\nEn otras palabras, sin importar la distribución original de la población, cuando el tamaño de la muestra es suficientemente grande, la distribución de las medias muestrales seguirá una forma de campana similar a la distribución normal. Este resultado es fundamental para realizar inferencias sobre la población a partir de muestras, ya que nos permite aplicar métodos basados en la distribución normal incluso cuando la población original no sigue una distribución normal.\n\n\nError estándar\n\nCorresponde a un estimador de la desviación estándar del estimador.\nIdentifica que tan lejos estamos del verdadero valor poblacional.\nPara la media muestral:\n\nSE=Syn SE = \\frac{S_y}{\\sqrt{n}}\nSe utiliza para evaluar a los estimadores, mediante pruebas de hipotesis y construir intervalos de confianza\n\nSi se conoce un estimador y su desviación estándar, podemos saber qué tan precisa es la estimación (mucha o poca varianza), pero no podemos saber si el estimador está cercano o no a su valor verdadero en la población (el cual no conocemos).\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población.\nSí se puede construir un conjunto de valores que contienen el parámetro poblacional con alguna probabilidad (llamada el nivel de confianza).\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n\n\nInferencia sobre Estadígrafos y parámetros - Conectados por el Teorema del Límite central\nx‾∼aN(μ,σn) \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hipótesis\np-valor\n\n\n\n\n\nIntervalos de confianza\nUna primera manera de aproximarnos a los parámetros poblacionales (particularmente a la esperanza) es mediante la construcción de intervalos de confianza.\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n¿De dónde saco los valores críticos?\nLos valores críticos de una distribución los obtenemos de una tabla de distribución o para calcular podemos usar excel, R o en python:\nscipy.stats.t.isf(alpha, n-p)\nSi estamos trabajando con dos colas, usar alpha/2 porque la probabilidad de error la estamos repartiendo a ambas colas.\n\n\n\n\n\n\n[Matemáticamente] Caso 1: Varianza conocida\n\n\n\nSumpongamos que tenemos una muestra aleatoria: y1,y2,…,yny_1, y_2, \\dots, y_n de una población Y∼N(μ,σ2)Y\\sim N(\\mu, \\sigma^2)\n\nLa media muestral y‾=1n∑i=1nyi\\bar{y}= \\frac{1}{n}\\sum_{i=1}{n}y_i\n\nSu esperanza es: E(y‾)=μE(\\bar{y}) =\\mu\nSu varianza es: var(y‾)=σ2nvar(\\bar{y}) = \\frac{\\sigma^2}{n}\nse distribuye normal, tal que podemos estandarrizar: $ N(0,1) $\n\n\nEntonces, podemos describir que:\n$ P( -1.96 &lt; &lt; 1.96 ) = 0.95 $$\n$ P( {y}- &lt; &lt; {y} + ) = 0.95 $\n\neste intervalo es aleatorio, porque y‾\\bar{y} es diferente en cada muestra.\npara el 95% de las muestras elatorias, el intervalo construido de esta manera contendrá a μ\\mu\n\n\n\n\n\n\n\n\n\n[Matemáticamente] Caso 2: Varianza desconocida\n\n\n\n\nSupongamos que tenemos una muestra aleatoria y1,y2,…,yny_1, y_2, \\dots, y_n de una población y∼N(μ,σ2)y\\sim N(\\mu, \\sigma^2 )\nUsamos la estimación de la desviación estándar muestral: Sy=1n−1∑i=1n(yi−y‾)2 S_y = \\sqrt{\\frac{1}{n-1} \\sum{i=1}{n}(y_i - \\bar{y})^2} \nY si estandarizamos y‾\\bar{y}: Y‾−μySn∼tn−1 \\frac{\\bar{Y} - \\mu_y }{ \\frac{S}{\\sqrt{n}}} \\sim t_{n-1} \nPodeos constrir un intervalo de la porbabilidad de estar al 95 con el valor critico c adecuado a los grados de libertad n-1:\n\nP(−c&lt;Y‾−μySn&lt;c)=0.95 P( -c &lt;   \\frac{\\bar{Y} - \\mu_y }{ \\frac{ S}{\\sqrt{n}}}  &lt; c  ) =0.95  \nP(y‾−c×Sn&lt;μ&lt;y‾+c×Sn)0.95 P(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) 0.95\n\nSi llamamos al error estandar SE: SE(y‾)=SnSE(\\bar{y})=\\frac{S}{\\sqrt{n}}\nEl IC es: (y‾−c×Sn,y‾+c×Sn) (\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) \n\n\n\n\n\nEl intervalo es una muestra aleatoria\nEsto quiere decir que para cada muestra podemos construir un intervalo.\nAsí como el estimador es una variable aleatoria, esto también es cierto para los intervalos de confianza. Por eso también se les llama intervalos aleatorios, ya que con diferentes muestras obtendremos un diferente estimador e intervalo.\nPor ende, supongamos que contamos con 20 muestras, entonces construiremos 20 intervalos de confanza diferentes para los 20 estimadores puntuales.\n\nInterpretación de intervalo de confianza\nPensemos en un 95% de confianza (un valor usual). Esto quiere decir, que si se repitiera este ejercicio muchas veces y construyéramos un intervalo de esta forma el 95% de ellos contendría el verdadero parámetro poblacional.\nUn elemento importante a considerar es que esto no significa que con 95% de certeza el parámetro está exactamente en estos valores. Por ejemplo, al 95% de confianza con 20 intervalos 19 contendrán el parámetro.\n\n\n\nPruebas de hipótesis\n\nUna forma de verificar hipotesis sobre los parámetros es mediante el contraste de hipótesis.\n\nEmpezamos suponiendo que hay una distribución conocida para el estadígrafo, centrada en un valor específico. Y nos preguntamos, si esto fuea verdad ¿qué tan probable es la muestra que tengo?\n\nLlamamos la hipótesis a probar Ho, y su alternativa H1.\n\n\n\n\n\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:\n\n\nTipo I: Rechazar Ho cuando es cierta\nTipo II: No rechazar Ho cuando es falsa.\n\n\n\n\nSe elige nivel de significancia de contraste (α) = probabilidad de cometer error Tipo I. Típicamente α = 0,01, 0,05, 0,10.\nPara contrastar una hipótesis con su alternativa, debemos elegir:\n\nUn estadístico de contraste\nUna regla de rechazo, la cual depende de un valor crítico.\n\n\n\n\n\nPrueba de significancia\nDefinimos la prueba de hipótesis de significancia como aquella que indica si un estimador T̂\\hat{T} es 0.\nH0:T=0 vs H1:T≠0 H_0: T =0\\text{ vs }H_1: T \\neq 0 \nEl Valor de probabilidad (ó p-valor) es el nivel probabilidad más alto para el cual no podemos rechazar la hipótesis nula de la prueba de significancia.\nEjemplo: $H_0: $ y en la muestra especifica t= 1.52:\nP−valor=P(T&gt;1.52|h0)=1−ϕ(1.52)=0.0065 P-valor = P(T&gt;1.52 \\vert h_0)= 1- \\phi(1.52)=0.0065\n\nel mayor nivel de significancia estadistica al cual no rechazamos H0H_0 es 6.5%\nla probabilidad de observar un velor T≥1.52T\\geq 1.52 cuando H0H_0 es cierta es en un 6.5 de las muestras.\nP-valores bajos dan evidencia en contra de H0H_0, ya que la probabilidad de observarlo si H0H_0 es cierta es bajo.\n\n\n\nEjemplo de aplicación: Peso de los Pingüinos Palmer\nConsideremos los datos de los pingüinos Palmer. Los datos “Palmer Penguins” son un conjunto que detalla medidas morfológicas y características de tres especies de pingüinos: Adelie, Gentoo y Chinstrap. Recopilados por el Dr. Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nEn el contexto de los pingüinos y el peso de su población, podríamos tomar una muestra de pingüinos y calcular un intervalo de confianza para el peso promedio. Esto nos daría una estimación del peso promedio de la población total, junto con la confianza en que este valor estimado es preciso.\nEs importante tener en cuenta que el proceso de inferencia estadística se basa en suposiciones y en el uso adecuado de técnicas estadísticas.\nLa elección de la muestra, la interpretación de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hipótesis, sobre el peso de los pingüinos.\nPrimero calcularemos el promedio muestral y lo veremos en el contexto de los datos observados:\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los pingüinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribución del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribución de Peso de Pingüinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral. De que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus características.\nPor ejemplo, consideremos que de esta población de pingüinos obtenemos 1000 muestras de 50 individuos cada una. Si graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tamaño de muestra, más nos alejamos de la distribución normal.\nSi reducimos el número de repeticiones tambieé.\n\n\nimport numpy as np\n\n# Definir el tamaño de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y cálculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gráfico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribución de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\n\nIntervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n# Obtener una muestra simple de 40 pingüinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error estándar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviación estándar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)\n\nIntervalo de Confianza para el Peso:\n(nan, nan)\n\n\nEl resultado será un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los pingüinos en la población, con un nivel de confianza del 95%.\n¿Como nos fue? ¿Contiene al verdadero valor?\n\n\n\nComparaciones de grupos\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribución de tamaño por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\ntabla_doble_entrada\n\n\n\n\n\n\n\n\nspecies\nsex\nPromedio\nVarianza\n\n\n\n\n0\nAdelie\nFemale\n3368.835616\n72565.639269\n\n\n1\nAdelie\nMale\n4043.493151\n120278.253425\n\n\n2\nChinstrap\nFemale\n3527.205882\n81415.441176\n\n\n3\nChinstrap\nMale\n3938.970588\n131143.605169\n\n\n4\nGentoo\nFemale\n4679.741379\n79286.335451\n\n\n5\nGentoo\nMale\n5484.836066\n98068.306011\n\n\n\n\n\n\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hipótesis:\n¿Existe una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”?\n\nHipótesis Nula (H0):\n\nNo hay diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\n\nHipótesis Alternativa (H1):\n\nExiste una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\nPara probar esta hipótesis, podrías utilizar una prueba de hipótesis para comparar las medias de las muestras de peso de los pingüinos machos y hembras en la especie “Adelie”. Esto te permitiría determinar si la diferencia observada en el peso promedio es lo suficientemente grande como para considerarse estadísticamente significativa.\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribución de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribución de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nA simple vista podriamos pensar ambos grupos son diferentes. Es más claro si dibujamos el promedio muestral observado.\n\n# Crear un gráfico de densidad con líneas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()\n\n\n\n\nSi construimos una prueba t de diferencia de medias:\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estadística t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gráfico de comparación de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Machos y Hembras de Pingüinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\nEstadística t: 13.126285923485874\nValor p: 6.402319748031793e-26\n\n\n\n\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas. Para esto podriamos usar una prueba ANOVA.\nEn este código, primero cargamos el conjunto de datos “Penguins” y luego creamos dos subconjuntos separados para machos y hembras. Después, utilizamos la función stats.f_oneway() para realizar una prueba ANOVA para comparar los pesos entre hembras y machos. El resultado incluye la estadística F y el valor p.\nEl valor p nos indica si hay una diferencia significativa entre los grupos. Si el valor p es menor que un umbral de significancia (por ejemplo, 0.05), podríamos rechazar la hipótesis nula y concluir que hay una diferencia significativa en el peso entre hembras y machos de diferentes islas.\nRecuerda que, antes de realizar una prueba ANOVA, es importante verificar las suposiciones necesarias, como la normalidad y la homogeneidad de varianzas en los grupos. Si estas suposiciones no se cumplen, podría ser necesario considerar otras pruebas estadísticas o transformaciones de los datos.\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estadística F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\nEstadística F: 72.96098633250911\nValor p: 4.897246751596325e-16\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Calcular las medias de peso por género e isla\nmedias_peso = penguins.groupby(['species', 'island', 'sex'])['body_mass_g'].mean().reset_index()\n\n# Crear un gráfico de barras con puntos y intervalos de confianza\nplt.figure(figsize=(10, 6))\nsns.barplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', errorbar='sd', palette=['pink', 'blue'])\n#sns.boxplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Hembras y Machos por Isla')\nplt.xlabel('Especie e Isla')\nplt.ylabel('Media de Peso (g)')\nplt.legend(title='Sexo')\nplt.show()\n\n\n\n\n\n\nExperimentos Aleatorios y pruebas A/B\nUn experimento estadístico es un enfoque científico que busca establecer relaciones de causalidad y obtener conclusiones sobre cómo ciertas variables afectan a otras. Los experimentos estadísticos se diseñan para manipular deliberadamente una o más variables independientes y observar los efectos que tienen sobre una variable dependiente. Al controlar y manipular las variables de interés, los experimentos permiten a los investigadores hacer afirmaciones más sólidas sobre las relaciones causales.\nUna prueba A/B, también conocida como prueba de división, es una técnica utilizada en la investigación y el análisis para comparar dos variantes o grupos con el fin de determinar cuál de ellos produce un mejor resultado en términos de rendimiento, efectividad o preferencia. En una prueba A/B, se selecciona un grupo de muestra y se divide en dos grupos, uno que experimenta la variante “A” (por ejemplo, una versión actual) y otro que experimenta la variante “B” (por ejemplo, una versión modificada). Luego, se recopilan datos y se comparan los resultados de ambos grupos para determinar cuál variante es más efectiva. Las pruebas A/B son comunes en marketing, diseño de productos y desarrollo web para tomar decisiones informadas sobre mejoras y optimizaciones.\nLas pruebas A/B es son ampliamente utilizado en diversas áreas, como el marketing, la investigación de usuarios y el diseño de productos. En una prueba A/B, se seleccionan dos grupos de muestra: uno experimenta la versión original (A) y el otro experimenta una variante modificada (B). La idea detrás de una prueba A/B es evaluar si la variante B produce un efecto significativamente diferente en una métrica de interés en comparación con la variante A.\nMediante la asignación aleatoria de los participantes a los grupos A y B, y al controlar las condiciones en las que se les presenta cada variante, se reduce la posibilidad de sesgos y se permite un análisis causal más confiable. Al comparar las diferencias observadas en los resultados entre los grupos A y B, es posible inferir si la variante B tiene un impacto significativo en la variable de interés.\nSin embargo, es importante tener en cuenta que aunque las pruebas A/B proporcionan evidencia de asociación causal, no garantizan que la causalidad sea absoluta. Otros factores no controlados pueden influir en los resultados. Para obtener una comprensión más completa de la causalidad, los experimentos controlados aleatorizados y el uso de métodos de diseño experimental sólidos son esenciales. Las pruebas A/B son una herramienta poderosa para explorar causas y efectos en condiciones controladas y analizar el rendimiento relativo de diferentes opciones.\nVeamos un ejemplo en la práctica. Este es parte del ejercicio de aplicación."
  },
  {
    "objectID": "sesion1_notas.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "href": "sesion1_notas.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Caso: Aplicación de A/B testing para promoción de Marketing",
    "text": "Caso: Aplicación de A/B testing para promoción de Marketing\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreación de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 3 -Ejemplo AB test en Marketing:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA.\n\n\n\n\n\nBuenas prácticas en análisis de datos\n\nImportancia de la Adquisición y Almacenamiento de Datos\nLa adquisición y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de análisis. La calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes. En esta sección, exploraremos la importancia de esta etapa y cómo afecta todo el flujo de trabajo de la ciencia de datos.\nGarantía de Calidad y Fiabilidad en la Obtención de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean sólidas. La calidad de los datos está relacionada con la precisión, integridad y consistencia de la información que recopilamos. Asegurarnos de que los datos sean precisos desde el principio minimiza la posibilidad de errores en análisis posteriores. Exploraremos técnicas y prácticas para verificar la calidad de los datos y cómo mitigar posibles fuentes de error.\nExploración de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros. Cada fuente tiene sus propias características y potenciales sesgos. Comprender las diferencias entre estas fuentes y cómo pueden influir en los resultados es crucial para tomar decisiones informadas. Analizaremos ejemplos de cómo la elección de la fuente de datos puede afectar las conclusiones y cómo evaluar la confiabilidad de las fuentes.\n\n\nMetodologías de Levantamiento y Adquisición de Datos:\nEl proceso de obtención de datos implica una planificación cuidadosa. Exploraremos diversas metodologías utilizadas para recopilar datos, desde encuestas y experimentos hasta scraping de datos en línea. Cada metodología tiene sus propias ventajas y desventajas, y es importante seleccionar la más adecuada para los objetivos del análisis. Discutiremos cómo diseñar encuestas efectivas, cómo considerar la ética en la recopilación de datos y cómo aprovechar las fuentes de datos existentes.\nEsta sección nos proporcionará una base sólida para comprender cómo adquirir y almacenar datos de manera efectiva y confiable. Una vez que comprendamos cómo obtener datos de calidad, podremos avanzar con confianza en las etapas posteriores del proceso de análisis, sabiendo que estamos trabajando con una base sólida y confiable.\n\n\nDesafíos y Consideraciones:\nA medida que ingresamos al emocionante mundo del análisis de datos, nos encontramos con una serie de desafíos y consideraciones que debemos abordar de manera efectiva para garantizar el éxito de nuestro proyecto. Estos desafíos abarcan desde la protección de la privacidad de los datos hasta las complejidades de la limpieza y transformación durante la etapa de preparación.\n\n\nPrivacidad y Seguridad de los Datos:\nUno de los aspectos más críticos en el análisis de datos es la privacidad y seguridad de la información. Los datos pueden contener información sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas. Exploraremos prácticas y regulaciones para garantizar que los datos se manejen de manera ética y legal. Discutiremos cómo anonimizar los datos, utilizar técnicas de enmascaramiento y seguir las mejores prácticas para resguardar la privacidad de los individuos.\n\n\nLimpieza y Transformación durante la Preparación de Datos:\nLa etapa de preparación de datos es crucial para asegurarse de que los datos sean aptos para el análisis. Sin embargo, este proceso no está exento de desafíos. Los datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada. Exploraremos técnicas para identificar y manejar valores atípicos y faltantes, así como la importancia de la normalización y estandarización de los datos. Aprenderemos cómo transformar los datos en un formato adecuado para el análisis, incluida la reorganización de variables y la creación de nuevas características. En resumen, enfrentamos una serie de desafíos y consideraciones clave en nuestro viaje hacia el análisis de datos significativo. Desde la protección de la privacidad hasta la preparación efectiva de los datos, abordar estos desafíos de manera adecuada es esencial para garantizar que nuestras conclusiones sean sólidas, confiables y éticas.\n\n\nReproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\nLa reproducibilidad y el control de versiones son componentes fundamentales para garantizar la integridad y la transparencia en el análisis de datos. Además de mantener un registro detallado de las decisiones tomadas durante el proceso, el uso de sistemas de control de versiones como GIT se vuelve esencial para mantener la trazabilidad y la colaboración efectiva en proyectos de preparación y análisis de datos.\nDocumentación Detallada del Análisis y Uso de Notebooks: Una documentación exhaustiva del análisis es esencial para comprender el flujo de trabajo, las decisiones tomadas y las transformaciones aplicadas a los datos. Los notebooks, como Jupyter Notebooks, ofrecen una herramienta excepcional para lograr esto. En cada celda de un notebook, es posible combinar explicaciones en lenguaje natural con código ejecutable y visualizaciones. Esto permite registrar no solo el qué y el cómo, sino también el porqué detrás de cada paso.\nImportancia de Mantener un Registro de los Cambios en los Datos: Cada decisión tomada durante la preparación y el análisis de datos puede tener un impacto significativo en los resultados finales. Mantener un registro detallado de estas decisiones, desde la limpieza de datos hasta la creación de variables derivadas, es crucial para comprender cómo se obtuvieron ciertos resultados. Una documentación precisa y detallada permite a otros analistas validar y replicar el análisis en el futuro.\nUso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que también es una herramienta poderosa en el análisis de datos. Permite rastrear cada modificación realizada en el código y en los documentos, incluidos los notebooks. Cada cambio es registrado como un “commit”, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\n\n\n\nUn esquema de git por Allison Horst @allison_horst\n\n\nAplicación de Control de Versiones en Proyectos de Preparación de Datos:* La aplicación de GIT en proyectos de preparación de datos agrega un nivel adicional de transparencia y colaboración. Los repositorios de GIT almacenan no solo los datos originales, sino también los notebooks y scripts utilizados en el proceso. Esto permite a los analistas colaborar en un entorno controlado y mantener un historial de cambios. En caso de que surjan problemas o se necesite retroceder en el tiempo, GIT ofrece la capacidad de volver a versiones anteriores de manera segura.\nLa combinación de documentación detallada a través de notebooks y el uso de sistemas de control de versiones como GIT proporciona una base sólida para el análisis de datos reproducible y transparente. Esto no solo facilita la comprensión y validación de los resultados, sino que también fomenta la colaboración y la mejora continua en proyectos de preparación y análisis de datos.\n\n\n\n\n\n\nActividad de proyecto - Inicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del análisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustaría trabajar.\nPropongan una o dos preguntas de investigación y las hipotesis que las responderían.\n\nLa siguiente sesión, vamos a explorar los datos y empezar los primeros pasos en su análisis."
  },
  {
    "objectID": "sesion0_notas.html",
    "href": "sesion0_notas.html",
    "title": "Presentación del curso",
    "section": "",
    "text": "Hola! Soy Melanie y seré la docente de este curso, en el que aprenderás los fundamentos del análisis de datos, tanto desde una perspectiva teórica como práctica (en Python).\nMe pueden contactar al mail melanie.oyarzun@udd.cl\n\n\n\n\n\n\n\nRevisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0_notas.html#en-la-sesión-de-hoy",
    "href": "sesion0_notas.html#en-la-sesión-de-hoy",
    "title": "Presentación del curso",
    "section": "",
    "text": "Revisaremos los objetivos del curso, metodología, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hipótesis."
  },
  {
    "objectID": "sesion0_notas.html#contexto-en-el-programa-de-magister",
    "href": "sesion0_notas.html#contexto-en-el-programa-de-magister",
    "title": "Presentación del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la línea de desarrollo de data science.\nEsta asignatura tributa, a través de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Magíster en Data Science:\n\nAplicar teorías, algoritmos, métodos, técnicas y herramientas básicas y avanzadas de Data Science para analizar, resolver y hacer una evaluación crítica de desafíos complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones.\nComunica efectivamente y argumenta sobre los resultados de su trabajo a públicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento ético, cumpliendo los protocolos y normas que guían su desempeño, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicación de estrategias para utilizar nuevo conocimiento en data science en su ámbito de desempeño."
  },
  {
    "objectID": "sesion0_notas.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "href": "sesion0_notas.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "title": "Presentación del curso",
    "section": "Objetivos de la asignatura (resultados de aprendizaje)",
    "text": "Objetivos de la asignatura (resultados de aprendizaje)\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el análisis de datos, utilizando lenguaje técnico afín.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso.\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer información y generar conclusiones basadas en evidencia.\nIdentificar las buenas prácticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0_notas.html#resumen",
    "href": "sesion0_notas.html#resumen",
    "title": "Presentación del curso",
    "section": "Resumen:",
    "text": "Resumen:"
  },
  {
    "objectID": "sesion0_notas.html#detallado",
    "href": "sesion0_notas.html#detallado",
    "title": "Presentación del curso",
    "section": "Detallado",
    "text": "Detallado\n\n\n\n\n\n\nSesión 1: Respondiendo Preguntas con datos\n\n\n\n\n\nFecha: 19 agosto\nObjetivos:\n\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados.\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos.\n\nContenidos:\n\nEl proceso de análisis de datos\n\nPlanteamiento de preguntas\nAdquision y almacenmiento de los datos\nPreparación de los datos\nUna visión general a las metodologías de análisis que veremos en el curso\n\nFormulación de Preguntas y Hipótesis:\n\nImportancia de definir preguntas claras y específicas.\nDiferenciación entre preguntas exploratorias y confirmatorias.\nCreación de hipótesis nulas y alternativas.\n\nHipótesis y Variables:\n\nIdentificación de variables independientes y dependientes.\nRelación entre hipótesis y variables a analizar.\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\nImportancia de la Adquisición y Almacenamiento de Datos:\n\nGarantía de calidad y fiabilidad en la obtención de datos.\nExploración de diferentes fuentes de datos y su impacto en los resultados.\nMetodologias de levantamiento y adquision\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\n\nBibliografia recomendada:\nActividades:\n\nTaller 1 (incluidas en slides 1)\nProyecto clase 1: Conformación de grupos, definición de temas, primeras hipótesis y datos.\n\n\n\n\n\n\n\n\n\n\nSesión 2:Preparando los datos\n\n\n\n\n\nFecha: 26 agosto\nObjetivos:\n\nComprender la importancia del proceso de preparación de datos para el análisis, reconociendo principios y enfoques clave junto con sus ventajas y desventajas.\nDesarrollar habilidades prácticas en la preparación de datos, identificando y abordando problemas comunes como valores faltantes, valores atípicos y formatos inconsistentes, así como enfoques de trabajo eficientes.\n\nContenidos:\n\nEl proceso de preparación de los datos\n\nSignificado y relevancia de la preparación de datos.\nEjemplos reales de cómo la falta de preparación puede afectar los resultados.\n\nPrincipios y enfoques\n\nExtract, Transform, Load (ETL): Proceso fundamental en la preparación de datos.\nData Wrangling: Técnicas para dar forma y estructura a los datos.\nDatos Tidy: Organización y reestructuración para un análisis eficaz.\n\nBuenas Prácticas en la Preparación de Datos\n\nDocumentación y Consistencia\n\nImportancia de la documentación detallada.\nMantener nomenclatura y convenciones consistentes.\n\nValidación y Verificación\n\nValidación cruzada y verificación de integridad.\nCumplir con reglas y restricciones esperadas.\n\nReproducibilidad y Versionado\n\nEntorno de trabajo reproducible (Jupyter Notebooks, R Markdown).\nUtilización de sistemas de control de versiones (GIT).\n\nComunicación y Validación Colaborativa\n\nComunicación clara de pasos y resultados.\nValidación intermedia con colaboradores para feedback.\n\nSeguridad y privacidad de los datos\n\nProblemas comunes presentes en datos\n\nValores faltantes: \n\nEstrategias para manejar valores faltantes.\nDecidir entre imputación, eliminación o conservación.\n\nValores atípicos\nNormalización y estandarización\nErrores de registro Bibliografia recomendada:\n\n\n\n“Practical Statistics for Data Scientists” (Capítulo 2).\n“Doing Data Science” (Capítulo 1).\n\nActividades de aplicación práctica:\n\nTaller 1: Limpieza y análisis descriptivo de datos en la practica con datos de educación (repasa elementos del curso anterior) (sesión 1)\nProyecto:\n\nInicie el proyecto, cree un documento notebook en el cual van a alojarsu proyecto\nExplorar los datos\nDiagnosticar problemas.\nLa hipótesis que pensamos, ¿tienen variables que pueda concretizarlas? ¿Qué variables usar?\n\n\n\n\n\n\n\n\n\n\n\nSesión 3: Introduccion al análisis de regresión\n\n\n\n\n\nFecha: 2 septiembre\nObjetivos:\n\nComprender los conceptos fundamentales del análisis de regresión lineal y su aplicación en la resolución de problemas.\nDesarrollar la habilidad de plantear modelos e interpretar los resultados obtenidos del análisis de regresión, para aplicarlos en la toma de decisiones.\n\nContenidos\n\nIntroducción al Análisis de Regresión:\n\nDefinición y concepto de regresión.\nUso y aplicabilidad en la toma de decisiones.\n\nRegresión Lineal Múltiple:\n\nExtensión del modelo de regresión a múltiples variables predictoras.\nEcuación de regresión lineal múltiple.\n\nInterpretación de Coeficientes:\n\nSignificado e interpretación de los coeficientes de regresión.\nInfluencia de las variables predictoras en la variable de respuesta.\n\nEvaluación de Modelos de Regresión:\n\nUso de medidas como el coeficiente de determinación (R²) y el error estándar de estimación.\nInterpretación de los resultados de evaluación.\n\nIncorporación de Variables Categóricas:\n\nTransformación de variables categóricas en variables numéricas.\nInterpretación de coeficientes para variables categóricas.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nPlantear modelos de regresión que implementen las hipótesis del proyecto\nEstimar e interpretar modelos\n\n\n\n\n\n\n\n\n\n\nSesión 4: Profundizando en Análisis de Regresión, Supuestos y Limitaciones\n\n\n\n\n\nFecha: 9 septiembre\nObjetivos:\n\nExplorar los supuestos y limitaciones asociados al análisis de regresión y desarrollar estrategias para manejar problemas comunes.\nAplicar estrategias prácticas para identificar y abordar problemas en el análisis de regresión.\n\nContenidos\n\nSupuestos en el Análisis de Regresión:\n\nIdentificación de supuestos clave: linealidad, independencia, homoscedasticidad y normalidad.\nSignificado de cada supuesto y su importancia en la interpretación de resultados.\n\nIdentificación de Problemas en la Regresión:\n\nIdentificación y manejo de outliers en los datos.\nReconocimiento de la heterocedasticidad y sus implicaciones.\nDetección de la no-normalidad de los residuos.\n\nEstrategias para Manejar Problemas:\n\nTransformación de variables para abordar problemas de linealidad.\nMétodos para reducir la influencia de outliers.\nUso de transformaciones para tratar la heterocedasticidad.\nPruebas y técnicas para verificar y mejorar la normalidad.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 2:\nProyecto:\nRevisar supuestos de modelo de regresión\nDiscutir problemas de identificación y limitaciones\n\n\n\n\n\n\n\n\n\n\nSesión 5: Introduccion al análisis de series de tiempo\n\n\n\n\n\nFecha: 30 septiembre\nObjetivos:\n\nIdentificar las características y particularidades de los datos de series de tiempo, comprendiendo sus aplicaciones profesionales.\nRealizar un análisis exploratorio de una serie de tiempo, identificando características clave para su modelamiento.\n\nContenidos\n\nConceptos Básicos de Series de Tiempo:\n\nDefinición y características de una serie de tiempo.\nEjemplos de aplicaciones en distintos campos profesionales.\n\nParticularidades de los Datos Temporales:\n\nDependencia temporal y autocorrelación.\nTendencias, estacionalidad y ciclos.\n\nAplicaciones Profesionales:\n\nCasos de estudio en finanzas, economía, medicina y otros campos.\nCómo el análisis de series de tiempo puede brindar insights valiosos.\n\nBúsqueda y Reorganización de Datos Temporales:\n\nFuentes de datos para series de tiempo (bases de datos, APIs, archivos).\nImportancia de la temporalidad y el orden en los datos.\n\nVisualización y Exploración Inicial:\n\nGráficos de línea y dispersión para identificar tendencias y patrones.\nEstudio de estacionalidad y ciclos mediante gráficos.\n\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 6: Modelando series temporales\n\n\n\n\n\nFecha: 7 occtubre\nObjetivos:\n\nComprender los conceptos y aplicaciones de los modelos ARIMA y VAR en el análisis de series temporales.\nEvaluar las ventajas y desventajas de los métodos estadísticos para el análisis de series de tiempo y seleccionar la técnica más adecuada.\n\nContenidos\n\nModelos ARIMA:\n\nDefinición y componentes de los modelos ARIMA.\nIdentificación, Estimación y Validación de un modelo ARIMA.\nUso de correlogramas y gráficos ACF/PACF para la identificación.\n\nModelos VAR (Vector Autoregressive):\n\nIntroducción a los modelos VAR y su aplicación.\nUso de matrices de coeficientes para representar relaciones entre variables.\n\nVentajas y Desventajas de los Métodos Estadísticos:\n\nUso de modelos estadísticos en comparación con otros enfoques.\nLimitaciones y supuestos asociados a los modelos ARIMA y VAR.\n\nSelección del Método Adecuado:\n\nCriterios para elegir entre modelos ARIMA y VAR.\nConsideraciones al evaluar las alternativas disponibles.\n\nOtros modelos\n\n\nGARCH\nSARIMA y SARIMAX\nAlisado exponencial\nCambio estructural\n\nBibliografía recomendada\nActividades de aplicación práctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesión 7: Principales lecciones para el análisis de datos y presentaciones de proyectos\n\n\n\n\n\nFecha: TBA\nObjetivos:\n\nCerrar el curso, poniendo en contexto las principales herramientas de análisis de datos.\nPresentar un proyecto de análisis de datos\nRecibir feedback y propuestas de mejoras, tanto del trabajo propio como el de sus compañeros.\n\nContenidos\nBibliografía recomendada\nActividades de aplicación práctica\n\nProyecto: Presentaciones finales"
  },
  {
    "objectID": "sesion3_notes.html",
    "href": "sesion3_notes.html",
    "title": "Sesión 3: Introducción al Análisis de Regresión",
    "section": "",
    "text": "El análisis de regersión es una técnica en la cual buscamos encontrar una función que pueda describir la relación observada en los datos entre dos o mas variables.\nPensemos en el caso más simple, una regresión lineal simple o univariada. Tenemos una variable que deseamos explicar o predecir (Y) como función de otra (X).\nPara esto, buscamos la pendiente e intercepto de una funciónla recta de la forma:\nY=α+βXY = \\alpha + \\beta X\nque se ajuste mejor al conjunto de datos con los que se cuenta.\n\nPara esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistemática o que se puede explicar directamente con una o más variables independientes (Xs o regresores) y otra que es no sistemática o error (μ\\mu o epsilonepsilon) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fenómeno.\n\nLa parte sistemática entonces la describimos con una forma funcional, que depende de otras variables o regresores.\nEsta forma funcional puede ser lineal univariada, lineal múltiple o no lineal. El tipo de forma funcional, definirá el tipo de regresión de la que estemos hablando.\nVentajas del análisis de regersión: es facil decsribir cuantitaivamente una rlación.\nEsquemáticamente, los elementos son:"
  },
  {
    "objectID": "sesion1_slides.html#contenidos",
    "href": "sesion1_slides.html#contenidos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Contenidos:",
    "text": "Contenidos:\n\n 1. El proceso de análisis de datos \n \n     Visión general\n      \n        Las metodologías de análisis que veremos en el curso\n        Adquisición y almacenamiento de los datos\n        Preparación de los datos\n      \n    \n     Preguntando a los datos\n      \n        Abstrayendo la realidad.\n        Planteamiento de preguntas.\n        El rol de las hipótesis.\n      \n    \n  \n\n\n\n   2. Respondiendo desde los datos: Pruebas de hipótesis  \n  \n    Conceptos Básicos de Pruebas de Hipótesis:\n      \n        Definición de hipótesis nula y alternativa.\n        Intervalos de confianza.\n        Niveles de significancia y p-values.\n        Errores tipo I y tipo II.\n      \n    \n    Tipos de Pruebas de Hipótesis:\n      \n        Pruebas t para comparación de medias.\n        Pruebas chi-cuadrado para variables categóricas.\n        Pruebas ANOVA para comparación de múltiples grupos.\n      \n    \n    Interpretación de Resultados:\n  \n\n\n\n\n   3. Buenas prácticas en análisis de datos \n  \n     Desafíos y Consideraciones:\n      \n        Privacidad y seguridad de los datos.\n        Limpieza y transformación durante la preparación de datos.\n      \n    \n     Reproducibilidad y Control de Versiones (GIT):\n      \n        Importancia de mantener un registro de los cambios en los datos.\n        Uso de sistemas de control de versiones como GIT para rastrear cambios.\n        Aplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-1",
    "href": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "El proceso de la ciencia de datos",
    "text": "El proceso de la ciencia de datos"
  },
  {
    "objectID": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-2",
    "href": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "El proceso de la ciencia de datos",
    "text": "El proceso de la ciencia de datos\nEn este curso nos enfocaremos en:\n\n\n\nLa preparación de los datos\nAnálisis mediante modelos de regresión"
  },
  {
    "objectID": "sesion1_slides.html#el-objetivo-dar-valor",
    "href": "sesion1_slides.html#el-objetivo-dar-valor",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "El objetivo: dar valor",
    "text": "El objetivo: dar valor\n\nEsto con el objetivo de responder preguntas desde los datos, que provean información valiosa."
  },
  {
    "objectID": "sesion1_slides.html#adquisición-de-datos",
    "href": "sesion1_slides.html#adquisición-de-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Adquisición de datos:",
    "text": "Adquisición de datos:\n\nEl primer paso en el proceso de análisis de datos implica la adquisición y el almacenamiento de los datos.\nEsto se refiere a la recolección de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilación de datos de fuentes diversas, como bases de datos, archivos CSV, páginas web o incluso sensores en tiempo real."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-i",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-i",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes I",
    "text": "Fuentes de datos comunes I\nExisten tantas fuentes de datos, como podríamos imaginar…\n\nEncuestas y Cuestionarios:\n\nDiseño y administración de encuestas para recopilar datos directamente de los participantes.\nPermite obtener información específica y detallada según las preguntas planteadas.\n\nExperimentos Controlados:\n\nDiseño de experimentos para recopilar datos bajo condiciones controladas.\nÚtil para establecer relaciones causales y evaluar efectos de cambios controlados."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-ii",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-ii",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes II",
    "text": "Fuentes de datos comunes II\n\nObservación y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar información.\nUtilización de sensores en dispositivos móviles y wearables para recopilar datos de ubicación, salud y actividad.\n\nRecopilación de Datos Existentes:\n\nUtilización de datos ya recopilados y disponibles en bases de datos o fuentes públicas.\nReduce el tiempo y costo de recopilación, pero puede tener limitaciones en términos de calidad y relevancia."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-iii",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-iii",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes III",
    "text": "Fuentes de datos comunes III\n\nWeb Scraping (Web Scrapping):\n\nExtracción de datos de sitios web utilizando herramientas y técnicas automatizadas.\nPermite recopilar información no estructurada de manera eficiente, pero requiere atención a la ética y términos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracción programática con sistemas y servicios para obtener datos en tiempo real.\nComún en la obtención de datos de redes sociales, información climática, finanzas, entre otros."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-iv",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-iv",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes IV",
    "text": "Fuentes de datos comunes IV\n\nColaboración y Participación Comunitaria:\n\nColaboración con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser útil para proyectos de mapeo colaborativo, ciencia ciudadana y recopilación de información local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes volúmenes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilación y posterior análisis de datos heterogéneos.\nUsualmente se accede a través de querys SQL"
  },
  {
    "objectID": "sesion1_slides.html#proyecto",
    "href": "sesion1_slides.html#proyecto",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Proyecto",
    "text": "Proyecto\n\n\n\n\n\n\nDatos disponibles\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos públicos sobre educación chilena\nDatos públicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad económica\n\nVeamos como acceder algunos de estos datos."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-encuesta-casen",
    "href": "sesion1_slides.html#ejemplo-encuesta-casen",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Encuesta Casen",
    "text": "Ejemplo: Encuesta Casen\nDatos públicos\nLa Encuesta de Caracterización Socioeconómica Nacional (CASEN), se realuza en chile:\n\nObjetivo: recopilar información detallada sobre la situación socioeconómica de los hogares y las personas en el país.\n\nEsta encuesta se lleva a cabo de manera periódica y abarca una amplia variedad de temas, como ingresos, educación, empleo, salud, vivienda y otros aspectos.\nSe utiliza para informar políticas públicas, tomar decisiones informadas y analizar la evolución de indicadores sociales a lo largo del tiempo.\n\nSitio Web oficial"
  },
  {
    "objectID": "sesion1_slides.html#encuesta-casen",
    "href": "sesion1_slides.html#encuesta-casen",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Encuesta Casen",
    "text": "Encuesta Casen\nEjemplo\n\n\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. . . .\n\n\ncodeOutput\n\n\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(4) # veamos las 4 primeros registros\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegión de Tarapacá\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegión de Tarapacá\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educación formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegión de Tarapacá\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nBásica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegión de Tarapacá\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nTécnico nivel superior incompleta\nSí\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n\n\n4 rows × 22 columns"
  },
  {
    "objectID": "sesion1_slides.html#encuesta-casen-1",
    "href": "sesion1_slides.html#encuesta-casen-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Encuesta Casen",
    "text": "Encuesta Casen\nEjemplo\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploración, para ser preparados para analizarlos.\nDe esto se tratará la siguiente sesión del curso."
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\nOtra opción es que los datos estén en una API:\n\nCodeOutput\n\n\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb \n# para instalar: conda install pandas-datareader  \n# o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. \n# En este caso revisare de PIB (GDP en ingés), \n# pero se pueden explorar muchas más opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-1",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\n\ncoderesults\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-2",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\n\ncoderesults\n\n\n\n#sabemos que queremos Chile, asi que busquemos su info\n\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n\n\n\n\nObservar que este es un data frame con dos índices: pais y año.\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-3",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del banco mundial",
    "text": "Datos desde la API del banco mundial\nEjemplo\nData frame con los datos de Chile, entre 1980 y 2020.\n\ncoderesults\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n2015\n13569.948127\n\n\n2014\n13421.538342\n\n\n2013\n13318.595215"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-4",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\nSi quisieramos, por simplicidad quedarnos solo con el indice del año y reordenar el dataframe:\n\ncoderesults\n\n\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el análisis es para un solo país\nreversed_df.head(5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-5",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del banco mundial",
    "text": "Datos desde la API del banco mundial\nEjemplo\nAhora, realicemos un grafico rápido con nuestros datos:\n\ncodeplot\n\n\n\n# Graficamos\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'Año')\n\n\n\n\n\n\n\n\nText(0.5, 0, 'Año')"
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicación-1",
    "href": "sesion1_slides.html#taller-de-aplicación-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicación 1",
    "text": "Taller de aplicación 1\n\n\n\n\n\n\nPregunta 1 - Bajando y formateando datos del Banco Mundial\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?"
  },
  {
    "objectID": "sesion1_slides.html#preguntando-a-los-datos",
    "href": "sesion1_slides.html#preguntando-a-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntando a los datos",
    "text": "Preguntando a los datos\n¿Cómo plantear preguntas y formular hipótesis en el contexto del análisis de datos?\n\nLa formulación de preguntas relevantes que se puedan responder mediante la exploración y el examen de los datos disponibles.\nPueden surgir de la necesidad de resolver un problema, entender un fenómeno o explorar patrones en los datos.\nUn buen planteamiento de preguntas es crucial, ya que guiará todo el proceso de análisis."
  },
  {
    "objectID": "sesion1_slides.html#abstrayendo-la-realidad",
    "href": "sesion1_slides.html#abstrayendo-la-realidad",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Abstrayendo la realidad",
    "text": "Abstrayendo la realidad\n\nEl proceso de abstraer la realidad"
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis",
    "href": "sesion1_slides.html#preguntas-e-hipótesis",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\n\nUna hipótesis es una afirmación, verificable con evidencia.\nEn este sentido, para toda pregunta podemos responderla mediante hipótesis.\nPara responder a las preguntas en el contexto de datos, es común formular hipótesis nulas y alternativas."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis-1",
    "href": "sesion1_slides.html#preguntas-e-hipótesis-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\n\nLa hipótesis nula es aquella que propone que algun parámetro toma cierto valor.\nEste generlamente es un punto de verdad.\nSi bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto.\nEn general, planteamos el problema de tal manera que podamos rechazar la hipótesis nula, en favor de otra que llamamos alternatiba."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis-2",
    "href": "sesion1_slides.html#preguntas-e-hipótesis-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\nPrueba de significancia\n\nQuizas, la hipotesis nula más famosa es la prueba de “significancia”.\nEn esta se propone que un parámetro (muchas veces un efecto, o correlación) es 0,\n\nes decir, plantea que no hay efecto o relación entre las variables\nmientras que la hipótesis alternativa sugiere que sí existe una relación o efecto significativo."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hipótesis-3",
    "href": "sesion1_slides.html#preguntas-e-hipótesis-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hipótesis:",
    "text": "Preguntas e hipótesis:\nUna guía clave para el análisis\n\nSon fundamentales para establecer una base objetiva para el análisis y para evaluar las evidencias encontradas en los datos.\nEl proceso de plantear preguntas y formular hipótesis es el primer paso en el análisis de datos, ya que establece una guía clara para el enfoque y la dirección del trabajo.\nAl identificar preguntas y establecer hipótesis, se crea un marco sólido que orientará la exploración y el análisis de los datos disponibles."
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicación-1-1",
    "href": "sesion1_slides.html#taller-de-aplicación-1-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicación 1",
    "text": "Taller de aplicación 1\n\n\n\n\n\n\nPregunta 2 - Investigando sobre países:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna característica de dicho país en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles.\n\n¿Cómo definiria la variable aleatoria relevante?\n¿Qué hipótesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estadística\nInferencia se refiere al proceso de hacer generalizaciones de una población a partir de una muestra de esa población.\n\nPoblación y Muestra"
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos-1",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estadística\n\nSi tenemos un sub-conjunto de datos representativos de una población\n\npodemos utilizar métodos estadísticos\npara sacar conclusiones sobre las características\nY propiedades de esa población en su totalidad."
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos-2",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estadística\n\nEl proceso de inferencia estadística se basa en el principio de que una muestra bien s eleccionada puede proporcionar información valiosa sobre la población en general.\nEl uso de la inferencia estadística es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una población en particular."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos",
    "href": "sesion1_slides.html#estadígrafos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos",
    "text": "Estadígrafos\nFunciones que aproximan parámetros\n\nEstadigrafos"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-1",
    "href": "sesion1_slides.html#estadígrafos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos",
    "text": "Estadígrafos\nson variables aleatorias\n\n\n\nDado que por cada muestra que tenemos, vamos a calcular un estadígrafo este es en si mismo una variable aleatoria.\nTiene su propia distribución, media y varianza!"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-2",
    "href": "sesion1_slides.html#estadígrafos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos",
    "text": "Estadígrafos\nlos más comunes\n\nEstadigrafos más comunes"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\nLa distribución de las medias muestrales de una población se aproxima a una distribución normal\nIndependientemente de la forma de la distribución original de la población.\nEste teorema es esencial en inferencia estadística y tiene amplias aplicaciones en análisis de datos y toma de decisiones."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-1",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\nLa media muestral se distribuye normal, sin importar la distribución de la variable subyacente"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-2",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\n\nFormalmente: \\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\nSea x con media μ y desviación estándar σ finitas.\n\n\n\nSi tomamos muestras aleatorias de tamaño n de esta población y calculamos la media muestral de cada muestra\nLas medias muestrales se aproximará a una distribución normal con media μ y desviación estándar σ/√n."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-3",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nConectados por el Teorema del Límite central\n\n\nFormalmente: \\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hipótesis\np-valor"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-4",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-5",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-6",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-6",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\nEste intervalo es aleatorio, porque \\(\\bar{y}\\) es diferente en cada muestra."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-7",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-7",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza\nMatemáticamente, para cada muestra podemos construir un intervalo.\n\nCon varianza conocidaCon varianza desconocida\n\n\n\\[ P\\left( \\bar{y}-\\frac{1.96\\sigma }{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{1.96\\sigma }{\\sqrt{n}}  \\right) = 0.95 \\]\n\n\n\\[ \\left(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} \\right) \\]"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-8",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-8",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza\n\nCon 20 muestras, tenemos 20 intervalos."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-9",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-9",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza - Interpretación\nPensemos en un 95% de confianza (un valor usual):\n\nEsto quiere decir, que si se repitiera este ejercicio muchas veces y construyéramos un intervalo de esta forma…\nel 95% de ellos contendría el verdadero parámetro poblacional.\nNo significa que con 95% de certeza el parámetro está exactamente en estos valores."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-10",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-10",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nIntervalos de confianza - Interpretación\n\nAl 95% de confianza con 20 intervalos 19 contendrán el parámetro."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-11",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-11",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nUna forma de verificar hipotesis sobre los parámetros es mediante el contraste de hipótesis."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-12",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-12",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nEmpezamos suponiendo que hay una distribución conocida para el estadígrafo, centrada en un valor específico."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-13",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-13",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nY nos preguntamos, si esto fuea verdad ¿qué tan probable es la muestra que tengo?"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-14",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-14",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nPruebas de hipótesis\nLlamamos la hipótesis a probar Ho, y su alternativa H1."
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-15",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-15",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-16",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-16",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nErrores y P-valor\n\nSe elige nivel de significancia de contraste (α) = probabilidad de cometer error Tipo I. Típicamente α = 0,01, 0,05, 0,10.\nDefinimos la prueba de hipótesis de significancia como aquella que indica si un estimador \\(\\hat{T}\\) es 0.\n\n\\[ H_0: T =0\\text{ vs }H_1: T \\neq 0 \\]"
  },
  {
    "objectID": "sesion1_slides.html#estadígrafos-y-parámetros-17",
    "href": "sesion1_slides.html#estadígrafos-y-parámetros-17",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estadígrafos y parámetros",
    "text": "Estadígrafos y parámetros\nErrores y P-valor\nEl Valor de probabilidad (ó p-valor) es el nivel probabilidad más alto para el cual no podemos rechazar la hipótesis nula de la prueba de significancia."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nLos datos “Palmer Penguins” son un conjunto que detalla medidas morfológicas y características de tres especies de pingüinos: Adelie, Gentoo y Chinstrap.\nRecopilados por el Dr. Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-1",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\n\n\n\n\ncodetabla\n\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head(6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-2",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nEn el contexto de los pingüinos y el peso de su población:\n\npodríamos tomar una muestra de pingüinos\ny calcular un intervalo de confianza para el peso promedio.\n\nEsto nos daría una estimación del peso promedio de la población total, junto con la confianza en que este valor estimado es preciso."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-3",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nLa elección de la muestra, la interpretación de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hipótesis, sobre el peso de los pingüinos.\nPor ahora, pensemos que nuestra información es la población completa"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-4",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\nCalcularemos el promedio muestral y lo veremos en el contexto de los datos observados: . . .\n\ncodeoutput\n\n\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los pingüinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribución del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribución de Peso de Pingüinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-5",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral.\nDe que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus características.\nPor ejemplo, consideremos que de esta población de pingüinos obtenemos 1000 muestras de 40 individuos cada una.\nSi graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tamaño de muestra, más nos alejamos de la distribución normal.\nSi reducimos el número de repeticiones tambieé."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-6",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-pingüinos-palmer-6",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Pingüinos Palmer",
    "text": "Ejemplo: Peso de los Pingüinos Palmer\n\ncodePlot\n\n\n\nimport numpy as np\n\n# Definir el tamaño de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y cálculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gráfico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribución de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#intervalo-de-confianza",
    "href": "sesion1_slides.html#intervalo-de-confianza",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Intervalo de confianza",
    "text": "Intervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n#  Obtener una muestra simple de 40 pingüinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error estándar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviación estándar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)"
  },
  {
    "objectID": "sesion1_slides.html#intervalo-de-confianza-1",
    "href": "sesion1_slides.html#intervalo-de-confianza-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Intervalo de confianza",
    "text": "Intervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nEl resultado será un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los pingüinos en la población, con un nivel de confianza del 95%.\n¿Como nos fue? ¿Contiene al verdadero valor?\n\n\n\nprint(\"Media poblacional:\", penguins['body_mass_g'].mean())\n\nprint(\"Intervalo de Confianza para el Peso:\", confidence_interval)\n\n\n\nMedia poblacional: 4201.754385964912\nIntervalo de Confianza para el Peso: (4057.6777501605625, 4561.072249839437)"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos",
    "href": "sesion1_slides.html#comparaciones-de-grupos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribución de tamaño por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n\n\ncodeTabla\n\n\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\nprint(tabla_doble_entrada)\n\n\n\n\n\n\n\n\n     species     sex     Promedio       Varianza\n0     Adelie  Female  3368.835616   72565.639269\n1     Adelie    Male  4043.493151  120278.253425\n2  Chinstrap  Female  3527.205882   81415.441176\n3  Chinstrap    Male  3938.970588  131143.605169\n4     Gentoo  Female  4679.741379   79286.335451\n5     Gentoo    Male  5484.836066   98068.306011"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-1",
    "href": "sesion1_slides.html#comparaciones-de-grupos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hipótesis: ¿Existe una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”?"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-2",
    "href": "sesion1_slides.html#comparaciones-de-grupos-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nHipótesis Nula (H0):\nNo hay diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\nHipótesis Alternativa (H1):\nExiste una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”."
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-3",
    "href": "sesion1_slides.html#comparaciones-de-grupos-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nPara probar esta hipótesis, podrías utilizar una prueba de hipótesis para comparar las medias de las muestras de peso de los pingüinos machos y hembras en la especie “Adelie”.\n\n\n\ncodePlot\n\n\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribución de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribución de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-4",
    "href": "sesion1_slides.html#comparaciones-de-grupos-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nA simple vista podriamos pensar ambos grupos son diferentes.\nEs más claro si dibujamos el promedio muestral observado.\n\n\n\ncodeplot\n\n\n\n# Crear un gráfico de densidad con líneas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-5",
    "href": "sesion1_slides.html#comparaciones-de-grupos-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\ncodeTest t diferenciaplot\n\n\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estadística t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gráfico de comparación de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Machos y Hembras de Pingüinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\n\n\n\n\n\n\n\nEstadística t: 13.126285923485874\nValor p: 6.402319748031793e-26"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-6",
    "href": "sesion1_slides.html#comparaciones-de-grupos-6",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas.\nPara esto podriamos usar una prueba ANOVA.\n\n\n\ncodeANOVA ResultsPlot\n\n\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estadística F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\n\n\n\n\n\n\n\nEstadística F: 72.96098633250911\nValor p: 4.897246751596325e-16"
  },
  {
    "objectID": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab",
    "href": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Experimentos Aleatorios y pruebas A/B",
    "text": "Experimentos Aleatorios y pruebas A/B\n\nUn experimento estadístico busca establecer relaciones causales entre variables y obtener conclusiones sobre su impacto.\nSe diseñan para manipular variables independientes y observar sus efectos en una variable dependiente.\nLos experimentos controlan y manipulan variables para hacer afirmaciones sólidas sobre relaciones causales.\nLas pruebas A/B son comunes en áreas como marketing y diseño de productos.\nEn una prueba A/B, se comparan dos grupos de muestra (A y B) para evaluar si la variante B produce cambios significativos en una métrica de interés."
  },
  {
    "objectID": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab-1",
    "href": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Experimentos Aleatorios y pruebas A/B",
    "text": "Experimentos Aleatorios y pruebas A/B\nCuidados:\n\nPruebas A/B ofrecen evidencia de asociación causal, pero no aseguran causalidad total debido a factores no controlados.\nExperimentos controlados y métodos de diseño sólidos son esenciales para una comprensión completa de la causalidad.\nPruebas A/B son herramientas poderosas para analizar efectos y comparar opciones en condiciones controladas."
  },
  {
    "objectID": "sesion1_slides.html#enunciado",
    "href": "sesion1_slides.html#enunciado",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Enunciado",
    "text": "Enunciado\n\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico"
  },
  {
    "objectID": "sesion1_slides.html#enunciado-1",
    "href": "sesion1_slides.html#enunciado-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Enunciado",
    "text": "Enunciado\n\nLos clientes son asignados a uno de los siguientes grupos:\n\nControl: no les da una promoción (mala suerte, intentalo otra vez)\nTratamiento 1: 20% de descuento en el producto\n-Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento."
  },
  {
    "objectID": "sesion1_slides.html#creación-de-los-datos",
    "href": "sesion1_slides.html#creación-de-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Creación de los datos",
    "text": "Creación de los datos\n\ncodeDataframe\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n#| output: false\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n#| echo: false\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns"
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicación-1-2",
    "href": "sesion1_slides.html#taller-de-aplicación-1-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicación 1:",
    "text": "Taller de aplicación 1:\n\n\n\n\n\n\nPregunta 3 -Ejemplo AB test en Marketing:\n\n\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones",
    "href": "sesion1_slides.html#desafíos-y-consideraciones",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nImportancia de la Adquisición y Almacenamiento de Datos\n\nLa adquisición y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de análisis.\nLa calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes.\nGarantía de Calidad y Fiabilidad en la Obtención de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean sólidas."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-1",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nImportancia de la Adquisición y Almacenamiento de Datos\n\nExploración de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros.\nCada fuente tiene sus propias características y potenciales sesgos.\nComprender las diferencias entre estas fuentes y cómo pueden influir en los resultados es crucial para tomar decisiones informadas."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-2",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-2",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nPrivacidad y Seguridad de los Datos:\n\nUno de los aspectos más críticos en el análisis de datos es la privacidad y seguridad de la información.\nLos datos pueden contener información sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-3",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-3",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nPrivacidad y Seguridad de los Datos:\n\nExploraremos prácticas y regulaciones para garantizar que los datos se manejen de manera ética y legal.\nDiscutiremos cómo anonimizar los datos, utilizar técnicas de enmascaramiento y seguir las mejores prácticas para resguardar la privacidad de los individuos."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-4",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-4",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nLimpieza y Transformación durante la Preparación de Datos:\n\nLa etapa de preparación de datos es crucial para asegurarse de que los datos sean aptos para el análisis.\nSin embargo, este proceso no está exento de desafíos.\nLos datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada."
  },
  {
    "objectID": "sesion1_slides.html#desafíos-y-consideraciones-5",
    "href": "sesion1_slides.html#desafíos-y-consideraciones-5",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desafíos y Consideraciones:",
    "text": "Desafíos y Consideraciones:\nLimpieza y Transformación durante la Preparación de Datos:\n\nExploraremos técnicas para identificar y manejar valores atípicos y faltantes, errores de digitación, etc.\nLos invetsigadores toman muchas decisiones en este proceso, que deben ser transparentes.\nAbordar estos desafíos de manera adecuada es esencial para garantizar que nuestras conclusiones sean sólidas, confiables y éticas."
  },
  {
    "objectID": "sesion1_slides.html#reproducibilidad-y-control-de-versiones-git",
    "href": "sesion1_slides.html#reproducibilidad-y-control-de-versiones-git",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Reproducibilidad y Control de Versiones (GIT):",
    "text": "Reproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios",
    "href": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:",
    "text": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\n\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que también es una herramienta poderosa en el análisis de datos.\nPermite rastrear cada modificación realizada en el código y en los documentos, incluidos los notebooks.\nCada cambio es registrado como un “commit”, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\nLa aplicación de GIT en proyectos de preparación de datos agrega un nivel adicional de transparencia y colaboració"
  },
  {
    "objectID": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios-1",
    "href": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:",
    "text": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\n\nUn esquema de git por Allison Horst @allison_horst"
  },
  {
    "objectID": "sesion1_slides.html#actividad-de-proyecto",
    "href": "sesion1_slides.html#actividad-de-proyecto",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos.",
    "section": "Actividad de proyecto",
    "text": "Actividad de proyecto\n\n\n\n\n\n\nInicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del análisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustaría trabajar.\nPropongan una o dos preguntas de investigación y las hipotesis que las responderían.\n\nLa siguiente sesión, vamos a explorar los datos y empezar los primeros pasos en su análisis.\n\n\n\n\n\n\n\nCurso Análisis de Datos - Sesión 1"
  },
  {
    "objectID": "sesion2_slides.html#la-preparación-de-los-datos",
    "href": "sesion2_slides.html#la-preparación-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "La preparación de los datos",
    "text": "La preparación de los datos\n\nLa preparación de datos es una fase esencial en el proceso de análisis de datos que involucra una serie de actividades destinadas a garantizar que los datos estén en condiciones óptimas para su posterior análisis.\n\nExtraccion de los datos\nLimpieza\nTransformación\nOrganización"
  },
  {
    "objectID": "sesion2_slides.html#tipos-de-datos",
    "href": "sesion2_slides.html#tipos-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tipos de datos",
    "text": "Tipos de datos\n\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\n\nPodemos clasificar estos datos que recopilamos de muchas maneras:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis."
  },
  {
    "objectID": "sesion2_slides.html#estructura-de-los-datos",
    "href": "sesion2_slides.html#estructura-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Estructura de los datos",
    "text": "Estructura de los datos\n\nLa estructura de un dato se refiere a su formato y codificación."
  },
  {
    "objectID": "sesion2_slides.html#datos-estructurados",
    "href": "sesion2_slides.html#datos-estructurados",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos estructurados",
    "text": "Datos estructurados\n\nConjuntos de tablas forman bases de datos estructuradas.\nTablas son formas organizadas y ordenadas de datos.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente."
  },
  {
    "objectID": "sesion2_slides.html#datos-no-estructurados",
    "href": "sesion2_slides.html#datos-no-estructurados",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos No estructurados",
    "text": "Datos No estructurados\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido."
  },
  {
    "objectID": "sesion2_slides.html#algunos-ejemplos-de-datos-no-estructurados-son",
    "href": "sesion2_slides.html#algunos-ejemplos-de-datos-no-estructurados-son",
    "title": "Sesión 2: Preparando los datos",
    "section": "Algunos ejemplos de datos no estructurados son:",
    "text": "Algunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos."
  },
  {
    "objectID": "sesion2_slides.html#estructurados-vs-no-estructuraods",
    "href": "sesion2_slides.html#estructurados-vs-no-estructuraods",
    "title": "Sesión 2: Preparando los datos",
    "section": "Estructurados vs no estructuraods",
    "text": "Estructurados vs no estructuraods\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes."
  },
  {
    "objectID": "sesion2_slides.html#datos-según-tamano",
    "href": "sesion2_slides.html#datos-según-tamano",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según tamaño",
    "text": "Datos según tamaño\n\nBig Data:\n\nSe refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales.\nBig Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos."
  },
  {
    "objectID": "sesion2_slides.html#datos-según-tamano-1",
    "href": "sesion2_slides.html#datos-según-tamano-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según tamaño",
    "text": "Datos según tamaño\n\nSmall Data:\n\nSe refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data.\nEstos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales.\nA menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas."
  },
  {
    "objectID": "sesion2_slides.html#datos-según-tipo-y-su-operacionalizacion",
    "href": "sesion2_slides.html#datos-según-tipo-y-su-operacionalizacion",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según tipo y su operacionalización",
    "text": "Datos según tipo y su operacionalización\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\nContinuas y discretas"
  },
  {
    "objectID": "sesion2_slides.html#datos-según-temporalidad-y-unidad-de-analisis.",
    "href": "sesion2_slides.html#datos-según-temporalidad-y-unidad-de-analisis.",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos según temporalidad y unidad de análisis.",
    "text": "Datos según temporalidad y unidad de análisis.\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nSerie de tiempo\nPanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_slides.html#corte-transversal",
    "href": "sesion2_slides.html#corte-transversal",
    "title": "Sesión 2: Preparando los datos",
    "section": "Corte transversal",
    "text": "Corte transversal"
  },
  {
    "objectID": "sesion2_slides.html#serie-temporal",
    "href": "sesion2_slides.html#serie-temporal",
    "title": "Sesión 2: Preparando los datos",
    "section": "Serie temporal",
    "text": "Serie temporal"
  },
  {
    "objectID": "sesion2_slides.html#panel",
    "href": "sesion2_slides.html#panel",
    "title": "Sesión 2: Preparando los datos",
    "section": "Panel",
    "text": "Panel"
  },
  {
    "objectID": "sesion2_slides.html#leyendo-datos-en-pandas",
    "href": "sesion2_slides.html#leyendo-datos-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo"
  },
  {
    "objectID": "sesion2_slides.html#leyendo-datos-en-pandas-1",
    "href": "sesion2_slides.html#leyendo-datos-en-pandas-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read:\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: automático o definido por el usuario\nDatetime parsing: puede combinar información de múltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con números con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2_slides.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2_slides.html#formato-csv-valores-separados-por-comas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que parece ser\n\n¿Y qué pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2_slides.html#csv-en-pandas",
    "href": "sesion2_slides.html#csv-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nBásica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nBásica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representación de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2_slides.html#json-java-script-object-notation",
    "href": "sesion2_slides.html#json-java-script-object-notation",
    "title": "Sesión 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, números, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores numéricos"
  },
  {
    "objectID": "sesion2_slides.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2_slides.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2_slides.html#orientaciones-posibles-de-json",
    "href": "sesion2_slides.html#orientaciones-posibles-de-json",
    "title": "Sesión 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2_slides.html#extensible-markup-language-xml",
    "href": "sesion2_slides.html#extensible-markup-language-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato más antiguo y auto descriptivo, con estructura jerárquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un método incorporado en Python.\nSe puede usar la librería lxml (también ElementTree)"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-xml",
    "href": "sesion2_slides.html#ejemplo-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo XML",
    "text": "Ejemplo XML"
  },
  {
    "objectID": "sesion2_slides.html#formatos-binarios",
    "href": "sesion2_slides.html#formatos-binarios",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¿Qué es un formato binario?\nPickle: Python’s built-in serialization\n\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\n\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}"
  },
  {
    "objectID": "sesion2_slides.html#formatos-binarios-1",
    "href": "sesion2_slides.html#formatos-binarios-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nHDF5: Librería para almacenar gran cantidad de datos científicos\n\nFormato jerárquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresión\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2_slides.html#bases-de-datos-relacionales",
    "href": "sesion2_slides.html#bases-de-datos-relacionales",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales"
  },
  {
    "objectID": "sesion2_slides.html#bases-de-datos-relacionales-1",
    "href": "sesion2_slides.html#bases-de-datos-relacionales-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\nLas bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\nvínculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos \n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayoría de los sistemas de bases de datos a través de un API común\nSintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.) \nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n\n\n\nimport sqlalchemy as sqla\ndb = sqla.create_engine('sqlite:///mydata.sqlite')\npd.read_sql('select \\* from test', db)"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-estadístico",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-estadístico",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista estadístico",
    "text": "Dirty data: punto de vista estadístico\n\nLos datos son generados desde algún proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsión: algunas muestras se corrompieron por un proceso\nSesgo de selección: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: hay valores que no observamos\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisión y simplicidad"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores están missing, corrompidos, erróneos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un área",
    "text": "Dirty Data: Punto de vista del experto en un área\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¿Qué ocurrió?\n\nLos expertos en un área llevan un modelo implícito de los datos que están testeando\n\nNo siempre necesitas ser un experto en un área para hacer esto\n\n¿Puede una persona correr 500 km por hora?\n¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido común"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinación de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregación es menos susceptible a los errores numéricos\nSer cuidadoso, puede que los datos estén bien…"
  },
  {
    "objectID": "sesion2_slides.html#dónde-se-origina-el-dirty-data",
    "href": "sesion2_slides.html#dónde-se-origina-el-dirty-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Dónde se origina el dirty data?",
    "text": "¿Dónde se origina el dirty data?\n\nLa fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integración de diferentes sets de datos causa problemas\nPropagación del error: se magnifica un error"
  },
  {
    "objectID": "sesion2_slides.html#problemas-dirty-comunes",
    "href": "sesion2_slides.html#problemas-dirty-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs. New York\nPérdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs. dos\nDatos truncados: “Janice Keihanaikukauakahihuliheekahaunaele” se vuelve “Janice - Keihanaikukauakahihuliheek” en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposición\nProblemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017"
  },
  {
    "objectID": "sesion2_slides.html#preparando-los-datos",
    "href": "sesion2_slides.html#preparando-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Preparando los datos",
    "text": "Preparando los datos\nMuchas veces los datos con los que queremos trabajar no están en el formato adecuado para los análisis que querenos realizar."
  },
  {
    "objectID": "sesion2_slides.html#data-wrangling",
    "href": "sesion2_slides.html#data-wrangling",
    "title": "Sesión 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato más significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representación a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2_slides.html#tidy-data",
    "href": "sesion2_slides.html#tidy-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para análisis de corte transversal nuestro ideal es trabajar con Tidy Data\n“Tidy Data is a standar way of mapping the meaning of a dataset to its structure¨\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un estándar deseable para analizar datos."
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-1",
    "href": "sesion2_slides.html#tidy-data-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observación (registro) forma una fila.\nCada dato (valor) está en una celda de la tabla."
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-2",
    "href": "sesion2_slides.html#tidy-data-2",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-3",
    "href": "sesion2_slides.html#tidy-data-3",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:"
  },
  {
    "objectID": "sesion2_slides.html#tidy-workflow",
    "href": "sesion2_slides.html#tidy-workflow",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy & workflow",
    "text": "Tidy & workflow\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados."
  },
  {
    "objectID": "sesion2_slides.html#porqué-datos-tidy",
    "href": "sesion2_slides.html#porqué-datos-tidy",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Porqué datos Tidy?",
    "text": "¿Porqué datos Tidy?\n\nEstandarizanción\n\n\n\n\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#facilitar-la-colaboración",
    "href": "sesion2_slides.html#facilitar-la-colaboración",
    "title": "Sesión 2: Preparando los datos",
    "section": "Facilitar la colaboración",
    "text": "Facilitar la colaboración\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#simplifica-la-reproducibilidad",
    "href": "sesion2_slides.html#simplifica-la-reproducibilidad",
    "title": "Sesión 2: Preparando los datos",
    "section": "Simplifica la reproducibilidad",
    "text": "Simplifica la reproducibilidad\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#siempre-tidy",
    "href": "sesion2_slides.html#siempre-tidy",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Siempre Tidy?",
    "text": "¿Siempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o “desordenadas”.\nEs importante tener en cuenta que siempre existen múltiples formas de representar la misma información."
  },
  {
    "objectID": "sesion2_slides.html#otras-estructuras",
    "href": "sesion2_slides.html#otras-estructuras",
    "title": "Sesión 2: Preparando los datos",
    "section": "Otras estructuras:",
    "text": "Otras estructuras:\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2_slides.html#algunas-tareas-comunes",
    "href": "sesion2_slides.html#algunas-tareas-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Algunas tareas comunes",
    "text": "Algunas tareas comunes\n\nDescartar e imputar missing data\nRemover duplicados.\nModificar datos\n\nmapear strings, expresiones aritméticas. Ejemplos:\n\nConvertir strings de mayúsculas a minúsculas (upper/lower case)\nConvertir T en Fahrenheit a Celsius\nCrear una nueva columna basada en la columna anterior."
  },
  {
    "objectID": "sesion2_slides.html#algunas-tareas-comunes-1",
    "href": "sesion2_slides.html#algunas-tareas-comunes-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Algunas tareas comunes",
    "text": "Algunas tareas comunes\n\nReemplazar valores\n\n(e.g. -999 → NaN). Usar método df[‘column’].replace()\n\nRestringir valores:\n\nvalores por encima o por debajo de los umbrales especificados se establecen en un valor máximo/mínimo."
  },
  {
    "objectID": "sesion2_slides.html#datos-perdidos-o-missing-na-nan",
    "href": "sesion2_slides.html#datos-perdidos-o-missing-na-nan",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos (o missing, NA, NAN)",
    "text": "Datos perdidos (o missing, NA, NAN)\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\nse pueden escoger filas o columnas\n\nLlenar/ reemplazar :\n\ncon un valor por default\ncon un valor interpolados, otros"
  },
  {
    "objectID": "sesion2_slides.html#datos-perdidos",
    "href": "sesion2_slides.html#datos-perdidos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-missing",
    "href": "sesion2_slides.html#ejemplo-missing",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo missing",
    "text": "Ejemplo missing\n\nimport pandas as pd\nimport numpy as np\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', np.nan, 'Carlos'],\n        'Edad': [25, 30, np.nan, 30, 28]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar filas con valores faltantes\ndf = df.dropna()\n\n# Llenar valores faltantes con un valor específico\ndf['Edad'].fillna(0, inplace=True)\n\nprint(df)\n\n\n\n   Nombre  Edad\n0    Juan  25.0\n1     Ana  30.0\n4  Carlos  28.0"
  },
  {
    "objectID": "sesion2_slides.html#fitrando-y-limpiando",
    "href": "sesion2_slides.html#fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cuál de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key."
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-fitrando-y-limpiando",
    "href": "sesion2_slides.html#ejemplo-fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo Fitrando y limpiando",
    "text": "Ejemplo Fitrando y limpiando\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-datos-de-educación",
    "href": "sesion2_slides.html#ejemplo-datos-de-educación",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo: Datos de educación",
    "text": "Ejemplo: Datos de educación\n\n\n\n\n\n\nTaller de aplicación 1: Pregunta 4"
  },
  {
    "objectID": "sesion2_slides.html#errores-de-registro-y-textos",
    "href": "sesion2_slides.html#errores-de-registro-y-textos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) “quiebra” o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14”"
  },
  {
    "objectID": "sesion2_slides.html#errores-de-registro-y-textos-1",
    "href": "sesion2_slides.html#errores-de-registro-y-textos-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\n\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14”\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD” \"AbCd\".lower() # \"abcd\""
  },
  {
    "objectID": "sesion2_slides.html#errores-de-formato",
    "href": "sesion2_slides.html#errores-de-formato",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de formato",
    "text": "Errores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0"
  },
  {
    "objectID": "sesion2_slides.html#transformando-strings",
    "href": "sesion2_slides.html#transformando-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformando strings",
    "text": "Transformando strings\nPodemos encontrar donde estan los problemas facilmente:\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n\n\n2\n\n\n\n\n\n#s.index(':') # ValueError raised"
  },
  {
    "objectID": "sesion2_slides.html#encontrar-problemas",
    "href": "sesion2_slides.html#encontrar-problemas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Encontrar problemas",
    "text": "Encontrar problemas\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado . . .\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n\n\n2\n\n\n\n\ns.find(':') # -1\n\n\n\n-1"
  },
  {
    "objectID": "sesion2_slides.html#encontrar-problemas-1",
    "href": "sesion2_slides.html#encontrar-problemas-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Encontrar problemas",
    "text": "Encontrar problemas\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string . . .\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\n\n\nFalse"
  },
  {
    "objectID": "sesion2_slides.html#métodos-para-strings",
    "href": "sesion2_slides.html#métodos-para-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Métodos para strings",
    "text": "Métodos para strings"
  },
  {
    "objectID": "sesion2_slides.html#métodos-para-strings-1",
    "href": "sesion2_slides.html#métodos-para-strings-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Métodos para strings",
    "text": "Métodos para strings\n\nCualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido)\nSe debe usar .str. (es importante el .str)."
  },
  {
    "objectID": "sesion2_slides.html#ejemplo",
    "href": "sesion2_slides.html#ejemplo",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo:",
    "text": "Ejemplo:\n\ngmailsplit3 char\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\n\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\n\n\n\n    \nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\n\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\n\n\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2_slides.html#expresiones-regulares-para-strings",
    "href": "sesion2_slides.html#expresiones-regulares-para-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Expresiones regulares para strings",
    "text": "Expresiones regulares para strings"
  },
  {
    "objectID": "sesion2_slides.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2_slides.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets"
  },
  {
    "objectID": "sesion2_slides.html#eliminar-elegir-columnas",
    "href": "sesion2_slides.html#eliminar-elegir-columnas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Eliminar/ elegir columnas",
    "text": "Eliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis. . . .\n\nCodetabla originaltabla sin peso\n\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\ndf.head(6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\nEdad\nPeso (kg)\n\n\n\n\n0\nJuan\n25\n70\n\n\n1\nAna\n30\n65\n\n\n2\nLuis\n35\n80\n\n\n3\nCarlos\n28\n75\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\nEdad\n\n\n\n\n0\nJuan\n25\n\n\n1\nAna\n30\n\n\n2\nLuis\n35\n\n\n3\nCarlos\n28"
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato",
    "href": "sesion2_slides.html#reshapesre-formato",
    "title": "Sesión 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato\n\nEl “reshape” o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico.\nEsto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos.\nEl “reshape” es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo."
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato-1",
    "href": "sesion2_slides.html#reshapesre-formato-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato"
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato-2",
    "href": "sesion2_slides.html#reshapesre-formato-2",
    "title": "Sesión 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato\n\nCodeDF anchoDF largo\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCiudad\nA\nB\n\n\nFecha\n\n\n\n\n\n\n2023-08-01\n25\n28\n\n\n2023-08-02\n26\n29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFecha\nCiudad\nTemperatura\n\n\n\n\n0\n2023-08-01\nA\n25\n\n\n1\n2023-08-01\nB\n28\n\n\n2\n2023-08-02\nA\n26\n\n\n3\n2023-08-02\nB\n29"
  },
  {
    "objectID": "sesion2_slides.html#crear-variables-dummies-dicotómicas",
    "href": "sesion2_slides.html#crear-variables-dummies-dicotómicas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Crear variables dummies / dicotómicas",
    "text": "Crear variables dummies / dicotómicas\n\nMuy útil para representar información cualitativa.\nMuy usando en análisis de regresión y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\nSe pueden generar mediante pd.get_dummies(df[‘key’])"
  },
  {
    "objectID": "sesion2_slides.html#one-hot-encoding-razas-de-perros",
    "href": "sesion2_slides.html#one-hot-encoding-razas-de-perros",
    "title": "Sesión 2: Preparando los datos",
    "section": "One hot encoding: Razas de perros",
    "text": "One hot encoding: Razas de perros"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-razas-de-perros",
    "href": "sesion2_slides.html#ejemplo-razas-de-perros",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo : Razas de perros",
    "text": "Ejemplo : Razas de perros\n\nCodetabla\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabrador\nPoodle\nBulldog\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n\n\n3\n0\n0\n0\n\n\n4\n0\n0\n0\n\n\n5\n0\n0\n0"
  },
  {
    "objectID": "sesion2_slides.html#unir-datasets",
    "href": "sesion2_slides.html#unir-datasets",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\nApliar/concatenar/append: Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join: Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como “inner”, “outer”, “left” y “right”"
  },
  {
    "objectID": "sesion2_slides.html#apilar-dataframes",
    "href": "sesion2_slides.html#apilar-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "1. Apilar dataframes",
    "text": "1. Apilar dataframes\nCreemos los dataframes: . . .\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\n\n\n   P  Q\n0  2  3\n1  4  5\n\n\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n\n\n   P  Q\n0  6  7\n1  8  9"
  },
  {
    "objectID": "sesion2_slides.html#apilar",
    "href": "sesion2_slides.html#apilar",
    "title": "Sesión 2: Preparando los datos",
    "section": "Apilar",
    "text": "Apilar\nHagamos el apilar. Atención a los index\n\n\ndf.append(df2)\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9"
  },
  {
    "objectID": "sesion2_slides.html#apilar-1",
    "href": "sesion2_slides.html#apilar-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Apilar",
    "text": "Apilar"
  },
  {
    "objectID": "sesion2_slides.html#ojo-al-apilar-con-los-index",
    "href": "sesion2_slides.html#ojo-al-apilar-con-los-index",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ojo al apilar con los index",
    "text": "Ojo al apilar con los index\nUsemos ignorar index . . .\n\ndf.append(df2, ignore_index=True)\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9"
  },
  {
    "objectID": "sesion2_slides.html#ojo-al-apilar-con-los-index-1",
    "href": "sesion2_slides.html#ojo-al-apilar-con-los-index-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ojo al apilar con los index",
    "text": "Ojo al apilar con los index"
  },
  {
    "objectID": "sesion2_slides.html#unir-dataframes",
    "href": "sesion2_slides.html#unir-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Unir dataframes",
    "text": "2. Unir dataframes"
  },
  {
    "objectID": "sesion2_slides.html#unir-datasets-1",
    "href": "sesion2_slides.html#unir-datasets-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nUtilizando la función pd.merge()\n\n\ncodeinner joinouter join\n\n\n\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nValue_x\nValue_y\n\n\n\n\n0\nB\n2\n4\n\n\n1\nC\n3\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nValue_x\nValue_y\n\n\n\n\n0\nA\n1.0\nNaN\n\n\n1\nB\n2.0\n4.0\n\n\n2\nC\n3.0\n5.0\n\n\n3\nD\nNaN\n6.0"
  },
  {
    "objectID": "sesion2_slides.html#transformaciones-estadísticas",
    "href": "sesion2_slides.html#transformaciones-estadísticas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones estadísticas:",
    "text": "Transformaciones estadísticas:\n1. Outliers\n\nLa identificación de valores atípicos es importante por varias razones:\n\n\nCalidad de los datos\nImpacto en estadísticas y modelos\nAnomalías y problemas reales\nToma de decisiones informada"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-outliers",
    "href": "sesion2_slides.html#ejemplo-outliers",
    "title": "Sesión 2: Preparando los datos",
    "section": "Ejemplo outliers",
    "text": "Ejemplo outliers\nDetección\n\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n\n\n\nValores atípicos:\n   Valor\n6    200"
  },
  {
    "objectID": "sesion2_slides.html#estandarización-de-datos",
    "href": "sesion2_slides.html#estandarización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Estandarización de Datos",
    "text": "2. Estandarización de Datos\n\nLa estandarización es un proceso importante en el análisis de datos y el aprendizaje automático.\nConsiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno.\nEsta transformación se logra mediante la fórmula:\n\n\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]"
  },
  {
    "objectID": "sesion2_slides.html#estandarización",
    "href": "sesion2_slides.html#estandarización",
    "title": "Sesión 2: Preparando los datos",
    "section": "Estandarización",
    "text": "Estandarización\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\n\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]"
  },
  {
    "objectID": "sesion2_slides.html#normalización-de-datos",
    "href": "sesion2_slides.html#normalización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Normalización de Datos",
    "text": "Normalización de Datos\n\nLa normalización es un proceso esencial en el análisis de datos y el aprendizaje automático.\nConsiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1.\nLa fórmula matemática utilizada para la normalización es:\n\n\n\\[x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\]"
  },
  {
    "objectID": "sesion2_slides.html#normalización-de-datos-1",
    "href": "sesion2_slides.html#normalización-de-datos-1",
    "title": "Sesión 2: Preparando los datos",
    "section": "Normalización de Datos",
    "text": "Normalización de Datos\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\n\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]\n\n\n\n\n\n\n\n\nCurso Análisis de Datos - Sesión 3"
  },
  {
    "objectID": "taller1_enunciado.html",
    "href": "taller1_enunciado.html",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Replique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\n\n\n\n\n\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\n\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df.head(5)\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n\n\n\n\n\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "taller1_enunciado.html#instrucciones",
    "href": "taller1_enunciado.html#instrucciones",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "",
    "text": "Replique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\n\n\n\n\n\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\n\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df.head(5)\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n\n\n\n\n\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "talleres.html",
    "href": "talleres.html",
    "title": "Talleres de aplicación",
    "section": "",
    "text": "Vamos a trabajar 3 tralleres de aplicación:\n\n\n\nActividad\nEnunciado\nFecha de Entrega\n\n\n\n\nTaller 1\nTaller 1 . Enunciado\n15 de septiembre\n\n\nTaller 2\n\nTBA\n\n\nTaller 3\n\nTBA"
  },
  {
    "objectID": "sesion2.html",
    "href": "sesion2.html",
    "title": "Sesión 2: Preparando los datos",
    "section": "",
    "text": "Slides Sesión 1"
  },
  {
    "objectID": "sesion2.html#tipos-de-datos",
    "href": "sesion2.html#tipos-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tipos de datos",
    "text": "Tipos de datos\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje común.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relación a:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis.\n\n\nSegún estructura de los datos\n\nLa estructura de un dato se refiere a su formato y codificación.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\nDatos estructurados\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\nDatos No estructurados\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\nEstructurados vs no estructuraods\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n\n\n\nDatos según tamaño\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n\n\nDatos según tipo y su operacionalización\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\n\n\n\nDatos según temporalidad y unidad de análisis.\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales\n\n\nCorte transversal\n\n\n\nSerie temproal\n\n\n\nPanel"
  },
  {
    "objectID": "sesion2.html#leyendo-datos-en-pandas",
    "href": "sesion2.html#leyendo-datos-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo\n\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read_*\n\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: automático o definido por el usuario\nDatetime parsing: puede combinar información de múltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con números con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2.html#formato-csv-valores-separados-por-comas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que parece ser\n\n¿Y qué pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2.html#csv-en-pandas",
    "href": "sesion2.html#csv-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nBásica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nBásica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representación de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2.html#json-java-script-object-notation",
    "href": "sesion2.html#json-java-script-object-notation",
    "title": "Sesión 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, números, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores numéricos"
  },
  {
    "objectID": "sesion2.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2.html#orientaciones-posibles-de-json",
    "href": "sesion2.html#orientaciones-posibles-de-json",
    "title": "Sesión 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2.html#extensible-markup-language-xml",
    "href": "sesion2.html#extensible-markup-language-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato más antiguo y auto descriptivo, con estructura jerárquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un método incorporado en Python.\nSe puede usar la librería lxml (también ElementTree)\n\n\nEjemplo XML"
  },
  {
    "objectID": "sesion2.html#formatos-binarios",
    "href": "sesion2.html#formatos-binarios",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¿Qué es un formato binario?\nPickle: Python’s built-in serialization\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n\n\nHDF5: Librería para almacenar gran cantidad de datos científicos\n\nFormato jerárquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresión\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2.html#bases-de-datos-relacionales",
    "href": "sesion2.html#bases-de-datos-relacionales",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\n\nLas bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\nvínculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos \n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayoría de los sistemas de bases de datos a través de un API común\nSintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.) \nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n`import sqlalchemy as sqla\ndb = sqla.create_engine(‘sqlite:///mydata.sqlite’)\npd.read_sql(‘select * from test’, db)`"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-estadístico",
    "href": "sesion2.html#dirty-data-punto-de-vista-estadístico",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista estadístico",
    "text": "Dirty data: punto de vista estadístico\n\nLos datos son generados desde algún proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsión: algunas muestras se corrompieron por un proceso\nSesgo de selección: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: Left and right censorship: los usuarios van y vienen del escrutinio\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisión y simplicidad"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores están missing, corrompidos, erróneos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "href": "sesion2.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un área",
    "text": "Dirty Data: Punto de vista del experto en un área\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¿Qué ocurrió?\nLos expertos en un área llevan un modelo implícito de los datos que están testeando\nNo siempre necesitas ser un experto en un área para hacer esto\n\n¿Puede una persona correr 500 km por hora?\n¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido común"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinación de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregación es menos susceptible a los errores numéricos\nSer cuidadoso, puede que los datos estén bien…"
  },
  {
    "objectID": "sesion2.html#dónde-se-origina-el-dirty-data",
    "href": "sesion2.html#dónde-se-origina-el-dirty-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Dónde se origina el dirty data?",
    "text": "¿Dónde se origina el dirty data?\n\nLa fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integración de diferentes sets de datos causa problemas\nPropagación del error: se magnifica un error"
  },
  {
    "objectID": "sesion2.html#problemas-dirty-comunes",
    "href": "sesion2.html#problemas-dirty-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs. New York\nPérdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs. dos\nDatos truncados: “Janice Keihanaikukauakahihuliheekahaunaele” se vuelve “Janice - Keihanaikukauakahihuliheek” en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposición\nProblemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017"
  },
  {
    "objectID": "sesion2.html#data-wrangling",
    "href": "sesion2.html#data-wrangling",
    "title": "Sesión 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato más significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representación a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2.html#tidy-data",
    "href": "sesion2.html#tidy-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para análisis de corte transversal nuestro ideal es trabajar con Tidy Data\n“Tidy Data is a standar way of mapping the meaning of a dataset to its structure¨\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un estándar deseable para analizar datos.\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observación (registro) forma una fila.\nCada dato (valor) está en una celda de la tabla.\n\n\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:\n\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n\n¿Porqué datos Tidy?\n\nEstandarizanción\n\nLos datos organizados te permiten ser más eficiente al utilizar herramientas existentes diseñadas específicamente para realizar las tareas que necesitas hacer, desde la selección de porciones de tus datos hasta la creación de mapas de tu área de estudio.\nUtilizar herramientas existentes te ahorra tener que construir todo desde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual puede ser consume tiempo y desmotivante).\nAfortunadamente, existen muchas herramientas diseñadas específicamente para transformar datos desorganizados en datos organizados (por ejemplo, en el paquete tidyr). Al estar mejor preparado para transformar tus datos en un formato organizado, podrás llegar más rápido a tus análisis y comenzar a responder las preguntas que estás planteando.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nFacilitar la colaboración\n\nDado que nuestros colegas pueden emplear las mismas herramientas de manera familiar. Tanto si consideramos a los colaboradores como compañeros actuales, su futuro propio o futuros colegas, la organización y compartición de datos de manera coherente y predecible implica menos ajustes, tiempo y esfuerzo para todos.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nSimplifica la reproducibilidad\n\nLos datos organizados también facilitan la reproducción de análisis, ya que son más fáciles de comprender, actualizar y reutilizar. Al utilizar herramientas que esperan datos organizados como entrada, puedes construir e iterar flujos de trabajo realmente potentes. Y cuando tienes entradas de datos adicionales, ¡no hay problema en volver a ejecutar tu código!\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\n\n¿Siempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o “desordenadas”.\nEs importante tener en cuenta que siempre existen múltiples formas de representar la misma información.\n\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2.html#datos-perdidos",
    "href": "sesion2.html#datos-perdidos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\n\nse pueden escoger filas o columnas\n\n\nLlenar/ reemplazar :\n\n\ncon un valor por default\ncon un valor interpolados, otros\n\n\n\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2.html#fitrando-y-limpiando",
    "href": "sesion2.html#fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cuál de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key.\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2.html#errores-de-registro-y-textos",
    "href": "sesion2.html#errores-de-registro-y-textos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) “quiebra” o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14”\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14”\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD” \"AbCd\".lower() # \"abcd\""
  },
  {
    "objectID": "sesion2.html#errores-de-formato",
    "href": "sesion2.html#errores-de-formato",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de formato",
    "text": "Errores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_94069/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True."
  },
  {
    "objectID": "sesion2.html#transformando-strings",
    "href": "sesion2.html#transformando-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformando strings",
    "text": "Transformando strings\n\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n2\n\n\n\n#s.index(':') # ValueError raised\n\n\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado\n\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n2\n\n\n\ns.find(':') # -1\n\n-1\n\n\n\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string\n\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\nFalse"
  },
  {
    "objectID": "sesion2.html#métodos-para-strings",
    "href": "sesion2.html#métodos-para-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Métodos para strings",
    "text": "Métodos para strings\n\n\nCualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido) Se debe usar .str. (es importante el .str).\nEjemplo:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets\n\n\nEliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis.\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n\n\n\n\nReshapes/re formato\n\nEl “reshape” o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico.\nEsto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos.\nEl “reshape” es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo.\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n\n\n\n\nCrear variables dummies / dicotómicas\n\nMuy útil para representar información cualitativa.\nMuy usando en análisis de regresión y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\nSe pueden generar mediante pd.get_dummies(df[‘key’])\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0"
  },
  {
    "objectID": "sesion2.html#unir-datasets",
    "href": "sesion2.html#unir-datasets",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\nApliar/concatenar/append Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como “inner”, “outer”, “left” y “right”"
  },
  {
    "objectID": "sesion2.html#apilar-dataframes",
    "href": "sesion2.html#apilar-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "1. Apilar dataframes",
    "text": "1. Apilar dataframes\nCreemos los dataframes:\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n\n\nHagamos el apilar. Atención a los index\n\ndf.append(df2)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_94069/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9\n\n\n\n\n\n\n\n\nUsemos ignorar index\n\ndf.append(df2, ignore_index=True)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_94069/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9"
  },
  {
    "objectID": "sesion2.html#unir-dataframes",
    "href": "sesion2.html#unir-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Unir dataframes",
    "text": "2. Unir dataframes\nUtilizando la función pd.merge().\n\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n\nUnión interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUnión externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0"
  },
  {
    "objectID": "sesion2.html#transformaciones-estadístocas",
    "href": "sesion2.html#transformaciones-estadístocas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones estadístocas:",
    "text": "Transformaciones estadístocas:\n\nOutliers\n\nUn valor atípico, también conocido como valor outlier en inglés, es un punto de datos que difiere significativamente del patrón general de los demás datos en un conjunto. Estos valores son inusuales en relación con el resto de la distribución de los datos y pueden ser considerablemente más altos o más bajos que los valores típicos del conjunto.\nLa identificación de valores atípicos es importante por varias razones: 1. Calidad de los datos 2. Impacto en estadísticas y modelos 3. Anomalías y problemas reales 4. Toma de decisiones informada\nVeamos un ejemplo:\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n\nValores atípicos:\n   Valor\n6    200"
  },
  {
    "objectID": "sesion2.html#estandarización-de-datos",
    "href": "sesion2.html#estandarización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Estandarización de Datos",
    "text": "2. Estandarización de Datos\nLa estandarización es un proceso importante en el análisis de datos y el aprendizaje automático. Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno. Esta transformación se logra mediante la fórmula:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nDonde $ x $ es el valor original, $ $ es la media de los valores y $ $ es la desviación estándar.\nLa estandarización es especialmente útil cuando se tienen variables con diferentes escalas y distribuciones, ya que permite compararlas de manera equitativa y mejorar la eficacia de los algoritmos de aprendizaje automático al mitigar el impacto de las diferencias en las magnitudes de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]"
  },
  {
    "objectID": "sesion2.html#normalización-de-datos",
    "href": "sesion2.html#normalización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Normalización de Datos",
    "text": "Normalización de Datos\nLa normalización es un proceso esencial en el análisis de datos y el aprendizaje automático. Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1. La fórmula matemática utilizada para la normalización es:\n\\[ x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\]\nDonde $ x $ es el valor original y $x_{} $ es el valor normalizado.\nLa normalización es beneficiosa para igualar la escala de las variables y mejorar el rendimiento de los modelos que son sensibles a la magnitud de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]"
  },
  {
    "objectID": "sesion1.html",
    "href": "sesion1.html",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "",
    "text": "Slides Sesión 1"
  },
  {
    "objectID": "sesion1.html#detalles",
    "href": "sesion1.html#detalles",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Detalles",
    "text": "Detalles\nNotas detalladas de la sesión 1, curso análisis de datos, magíster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versión 1"
  },
  {
    "objectID": "sesion1.html#objetivos-de-aprendizaje-de-la-sesión",
    "href": "sesion1.html#objetivos-de-aprendizaje-de-la-sesión",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Objetivos de aprendizaje de la sesión",
    "text": "Objetivos de aprendizaje de la sesión\n\nComprender el papel del proceso de adquisición y almacenamiento en un proyecto de análisis de datos, junto a buenas prácticas que promuevan la transparencia y replicabilidad.\nAprender a formular preguntas y plantear hipótesis que puedan ser abordadas mediante el análisis de datos.\nDesarrollar la habilidad de realizar pruebas de hipótesis y comprender la interpretación de sus resultados."
  },
  {
    "objectID": "sesion1.html#contenidos",
    "href": "sesion1.html#contenidos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Contenidos:",
    "text": "Contenidos:\n\nEl proceso de análisis de datos\n\nEl proceso de análisis de datos\n\nUna visión general a las metodologías de análisis que veremos en el curso\nAdquision y almacenmiento de los datos\nPreparación de los datos\n\nPreguntando a los datos\n\nAsbtrayendo la realidad, variables aleatorias y probabilidades.\nPlanteamiento de preguntas.\nPreguntas y respuestas: el rol de las hipótesis.\n\n\n\n\nRespondiendo desde los datos: Pruebas de hipótesis\n\nConceptos Básicos de Pruebas de Hipótesis:\n\nDefinición de hipótesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hipótesis:\n\nPruebas t para comparación de medias.\nPruebas chi-cuadrado para variables categóricas.\nPruebas ANOVA para comparación de múltiples grupos.\n\nInterpretación de Resultados:\n\nEvaluación de p-values y toma de decisiones.\nSignificación estadística vs. significación práctica.\nComunicación de los resultados de las pruebas de hipótesis.\n\n\n\n\nBuenas prácticas en análisis de datos\n\nDesafíos y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformación durante la preparación de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos."
  },
  {
    "objectID": "sesion1.html#el-proceso-de-análisis-de-datos-1",
    "href": "sesion1.html#el-proceso-de-análisis-de-datos-1",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "El proceso de análisis de datos",
    "text": "El proceso de análisis de datos\nEn el mundo actual, la generación y recopilación de datos se ha vuelto más accesible y significativa que nunca antes. Esta abundancia de información ofrece la oportunidad de extraer conocimientos valiosos que pueden influir en la toma de decisiones y el desarrollo de soluciones eficientes.\nSin embargo, el proceso de transformar estos datos crudos en información útil y significativa requiere una serie de pasos fundamentales que forman parte integral de la disciplina conocida como Ciencia de Datos.\n\nEn esta primera parte, daremos un vistazo general a las metodologías y enfoques clave que exploraremos a lo largo del curso, con énfasis en la importancia de la preparación de los datos.\nEl proceso de análisis de datos se puede dividir en varias etapas interconectadas, cada una con su propio conjunto de desafíos y consideraciones.\n\nBajo esta mirada, tenemos varias fases clave que están interconectadas. En este curso nos enfocaremos en la preparación de los datos y en su análisis mediante modelos de regresión. Esto con el objetivo de responder preguntas desde los datos, que provean información valiosa.\n\nAdquisición de datos:\nEl primer paso en el proceso de análisis de datos implica la adquisición y el almacenamiento de los datos. Esto se refiere a la recolección de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilación de datos de fuentes diversas, como bases de datos, archivos CSV, páginas web o incluso sensores en tiempo real.\nEs crucial comprender cómo recopilar y almacenar estos datos de manera adecuada, garantizando su calidad, integridad y seguridad.\nExisten tantas fuentes de datos, como podríamos imaginar. ALgunas de las más comunes son las siguientes:\n\nEncuestas y Cuestionarios:\n\nDiseño y administración de encuestas para recopilar datos directamente de los participantes.\nPermite obtener información específica y detallada según las preguntas planteadas.\n\nExperimentos Controlados:\n\nDiseño de experimentos para recopilar datos bajo condiciones controladas.\nÚtil para establecer relaciones causales y evaluar efectos de cambios controlados.\n\nObservación y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar información ambiental.\nUtilización de sensores en dispositivos móviles y wearables para recopilar datos de ubicación, salud y actividad.\n\nRecopilación de Datos Existentes:\n\nUtilización de datos ya recopilados y disponibles en bases de datos o fuentes públicas.\nReduce el tiempo y costo de recopilación, pero puede tener limitaciones en términos de calidad y relevancia.\n\nWeb Scraping (Web Scrapping):\n\nExtracción de datos de sitios web utilizando herramientas y técnicas automatizadas.\nPermite recopilar información no estructurada de manera eficiente, pero requiere atención a la ética y términos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracción programática con sistemas y servicios para obtener datos en tiempo real.\nComún en la obtención de datos de redes sociales, información climática, finanzas, entre otros.\n\nColaboración y Participación Comunitaria:\n\nColaboración con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser útil para proyectos de mapeo colaborativo, ciencia ciudadana y recopilación de información local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes volúmenes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilación y posterior análisis de datos heterogéneos.\nUsualmente se accede a través de querys SQL\n\n\n\n\n\n\n\n\nDatos disponibles para el proyecto\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos públicos sobre educación chilena\nDatos públicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad económica\n\nVeamos como acceder algunos de estos datos.\n\n\nEjemplo: Datos públicos sobre individuos en comunas chilenas (encuesta Casen)\nLa Encuesta de Caracterización Socioeconómica Nacional (CASEN) es una investigación realizada en Chile que tiene como objetivo principal recopilar información detallada sobre la situación socioeconómica de los hogares y las personas en el país. Esta encuesta se lleva a cabo de manera periódica y abarca una amplia variedad de temas, como ingresos, educación, empleo, salud, vivienda y otros aspectos relevantes para comprender la realidad socioeconómica de la población chilena. La información recopilada en la Encuesta CASEN se utiliza para informar políticas públicas, tomar decisiones informadas y analizar la evolución de indicadores sociales a lo largo del tiempo.\nSitio Web oficial\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. El formato mas comun es .csv, pero a veces estan en formatos extraños. Por ejemplo, .dta de STATA.\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(5)\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegión de Tarapacá\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegión de Tarapacá\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educación formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegión de Tarapacá\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nBásica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegión de Tarapacá\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nTécnico nivel superior incompleta\nSí\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n4\n1.101100e+11\n3\n30\nRegión de Tarapacá\nIquique\nUrbano\n67\n19\nMujer\n3\n...\nNaN\nNo sabe\nNo\n27083.0\n941583\n27083.0\n941583\n2083.0\n891583.0\n439170.0\n\n\n\n\n5 rows × 22 columns\n\n\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploración, para ser preparados para analizarlos. De esto se tratará la siguiente sesión del curso.\nEjemplo: Datos desde la API del banco mundial Primero siga este ejemplo practico de importar datos, luego será facil responder la pregunta anterior.\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb # para instalar: conda install pandas-datareader  o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. En este caso revisare de PIB (GDP en ingés), pero se pueden explorar muchas más opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n1503\nBG.GSR.NFSV.GD.ZS\nTrade in services (% of GDP)\n\nWorld Development Indicators\nTrade in services is the sum of service export...\nb'International Monetary Fund, Balance of Paym...\nEconomy & Growth ; Private Sector ; Trade\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16626\nUIS.XUNIT.GDPCAP.23.FSGOV\nInitial government funding per secondary stude...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16627\nUIS.XUNIT.GDPCAP.23.FSHH\nInitial household funding per secondary studen...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n16628\nUIS.XUNIT.GDPCAP.3.FSGOV\nInitial government funding per upper secondary...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16629\nUIS.XUNIT.GDPCAP.5T8.FSGOV\nInitial government funding per tertiary studen...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16630\nUIS.XUNIT.GDPCAP.5T8.FSHH\nInitial household funding per tertiary student...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n\n\n540 rows × 7 columns\n\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n\n\n\n\n\nObservar que este es un data frame con dos índices: pais y año. Para mayor referencia coo tratar este tipo de datos ver en https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\nObtengamos un data frame con los datos de Chile, entre 1980 y 2020.\n\n#sabemos que queremos Chile, asi que busquemos su info\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n\n\n\n\n\nSi quisieramos, por simplicidad quedarnos solo con el indice del año y reordenar el dataframe:\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el análisis es para un solo país\nreversed_df.head(5)\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068\n\n\n\n\n\n\n\nAhora, realicemos un grafico rápido con nuestros datos:\n\n# Graficamos\n\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'Año')\n\nText(0.5, 0, 'Año')\n\n\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 1 - Bajando y formateando datos del Banco Mundial**\n\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\n\nPreguntando a los datos\n¿Cómo plantear preguntas y formular hipótesis en el contexto del análisis de datos? El proceso de análisis comienza con la curiosidad y/o necesidad.cLa formulación de preguntas relevantes que se puedan responder mediante la exploración y el examen de los datos disponibles.\nInicia con la identificación de áreas de interés y la formulación de preguntas específicas relacionadas con esos temas. Estas preguntas pueden surgir de la necesidad de resolver un problema, entender un fenómeno o explorar patrones en los datos. Un buen planteamiento de preguntas es crucial, ya que guiará todo el proceso de análisis.\n\n\n\nEl proceso de abstraer la realidad\n\n\nPreguntas e hipótesis:\nUna hipótesis es una afirmación, verificable con evidencia. En este sentido, para toda pregunta podemos responderla mediante hipótesis.\nEn particular, para responder a las preguntas en el contexto de datos, es común formular hipótesis nulas y alternativas.\nLa hipótesis nula es aquella que propone que algun parámetro toma cierto valor. Este generlamente es un punto de verdad. Si bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto. En general, planteamos el problema de tal manera que podamos rechazar la hipótesis nula, en favor de otra que llamamos alternatiba.\nQuizas, la hipotesis nula más famosa es la prueba de “significancia”. En esta se propone que un parámetro (muchas veces un efecto, o correlación) es 0, es decir, plantea que no hay efecto o relación entre las variables, mientras que la hipótesis alternativa sugiere que sí existe una relación o efecto significativo.\nEstas hipótesis son fundamentales para establecer una base objetiva para el análisis y para evaluar las evidencias encontradas en los datos. El proceso de plantear preguntas y formular hipótesis es el primer paso en el análisis de datos, ya que establece una guía clara para el enfoque y la dirección del trabajo. Al identificar preguntas y establecer hipótesis, se crea un marco sólido que orientará la exploración y el análisis de los datos disponibles.\n\n\n\n\n\n\nTaller 1: Pregunta 2 - Investigando sobre países:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1.html#respondiendo-desde-los-datos",
    "href": "sesion1.html#respondiendo-desde-los-datos",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\n\nInferencia estadística\nInferencia se refiere al proceso de hacer generalizaciones de una población a partir de una muestra de esa población. En particular, la idea es que si tenemos un conjunto de datos (muestra) obtenido de una población más grande, el cual es representativo de esta, podemos utilizar métodos estadísticos para sacar conclusiones sobre las características y propiedades de esa población en su totalidad.\n\n\n\nPoblación y Muestra\n\n\nEl proceso de inferencia estadística se basa en el principio de que una muestra bien seleccionada puede proporcionar información valiosa sobre la población en general. Mediante el análisis de la muestra, podemos estimar parámetros poblacionales, como la media, la proporción o la desviación estándar, y también podemos construir intervalos de confianza para estimar el rango dentro del cual se espera que se encuentren estos parámetros.\nEl uso de la inferencia estadística es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una población en particular. Por ejemplo, en lugar de encuestar a todos los ciudadanos de un país, es mucho más factible encuestar una muestra representativa y utilizar esa información para hacer suposiciones sobre la opinión de la población en general.\n\nEstadígrafos y el Teorema del Límite central\nEntonces, en cada muestra que tenemos podemos calcular aproximaciones a los parámetros poblacionales de interés. Estos son los llamados estadísgrafos \nDado que por cada muestra que tenemos, vamos a calcular un estadígrafo este es en si mismo una variable aleatoria. Tiene su propia distribución, media y varianza!\n\nEl estadígrafo más conocido es el promedio o media muestral.\n\n\n\nEstadigrafos más comunes\n\n\nCada estimador es una función de la muestra, por ende para cada muestra que tengamos obtendremos un valor numérico específico para el estimador. Por este motivo, cuando estamos trabajando con una única muestra específica, tenemos un único valor del estimador, o estimador puntual.\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población, por lo cual un primer camino tentador es usar el estimador puntual para tomar una decisión. Como nunca podemos conocer el verdadero parámetro, tampoco podemos saber a ciencia cierta si el estimador puntual es cercano a este.\n¿Cómo conectamos estadpigrafos y parámetros?\nEl teorema del límite central, nos dice que, bajo ciertas condiciones, la distribución de las medias muestrales de una población se aproxima a una distribución normal a medida que el tamaño de la muestra aumenta, independientemente de la forma de la distribución original de la población. Este teorema es esencial en inferencia estadística y tiene amplias aplicaciones en análisis de datos y toma de decisiones.\n\n\n\nLa media muestral se distribuye normal, sin importar la distribución de la variable subyacente\n\n\nFormalmente, el Teorema del Límite Central establece lo siguiente:\n\\[\\bar{x} \\sim_a N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) \\]\nSupongamos que tenemos una población con media μ y desviación estándar σ finitas. Si tomamos muestras aleatorias de tamaño n de esta población y calculamos la media muestral de cada muestra, entonces, a medida que n tiende a infinito, la distribución de estas medias muestrales se aproximará a una distribución normal con media μ y desviación estándar σ/√n.\nEn otras palabras, sin importar la distribución original de la población, cuando el tamaño de la muestra es suficientemente grande, la distribución de las medias muestrales seguirá una forma de campana similar a la distribución normal. Este resultado es fundamental para realizar inferencias sobre la población a partir de muestras, ya que nos permite aplicar métodos basados en la distribución normal incluso cuando la población original no sigue una distribución normal.\n\n\nError estándar\n\nCorresponde a un estimador de la desviación estándar del estimador.\nIdentifica que tan lejos estamos del verdadero valor poblacional.\nPara la media muestral:\n\n\\[ SE = \\frac{S_y}{\\sqrt{n}}\\]\nSe utiliza para evaluar a los estimadores, mediante pruebas de hipotesis y construir intervalos de confianza\n\nSi se conoce un estimador y su desviación estándar, podemos saber qué tan precisa es la estimación (mucha o poca varianza), pero no podemos saber si el estimador está cercano o no a su valor verdadero en la población (el cual no conocemos).\nNunca (o casi nunca) podemos conocer el valor verdadero de los parámetros en la población.\nSí se puede construir un conjunto de valores que contienen el parámetro poblacional con alguna probabilidad (llamada el nivel de confianza).\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n\n\nInferencia sobre Estadígrafos y parámetros - Conectados por el Teorema del Límite central\n\\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hipótesis\np-valor\n\n\n\n\n\nIntervalos de confianza\nUna primera manera de aproximarnos a los parámetros poblacionales (particularmente a la esperanza) es mediante la construcción de intervalos de confianza.\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un límite inferior y un límite superior, con cierta probabilidad.\n\n¿De dónde saco los valores críticos?\nLos valores críticos de una distribución los obtenemos de una tabla de distribución o para calcular podemos usar excel, R o en python:\nscipy.stats.t.isf(alpha, n-p)\nSi estamos trabajando con dos colas, usar alpha/2 porque la probabilidad de error la estamos repartiendo a ambas colas.\n\n\n\n\n\n\n[Matemáticamente] Caso 1: Varianza conocida\n\n\n\nSumpongamos que tenemos una muestra aleatoria: \\(y_1, y_2, \\dots, y_n\\) de una población \\(Y\\sim N(\\mu, \\sigma^2)\\)\n\nLa media muestral \\(\\bar{y}= \\frac{1}{n}\\sum_{i=1}{n}y_i\\)\n\nSu esperanza es: \\(E(\\bar{y}) =\\mu\\)\nSu varianza es: \\(var(\\bar{y}) = \\frac{\\sigma^2}{n}\\)\nse distribuye normal, tal que podemos estandarrizar: $ N(0,1) $\n\n\nEntonces, podemos describir que:\n$ P( -1.96 &lt; &lt; 1.96 ) = 0.95 $$\n$ P( {y}- &lt; &lt; {y} + ) = 0.95 $\n\neste intervalo es aleatorio, porque \\(\\bar{y}\\) es diferente en cada muestra.\npara el 95% de las muestras elatorias, el intervalo construido de esta manera contendrá a \\(\\mu\\)\n\n\n\n\n\n\n\n\n\n[Matemáticamente] Caso 2: Varianza desconocida\n\n\n\n\nSupongamos que tenemos una muestra aleatoria \\(y_1, y_2, \\dots, y_n\\) de una población \\(y\\sim N(\\mu, \\sigma^2 )\\)\nUsamos la estimación de la desviación estándar muestral: \\[ S_y = \\sqrt{\\frac{1}{n-1} \\sum{i=1}{n}(y_i - \\bar{y})^2} \\]\nY si estandarizamos \\(\\bar{y}\\): \\[ \\frac{\\bar{Y} - \\mu_y }{ \\frac{S}{\\sqrt{n}}} \\sim t_{n-1} \\]\nPodeos constrir un intervalo de la porbabilidad de estar al 95 con el valor critico c adecuado a los grados de libertad n-1:\n\n\\[ P( -c &lt;   \\frac{\\bar{Y} - \\mu_y }{ \\frac{ S}{\\sqrt{n}}}  &lt; c  ) =0.95  \\]\n\\[ P(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) 0.95\\]\n\nSi llamamos al error estandar SE: \\(SE(\\bar{y})=\\frac{S}{\\sqrt{n}}\\)\nEl IC es: \\[ (\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) \\]\n\n\n\n\n\nEl intervalo es una muestra aleatoria\nEsto quiere decir que para cada muestra podemos construir un intervalo.\nAsí como el estimador es una variable aleatoria, esto también es cierto para los intervalos de confianza. Por eso también se les llama intervalos aleatorios, ya que con diferentes muestras obtendremos un diferente estimador e intervalo.\nPor ende, supongamos que contamos con 20 muestras, entonces construiremos 20 intervalos de confanza diferentes para los 20 estimadores puntuales.\n\nInterpretación de intervalo de confianza\nPensemos en un 95% de confianza (un valor usual). Esto quiere decir, que si se repitiera este ejercicio muchas veces y construyéramos un intervalo de esta forma el 95% de ellos contendría el verdadero parámetro poblacional.\nUn elemento importante a considerar es que esto no significa que con 95% de certeza el parámetro está exactamente en estos valores. Por ejemplo, al 95% de confianza con 20 intervalos 19 contendrán el parámetro.\n\n\n\nPruebas de hipótesis\n\nUna forma de verificar hipotesis sobre los parámetros es mediante el contraste de hipótesis.\n\nEmpezamos suponiendo que hay una distribución conocida para el estadígrafo, centrada en un valor específico. Y nos preguntamos, si esto fuea verdad ¿qué tan probable es la muestra que tengo?\n\nLlamamos la hipótesis a probar Ho, y su alternativa H1.\n\n\n\n\n\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:\n\n\nTipo I: Rechazar Ho cuando es cierta\nTipo II: No rechazar Ho cuando es falsa.\n\n\n\n\nSe elige nivel de significancia de contraste (α) = probabilidad de cometer error Tipo I. Típicamente α = 0,01, 0,05, 0,10.\nPara contrastar una hipótesis con su alternativa, debemos elegir:\n\nUn estadístico de contraste\nUna regla de rechazo, la cual depende de un valor crítico.\n\n\n\n\n\nPrueba de significancia\nDefinimos la prueba de hipótesis de significancia como aquella que indica si un estimador \\(\\hat{T}\\) es 0.\n\\[ H_0: T =0\\text{ vs }H_1: T \\neq 0 \\]\nEl Valor de probabilidad (ó p-valor) es el nivel probabilidad más alto para el cual no podemos rechazar la hipótesis nula de la prueba de significancia.\nEjemplo: $H_0: $ y en la muestra especifica t= 1.52:\n\\[ P-valor = P(T&gt;1.52 \\vert h_0)= 1- \\phi(1.52)=0.0065\\]\n\nel mayor nivel de significancia estadistica al cual no rechazamos \\(H_0\\) es 6.5%\nla probabilidad de observar un velor \\(T\\geq 1.52\\) cuando \\(H_0\\) es cierta es en un 6.5 de las muestras.\nP-valores bajos dan evidencia en contra de \\(H_0\\), ya que la probabilidad de observarlo si \\(H_0\\) es cierta es bajo.\n\n\n\nEjemplo de aplicación: Peso de los Pingüinos Palmer\nConsideremos los datos de los pingüinos Palmer. Los datos “Palmer Penguins” son un conjunto que detalla medidas morfológicas y características de tres especies de pingüinos: Adelie, Gentoo y Chinstrap. Recopilados por el Dr. Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nEn el contexto de los pingüinos y el peso de su población, podríamos tomar una muestra de pingüinos y calcular un intervalo de confianza para el peso promedio. Esto nos daría una estimación del peso promedio de la población total, junto con la confianza en que este valor estimado es preciso.\nEs importante tener en cuenta que el proceso de inferencia estadística se basa en suposiciones y en el uso adecuado de técnicas estadísticas.\nLa elección de la muestra, la interpretación de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hipótesis, sobre el peso de los pingüinos.\nPrimero calcularemos el promedio muestral y lo veremos en el contexto de los datos observados:\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los pingüinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribución del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribución de Peso de Pingüinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral. De que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus características.\nPor ejemplo, consideremos que de esta población de pingüinos obtenemos 1000 muestras de 50 individuos cada una. Si graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tamaño de muestra, más nos alejamos de la distribución normal.\nSi reducimos el número de repeticiones tambieé.\n\n\nimport numpy as np\n\n# Definir el tamaño de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y cálculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gráfico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribución de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\n\nIntervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n# Obtener una muestra simple de 40 pingüinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error estándar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviación estándar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)\n\nIntervalo de Confianza para el Peso:\n(3669.674745507001, 4119.075254492999)\n\n\nEl resultado será un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los pingüinos en la población, con un nivel de confianza del 95%.\n¿Como nos fue? ¿Contiene al verdadero valor?\n\n\n\nComparaciones de grupos\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribución de tamaño por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\ntabla_doble_entrada\n\n\n\n\n\n\n\n\nspecies\nsex\nPromedio\nVarianza\n\n\n\n\n0\nAdelie\nFemale\n3368.835616\n72565.639269\n\n\n1\nAdelie\nMale\n4043.493151\n120278.253425\n\n\n2\nChinstrap\nFemale\n3527.205882\n81415.441176\n\n\n3\nChinstrap\nMale\n3938.970588\n131143.605169\n\n\n4\nGentoo\nFemale\n4679.741379\n79286.335451\n\n\n5\nGentoo\nMale\n5484.836066\n98068.306011\n\n\n\n\n\n\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hipótesis:\n¿Existe una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”?\n\nHipótesis Nula (H0):\n\nNo hay diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\n\nHipótesis Alternativa (H1):\n\nExiste una diferencia significativa en el peso promedio entre los pingüinos machos y las pingüinas hembras en la especie “Adelie”.\nPara probar esta hipótesis, podrías utilizar una prueba de hipótesis para comparar las medias de las muestras de peso de los pingüinos machos y hembras en la especie “Adelie”. Esto te permitiría determinar si la diferencia observada en el peso promedio es lo suficientemente grande como para considerarse estadísticamente significativa.\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribución de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribución de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nA simple vista podriamos pensar ambos grupos son diferentes. Es más claro si dibujamos el promedio muestral observado.\n\n# Crear un gráfico de densidad con líneas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Pingüinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()\n\n\n\n\nSi construimos una prueba t de diferencia de medias:\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los pingüinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estadística t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gráfico de comparación de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Machos y Hembras de Pingüinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\nEstadística t: 13.126285923485874\nValor p: 6.402319748031793e-26\n\n\n\n\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas. Para esto podriamos usar una prueba ANOVA.\nEn este código, primero cargamos el conjunto de datos “Penguins” y luego creamos dos subconjuntos separados para machos y hembras. Después, utilizamos la función stats.f_oneway() para realizar una prueba ANOVA para comparar los pesos entre hembras y machos. El resultado incluye la estadística F y el valor p.\nEl valor p nos indica si hay una diferencia significativa entre los grupos. Si el valor p es menor que un umbral de significancia (por ejemplo, 0.05), podríamos rechazar la hipótesis nula y concluir que hay una diferencia significativa en el peso entre hembras y machos de diferentes islas.\nRecuerda que, antes de realizar una prueba ANOVA, es importante verificar las suposiciones necesarias, como la normalidad y la homogeneidad de varianzas en los grupos. Si estas suposiciones no se cumplen, podría ser necesario considerar otras pruebas estadísticas o transformaciones de los datos.\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estadística F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\nEstadística F: 72.96098633250911\nValor p: 4.897246751596325e-16\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Calcular las medias de peso por género e isla\nmedias_peso = penguins.groupby(['species', 'island', 'sex'])['body_mass_g'].mean().reset_index()\n\n# Crear un gráfico de barras con puntos y intervalos de confianza\nplt.figure(figsize=(10, 6))\nsns.barplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', errorbar='sd', palette=['pink', 'blue'])\n#sns.boxplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', palette=['blue', 'pink'])\nplt.title('Comparación de Peso entre Hembras y Machos por Isla')\nplt.xlabel('Especie e Isla')\nplt.ylabel('Media de Peso (g)')\nplt.legend(title='Sexo')\nplt.show()\n\n\n\n\n\n\nExperimentos Aleatorios y pruebas A/B\nUn experimento estadístico es un enfoque científico que busca establecer relaciones de causalidad y obtener conclusiones sobre cómo ciertas variables afectan a otras. Los experimentos estadísticos se diseñan para manipular deliberadamente una o más variables independientes y observar los efectos que tienen sobre una variable dependiente. Al controlar y manipular las variables de interés, los experimentos permiten a los investigadores hacer afirmaciones más sólidas sobre las relaciones causales.\nUna prueba A/B, también conocida como prueba de división, es una técnica utilizada en la investigación y el análisis para comparar dos variantes o grupos con el fin de determinar cuál de ellos produce un mejor resultado en términos de rendimiento, efectividad o preferencia. En una prueba A/B, se selecciona un grupo de muestra y se divide en dos grupos, uno que experimenta la variante “A” (por ejemplo, una versión actual) y otro que experimenta la variante “B” (por ejemplo, una versión modificada). Luego, se recopilan datos y se comparan los resultados de ambos grupos para determinar cuál variante es más efectiva. Las pruebas A/B son comunes en marketing, diseño de productos y desarrollo web para tomar decisiones informadas sobre mejoras y optimizaciones.\nLas pruebas A/B es son ampliamente utilizado en diversas áreas, como el marketing, la investigación de usuarios y el diseño de productos. En una prueba A/B, se seleccionan dos grupos de muestra: uno experimenta la versión original (A) y el otro experimenta una variante modificada (B). La idea detrás de una prueba A/B es evaluar si la variante B produce un efecto significativamente diferente en una métrica de interés en comparación con la variante A.\nMediante la asignación aleatoria de los participantes a los grupos A y B, y al controlar las condiciones en las que se les presenta cada variante, se reduce la posibilidad de sesgos y se permite un análisis causal más confiable. Al comparar las diferencias observadas en los resultados entre los grupos A y B, es posible inferir si la variante B tiene un impacto significativo en la variable de interés.\nSin embargo, es importante tener en cuenta que aunque las pruebas A/B proporcionan evidencia de asociación causal, no garantizan que la causalidad sea absoluta. Otros factores no controlados pueden influir en los resultados. Para obtener una comprensión más completa de la causalidad, los experimentos controlados aleatorizados y el uso de métodos de diseño experimental sólidos son esenciales. Las pruebas A/B son una herramienta poderosa para explorar causas y efectos en condiciones controladas y analizar el rendimiento relativo de diferentes opciones.\nVeamos un ejemplo en la práctica. Este es parte del ejercicio de aplicación."
  },
  {
    "objectID": "sesion1.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "href": "sesion1.html#caso-aplicación-de-ab-testing-para-promoción-de-marketing",
    "title": "Sesión 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hipótesis)",
    "section": "Caso: Aplicación de A/B testing para promoción de Marketing",
    "text": "Caso: Aplicación de A/B testing para promoción de Marketing\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreación de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows × 3 columns\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 3 -Ejemplo AB test en Marketing:\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA.\n\n\n\n\n\nBuenas prácticas en análisis de datos\n\nImportancia de la Adquisición y Almacenamiento de Datos\nLa adquisición y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de análisis. La calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes. En esta sección, exploraremos la importancia de esta etapa y cómo afecta todo el flujo de trabajo de la ciencia de datos.\nGarantía de Calidad y Fiabilidad en la Obtención de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean sólidas. La calidad de los datos está relacionada con la precisión, integridad y consistencia de la información que recopilamos. Asegurarnos de que los datos sean precisos desde el principio minimiza la posibilidad de errores en análisis posteriores. Exploraremos técnicas y prácticas para verificar la calidad de los datos y cómo mitigar posibles fuentes de error.\nExploración de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros. Cada fuente tiene sus propias características y potenciales sesgos. Comprender las diferencias entre estas fuentes y cómo pueden influir en los resultados es crucial para tomar decisiones informadas. Analizaremos ejemplos de cómo la elección de la fuente de datos puede afectar las conclusiones y cómo evaluar la confiabilidad de las fuentes.\n\n\nMetodologías de Levantamiento y Adquisición de Datos:\nEl proceso de obtención de datos implica una planificación cuidadosa. Exploraremos diversas metodologías utilizadas para recopilar datos, desde encuestas y experimentos hasta scraping de datos en línea. Cada metodología tiene sus propias ventajas y desventajas, y es importante seleccionar la más adecuada para los objetivos del análisis. Discutiremos cómo diseñar encuestas efectivas, cómo considerar la ética en la recopilación de datos y cómo aprovechar las fuentes de datos existentes.\nEsta sección nos proporcionará una base sólida para comprender cómo adquirir y almacenar datos de manera efectiva y confiable. Una vez que comprendamos cómo obtener datos de calidad, podremos avanzar con confianza en las etapas posteriores del proceso de análisis, sabiendo que estamos trabajando con una base sólida y confiable.\n\n\nDesafíos y Consideraciones:\nA medida que ingresamos al emocionante mundo del análisis de datos, nos encontramos con una serie de desafíos y consideraciones que debemos abordar de manera efectiva para garantizar el éxito de nuestro proyecto. Estos desafíos abarcan desde la protección de la privacidad de los datos hasta las complejidades de la limpieza y transformación durante la etapa de preparación.\n\n\nPrivacidad y Seguridad de los Datos:\nUno de los aspectos más críticos en el análisis de datos es la privacidad y seguridad de la información. Los datos pueden contener información sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas. Exploraremos prácticas y regulaciones para garantizar que los datos se manejen de manera ética y legal. Discutiremos cómo anonimizar los datos, utilizar técnicas de enmascaramiento y seguir las mejores prácticas para resguardar la privacidad de los individuos.\n\n\nLimpieza y Transformación durante la Preparación de Datos:\nLa etapa de preparación de datos es crucial para asegurarse de que los datos sean aptos para el análisis. Sin embargo, este proceso no está exento de desafíos. Los datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada. Exploraremos técnicas para identificar y manejar valores atípicos y faltantes, así como la importancia de la normalización y estandarización de los datos. Aprenderemos cómo transformar los datos en un formato adecuado para el análisis, incluida la reorganización de variables y la creación de nuevas características. En resumen, enfrentamos una serie de desafíos y consideraciones clave en nuestro viaje hacia el análisis de datos significativo. Desde la protección de la privacidad hasta la preparación efectiva de los datos, abordar estos desafíos de manera adecuada es esencial para garantizar que nuestras conclusiones sean sólidas, confiables y éticas.\n\n\nReproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicación de control de versiones en proyectos de preparación de datos.\n\nLa reproducibilidad y el control de versiones son componentes fundamentales para garantizar la integridad y la transparencia en el análisis de datos. Además de mantener un registro detallado de las decisiones tomadas durante el proceso, el uso de sistemas de control de versiones como GIT se vuelve esencial para mantener la trazabilidad y la colaboración efectiva en proyectos de preparación y análisis de datos.\nDocumentación Detallada del Análisis y Uso de Notebooks: Una documentación exhaustiva del análisis es esencial para comprender el flujo de trabajo, las decisiones tomadas y las transformaciones aplicadas a los datos. Los notebooks, como Jupyter Notebooks, ofrecen una herramienta excepcional para lograr esto. En cada celda de un notebook, es posible combinar explicaciones en lenguaje natural con código ejecutable y visualizaciones. Esto permite registrar no solo el qué y el cómo, sino también el porqué detrás de cada paso.\nImportancia de Mantener un Registro de los Cambios en los Datos: Cada decisión tomada durante la preparación y el análisis de datos puede tener un impacto significativo en los resultados finales. Mantener un registro detallado de estas decisiones, desde la limpieza de datos hasta la creación de variables derivadas, es crucial para comprender cómo se obtuvieron ciertos resultados. Una documentación precisa y detallada permite a otros analistas validar y replicar el análisis en el futuro.\nUso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que también es una herramienta poderosa en el análisis de datos. Permite rastrear cada modificación realizada en el código y en los documentos, incluidos los notebooks. Cada cambio es registrado como un “commit”, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\n\n\n\nUn esquema de git por Allison Horst @allison_horst\n\n\nAplicación de Control de Versiones en Proyectos de Preparación de Datos:* La aplicación de GIT en proyectos de preparación de datos agrega un nivel adicional de transparencia y colaboración. Los repositorios de GIT almacenan no solo los datos originales, sino también los notebooks y scripts utilizados en el proceso. Esto permite a los analistas colaborar en un entorno controlado y mantener un historial de cambios. En caso de que surjan problemas o se necesite retroceder en el tiempo, GIT ofrece la capacidad de volver a versiones anteriores de manera segura.\nLa combinación de documentación detallada a través de notebooks y el uso de sistemas de control de versiones como GIT proporciona una base sólida para el análisis de datos reproducible y transparente. Esto no solo facilita la comprensión y validación de los resultados, sino que también fomenta la colaboración y la mejora continua en proyectos de preparación y análisis de datos.\n\n\n\n\n\n\nActividad de proyecto - Inicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del análisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustaría trabajar.\nPropongan una o dos preguntas de investigación y las hipotesis que las responderían.\n\nLa siguiente sesión, vamos a explorar los datos y empezar los primeros pasos en su análisis."
  },
  {
    "objectID": "sesion3.html#tipos-de-datos",
    "href": "sesion3.html#tipos-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tipos de datos",
    "text": "Tipos de datos\nAntes de adentrarnos en la preparación de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje común.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relación a:\n\nEstructura\nTamaño\nOperacionalización\nTemporalidad y unidad de análisis.\n\n\nSegún estructura de los datos\n\nLa estructura de un dato se refiere a su formato y codificación.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\nDatos estructurados\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o números.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\nDatos No estructurados\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, imágenes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteorológicos\nColecciones de documentos digitalizados: Facturas, registros, correos electrónicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\nEstructurados vs no estructuraods\n\nDatos estructurados están en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la información, pero los no estructurados son más abundantes.\n\n\n\n\nDatos según tamaño\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son difíciles de gestionar, procesar y analizar con herramientas y métodos tradicionales. Big Data involucra terabytes o incluso petabytes de información y generalmente requiere tecnologías y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos más pequeños y manejables en comparación con Big Data. Estos conjuntos de datos son más accesibles y pueden ser procesados y analizados utilizando herramientas y métodos convencionales. A menudo, Small Data se centra en obtener información valiosa de fuentes limitadas y específicas.\n\n\n\nDatos según tipo y su operacionalización\n\nLas variables en análisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan números medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos números o factores que representen categorías, facilitando su análisis cuantitativo.\n\n\n\n\n\nDatos según temporalidad y unidad de análisis.\nOtra manera de clasificar los datos con relación a la temporalidad en la cual son tomados y cuál es la unidad de análisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales\n\n\nCorte transversal\n\n\n\nSerie temproal\n\n\n\nPanel"
  },
  {
    "objectID": "sesion3.html#leyendo-datos-en-pandas",
    "href": "sesion3.html#leyendo-datos-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a través de la familia de funciones read_tipo\n\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read_*\n\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: automático o definido por el usuario\nDatetime parsing: puede combinar información de múltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con números con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion3.html#formato-csv-valores-separados-por-comas",
    "href": "sesion3.html#formato-csv-valores-separados-por-comas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de información: no sabemos qué son las columnas (números, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qué tipo para transformar cada columna basado en lo que parece ser\n\n¿Y qué pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion3.html#csv-en-pandas",
    "href": "sesion3.html#csv-en-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nBásica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nBásica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representación de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion3.html#json-java-script-object-notation",
    "href": "sesion3.html#json-java-script-object-notation",
    "title": "Sesión 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, números, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores numéricos"
  },
  {
    "objectID": "sesion3.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion3.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion3.html#orientaciones-posibles-de-json",
    "href": "sesion3.html#orientaciones-posibles-de-json",
    "title": "Sesión 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion3.html#extensible-markup-language-xml",
    "href": "sesion3.html#extensible-markup-language-xml",
    "title": "Sesión 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato más antiguo y auto descriptivo, con estructura jerárquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un método incorporado en Python.\nSe puede usar la librería lxml (también ElementTree)\n\n\nEjemplo XML"
  },
  {
    "objectID": "sesion3.html#formatos-binarios",
    "href": "sesion3.html#formatos-binarios",
    "title": "Sesión 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¿Qué es un formato binario?\nPickle: Python’s built-in serialization\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n\n\nHDF5: Librería para almacenar gran cantidad de datos científicos\n\nFormato jerárquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresión\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de cálculo tiene múltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion3.html#bases-de-datos-relacionales",
    "href": "sesion3.html#bases-de-datos-relacionales",
    "title": "Sesión 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\n\nLas bases de datos relacionales son similares a los marcos de datos múltiples pero tienen muchas más características\n\nvínculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos \n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayoría de los sistemas de bases de datos a través de un API común\nSintaxis similar (¡pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.) \nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n`import sqlalchemy as sqla\ndb = sqla.create_engine(‘sqlite:///mydata.sqlite’)\npd.read_sql(‘select * from test’, db)`"
  },
  {
    "objectID": "sesion3.html#dirty-data-punto-de-vista-estadístico",
    "href": "sesion3.html#dirty-data-punto-de-vista-estadístico",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista estadístico",
    "text": "Dirty data: punto de vista estadístico\n\nLos datos son generados desde algún proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsión: algunas muestras se corrompieron por un proceso\nSesgo de selección: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: Left and right censorship: los usuarios van y vienen del escrutinio\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisión y simplicidad"
  },
  {
    "objectID": "sesion3.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion3.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores están missing, corrompidos, erróneos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion3.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "href": "sesion3.html#dirty-data-punto-de-vista-del-experto-en-un-área",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un área",
    "text": "Dirty Data: Punto de vista del experto en un área\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¿Qué ocurrió?\nLos expertos en un área llevan un modelo implícito de los datos que están testeando\nNo siempre necesitas ser un experto en un área para hacer esto\n\n¿Puede una persona correr 500 km por hora?\n¿Puede una montaña en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido común"
  },
  {
    "objectID": "sesion3.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion3.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesión 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinación de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregación es menos susceptible a los errores numéricos\nSer cuidadoso, puede que los datos estén bien…"
  },
  {
    "objectID": "sesion3.html#dónde-se-origina-el-dirty-data",
    "href": "sesion3.html#dónde-se-origina-el-dirty-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "¿Dónde se origina el dirty data?",
    "text": "¿Dónde se origina el dirty data?\n\nLa fuente está mal, por ejemplo, una persona la ingresó de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integración de diferentes sets de datos causa problemas\nPropagación del error: se magnifica un error"
  },
  {
    "objectID": "sesion3.html#problemas-dirty-comunes",
    "href": "sesion3.html#problemas-dirty-comunes",
    "title": "Sesión 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs. New York\nPérdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs. dos\nDatos truncados: “Janice Keihanaikukauakahihuliheekahaunaele” se vuelve “Janice - Keihanaikukauakahihuliheek” en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposición\nProblemas de formato: 2017-11-07 vs. 07/11/2017 vs. 11/07/2017"
  },
  {
    "objectID": "sesion3.html#data-wrangling",
    "href": "sesion3.html#data-wrangling",
    "title": "Sesión 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato más significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representación a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion3.html#tidy-data",
    "href": "sesion3.html#tidy-data",
    "title": "Sesión 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para análisis de corte transversal nuestro ideal es trabajar con Tidy Data\n“Tidy Data is a standar way of mapping the meaning of a dataset to its structure¨\nUn tipo de estructura de datos útil para poder realizar modelos y análisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un estándar deseable para analizar datos.\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observación (registro) forma una fila.\nCada dato (valor) está en una celda de la tabla.\n\n\nCon Tidy data podemos tener una estandarización en el tratamiento de los datos:\n\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n\n¿Porqué datos Tidy?\n\nEstandarizanción\n\nLos datos organizados te permiten ser más eficiente al utilizar herramientas existentes diseñadas específicamente para realizar las tareas que necesitas hacer, desde la selección de porciones de tus datos hasta la creación de mapas de tu área de estudio.\nUtilizar herramientas existentes te ahorra tener que construir todo desde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual puede ser consume tiempo y desmotivante).\nAfortunadamente, existen muchas herramientas diseñadas específicamente para transformar datos desorganizados en datos organizados (por ejemplo, en el paquete tidyr). Al estar mejor preparado para transformar tus datos en un formato organizado, podrás llegar más rápido a tus análisis y comenzar a responder las preguntas que estás planteando.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nFacilitar la colaboración\n\nDado que nuestros colegas pueden emplear las mismas herramientas de manera familiar. Tanto si consideramos a los colaboradores como compañeros actuales, su futuro propio o futuros colegas, la organización y compartición de datos de manera coherente y predecible implica menos ajustes, tiempo y esfuerzo para todos.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nSimplifica la reproducibilidad\n\nLos datos organizados también facilitan la reproducción de análisis, ya que son más fáciles de comprender, actualizar y reutilizar. Al utilizar herramientas que esperan datos organizados como entrada, puedes construir e iterar flujos de trabajo realmente potentes. Y cuando tienes entradas de datos adicionales, ¡no hay problema en volver a ejecutar tu código!\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\n\n¿Siempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean útiles o “desordenadas”.\nEs importante tener en cuenta que siempre existen múltiples formas de representar la misma información.\n\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempeño computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion3.html#datos-perdidos",
    "href": "sesion3.html#datos-perdidos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\n\nse pueden escoger filas o columnas\n\n\nLlenar/ reemplazar :\n\n\ncon un valor por default\ncon un valor interpolados, otros\n\n\n\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion3.html#fitrando-y-limpiando",
    "href": "sesion3.html#fitrando-y-limpiando",
    "title": "Sesión 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cuál de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas específics para chequear por duplicados, e.g. chequear solo la columna key.\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion3.html#errores-de-registro-y-textos",
    "href": "sesion3.html#errores-de-registro-y-textos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\nUno de los errores de registro más típico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) “quiebra” o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14”\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14”\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD” \"AbCd\".lower() # \"abcd\""
  },
  {
    "objectID": "sesion3.html#errores-de-formato",
    "href": "sesion3.html#errores-de-formato",
    "title": "Sesión 2: Preparando los datos",
    "section": "Errores de formato",
    "text": "Errores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a números eliminando el símbolo de dólar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_95173/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True."
  },
  {
    "objectID": "sesion3.html#transformando-strings",
    "href": "sesion3.html#transformando-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformando strings",
    "text": "Transformando strings\n\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n2\n\n\n\n#s.index(':') # ValueError raised\n\n\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado\n\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n2\n\n\n\ns.find(':') # -1\n\n-1\n\n\n\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string\n\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\nFalse"
  },
  {
    "objectID": "sesion3.html#métodos-para-strings",
    "href": "sesion3.html#métodos-para-strings",
    "title": "Sesión 2: Preparando los datos",
    "section": "Métodos para strings",
    "text": "Métodos para strings\n\n\nCualquier columna o serie puede tener métodos string (e.g. replace, split) aplicado a la serie completa\nEstá vectorizado para columnas completas o incluso el dataframe (lo cual lo hace rápido) Se debe usar .str. (es importante el .str).\nEjemplo:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion3.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion3.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets\n\n\nEliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro análisis.\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n\n\n\n\nReshapes/re formato\n\nEl “reshape” o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposición de filas y columnas para adaptarse a un formato específico.\nEsto puede implicar la transformación de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el análisis, la visualización o la aplicación de modelos estadísticos.\nEl “reshape” es comúnmente realizado en la manipulación de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de cálculo.\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n\n\n\n\nCrear variables dummies / dicotómicas\n\nMuy útil para representar información cualitativa.\nMuy usando en análisis de regresión y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o más indicadores que toman valor 0 ó 1.\nSe pueden generar mediante pd.get_dummies(df[‘key’])\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicotómicas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicotómicas para las razas de interés\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0"
  },
  {
    "objectID": "sesion3.html#unir-datasets",
    "href": "sesion3.html#unir-datasets",
    "title": "Sesión 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos métodos principales:\n\nApliar/concatenar/append Consiste en agregar un dataset a continuación de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join Las uniones combinan DataFrames utilizando columnas en común como clave. Puedes realizar diferentes tipos de uniones, como “inner”, “outer”, “left” y “right”"
  },
  {
    "objectID": "sesion3.html#apilar-dataframes",
    "href": "sesion3.html#apilar-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "1. Apilar dataframes",
    "text": "1. Apilar dataframes\nCreemos los dataframes:\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n\n\nHagamos el apilar. Atención a los index\n\ndf.append(df2)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_95173/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9\n\n\n\n\n\n\n\n\nUsemos ignorar index\n\ndf.append(df2, ignore_index=True)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_95173/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9"
  },
  {
    "objectID": "sesion3.html#unir-dataframes",
    "href": "sesion3.html#unir-dataframes",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Unir dataframes",
    "text": "2. Unir dataframes\nUtilizando la función pd.merge().\n\n# Crear dos DataFrames con columnas en común\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Unión interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Unión externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Unión interna:\")\nprint(inner_join)\n\nprint(\"\\nUnión externa:\")\nprint(outer_join)\n\nUnión interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUnión externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0"
  },
  {
    "objectID": "sesion3.html#transformaciones-estadístocas",
    "href": "sesion3.html#transformaciones-estadístocas",
    "title": "Sesión 2: Preparando los datos",
    "section": "Transformaciones estadístocas:",
    "text": "Transformaciones estadístocas:\n\nOutliers\n\nUn valor atípico, también conocido como valor outlier en inglés, es un punto de datos que difiere significativamente del patrón general de los demás datos en un conjunto. Estos valores son inusuales en relación con el resto de la distribución de los datos y pueden ser considerablemente más altos o más bajos que los valores típicos del conjunto.\nLa identificación de valores atípicos es importante por varias razones: 1. Calidad de los datos 2. Impacto en estadísticas y modelos 3. Anomalías y problemas reales 4. Toma de decisiones informada\nVeamos un ejemplo:\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores atípicos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuartílico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los límites para identificar valores atípicos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores atípicos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores atípicos:\")\nprint(outliers)\n\nValores atípicos:\n   Valor\n6    200"
  },
  {
    "objectID": "sesion3.html#estandarización-de-datos",
    "href": "sesion3.html#estandarización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "2. Estandarización de Datos",
    "text": "2. Estandarización de Datos\nLa estandarización es un proceso importante en el análisis de datos y el aprendizaje automático. Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviación estándar de uno. Esta transformación se logra mediante la fórmula:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nDonde $ x $ es el valor original, $ $ es la media de los valores y $ $ es la desviación estándar.\nLa estandarización es especialmente útil cuando se tienen variables con diferentes escalas y distribuciones, ya que permite compararlas de manera equitativa y mejorar la eficacia de los algoritmos de aprendizaje automático al mitigar el impacto de las diferencias en las magnitudes de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviación estándar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarización de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviación estándar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarización de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]"
  },
  {
    "objectID": "sesion3.html#normalización-de-datos",
    "href": "sesion3.html#normalización-de-datos",
    "title": "Sesión 2: Preparando los datos",
    "section": "Normalización de Datos",
    "text": "Normalización de Datos\nLa normalización es un proceso esencial en el análisis de datos y el aprendizaje automático. Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango específico, generalmente entre 0 y 1. La fórmula matemática utilizada para la normalización es:\n\\[ x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\]\nDonde $ x $ es el valor original y $x_{} $ es el valor normalizado.\nLa normalización es beneficiosa para igualar la escala de las variables y mejorar el rendimiento de los modelos que son sensibles a la magnitud de las características.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores mínimo y máximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalización min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores mínimo y máximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalización min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]"
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión",
    "href": "sesion3_slides.html#el-análisis-de-regresión",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\n\nEl análisis de regersión es una técnica en la cual buscamos encontrar una función que pueda describir la relación observada en los datos entre dos o mas variables."
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-1",
    "href": "sesion3_slides.html#el-análisis-de-regresión-1",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\n\nCaso más simple: una regresión lineal simple o univariada. T\n\nUna variable que deseamos explicar o predecir (Y) como función de otra (X).\nBuscamos la pendiente e intercepto de una funciónla recta de la forma:\n\n\n\n\\[Y = \\alpha + \\beta X\\]"
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-2",
    "href": "sesion3_slides.html#el-análisis-de-regresión-2",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión"
  },
  {
    "objectID": "sesion3_slides.html#una-perspectiva-histórica",
    "href": "sesion3_slides.html#una-perspectiva-histórica",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "Una perspectiva histórica:",
    "text": "Una perspectiva histórica:\n\n\n\nCurso Análisis de Datos - Sesión 3"
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-3",
    "href": "sesion3_slides.html#el-análisis-de-regresión-3",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\nPara esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistemática o que se puede explicar directamente con una o más variables independientes (Xs o regresores) y otra que es no sistemática o error (\\(\\mu\\) o \\(epsilon\\)) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fenómeno.\n\nLa parte sistemática entonces la describimos con una forma funcional, que depende de otras variables o regresores."
  },
  {
    "objectID": "sesion3_slides.html#el-análisis-de-regresión-4",
    "href": "sesion3_slides.html#el-análisis-de-regresión-4",
    "title": "Sesión 3: Introducción al análisis de regresión",
    "section": "El análisis de regresión",
    "text": "El análisis de regresión\nEsta forma funcional puede ser lineal univariada, lineal múltiple o no lineal. El tipo de forma funcional, definirá el tipo de regresión de la que estemos hablando.\nVentajas del análisis de regersión: es facil decsribir cuantitaivamente una rlación.\nEsquemáticamente, los elementos son:"
  },
  {
    "objectID": "taller1_aplicacion_educ.html",
    "href": "taller1_aplicacion_educ.html",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "Vamos a usar una situación ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de identificación revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy rápidamente y en otras, vamos a tener mayor énfasis.\n\n\nNuestro objetivo es responder la siguiente pregunta ficticia de investigación:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\n\nLa pregunta de investigación se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioeconómico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opción de asistir a un curso de verano intensivo durante el verano entre 5 y 6to básico.\nEl curso de verano se enfoca en mejorar las habilidades académicas de preparar la prueba de admisión a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participación en el curso en los resultados académicos de los estudiantes.\n\n\n\n\nLos datos estan disponibles en\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene información sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si participó en el curso de verano, sexo, ingreso del hogar (en logaritmo), educación de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el año 5 como para el año 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene información de cada individuo (con identificador id).\nEste dataset tiene la información si el individuo recibió la carta de invitación para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene información de rendimiento académico antes y después del curso de verano.\n\n\n\n\n\nLa idea de este taller es poner en práctica los primeros pasos para un análisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n\nTambién exploraremos los datos, usaremos estadísticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\n\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre lso pasos anteriores múltiples veces. También invlocura tomar desiciones por parte de los investigadores, por lo cual la documentación de esta fase es especialmente importante.\nEn nuestro caso, será bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser así.\nPasos que debe realizar:\n\nPreparación de los datos\n\n1.1. Cargue las tres bases de datos\n1.2. Unir los datasets\nTenemos 3 bases de datos con información diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta información, especialmente si estimaremos modelos en base a ésta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido debería tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de información del estudiante si recibió o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de unión person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que acá unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id única tiene un solo colegio, pero sirve de ejemplo en como usar más de una columna mediante.\nUsamos la función summary() para obtener una estadística descriptiva de las variables en el dataset unido.\nPreparar los datos\n\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera “tidy” (Según Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observación.\n\n(puedes profundizar y ver más ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observación.\nSi inspeccionamos el número de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del año 2 al 10. Este tipo de datos son de panel\n\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y análisi preferimos que esté en formato long. Especialmente cuando modelamos incluyendo efectos fijos También es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos melt()\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selección de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podríamos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y número de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn último paso que haremos antes de hacer estadística decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviación estándar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto específico de la medida y es más facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estadística descriptiva.\n\n\n\nHasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estadísticas descriptivas y también algunos graficos descriptivos.\n3.1 Tablas de etsádistocas descriptivas Incluya la media, la desviación estandar, la mediana, max y min, al menos.\n3.2 Gráficos de estadística descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#taller-1-pregunta-4---aplicación-datos-de-educación",
    "href": "taller1_aplicacion_educ.html#taller-1-pregunta-4---aplicación-datos-de-educación",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "Vamos a usar una situación ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de identificación revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy rápidamente y en otras, vamos a tener mayor énfasis.\n\n\nNuestro objetivo es responder la siguiente pregunta ficticia de investigación:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\n\nLa pregunta de investigación se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioeconómico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opción de asistir a un curso de verano intensivo durante el verano entre 5 y 6to básico.\nEl curso de verano se enfoca en mejorar las habilidades académicas de preparar la prueba de admisión a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participación en el curso en los resultados académicos de los estudiantes.\n\n\n\n\nLos datos estan disponibles en\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene información sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si participó en el curso de verano, sexo, ingreso del hogar (en logaritmo), educación de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el año 5 como para el año 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene información de cada individuo (con identificador id).\nEste dataset tiene la información si el individuo recibió la carta de invitación para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene información de rendimiento académico antes y después del curso de verano."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#objetivos",
    "href": "taller1_aplicacion_educ.html#objetivos",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "La idea de este taller es poner en práctica los primeros pasos para un análisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n\nTambién exploraremos los datos, usaremos estadísticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\n\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre lso pasos anteriores múltiples veces. También invlocura tomar desiciones por parte de los investigadores, por lo cual la documentación de esta fase es especialmente importante.\nEn nuestro caso, será bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser así.\nPasos que debe realizar:\n\nPreparación de los datos\n\n1.1. Cargue las tres bases de datos\n1.2. Unir los datasets\nTenemos 3 bases de datos con información diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta información, especialmente si estimaremos modelos en base a ésta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido debería tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de información del estudiante si recibió o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de unión person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que acá unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id única tiene un solo colegio, pero sirve de ejemplo en como usar más de una columna mediante.\nUsamos la función summary() para obtener una estadística descriptiva de las variables en el dataset unido.\nPreparar los datos\n\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera “tidy” (Según Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observación.\n\n(puedes profundizar y ver más ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observación.\nSi inspeccionamos el número de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del año 2 al 10. Este tipo de datos son de panel\n\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y análisi preferimos que esté en formato long. Especialmente cuando modelamos incluyendo efectos fijos También es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos melt()\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selección de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podríamos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y número de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn último paso que haremos antes de hacer estadística decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviación estándar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto específico de la medida y es más facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estadística descriptiva."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#estadística-descriptiva",
    "href": "taller1_aplicacion_educ.html#estadística-descriptiva",
    "title": "Caso aplicación: Cursos de Verano",
    "section": "",
    "text": "Hasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estadísticas descriptivas y también algunos graficos descriptivos.\n3.1 Tablas de etsádistocas descriptivas Incluya la media, la desviación estandar, la mediana, max y min, al menos.\n3.2 Gráficos de estadística descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  }
]