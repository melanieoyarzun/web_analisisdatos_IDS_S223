[
  {
    "objectID": "sesion0.html",
    "href": "sesion0.html",
    "title": "Presentaci√≥n del curso",
    "section": "",
    "text": "Hola! Soy Melanie y ser√© la docente de este curso, en el que aprender√°s los fundamentos del an√°lisis de datos, tanto desde una perspectiva te√≥rica como pr√°ctica (en Python).\nMe pueden contactar al mail melanie.oyarzun@udd.cl\n\n\nPuedes ver las slides de este documento en slides seseion introductoria\n\n\n\n\n\n\n\n\nRevisaremos los objetivos del curso, metodolog√≠a, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hip√≥tesis."
  },
  {
    "objectID": "sesion0.html#links",
    "href": "sesion0.html#links",
    "title": "Presentaci√≥n del curso",
    "section": "",
    "text": "Puedes ver las slides de este documento en slides seseion introductoria"
  },
  {
    "objectID": "sesion0.html#en-la-sesi√≥n-de-hoy",
    "href": "sesion0.html#en-la-sesi√≥n-de-hoy",
    "title": "Presentaci√≥n del curso",
    "section": "",
    "text": "Revisaremos los objetivos del curso, metodolog√≠a, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hip√≥tesis."
  },
  {
    "objectID": "sesion0.html#contexto-en-el-programa-de-magister",
    "href": "sesion0.html#contexto-en-el-programa-de-magister",
    "title": "Presentaci√≥n del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la l√≠nea de desarrollo de data science.\nEsta asignatura tributa, a trav√©s de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Mag√≠ster en Data Science:\n\nAplicar teor√≠as, algoritmos, m√©todos, t√©cnicas y herramientas b√°sicas y avanzadas de Data Science para analizar, resolver y hacer una evaluaci√≥n cr√≠tica de desaf√≠os complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones.\nComunica efectivamente y argumenta sobre los resultados de su trabajo a p√∫blicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento √©tico, cumpliendo los protocolos y normas que gu√≠an su desempe√±o, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicaci√≥n de estrategias para utilizar nuevo conocimiento en data science en su √°mbito de desempe√±o."
  },
  {
    "objectID": "sesion0.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "href": "sesion0.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "title": "Presentaci√≥n del curso",
    "section": "Objetivos de la asignatura (resultados de aprendizaje)",
    "text": "Objetivos de la asignatura (resultados de aprendizaje)\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el an√°lisis de datos, utilizando lenguaje t√©cnico af√≠n.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso.\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer informaci√≥n y generar conclusiones basadas en evidencia.\nIdentificar las buenas pr√°cticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0.html#resumen",
    "href": "sesion0.html#resumen",
    "title": "Presentaci√≥n del curso",
    "section": "Resumen:",
    "text": "Resumen:"
  },
  {
    "objectID": "sesion0.html#detallado",
    "href": "sesion0.html#detallado",
    "title": "Presentaci√≥n del curso",
    "section": "Detallado",
    "text": "Detallado\n\n\n\n\n\n\nSesi√≥n 1: Respondiendo Preguntas con datos\n\n\n\n\n\nFecha: 19 agosto\nObjetivos:\n\nAprender a formular preguntas y plantear hip√≥tesis que puedan ser abordadas mediante el an√°lisis de datos.\nDesarrollar la habilidad de realizar pruebas de hip√≥tesis y comprender la interpretaci√≥n de sus resultados.\nComprender el papel del proceso de adquisici√≥n y almacenamiento en un proyecto de an√°lisis de datos.\n\nContenidos:\n\nEl proceso de an√°lisis de datos\n\nPlanteamiento de preguntas\nAdquision y almacenmiento de los datos\nPreparaci√≥n de los datos\nUna visi√≥n general a las metodolog√≠as de an√°lisis que veremos en el curso\n\nFormulaci√≥n de Preguntas y Hip√≥tesis:\n\nImportancia de definir preguntas claras y espec√≠ficas.\nDiferenciaci√≥n entre preguntas exploratorias y confirmatorias.\nCreaci√≥n de hip√≥tesis nulas y alternativas.\n\nHip√≥tesis y Variables:\n\nIdentificaci√≥n de variables independientes y dependientes.\nRelaci√≥n entre hip√≥tesis y variables a analizar.\n\nConceptos B√°sicos de Pruebas de Hip√≥tesis:\n\nDefinici√≥n de hip√≥tesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hip√≥tesis:\n\nPruebas t para comparaci√≥n de medias.\nPruebas chi-cuadrado para variables categ√≥ricas.\nPruebas ANOVA para comparaci√≥n de m√∫ltiples grupos.\n\nInterpretaci√≥n de Resultados:\n\nEvaluaci√≥n de p-values y toma de decisiones.\nSignificaci√≥n estad√≠stica vs.¬†significaci√≥n pr√°ctica.\nComunicaci√≥n de los resultados de las pruebas de hip√≥tesis.\n\nImportancia de la Adquisici√≥n y Almacenamiento de Datos:\n\nGarant√≠a de calidad y fiabilidad en la obtenci√≥n de datos.\nExploraci√≥n de diferentes fuentes de datos y su impacto en los resultados.\nMetodologias de levantamiento y adquision\n\nDesaf√≠os y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformaci√≥n durante la preparaci√≥n de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos.\n\n\nBibliografia recomendada:\nActividades:\n\nTaller 1 (incluidas en slides 1)\nProyecto clase 1: Conformaci√≥n de grupos, definici√≥n de temas, primeras hip√≥tesis y datos.\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 2:Preparando los datos\n\n\n\n\n\nFecha: 26 agosto\nObjetivos:\n\nComprender la importancia del proceso de preparaci√≥n de datos para el an√°lisis, reconociendo principios y enfoques clave junto con sus ventajas y desventajas.\nDesarrollar habilidades pr√°cticas en la preparaci√≥n de datos, identificando y abordando problemas comunes como valores faltantes, valores at√≠picos y formatos inconsistentes, as√≠ como enfoques de trabajo eficientes.\n\nContenidos:\n\nEl proceso de preparaci√≥n de los datos\n\nSignificado y relevancia de la preparaci√≥n de datos.\nEjemplos reales de c√≥mo la falta de preparaci√≥n puede afectar los resultados.\n\nPrincipios y enfoques\n\nExtract, Transform, Load (ETL): Proceso fundamental en la preparaci√≥n de datos.\nData Wrangling: T√©cnicas para dar forma y estructura a los datos.\nDatos Tidy: Organizaci√≥n y reestructuraci√≥n para un an√°lisis eficaz.\n\nBuenas Pr√°cticas en la Preparaci√≥n de Datos\n\nDocumentaci√≥n y Consistencia\n\nImportancia de la documentaci√≥n detallada.\nMantener nomenclatura y convenciones consistentes.\n\nValidaci√≥n y Verificaci√≥n\n\nValidaci√≥n cruzada y verificaci√≥n de integridad.\nCumplir con reglas y restricciones esperadas.\n\nReproducibilidad y Versionado\n\nEntorno de trabajo reproducible (Jupyter Notebooks, R Markdown).\nUtilizaci√≥n de sistemas de control de versiones (GIT).\n\nComunicaci√≥n y Validaci√≥n Colaborativa\n\nComunicaci√≥n clara de pasos y resultados.\nValidaci√≥n intermedia con colaboradores para feedback.\n\nSeguridad y privacidad de los datos\n\nProblemas comunes¬†presentes en datos\n\nValores faltantes:¬†\n\nEstrategias para manejar valores faltantes.\nDecidir entre imputaci√≥n, eliminaci√≥n o conservaci√≥n.\n\nValores at√≠picos\nNormalizaci√≥n y estandarizaci√≥n\nErrores de registro Bibliografia recomendada:\n\n\n\n‚ÄúPractical Statistics for Data Scientists‚Äù (Cap√≠tulo 2).\n‚ÄúDoing Data Science‚Äù (Cap√≠tulo 1).\n\nActividades de aplicaci√≥n pr√°ctica:\n\nTaller 1: Limpieza y an√°lisis descriptivo de datos en la practica con datos de educaci√≥n (repasa elementos del curso anterior) (sesi√≥n 1)\nProyecto:\n\nInicie el proyecto, cree un documento notebook en el cual van a alojarsu proyecto\nExplorar los datos\nDiagnosticar problemas.\nLa hip√≥tesis que pensamos, ¬øtienen variables que pueda concretizarlas? ¬øQu√© variables usar?\n\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 3: Introduccion al an√°lisis de regresi√≥n\n\n\n\n\n\nFecha: 2 septiembre\nObjetivos:\n\nComprender los conceptos fundamentales del an√°lisis de regresi√≥n lineal y su aplicaci√≥n en la resoluci√≥n de problemas.\nDesarrollar la habilidad de plantear modelos e interpretar los resultados obtenidos del an√°lisis de regresi√≥n, para aplicarlos en la toma de decisiones.\n\nContenidos\n\nIntroducci√≥n al An√°lisis de Regresi√≥n:\n\nDefinici√≥n y concepto de regresi√≥n.\nUso y aplicabilidad en la toma de decisiones.\n\nRegresi√≥n Lineal M√∫ltiple:\n\nExtensi√≥n del modelo de regresi√≥n a m√∫ltiples variables predictoras.\nEcuaci√≥n de regresi√≥n lineal m√∫ltiple.\n\nInterpretaci√≥n de Coeficientes:\n\nSignificado e interpretaci√≥n de los coeficientes de regresi√≥n.\nInfluencia de las variables predictoras en la variable de respuesta.\n\nEvaluaci√≥n de Modelos de Regresi√≥n:\n\nUso de medidas como el coeficiente de determinaci√≥n (R¬≤) y el error est√°ndar de estimaci√≥n.\nInterpretaci√≥n de los resultados de evaluaci√≥n.\n\nIncorporaci√≥n de Variables Categ√≥ricas:\n\nTransformaci√≥n de variables categ√≥ricas en variables num√©ricas.\nInterpretaci√≥n de coeficientes para variables categ√≥ricas.\n\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 2:\nProyecto:\nPlantear modelos de regresi√≥n que implementen las hip√≥tesis del proyecto\nEstimar e interpretar modelos\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 4: Profundizando en An√°lisis de Regresi√≥n, Supuestos y Limitaciones\n\n\n\n\n\nFecha: 9 septiembre\nObjetivos:\n\nExplorar los supuestos y limitaciones asociados al an√°lisis de regresi√≥n y desarrollar estrategias para manejar problemas comunes.\nAplicar estrategias pr√°cticas para identificar y abordar problemas en el an√°lisis de regresi√≥n.\n\nContenidos\n\nSupuestos en el An√°lisis de Regresi√≥n:\n\nIdentificaci√≥n de supuestos clave: linealidad, independencia, homoscedasticidad y normalidad.\nSignificado de cada supuesto y su importancia en la interpretaci√≥n de resultados.\n\nIdentificaci√≥n de Problemas en la Regresi√≥n:\n\nIdentificaci√≥n y manejo de outliers en los datos.\nReconocimiento de la heterocedasticidad y sus implicaciones.\nDetecci√≥n de la no-normalidad de los residuos.\n\nEstrategias para Manejar Problemas:\n\nTransformaci√≥n de variables para abordar problemas de linealidad.\nM√©todos para reducir la influencia de outliers.\nUso de transformaciones para tratar la heterocedasticidad.\nPruebas y t√©cnicas para verificar y mejorar la normalidad.\n\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 2:\nProyecto:\nRevisar supuestos de modelo de regresi√≥n\nDiscutir problemas de identificaci√≥n y limitaciones\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 5: Introduccion al an√°lisis de series de tiempo\n\n\n\n\n\nFecha: 30 septiembre\nObjetivos:\n\nIdentificar las caracter√≠sticas y particularidades de los datos de series de tiempo, comprendiendo sus aplicaciones profesionales.\nRealizar un an√°lisis exploratorio de una serie de tiempo, identificando caracter√≠sticas clave para su modelamiento.\n\nContenidos\n\nConceptos B√°sicos de Series de Tiempo:\n\nDefinici√≥n y caracter√≠sticas de una serie de tiempo.\nEjemplos de aplicaciones en distintos campos profesionales.\n\nParticularidades de los Datos Temporales:\n\nDependencia temporal y autocorrelaci√≥n.\nTendencias, estacionalidad y ciclos.\n\nAplicaciones Profesionales:\n\nCasos de estudio en finanzas, econom√≠a, medicina y otros campos.\nC√≥mo el an√°lisis de series de tiempo puede brindar insights valiosos.\n\nB√∫squeda y Reorganizaci√≥n de Datos Temporales:\n\nFuentes de datos para series de tiempo (bases de datos, APIs, archivos).\nImportancia de la temporalidad y el orden en los datos.\n\nVisualizaci√≥n y Exploraci√≥n Inicial:\n\nGr√°ficos de l√≠nea y dispersi√≥n para identificar tendencias y patrones.\nEstudio de estacionalidad y ciclos mediante gr√°ficos.\n\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 6: Modelando series temporales\n\n\n\n\n\nFecha: 7 occtubre\nObjetivos:\n\nComprender los conceptos y aplicaciones de los modelos ARIMA y VAR en el an√°lisis de series temporales.\nEvaluar las ventajas y desventajas de los m√©todos estad√≠sticos para el an√°lisis de series de tiempo y seleccionar la t√©cnica m√°s adecuada.\n\nContenidos\n\nModelos ARIMA:\n\nDefinici√≥n y componentes de los modelos ARIMA.\nIdentificaci√≥n, Estimaci√≥n y Validaci√≥n de un modelo ARIMA.\nUso de correlogramas y gr√°ficos ACF/PACF para la identificaci√≥n.\n\nModelos VAR (Vector Autoregressive):\n\nIntroducci√≥n a los modelos VAR y su aplicaci√≥n.\nUso de matrices de coeficientes para representar relaciones entre variables.\n\nVentajas y Desventajas de los M√©todos Estad√≠sticos:\n\nUso de modelos estad√≠sticos en comparaci√≥n con otros enfoques.\nLimitaciones y supuestos asociados a los modelos ARIMA y VAR.\n\nSelecci√≥n del M√©todo Adecuado:\n\nCriterios para elegir entre modelos ARIMA y VAR.\nConsideraciones al evaluar las alternativas disponibles.\n\nOtros modelos\n\n\nGARCH\nSARIMA y SARIMAX\nAlisado exponencial\nCambio estructural\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 7: Principales lecciones para el an√°lisis de datos y presentaciones de proyectos\n\n\n\n\n\nFecha: TBA\nObjetivos:\n\nCerrar el curso, poniendo en contexto las principales herramientas de an√°lisis de datos.\nPresentar un proyecto de an√°lisis de datos\nRecibir feedback y propuestas de mejoras, tanto del trabajo propio como el de sus compa√±eros.\n\nContenidos\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nProyecto: Presentaciones finales"
  },
  {
    "objectID": "sesion3.html",
    "href": "sesion3.html",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Slides Sesi√≥n 3"
  },
  {
    "objectID": "sesion3.html#usos-de-las-regresiones",
    "href": "sesion3.html#usos-de-las-regresiones",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Usos de las regresiones",
    "text": "Usos de las regresiones\nLas regresiones tienen tres principales usos:\n\nDescribir un fen√≥meno\nProbar hip√≥tesis sobre ciertas teor√≠as\nRealizar predicciones"
  },
  {
    "objectID": "sesion3.html#regresi√≥n-simple-y-scatterplot",
    "href": "sesion3.html#regresi√≥n-simple-y-scatterplot",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Regresi√≥n simple y scatterplot",
    "text": "Regresi√≥n simple y scatterplot\nPor ejemplo, pensemos en la relaci√≥n entre los a√±os de educaci√≥n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom√≠a.\nPodriamos pensar que ambas variables se encuentras relacionadas.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a√±os\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nB√°sica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nS√≠\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nB√°sica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegi√≥n de √ëuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nT√©cnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserci√≥n\nMedia humanista completa\nNaN\n\n\n\n\n\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregaci√≥n\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por regi√≥n\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegi√≥n de Tarapac√°\n658026.6250\n11.679582\n\n\n1\nRegi√≥n de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegi√≥n de Atacama\n666128.3125\n11.126735\n\n\n3\nRegi√≥n de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegi√≥n de Valpara√≠so\n611298.1250\n11.559877\n\n\n\n\n\n\n\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por regi√≥n)')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de regi√≥n a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la √∫ltima palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Etiquetas de Regi√≥n (√öltima Palabra)')\nplt.show()\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relaci√≥n positiva: a mayor escolaridad promedio, mayor salario promedio por regi√≥n."
  },
  {
    "objectID": "sesion3.html#especificaci√≥n",
    "href": "sesion3.html#especificaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Especificaci√≥n",
    "text": "Especificaci√≥n\nLlamamos especifiaci√≥n al precisar la relaci√≥n entre las variables que deseamos estimar.\nEn nuestro caso, la funci√≥n base que queremos entender es entre salario y educaci√≥n:\n\\[ \\text{Salario} = f(Educacion))\\]\nEste es una relaci√≥n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales: - agregar el error aleatorio - especificar una forma funcional - definir una forma de medir las variables en los datos\nEn nuestro caso, entonces el modelo especificado ser√≠a:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\\]"
  },
  {
    "objectID": "sesion3.html#interpretaci√≥n",
    "href": "sesion3.html#interpretaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Interpretaci√≥n",
    "text": "Interpretaci√≥n\nCon nuestro modelo especificado:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\\]\nPodemos interpretar \\(\\beta\\) y \\(alpha\\):\n\n\\(\\beta = \\frac{\\partial ingr}{\\partial educ}\\): un a√±o adici√≥nal de educaci√≥n, en cuanto incrementa el salario (si nada m√°s cambia)\n\\(\\alpha\\) valor esperado de y, si x=0‚Ä¶"
  },
  {
    "objectID": "sesion3.html#modelo-poblaci√≥nal-y-estimaci√≥n",
    "href": "sesion3.html#modelo-poblaci√≥nal-y-estimaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Modelo poblaci√≥nal y estimaci√≥n",
    "text": "Modelo poblaci√≥nal y estimaci√≥n\nEste modelo especificado esta definido en la poblaci√≥n:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\\]\npero necesitamos calcularlo con la muestra‚Ä¶. por lo cual tenemos estimadores para los coeficientes poblacionales!\n\\[\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{a√±os educaci√≥n}_i \\]"
  },
  {
    "objectID": "sesion3.html#modelo-poblaci√≥nal-y-estimaci√≥n-1",
    "href": "sesion3.html#modelo-poblaci√≥nal-y-estimaci√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Modelo poblaci√≥nal y estimaci√≥n",
    "text": "Modelo poblaci√≥nal y estimaci√≥n\nEl m√©todo m√°s comun de estimaci√≥n es el de los m√≠nimos cuadrados ordinarios. Veremos detalles sobre la estimaci√≥n, supuestos, propiedades estad√≠sticas la proxima sesi√≥n.\nPor ahora, pensaremos que es el m√©todo que busca la l√≠nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi√≥n).\n\\[ \\hat{\\mu}_i= y_i-\\hat{y}_i\\]\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuraci√≥n del estilo del gr√°fico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gr√°fico de dispersi√≥n con la l√≠nea de regresi√≥n\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresi√≥n lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t√©rmino constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar l√≠neas que conecten cada punto a la l√≠nea de regresi√≥n\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # L√≠nea que conecta el punto a la l√≠nea de regresi√≥n\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresi√≥n y residuos')\nplt.show()\n\n\n\n\nEs decir, minimiza $_{i}^{n} _i $"
  },
  {
    "objectID": "sesion3.html#modelo-estimado",
    "href": "sesion3.html#modelo-estimado",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Modelo estimado",
    "text": "Modelo estimado\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regi√≥nTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        13:57:56   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n&gt;=20 ... continuing anyway, n=16\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        13:57:56   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\nPodemos ver que un a√±o adicional de educaci√≥n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¬øy la constante, como la podemos interpretar?"
  },
  {
    "objectID": "sesion3.html#modelos-simples-y-m√∫ltiples",
    "href": "sesion3.html#modelos-simples-y-m√∫ltiples",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Modelos simples y m√∫ltiples",
    "text": "Modelos simples y m√∫ltiples\nMuchas veces una sola variable no es suficiente para describir bien un fen√≥meno. Necesitamos incluir m√°s variables.\nEsto puede ser: - Una nueva variable - Una forma funcional no lineal de la variable ya incluida\nNuestra interpretaci√≥n del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores.\nProbemos, agregar edad al modelo:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        13:57:56   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nEs muy usual, agregar edad al cuadrado‚Ä¶. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer‚Ä¶\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46433/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        13:57:56   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "sesion3.html#un-poco-m√°s-sobre-interpretaci√≥n",
    "href": "sesion3.html#un-poco-m√°s-sobre-interpretaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Un poco m√°s sobre interpretaci√≥n",
    "text": "Un poco m√°s sobre interpretaci√≥n\nLos principales elementos que hay que interpretar en un modelo de regresi√≥n lineal son los coeficientes de los predictores:\n\n\\(\\beta_0\\) es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta \\(y\\), cuando todos los predictores son cero.\n\\(\\beta_j\\) los coeficientes de regresi√≥n parcial de cada predictor indican el cambio promedio esperado de la variable respuesta ùë¶ al incrementar en una unidad de la variable predictora \\(x_j\\), manteni√©ndose constantes el resto de variables. (‚ÄúCeteris paribus‚Äù))\n\nLa magnitud de cada coeficiente parcial de regresi√≥n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est√° asociada con la importancia de cada predictor.\n\nPara poder determinar qu√© impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviaci√≥n est√°ndar) las variables predictoras previo ajuste del modelo. En este caso, \\(\\beta_0\\) se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y \\(\\beta_j\\) el cambio promedio esperado de la variable respuesta al incrementar en una desviaci√≥n est√°ndar la variable predictora \\(x_j\\), manteni√©ndose constantes el resto de variables.\nSi bien los coeficientes de regresi√≥n suelen ser el primer objetivo de la interpretaci√≥n de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condici√≥n de normalidad‚Ä¶etc.). Estos √∫ltimos suelen ser tratados con poco detalle cuando el √∫nico objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta."
  },
  {
    "objectID": "sesion3.html#causalidad-regresi√≥n-y-correlaci√≥n",
    "href": "sesion3.html#causalidad-regresi√≥n-y-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Causalidad, regresi√≥n y correlaci√≥n",
    "text": "Causalidad, regresi√≥n y correlaci√≥n\nImportante tener en cuenta\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relaci√≥n entre las variables de inter√©s. Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes m√°s altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociaci√≥n significativa entre las dos variables.\nUn diagrama de dispersi√≥n puede ser una herramienta √∫til para determinar la fuerza de la relaci√≥n entre dos variables. Si parece no haber asociaci√≥n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi√≥n no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresi√≥n lineal a los datos probablemente no proporcionar√° un modelo √∫til.\nUna valiosa medida num√©rica de asociaci√≥n entre dos variables es el coeficiente de correlaci√≥n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci√≥n de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3.html#estudio-de-caso-es-hereditaria-la-altura",
    "href": "sesion3.html#estudio-de-caso-es-hereditaria-la-altura",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Estudio de caso: ¬øes hereditaria la altura?",
    "text": "Estudio de caso: ¬øes hereditaria la altura?\nTenemos acceso a los datos de altura de familias recolectado por Galton, a trav√©s del paquete HistData. Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\ngalton_data.head(4)\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\nmidparentHeight\nchildren\nchildNum\ngender\nchildHeight\n\n\n\n\n0\n001\n78.5\n67.0\n75.43\n4\n1\nmale\n73.2\n\n\n1\n001\n78.5\n67.0\n75.43\n4\n2\nfemale\n69.2\n\n\n2\n001\n78.5\n67.0\n75.43\n4\n3\nfemale\n69.0\n\n\n3\n001\n78.5\n67.0\n75.43\n4\n4\nfemale\n69.0\n\n\n\n\n\n\n\nPara imitar el an√°lisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head(4)\n\n\n\n\n\n\n\n\nfather\nson\n\n\n\n\n0\n78.5\n73.2\n\n\n1\n75.5\n72.5\n\n\n2\n75.0\n71.0\n\n\n3\n75.0\n68.5\n\n\n\n\n\n\n\nEn los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.\nSupongamos que se nos pidiera que resumi√©ramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones est√°n bien aproximadas por la distribuci√≥n normal, podr√≠amos usar los dos promedios y dos desviaciones est√°ndar como res√∫menes:\n\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nresumen_estadistico.head()\n\n\n\n\n\n\n\n\npromedio_padre\nsd_padre\npromedio_hijo\nsd_hijo\n\n\n\n\n0\n69.098883\n2.546555\n69.120112\n2.622362\n\n\n\n\n\n\n\nSin embargo, este resumen no describe una caracter√≠stica importante de los datos: la tendencia de que cuanto m√°s alto es el padre, m√°s alto es el hijo.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tama√±o de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relaci√≥n entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gr√°fico\nplt.show()\n\n\n\n\nAprenderemos que el coeficiente de correlaci√≥n es un resumen informativo de c√≥mo dos variables se mueven juntas y luego veremos c√≥mo esto puede ser usado para predecir una variable usando la otra, en una regresi√≥n."
  },
  {
    "objectID": "sesion3.html#taller-de-aplicaci√≥n-2-caso-aplicaci√≥n-cursos-de-verano",
    "href": "sesion3.html#taller-de-aplicaci√≥n-2-caso-aplicaci√≥n-cursos-de-verano",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Taller de aplicaci√≥n 2: Caso aplicaci√≥n: Cursos de Verano",
    "text": "Taller de aplicaci√≥n 2: Caso aplicaci√≥n: Cursos de Verano\n\n\n\n\n\n\nTaller de aplicaci√≥n 2: Pregunta 1\n\n\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que quer√≠amos responder:\n\nAsistir a cursos de verano mejora los resultados acad√©micos?\n\n\nPlantee un modelo de regresi√≥n que con los datos disponibles quisieramos estimar.\nGrafique la dispersi√≥n y la recta de regresi√≥n estimada.\nEstime el modelo simple e interprete"
  },
  {
    "objectID": "sesion3.html#regresi√≥n-pero-y-la-correlaci√≥n",
    "href": "sesion3.html#regresi√≥n-pero-y-la-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "¬øRegresi√≥n?‚Ä¶ pero ¬øY la correlaci√≥n?",
    "text": "¬øRegresi√≥n?‚Ä¶ pero ¬øY la correlaci√≥n?\n\nAmbos est√°n muy relacionados.\nAprenderemos que el coeficiente de correlaci√≥n es un resumen informativo de c√≥mo dos variables se mueven juntas‚Ä¶\ny luego veremos c√≥mo esto puede ser usado para predecir una variable usando la otra y modelado en una regresi√≥n"
  },
  {
    "objectID": "sesion3.html#el-coeficiente-de-correlaci√≥n",
    "href": "sesion3.html#el-coeficiente-de-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "El coeficiente de correlaci√≥n",
    "text": "El coeficiente de correlaci√≥n\nEl coeficiente de correlaci√≥n se define para una lista de pares \\((x_1,y_1),...(x_n,y_n)\\) como la media de los productos de los valores normalizados:\n\\[\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n\\]\nD√≥nde \\(\\mu\\) son promedios y \\(\\sigma\\) son desviaciones est√°ndar. La letra griega para r, \\(\\rho\\) se utiliza com√∫nmente en los libros de estad√≠stica para denotar la correlaci√≥n, porque es la primera letra de regresi√≥n. Pronto aprenderemos sobre la conexi√≥n entre correlaci√≥n y regresi√≥n.\nPodemos representar la f√≥rmula anterior con el c√≥digo usando:\nrho &lt;- np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\nPodemos representar la f√≥rmula anterior con el siguiente c√≥digo usando:\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aqu√≠\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aqu√≠\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n\n0.9999999999999998\n\n\nLa correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\):\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviaci√≥n est√°ndar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviaci√≥n Est√°ndar de la Altura del Padre:\", sd_father)\n\n# Crear el gr√°fico de dispersi√≥n\nplt.scatter(galton_heights['father'], StandardScaler().fit_transform(galton_heights[['father']]))\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Padre Estandarizada\")\nplt.title(\"Relaci√≥n entre Altura del Padre y Altura del Padre Estandarizada\")\nplt.show()\n\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviaci√≥n Est√°ndar de la Altura del Padre: 2.546555038637643\n\n\n\n\n\nLa correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\).\n\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlaci√≥n:\", correlation_coefficient)\n\nCoeficiente de Correlaci√≥n: 0.4868259925112461\n\n\n\nimport pandas as pd\n\n# Generar datos simulados usando la biblioteca faux\ndat = pd.DataFrame(np.random.multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([1, 1, 1, 1, 1, 1]), size=100),\n                   columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n\nprint(dat)\n\n# Calcular la correlaci√≥n entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n           A         B         C         D         E         F\n0  -0.341648  0.116020  2.277212  0.292145  1.164207  1.454844\n1  -0.574601 -1.376076  0.849175 -0.331692  0.534056  0.616964\n2  -1.346341 -1.466495 -1.525634  0.074053  0.632263  0.373107\n3  -1.317879  1.225713 -0.363807  0.544034 -1.344250  1.198290\n4  -0.443862 -1.315814 -0.916430  0.419516 -0.771529  1.095978\n..       ...       ...       ...       ...       ...       ...\n95  0.063052 -0.305811 -0.986839  0.926453  0.191580 -2.430414\n96 -2.056289  1.048124  0.958648 -0.424067  1.561997 -0.168998\n97 -0.685922 -2.506629  2.436036 -1.473975 -1.937537 -0.549623\n98  1.008418  0.230475  0.260994  1.729545 -1.918848 -0.709950\n99 -1.046753  2.880363  0.154750  0.715915 -0.156903 -0.364088\n\n[100 rows x 6 columns]\n0.4075594018787953\n\n\nPara ver c√≥mo se ven los datos para los diferentes valores de \\(\\rho\\) aqu√≠ hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\n\n\nimage"
  },
  {
    "objectID": "sesion3.html#la-correlaci√≥n-de-la-muestra-es-una-variable-aleatoria",
    "href": "sesion3.html#la-correlaci√≥n-de-la-muestra-es-una-variable-aleatoria",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "La correlaci√≥n de la muestra es una variable aleatoria",
    "text": "La correlaci√≥n de la muestra es una variable aleatoria\nAntes de continuar conectando la correlaci√≥n con la regresi√≥n, recordemos la variabilidad aleatoria.\nEn la mayor√≠a de las aplicaciones de la ciencia de datos, observamos datos que incluyen variaci√≥n aleatoria.\nPor ejemplo, en muchos casos, no se observan datos para toda la poblaci√≥n de inter√©s, sino para una muestra aleatoria. Al igual que con el promedio y la desviaci√≥n est√°ndar, la correlaci√≥n de la muestra es la estimaci√≥n m√°s com√∫nmente utilizada de la correlaci√≥n de la poblaci√≥n. Esto implica que la correlaci√≥n que calculamos y usamos como resumen es una variable aleatoria.\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra poblaci√≥n. Un genetista menos afortunado s√≥lo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlaci√≥n de la muestra se puede calcular con:\n\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tama√±o 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlaci√≥n entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlaci√≥n en la Muestra:\", correlation_coefficient)\n\nCoeficiente de Correlaci√≥n en la Muestra: 0.5461944437719961\n\n\nR es una variable aleatoria. Podemos ejecutar una simulaci√≥n de Monte Carlo para ver su distribuci√≥n:\n\nNota: el objetivo principal de la simulaci√≥n de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir c√≥mo van a evolucionar.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlaci√≥n\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlaci√≥n\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlaci√≥n\")\nplt.show()\n\n\n\n\nVemos que el valor esperado de R es la correlaci√≥n de la poblaci√≥n:\n\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlaci√≥n:\", mean_R)\n\nMedia de Coeficientes de Correlaci√≥n: 0.48653710780288484\n\n\ny que tiene un error est√°ndar relativamente alto en relaci√≥n con el rango de valores que puede tomar R:\n\nsd_R = np.std(R)\nprint(\"Desviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n:\", sd_R)\n\nDesviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n: 0.051105220051558105\n\n\nPor lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\nAdem√°s, tenga en cuenta que debido a que la correlaci√≥n de la muestra es un promedio de extracciones independientes, el teorema del l√≠mite central realmente funciona. Por lo tanto, para \\(N\\) lo suficientemente grande la distribuci√≥n de \\(R\\) es aproximadamente normal con el valor esperado \\(\\rho\\). La desviaci√≥n est√°ndar, que es algo compleja de derivar, es: \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nEn nuestro ejemplo, \\(N=25\\) no parece ser lo suficientemente grande para que la aproximaci√≥n sea buena:\n\nNota: El gr√°fico Q-Q, o gr√°fico cuantitativo, es una herramienta gr√°fica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribuci√≥n te√≥rica como una Normal o exponencial. Por ejemplo, si realizamos un an√°lisis estad√≠stico que asume que nuestra variable dependiente est√° Normalmente distribuida, podemos usar un gr√°fico Q-Q-Normal para verificar esa suposici√≥n. https://data.library.virginia.edu/understanding-q-q-plots/\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlaci√≥n\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tama√±o de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gr√°fico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Te√≥ricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gr√°fico QQ-plot para los Coeficientes de Correlaci√≥n\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # L√≠nea de referencia\nplt.show()\n\n\n\n\nSi N aumenta ver√°s que la distribuci√≥n converge a una normal."
  },
  {
    "objectID": "sesion3.html#la-correlaci√≥n-no-siempre-es-un-resumen-√∫til",
    "href": "sesion3.html#la-correlaci√≥n-no-siempre-es-un-resumen-√∫til",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "La correlaci√≥n no siempre es un resumen √∫til",
    "text": "La correlaci√≥n no siempre es un resumen √∫til\nLa correlaci√≥n no siempre es un buen resumen de la relaci√≥n entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlaci√≥n de 0,82:\n\n\n\nimage\n\n\nLa correlaci√≥n s√≥lo tiene sentido en un contexto particular. Para ayudarnos a entender cu√°ndo es que la correlaci√≥n es significativa como estad√≠stica de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudar√° a motivar y definir la regresi√≥n lineal. Comenzamos demostrando c√≥mo la correlaci√≥n puede ser √∫til para la predicci√≥n."
  },
  {
    "objectID": "sesion3.html#correlaci√≥n-espuria",
    "href": "sesion3.html#correlaci√≥n-espuria",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Correlaci√≥n espuria",
    "text": "Correlaci√≥n espuria\nEl siguiente ejemplo c√≥mico subraya que la correlaci√≥n no es causalidad. Muestra una fuerte correlaci√≥n entre las tasas de divorcio y el consumo de margarina.\n\n\n\nimage\n\n\n(Ac√° pueden encontrar m√°s http://tylervigen.com/old-version.html)\n¬øSignifica esto que la margarina causa divorcios? ¬øO los divorcios hacen que la gente coma m√°s margarina? Por supuesto que la respuesta a estas dos preguntas es no. Esto es s√≥lo un ejemplo de lo que llamamos una correlaci√≥n espuria.\nLos casos presentados en el sitio de correlaci√≥n espuria son todos casos de lo que generalmente se denomina dragado de datos (data dredging), pesca de datos (data fishing) o espionaje de datos (data snooping). Es b√°sicamente una forma de lo que en los EE.UU. se llama ‚Äúcherry picking‚Äù. Un ejemplo de dragado de datos ser√≠a si miras a trav√©s de muchos resultados producidos por un proceso aleatorio y escoges el que muestra una relaci√≥n que apoya una teor√≠a que se quiere defender."
  },
  {
    "objectID": "sesion3.html#varianza-explicada",
    "href": "sesion3.html#varianza-explicada",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Varianza explicada",
    "text": "Varianza explicada\nLa teor√≠a de la normalidad bivariada tambi√©n nos dice que la desviaci√≥n est√°ndar de la distribuci√≥n condicional descrita anteriormente es:\n\\[\nSD(Y|X=x)=\\sigma_Y\\sqrt{1-\\rho^2}\n\\]\nPara ver por qu√© esto es intuitivo, note que sin condicionamiento, \\(SD(Y)=\\sigma_Y\\) estamos viendo la variabilidad de todos los hijos. Pero una vez que los condicionamos, s√≥lo estamos viendo la variabilidad de los hijos con un padre que mide 72 pulgadas de alto. Este grupo tender√° a ser ‚Äúalgo m√°s‚Äù alto (que el promedio), por lo que la desviaci√≥n est√°ndar se reduce.\nEspec√≠ficamente, se reduce a \\(\\sqrt{1-\\rho^2}=\\sqrt{1-0.25}=0.86\\) de lo que era originalmente. Podr√≠amos decir que la estatura del padre ‚Äúexplica‚Äù el 14% de la variabilidad de estatura del hijo.\nLa frase ‚Äú\\(X\\) explica tal o cual porcentaje de la variabilidad‚Äù se utiliza com√∫nmente en papers acad√©micos. En este caso, este porcentaje se refiere realmente a la desviaci√≥n (SD al cuadrado). Por lo tanto, si los datos son normales bivariados, la varianza se reduce en \\(1-\\rho^2\\) por lo que decimos que \\(X\\) explica \\(1-(1-\\rho^2)=\\rho^2\\) (la correlaci√≥n al cuadrado) de la varianza.\nPero es importante recordar que la afirmaci√≥n de ‚Äúvarianza explicada‚Äù s√≥lo tiene sentido cuando los datos se aproximan mediante una distribuci√≥n normal bivariada."
  },
  {
    "objectID": "sesion3.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n",
    "href": "sesion3.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Cuidado: hay dos l√≠neas de regresi√≥n",
    "text": "Cuidado: hay dos l√≠neas de regresi√≥n\nCalculamos una l√≠nea de regresi√≥n para predecir la altura del hijo desde la altura del padre.\nUsamos estos c√°lculos:\n\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviaci√≥n est√°ndar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviaci√≥n est√°ndar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlaci√≥n entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n\n0.489950313647045\n2.5644170903922188\n2.5850810242285687\n69.08938547486034\n69.2223463687151\n\n\n\n# Calcular la pendiente de la primera l√≠nea de regresi√≥n\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera l√≠nea de regresi√≥n\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n\npendiente 0.4938983067025557\nconstante 35.09921587156143\n\n\nLo que nos da la funci√≥n \\(E(Y|X=x)=28,8+0.44x\\).\n¬øY si queremos predecir la estatura del padre bas√°ndonos en la del hijo?\nEs importante saber que esto no se determina calculando la funci√≥n inversa!.\nNecesitamos computar \\(E(X‚à£Y=y)\\). Dado que los datos son aproximadamente normales bivariados, la teor√≠a descrita anteriormente nos dice que esta expectativa condicional seguir√° una l√≠nea con pendiente e intercepto:\n\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n\npendiente 0.486033879009441\nconstante 35.44497995513865\n\n\nDe nuevo vemos una regresi√≥n a la media: la predicci√≥n para el padre est√° m√°s cerca de la media del padre que lo que estan las alturas del hijo \\(y\\) al promedio del hijo.\nAqu√≠ hay un gr√°fico que muestra las dos l√≠neas de regresi√≥n:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gr√°fico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()\n\n\n\n\ncon azul para la predicci√≥n de las alturas del hijo con las alturas del padre y rojo para la predicci√≥n de las alturas del padre con las alturas del hijo.\n\n\n\n\n\n\nTaller aplicacci√≥n 2: Altura de padres e hijos\n\n\n\n\nCargue los datos de GaltonFamilies desde el HistData. Los ni√±os de cada familia est√°n ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado galton_heights seleccionando ni√±os y ni√±as al azar. (HINT: use sample).\nHaga una gr√°fica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nCalcular la correlaci√≥n para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nPlotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\nObtener el modelo de regresi√≥n e interpretar los coeficientes."
  },
  {
    "objectID": "sesion3_notas.html",
    "href": "sesion3_notas.html",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "En las aplicaciones de la ciencia de datos, es muy com√∫n estar interesado en la relaci√≥n entre dos o m√°s variables.\nEl an√°lisis de regresi√≥n es una t√©cnica en la cual buscamos encontrar una funci√≥n que pueda describir la relaci√≥n observada en los datos entre dos o mas variables.\nPor ejemplo, una persona podr√≠a querer relacionar los pesos de los individuos con sus alturas‚Ä¶\n\n¬øSon los m√°s altos m√°s pesados?\n‚Ä¶y¬øcu√°nto m√°s pesados?\n\nPensemos en el caso m√°s sencillo: una regresi√≥n lineal simple o univariada. Tenemos una variable que deseamos explicar o predecir (Y) como funci√≥n de otra (X).\nPara esto, buscamos la pendiente e intercepto de una funci√≥nla recta de la forma:\nY=Œ±+Œ≤XY = \\alpha + \\beta X\nque se ajuste mejor al conjunto de datos con los que se cuenta.\ndonde XX es la variable explicativa e YY es la variable dependiente. La pendiente de la recta es bb, y aa es la intersecci√≥n (el valor de yy cuando x=0x = 0).\n\nPara esto, entendemos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes: una que es sistem√°tica o que se puede explicar directamente con una o m√°s variables independientes (Xs o regresores) y otra que es no sistem√°tica o error (Œº\\mu o epsilonepsilon) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fen√≥meno.\n\nLa parte sistem√°tica entonces la describimos con una forma funcional, que depende de otras variables o regresores.\nEsta forma funcional puede ser lineal univariada, lineal m√∫ltiple o no lineal. El tipo de forma funcional, definir√° el tipo de regresi√≥n de la que estemos hablando.\nVentajas del an√°lisis de regersi√≥n: es facil describir cuantitaivamente una relaci√≥n.\nEsquem√°ticamente, los elementos son:\n\n\n\nLas regresiones tienen tres principales usos:\n\nDescribir un fen√≥meno\nProbar hip√≥tesis sobre ciertas teor√≠as\nRealizar predicciones\n\n\n\n\nPor ejemplo, pensemos en la relaci√≥n entre los a√±os de educaci√≥n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom√≠a.\nPodriamos pensar que ambas variables se encuentras relacionadas.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a√±os\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nB√°sica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nS√≠\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nB√°sica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegi√≥n de √ëuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nT√©cnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserci√≥n\nMedia humanista completa\nNaN\n\n\n\n\n\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregaci√≥n\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por regi√≥n\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegi√≥n de Tarapac√°\n658026.6250\n11.679582\n\n\n1\nRegi√≥n de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegi√≥n de Atacama\n666128.3125\n11.126735\n\n\n3\nRegi√≥n de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegi√≥n de Valpara√≠so\n611298.1250\n11.559877\n\n\n\n\n\n\n\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por regi√≥n)')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de regi√≥n a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la √∫ltima palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Etiquetas de Regi√≥n (√öltima Palabra)')\nplt.show()\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relaci√≥n positiva: a mayor escolaridad promedio, mayor salario promedio por regi√≥n.\n\n\n\nLlamamos especifiaci√≥n al precisar la relaci√≥n entre las variables que deseamos estimar.\nEn nuestro caso, la funci√≥n base que queremos entender es entre salario y educaci√≥n:\nSalario=f(Educacion)) \\text{Salario} = f(Educacion))\nEste es una relaci√≥n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales: - agregar el error aleatorio - especificar una forma funcional - definir una forma de medir las variables en los datos\nEn nuestro caso, entonces el modelo especificado ser√≠a:\ningreso del trabajoi=Œ±+Œ≤a√±os educaci√≥ni+Œºi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\n\n\n\nCon nuestro modelo especificado:\ningreso del trabajoi=Œ±+Œ≤a√±os educaci√≥ni+Œºi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\nPodemos interpretar Œ≤\\beta y alphaalpha:\n\nŒ≤=‚àÇingr‚àÇeduc\\beta = \\frac{\\partial ingr}{\\partial educ}: un a√±o adici√≥nal de educaci√≥n, en cuanto incrementa el salario (si nada m√°s cambia)\nŒ±\\alpha valor esperado de y, si x=0‚Ä¶\n\n\n\n\nEste modelo especificado esta definido en la poblaci√≥n:\ningreso del trabajoi=Œ±+Œ≤a√±os educaci√≥ni+Œºi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\npero necesitamos calcularlo con la muestra‚Ä¶. por lo cual tenemos estimadores para los coeficientes poblacionales!\ningreso del trabajoÃÇi=Œ±ÃÇ+Œ≤ÃÇa√±os educaci√≥ni\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{a√±os educaci√≥n}_i \n\n\n\nEl m√©todo m√°s comun de estimaci√≥n es el de los m√≠nimos cuadrados ordinarios. Veremos detalles sobre la estimaci√≥n, supuestos, propiedades estad√≠sticas la proxima sesi√≥n.\nPor ahora, pensaremos que es el m√©todo que busca la l√≠nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi√≥n).\nŒºÃÇi=yi‚àíyÃÇi \\hat{\\mu}_i= y_i-\\hat{y}_i\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuraci√≥n del estilo del gr√°fico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gr√°fico de dispersi√≥n con la l√≠nea de regresi√≥n\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresi√≥n lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t√©rmino constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar l√≠neas que conecten cada punto a la l√≠nea de regresi√≥n\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # L√≠nea que conecta el punto a la l√≠nea de regresi√≥n\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresi√≥n y residuos')\nplt.show()\n\n\n\n\nEs decir, minimiza $_{i}^{n} _i $\n\n\n\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regi√≥nTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:33:51   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n&gt;=20 ... continuing anyway, n=16\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\nPodemos ver que un a√±o adicional de educaci√≥n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¬øy la constante, como la podemos interpretar?\n\n\n\nMuchas veces una sola variable no es suficiente para describir bien un fen√≥meno. Necesitamos incluir m√°s variables.\nEsto puede ser: - Una nueva variable - Una forma funcional no lineal de la variable ya incluida\nNuestra interpretaci√≥n del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores.\nProbemos, agregar edad al modelo:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nEs muy usual, agregar edad al cuadrado‚Ä¶. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer‚Ä¶\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\nLos principales elementos que hay que interpretar en un modelo de regresi√≥n lineal son los coeficientes de los predictores:\n\nŒ≤0\\beta_0 es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta yy, cuando todos los predictores son cero.\nŒ≤j\\beta_j los coeficientes de regresi√≥n parcial de cada predictor indican el cambio promedio esperado de la variable respuesta ùë¶ al incrementar en una unidad de la variable predictora xjx_j, manteni√©ndose constantes el resto de variables. (‚ÄúCeteris paribus‚Äù))\n\nLa magnitud de cada coeficiente parcial de regresi√≥n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est√° asociada con la importancia de cada predictor.\n\nPara poder determinar qu√© impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviaci√≥n est√°ndar) las variables predictoras previo ajuste del modelo. En este caso, Œ≤0\\beta_0 se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y Œ≤j\\beta_j el cambio promedio esperado de la variable respuesta al incrementar en una desviaci√≥n est√°ndar la variable predictora xjx_j, manteni√©ndose constantes el resto de variables.\nSi bien los coeficientes de regresi√≥n suelen ser el primer objetivo de la interpretaci√≥n de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condici√≥n de normalidad‚Ä¶etc.). Estos √∫ltimos suelen ser tratados con poco detalle cuando el √∫nico objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta.\n\n\n\nImportante tener en cuenta\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relaci√≥n entre las variables de inter√©s. Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes m√°s altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociaci√≥n significativa entre las dos variables.\nUn diagrama de dispersi√≥n puede ser una herramienta √∫til para determinar la fuerza de la relaci√≥n entre dos variables. Si parece no haber asociaci√≥n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi√≥n no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresi√≥n lineal a los datos probablemente no proporcionar√° un modelo √∫til.\nUna valiosa medida num√©rica de asociaci√≥n entre dos variables es el coeficiente de correlaci√≥n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci√≥n de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3_notas.html#usos-de-las-regresiones",
    "href": "sesion3_notas.html#usos-de-las-regresiones",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Las regresiones tienen tres principales usos:\n\nDescribir un fen√≥meno\nProbar hip√≥tesis sobre ciertas teor√≠as\nRealizar predicciones"
  },
  {
    "objectID": "sesion3_notas.html#regresi√≥n-simple-y-scatterplot",
    "href": "sesion3_notas.html#regresi√≥n-simple-y-scatterplot",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Por ejemplo, pensemos en la relaci√≥n entre los a√±os de educaci√≥n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom√≠a.\nPodriamos pensar que ambas variables se encuentras relacionadas.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a√±os\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nB√°sica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nS√≠\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nB√°sica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegi√≥n de √ëuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nT√©cnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserci√≥n\nMedia humanista completa\nNaN\n\n\n\n\n\n\n\nY lo agrparemos por ragion, para facilitar el ejemplo:\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregaci√≥n\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por regi√≥n\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegi√≥n de Tarapac√°\n658026.6250\n11.679582\n\n\n1\nRegi√≥n de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegi√≥n de Atacama\n666128.3125\n11.126735\n\n\n3\nRegi√≥n de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegi√≥n de Valpara√≠so\n611298.1250\n11.559877\n\n\n\n\n\n\n\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por regi√≥n)')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de regi√≥n a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la √∫ltima palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Etiquetas de Regi√≥n (√öltima Palabra)')\nplt.show()\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relaci√≥n positiva: a mayor escolaridad promedio, mayor salario promedio por regi√≥n."
  },
  {
    "objectID": "sesion3_notas.html#especificaci√≥n",
    "href": "sesion3_notas.html#especificaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Llamamos especifiaci√≥n al precisar la relaci√≥n entre las variables que deseamos estimar.\nEn nuestro caso, la funci√≥n base que queremos entender es entre salario y educaci√≥n:\nSalario=f(Educacion)) \\text{Salario} = f(Educacion))\nEste es una relaci√≥n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales: - agregar el error aleatorio - especificar una forma funcional - definir una forma de medir las variables en los datos\nEn nuestro caso, entonces el modelo especificado ser√≠a:\ningreso del trabajoi=Œ±+Œ≤a√±os educaci√≥ni+Œºi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i"
  },
  {
    "objectID": "sesion3_notas.html#interpretaci√≥n",
    "href": "sesion3_notas.html#interpretaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Con nuestro modelo especificado:\ningreso del trabajoi=Œ±+Œ≤a√±os educaci√≥ni+Œºi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\nPodemos interpretar Œ≤\\beta y alphaalpha:\n\nŒ≤=‚àÇingr‚àÇeduc\\beta = \\frac{\\partial ingr}{\\partial educ}: un a√±o adici√≥nal de educaci√≥n, en cuanto incrementa el salario (si nada m√°s cambia)\nŒ±\\alpha valor esperado de y, si x=0‚Ä¶"
  },
  {
    "objectID": "sesion3_notas.html#modelo-poblaci√≥nal-y-estimaci√≥n",
    "href": "sesion3_notas.html#modelo-poblaci√≥nal-y-estimaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Este modelo especificado esta definido en la poblaci√≥n:\ningreso del trabajoi=Œ±+Œ≤a√±os educaci√≥ni+Œºi \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\npero necesitamos calcularlo con la muestra‚Ä¶. por lo cual tenemos estimadores para los coeficientes poblacionales!\ningreso del trabajoÃÇi=Œ±ÃÇ+Œ≤ÃÇa√±os educaci√≥ni\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{a√±os educaci√≥n}_i"
  },
  {
    "objectID": "sesion3_notas.html#modelo-poblaci√≥nal-y-estimaci√≥n-1",
    "href": "sesion3_notas.html#modelo-poblaci√≥nal-y-estimaci√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "El m√©todo m√°s comun de estimaci√≥n es el de los m√≠nimos cuadrados ordinarios. Veremos detalles sobre la estimaci√≥n, supuestos, propiedades estad√≠sticas la proxima sesi√≥n.\nPor ahora, pensaremos que es el m√©todo que busca la l√≠nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi√≥n).\nŒºÃÇi=yi‚àíyÃÇi \\hat{\\mu}_i= y_i-\\hat{y}_i\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuraci√≥n del estilo del gr√°fico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gr√°fico de dispersi√≥n con la l√≠nea de regresi√≥n\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresi√≥n lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t√©rmino constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar l√≠neas que conecten cada punto a la l√≠nea de regresi√≥n\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # L√≠nea que conecta el punto a la l√≠nea de regresi√≥n\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresi√≥n y residuos')\nplt.show()\n\n\n\n\nEs decir, minimiza $_{i}^{n} _i $"
  },
  {
    "objectID": "sesion3_notas.html#modelo-estimado",
    "href": "sesion3_notas.html#modelo-estimado",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Por ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regi√≥nTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:33:51   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/melita/anaconda3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1736: UserWarning:\n\nkurtosistest only valid for n&gt;=20 ... continuing anyway, n=16\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/626282569.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\nPodemos ver que un a√±o adicional de educaci√≥n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¬øy la constante, como la podemos interpretar?"
  },
  {
    "objectID": "sesion3_notas.html#modelos-simples-y-m√∫ltiples",
    "href": "sesion3_notas.html#modelos-simples-y-m√∫ltiples",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Muchas veces una sola variable no es suficiente para describir bien un fen√≥meno. Necesitamos incluir m√°s variables.\nEsto puede ser: - Una nueva variable - Una forma funcional no lineal de la variable ya incluida\nNuestra interpretaci√≥n del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores.\nProbemos, agregar edad al modelo:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/2633333245.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nEs muy usual, agregar edad al cuadrado‚Ä¶. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer‚Ä¶\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_44100/847230214.py:14: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:33:51   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "sesion3_notas.html#un-poco-m√°s-sobre-interpretaci√≥n",
    "href": "sesion3_notas.html#un-poco-m√°s-sobre-interpretaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Los principales elementos que hay que interpretar en un modelo de regresi√≥n lineal son los coeficientes de los predictores:\n\nŒ≤0\\beta_0 es la ordenada en el origen o intercept, se corresponde con el valor esperado de la variable respuesta yy, cuando todos los predictores son cero.\nŒ≤j\\beta_j los coeficientes de regresi√≥n parcial de cada predictor indican el cambio promedio esperado de la variable respuesta ùë¶ al incrementar en una unidad de la variable predictora xjx_j, manteni√©ndose constantes el resto de variables. (‚ÄúCeteris paribus‚Äù))\n\nLa magnitud de cada coeficiente parcial de regresi√≥n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est√° asociada con la importancia de cada predictor.\n\nPara poder determinar qu√© impacto tienen en el modelo cada una de las variables, se emplean los coeficientes parciales estandarizados, que se obtienen al estandarizar (sustraer la media y dividir entre la desviaci√≥n est√°ndar) las variables predictoras previo ajuste del modelo. En este caso, Œ≤0\\beta_0 se corresponde con el valor esperado de la variable respuesta cuando todos los predictores se encuentran en su valor promedio, y Œ≤j\\beta_j el cambio promedio esperado de la variable respuesta al incrementar en una desviaci√≥n est√°ndar la variable predictora xjx_j, manteni√©ndose constantes el resto de variables.\nSi bien los coeficientes de regresi√≥n suelen ser el primer objetivo de la interpretaci√≥n de un modelo lineal, existen muchos otros aspectos (significancia del modelo en su conjunto, significancia de los predictores, condici√≥n de normalidad‚Ä¶etc.). Estos √∫ltimos suelen ser tratados con poco detalle cuando el √∫nico objetivo del modelo es realizar predicciones, sin embargo, son muy relevantes si se quiere realizar inferencia, es decir, explicar las relaciones entre los predictores y la variable respuesta."
  },
  {
    "objectID": "sesion3_notas.html#causalidad-regresi√≥n-y-correlaci√≥n",
    "href": "sesion3_notas.html#causalidad-regresi√≥n-y-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Importante tener en cuenta\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relaci√≥n entre las variables de inter√©s. Esto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes m√°s altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociaci√≥n significativa entre las dos variables.\nUn diagrama de dispersi√≥n puede ser una herramienta √∫til para determinar la fuerza de la relaci√≥n entre dos variables. Si parece no haber asociaci√≥n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi√≥n no indica ninguna tendencia creciente o decreciente), entonces ajustar un modelo de regresi√≥n lineal a los datos probablemente no proporcionar√° un modelo √∫til.\nUna valiosa medida num√©rica de asociaci√≥n entre dos variables es el coeficiente de correlaci√≥n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci√≥n de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3_notas.html#estudio-de-caso-es-hereditaria-la-altura",
    "href": "sesion3_notas.html#estudio-de-caso-es-hereditaria-la-altura",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Estudio de caso: ¬øes hereditaria la altura?",
    "text": "Estudio de caso: ¬øes hereditaria la altura?\nTenemos acceso a los datos de altura de familias recolectado por Galton, a trav√©s del paquete HistData. Estos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\ngalton_data.head(4)\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\nmidparentHeight\nchildren\nchildNum\ngender\nchildHeight\n\n\n\n\n0\n001\n78.5\n67.0\n75.43\n4\n1\nmale\n73.2\n\n\n1\n001\n78.5\n67.0\n75.43\n4\n2\nfemale\n69.2\n\n\n2\n001\n78.5\n67.0\n75.43\n4\n3\nfemale\n69.0\n\n\n3\n001\n78.5\n67.0\n75.43\n4\n4\nfemale\n69.0\n\n\n\n\n\n\n\nPara imitar el an√°lisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head(4)\n\n\n\n\n\n\n\n\nfather\nson\n\n\n\n\n0\n78.5\n73.2\n\n\n1\n75.5\n73.5\n\n\n2\n75.0\n71.0\n\n\n3\n75.0\n68.5\n\n\n\n\n\n\n\nEn los ejercicios, examinaremos otras relaciones, incluidas las de madres e hijas.\nSupongamos que se nos pidiera que resumi√©ramos (describieramos) los datos de padres e hijos. Dado que ambas distribuciones est√°n bien aproximadas por la distribuci√≥n normal, podr√≠amos usar los dos promedios y dos desviaciones est√°ndar como res√∫menes:\n\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nresumen_estadistico.head()\n\n\n\n\n\n\n\n\npromedio_padre\nsd_padre\npromedio_hijo\nsd_hijo\n\n\n\n\n0\n69.098883\n2.546555\n69.263687\n2.567837\n\n\n\n\n\n\n\nSin embargo, este resumen no describe una caracter√≠stica importante de los datos: la tendencia de que cuanto m√°s alto es el padre, m√°s alto es el hijo.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tama√±o de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relaci√≥n entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gr√°fico\nplt.show()\n\n\n\n\nAprenderemos que el coeficiente de correlaci√≥n es un resumen informativo de c√≥mo dos variables se mueven juntas y luego veremos c√≥mo esto puede ser usado para predecir una variable usando la otra, en una regresi√≥n."
  },
  {
    "objectID": "sesion3_notas.html#taller-de-aplicaci√≥n-2-caso-aplicaci√≥n-cursos-de-verano",
    "href": "sesion3_notas.html#taller-de-aplicaci√≥n-2-caso-aplicaci√≥n-cursos-de-verano",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Taller de aplicaci√≥n 2: Caso aplicaci√≥n: Cursos de Verano",
    "text": "Taller de aplicaci√≥n 2: Caso aplicaci√≥n: Cursos de Verano\n\n\n\n\n\n\nTaller de aplicaci√≥n 2: Pregunta 1\n\n\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que quer√≠amos responder:\n\nAsistir a cursos de verano mejora los resultados acad√©micos?\n\n\nPlantee un modelo de regresi√≥n que con los datos disponibles quisieramos estimar.\nGrafique la dispersi√≥n y la recta de regresi√≥n estimada.\nEstime el modelo simple e interprete"
  },
  {
    "objectID": "sesion3_notas.html#regresi√≥n-pero-y-la-correlaci√≥n",
    "href": "sesion3_notas.html#regresi√≥n-pero-y-la-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "¬øRegresi√≥n?‚Ä¶ pero ¬øY la correlaci√≥n?",
    "text": "¬øRegresi√≥n?‚Ä¶ pero ¬øY la correlaci√≥n?\n\nAmbos est√°n muy relacionados.\nAprenderemos que el coeficiente de correlaci√≥n es un resumen informativo de c√≥mo dos variables se mueven juntas‚Ä¶\ny luego veremos c√≥mo esto puede ser usado para predecir una variable usando la otra y modelado en una regresi√≥n"
  },
  {
    "objectID": "sesion3_notas.html#el-coeficiente-de-correlaci√≥n",
    "href": "sesion3_notas.html#el-coeficiente-de-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "El coeficiente de correlaci√≥n",
    "text": "El coeficiente de correlaci√≥n\nEl coeficiente de correlaci√≥n se define para una lista de pares (x1,y1),...(xn,yn)(x_1,y_1),...(x_n,y_n) como la media de los productos de los valores normalizados:\nœÅ=1n‚àëi=1n(xi‚àíŒºxœÉx)(yi‚àíŒºyœÉy)\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n\nD√≥nde Œº\\mu son promedios y œÉ\\sigma son desviaciones est√°ndar. La letra griega para r, œÅ\\rho se utiliza com√∫nmente en los libros de estad√≠stica para denotar la correlaci√≥n, porque es la primera letra de regresi√≥n. Pronto aprenderemos sobre la conexi√≥n entre correlaci√≥n y regresi√≥n.\nPodemos representar la f√≥rmula anterior con el c√≥digo usando:\nrho &lt;- np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\nPodemos representar la f√≥rmula anterior con el siguiente c√≥digo usando:\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aqu√≠\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aqu√≠\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n\n0.9999999999999998\n\n\nLa correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente 0,40,4:\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviaci√≥n est√°ndar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviaci√≥n Est√°ndar de la Altura del Padre:\", sd_father)\n\n# Crear el gr√°fico de dispersi√≥n\nplt.scatter(galton_heights['father'], StandardScaler().fit_transform(galton_heights[['father']]))\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Padre Estandarizada\")\nplt.title(\"Relaci√≥n entre Altura del Padre y Altura del Padre Estandarizada\")\nplt.show()\n\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviaci√≥n Est√°ndar de la Altura del Padre: 2.546555038637643\n\n\n\n\n\nLa correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente 0,40,4.\n\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlaci√≥n:\", correlation_coefficient)\n\nCoeficiente de Correlaci√≥n: 0.42699639842017706\n\n\n\nimport pandas as pd\n\n# Generar datos simulados usando la biblioteca faux\ndat = pd.DataFrame(np.random.multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([1, 1, 1, 1, 1, 1]), size=100),\n                   columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])\n\nprint(dat)\n\n# Calcular la correlaci√≥n entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n           A         B         C         D         E         F\n0  -0.487216 -0.103054 -0.406291  0.144734  0.482508 -1.065099\n1  -2.619254 -0.482619  0.373571 -0.031262 -0.059186  1.284221\n2  -0.606478  0.793955 -0.923926 -0.833852  0.038484  0.057210\n3   0.748645  0.858455  0.849438  0.973093 -0.139669  0.295479\n4  -1.394435  0.086160  0.131287  0.053497 -0.113966  1.327921\n..       ...       ...       ...       ...       ...       ...\n95  0.066146 -0.052701  0.776452  0.885253  0.336985  1.836124\n96 -0.189615  0.674505  0.660422  0.862998 -1.177144  0.924969\n97 -1.168625  1.464250  0.373704 -0.818466 -1.300497 -0.431909\n98 -0.177136  0.473261  0.702792  3.293837  0.889548  0.447885\n99  0.637694 -0.315696  0.463798 -0.975720 -1.828953  2.008513\n\n[100 rows x 6 columns]\n0.4922242607689742\n\n\nPara ver c√≥mo se ven los datos para los diferentes valores de œÅ\\rho aqu√≠ hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\n\n\nimage"
  },
  {
    "objectID": "sesion3_notas.html#la-correlaci√≥n-de-la-muestra-es-una-variable-aleatoria",
    "href": "sesion3_notas.html#la-correlaci√≥n-de-la-muestra-es-una-variable-aleatoria",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "La correlaci√≥n de la muestra es una variable aleatoria",
    "text": "La correlaci√≥n de la muestra es una variable aleatoria\nAntes de continuar conectando la correlaci√≥n con la regresi√≥n, recordemos la variabilidad aleatoria.\nEn la mayor√≠a de las aplicaciones de la ciencia de datos, observamos datos que incluyen variaci√≥n aleatoria.\nPor ejemplo, en muchos casos, no se observan datos para toda la poblaci√≥n de inter√©s, sino para una muestra aleatoria. Al igual que con el promedio y la desviaci√≥n est√°ndar, la correlaci√≥n de la muestra es la estimaci√≥n m√°s com√∫nmente utilizada de la correlaci√≥n de la poblaci√≥n. Esto implica que la correlaci√≥n que calculamos y usamos como resumen es una variable aleatoria.\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra poblaci√≥n. Un genetista menos afortunado s√≥lo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlaci√≥n de la muestra se puede calcular con:\n\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tama√±o 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlaci√≥n entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlaci√≥n en la Muestra:\", correlation_coefficient)\n\nCoeficiente de Correlaci√≥n en la Muestra: 0.4105312650311907\n\n\nR es una variable aleatoria. Podemos ejecutar una simulaci√≥n de Monte Carlo para ver su distribuci√≥n:\n\nNota: el objetivo principal de la simulaci√≥n de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir c√≥mo van a evolucionar.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlaci√≥n\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlaci√≥n\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlaci√≥n\")\nplt.show()\n\n\n\n\nVemos que el valor esperado de R es la correlaci√≥n de la poblaci√≥n:\n\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlaci√≥n:\", mean_R)\n\nMedia de Coeficientes de Correlaci√≥n: 0.42583844932004655\n\n\ny que tiene un error est√°ndar relativamente alto en relaci√≥n con el rango de valores que puede tomar R:\n\nsd_R = np.std(R)\nprint(\"Desviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n:\", sd_R)\n\nDesviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n: 0.053483610252486324\n\n\nPor lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\nAdem√°s, tenga en cuenta que debido a que la correlaci√≥n de la muestra es un promedio de extracciones independientes, el teorema del l√≠mite central realmente funciona. Por lo tanto, para NN lo suficientemente grande la distribuci√≥n de RR es aproximadamente normal con el valor esperado œÅ\\rho. La desviaci√≥n est√°ndar, que es algo compleja de derivar, es: 1‚àír2N‚àí2\\sqrt{\\frac{1-r^2}{N-2}}.\nEn nuestro ejemplo, N=25N=25 no parece ser lo suficientemente grande para que la aproximaci√≥n sea buena:\n\nNota: El gr√°fico Q-Q, o gr√°fico cuantitativo, es una herramienta gr√°fica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribuci√≥n te√≥rica como una Normal o exponencial. Por ejemplo, si realizamos un an√°lisis estad√≠stico que asume que nuestra variable dependiente est√° Normalmente distribuida, podemos usar un gr√°fico Q-Q-Normal para verificar esa suposici√≥n. https://data.library.virginia.edu/understanding-q-q-plots/\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlaci√≥n\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tama√±o de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gr√°fico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Te√≥ricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gr√°fico QQ-plot para los Coeficientes de Correlaci√≥n\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # L√≠nea de referencia\nplt.show()\n\n\n\n\nSi N aumenta ver√°s que la distribuci√≥n converge a una normal."
  },
  {
    "objectID": "sesion3_notas.html#la-correlaci√≥n-no-siempre-es-un-resumen-√∫til",
    "href": "sesion3_notas.html#la-correlaci√≥n-no-siempre-es-un-resumen-√∫til",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "La correlaci√≥n no siempre es un resumen √∫til",
    "text": "La correlaci√≥n no siempre es un resumen √∫til\nLa correlaci√≥n no siempre es un buen resumen de la relaci√≥n entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlaci√≥n de 0,82:\n\n\n\nimage\n\n\nLa correlaci√≥n s√≥lo tiene sentido en un contexto particular. Para ayudarnos a entender cu√°ndo es que la correlaci√≥n es significativa como estad√≠stica de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudar√° a motivar y definir la regresi√≥n lineal. Comenzamos demostrando c√≥mo la correlaci√≥n puede ser √∫til para la predicci√≥n."
  },
  {
    "objectID": "sesion3_notas.html#correlaci√≥n-espuria",
    "href": "sesion3_notas.html#correlaci√≥n-espuria",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Correlaci√≥n espuria",
    "text": "Correlaci√≥n espuria\nEl siguiente ejemplo c√≥mico subraya que la correlaci√≥n no es causalidad. Muestra una fuerte correlaci√≥n entre las tasas de divorcio y el consumo de margarina.\n\n\n\nimage\n\n\n(Ac√° pueden encontrar m√°s http://tylervigen.com/old-version.html)\n¬øSignifica esto que la margarina causa divorcios? ¬øO los divorcios hacen que la gente coma m√°s margarina? Por supuesto que la respuesta a estas dos preguntas es no. Esto es s√≥lo un ejemplo de lo que llamamos una correlaci√≥n espuria.\nLos casos presentados en el sitio de correlaci√≥n espuria son todos casos de lo que generalmente se denomina dragado de datos (data dredging), pesca de datos (data fishing) o espionaje de datos (data snooping). Es b√°sicamente una forma de lo que en los EE.UU. se llama ‚Äúcherry picking‚Äù. Un ejemplo de dragado de datos ser√≠a si miras a trav√©s de muchos resultados producidos por un proceso aleatorio y escoges el que muestra una relaci√≥n que apoya una teor√≠a que se quiere defender."
  },
  {
    "objectID": "sesion3_notas.html#varianza-explicada",
    "href": "sesion3_notas.html#varianza-explicada",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Varianza explicada",
    "text": "Varianza explicada\nLa teor√≠a de la normalidad bivariada tambi√©n nos dice que la desviaci√≥n est√°ndar de la distribuci√≥n condicional descrita anteriormente es:\nSD(Y|X=x)=œÉY1‚àíœÅ2\nSD(Y|X=x)=\\sigma_Y\\sqrt{1-\\rho^2}\n\nPara ver por qu√© esto es intuitivo, note que sin condicionamiento, SD(Y)=œÉYSD(Y)=\\sigma_Y estamos viendo la variabilidad de todos los hijos. Pero una vez que los condicionamos, s√≥lo estamos viendo la variabilidad de los hijos con un padre que mide 72 pulgadas de alto. Este grupo tender√° a ser ‚Äúalgo m√°s‚Äù alto (que el promedio), por lo que la desviaci√≥n est√°ndar se reduce.\nEspec√≠ficamente, se reduce a 1‚àíœÅ2=1‚àí0.25=0.86\\sqrt{1-\\rho^2}=\\sqrt{1-0.25}=0.86 de lo que era originalmente. Podr√≠amos decir que la estatura del padre ‚Äúexplica‚Äù el 14% de la variabilidad de estatura del hijo.\nLa frase ‚ÄúXX explica tal o cual porcentaje de la variabilidad‚Äù se utiliza com√∫nmente en papers acad√©micos. En este caso, este porcentaje se refiere realmente a la desviaci√≥n (SD al cuadrado). Por lo tanto, si los datos son normales bivariados, la varianza se reduce en 1‚àíœÅ21-\\rho^2 por lo que decimos que XX explica 1‚àí(1‚àíœÅ2)=œÅ21-(1-\\rho^2)=\\rho^2 (la correlaci√≥n al cuadrado) de la varianza.\nPero es importante recordar que la afirmaci√≥n de ‚Äúvarianza explicada‚Äù s√≥lo tiene sentido cuando los datos se aproximan mediante una distribuci√≥n normal bivariada."
  },
  {
    "objectID": "sesion3_notas.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n",
    "href": "sesion3_notas.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Cuidado: hay dos l√≠neas de regresi√≥n",
    "text": "Cuidado: hay dos l√≠neas de regresi√≥n\nCalculamos una l√≠nea de regresi√≥n para predecir la altura del hijo desde la altura del padre.\nUsamos estos c√°lculos:\n\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviaci√≥n est√°ndar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviaci√≥n est√°ndar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlaci√≥n entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n\n0.4282667416279721\n2.5644170903922188\n2.6860317955777377\n69.08938547486034\n69.23184357541899\n\n\n\n# Calcular la pendiente de la primera l√≠nea de regresi√≥n\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera l√≠nea de regresi√≥n\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n\npendiente 0.44857682836034635\nconstante 38.23994616574076\n\n\nLo que nos da la funci√≥n E(Y|X=x)=28,8+0.44xE(Y|X=x)=28,8+0.44x.\n¬øY si queremos predecir la estatura del padre bas√°ndonos en la del hijo?\nEs importante saber que esto no se determina calculando la funci√≥n inversa!.\nNecesitamos computar E(X‚à£Y=y)E(X‚à£Y=y). Dado que los datos son aproximadamente normales bivariados, la teor√≠a descrita anteriormente nos dice que esta expectativa condicional seguir√° una l√≠nea con pendiente e intercepto:\n\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n\npendiente 0.4088762289729847\nconstante 40.782130348895464\n\n\nDe nuevo vemos una regresi√≥n a la media: la predicci√≥n para el padre est√° m√°s cerca de la media del padre que lo que estan las alturas del hijo yy al promedio del hijo.\nAqu√≠ hay un gr√°fico que muestra las dos l√≠neas de regresi√≥n:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gr√°fico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()\n\n\n\n\ncon azul para la predicci√≥n de las alturas del hijo con las alturas del padre y rojo para la predicci√≥n de las alturas del padre con las alturas del hijo.\n\n\n\n\n\n\nTaller aplicacci√≥n 2: Altura de padres e hijos\n\n\n\n\nCargue los datos de GaltonFamilies desde el HistData. Los ni√±os de cada familia est√°n ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado galton_heights seleccionando ni√±os y ni√±as al azar. (HINT: use sample).\nHaga una gr√°fica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nCalcular la correlaci√≥n para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nPlotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\nObtener el modelo de regresi√≥n e interpretar los coeficientes."
  },
  {
    "objectID": "talleres.html",
    "href": "talleres.html",
    "title": "Talleres de aplicaci√≥n",
    "section": "",
    "text": "Vamos a trabajar 3 tralleres de aplicaci√≥n:\n\n\n\nActividad\nEnunciado\nFecha de Entrega\n\n\n\n\nTaller 1\nTaller 1 . Enunciado\n15 de septiembre\n\n\nTaller 2\n\nTBA\n\n\nTaller 3\n\nTBA"
  },
  {
    "objectID": "taller1_enunciado.html",
    "href": "taller1_enunciado.html",
    "title": "Taller 1: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Este taller es un conjunto de actividades para poner en pr√°ctica y consolidar los contenidos revisados en la primera unidad del curso, en las sesiones 1 y 2.\n\nPuedes realizarlo individualmente o hasta en grupos de 3 personas.\nLa entrega es hasta el 15 de septiembre (para todo el cr√©dito)\nDebes entregar un notebook en el que desarrolle las preguntas y no olvide concluir sus respuestas.\nSer√° evaluado mediante la r√∫brica de evaluaci√≥n disponible."
  },
  {
    "objectID": "taller1_enunciado.html#instrucciones",
    "href": "taller1_enunciado.html#instrucciones",
    "title": "Taller 1: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "",
    "text": "Este taller es un conjunto de actividades para poner en pr√°ctica y consolidar los contenidos revisados en la primera unidad del curso, en las sesiones 1 y 2.\n\nPuedes realizarlo individualmente o hasta en grupos de 3 personas.\nLa entrega es hasta el 15 de septiembre (para todo el cr√©dito)\nDebes entregar un notebook en el que desarrolle las preguntas y no olvide concluir sus respuestas.\nSer√° evaluado mediante la r√∫brica de evaluaci√≥n disponible."
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-1---bajando-y-formateando-datos-del-banco-mundial",
    "href": "taller1_enunciado.html#pregunta-1---bajando-y-formateando-datos-del-banco-mundial",
    "title": "Taller 1: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Pregunta 1 - Bajando y formateando datos del Banco Mundial",
    "text": "Pregunta 1 - Bajando y formateando datos del Banco Mundial\nReplique el ejemplo pr√°ctico de importar datos desde la API del Banco Mundial y empezar la base para su an√°lisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro pa√≠s serie desde la API del Banco mundial, muestre sus principales caracter√≠sticas y realice un grafico.\n¬øPareciera haber tendencias? Explique."
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-2---investigando-sobre-pa√≠ses",
    "href": "taller1_enunciado.html#pregunta-2---investigando-sobre-pa√≠ses",
    "title": "Taller 1: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Pregunta 2 - Investigando sobre pa√≠ses:",
    "text": "Pregunta 2 - Investigando sobre pa√≠ses:\nConsidere que tenemos los datos del banco mundial, del pa√≠s que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigaci√≥n que se pueda responder con los datos disponibles. ¬øC√≥mo definiria la variable aleatoria relevante? ¬øQu√© hip√≥tesis podria responder su pregunta?"
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-3---caso-aplicaci√≥n-ejemplo-ab-test-en-marketing",
    "href": "taller1_enunciado.html#pregunta-3---caso-aplicaci√≥n-ejemplo-ab-test-en-marketing",
    "title": "Taller 1: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Pregunta 3 - Caso aplicaci√≥n: Ejemplo AB test en Marketing:",
    "text": "Pregunta 3 - Caso aplicaci√≥n: Ejemplo AB test en Marketing:\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electr√≥nicos y queremos aumentar las ventas en una l√≠nea de productos espec√≠fica, como tel√©fonos m√≥viles.\nPara ello, decidimos utilizar una promoci√≥n de ventas basada en una ruleta l√∫dica que ofrecer√° descuentos a los clientes que la utilicen.\nPara implementar la promoci√≥n, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electr√≥nico con un enlace a la ruleta l√∫dica. Al hacer clic en el enlace, los clientes son redirigidos a una p√°gina en la que pueden girar la ruleta y ganar un descuento en su pr√≥xima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoci√≥n (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreaci√≥n de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste c√≥digo crear√° un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generaci√≥n de n√∫meros aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de n√∫mero de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df.head(5)\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n\n\n\n\n\n\n\nPreguntas:\nEstudiemos si la promoci√≥n fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¬øFue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¬øCual de las promociones fue m√°s efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "taller1_enunciado.html#pregunta-4---caso-de-aplicaci√≥n-datos-de-educaci√≥n",
    "href": "taller1_enunciado.html#pregunta-4---caso-de-aplicaci√≥n-datos-de-educaci√≥n",
    "title": "Taller 1: Introducci√≥n al An√°lisis de Regresi√≥n",
    "section": "Pregunta 4 - Caso de aplicaci√≥n datos de educaci√≥n",
    "text": "Pregunta 4 - Caso de aplicaci√≥n datos de educaci√≥n\n\nPregunta de investigaci√≥n\nNuestro objetivo es responder la siguiente pregunta ficticia de investigaci√≥n:\n\nAsistir a cursos de verano mejora los resultados acad√©micos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\nContexto\nLa pregunta de investigaci√≥n se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioecon√≥mico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opci√≥n de asistir a un curso de verano intensivo durante el verano entre 5 y 6to b√°sico.\nEl curso de verano se enfoca en mejorar las habilidades acad√©micas de preparar la prueba de admisi√≥n a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participaci√≥n en el curso en los resultados acad√©micos de los estudiantes.\n\n\n\nDatos ficticios dispobibles\nLos datos estan disponibles en https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene informaci√≥n sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si particip√≥ en el curso de verano, sexo, ingreso del hogar (en logaritmo), educaci√≥n de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el a√±o 5 como para el a√±o 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene informaci√≥n de cada individuo (con identificador id).\nEste dataset tiene la informaci√≥n si el individuo recibi√≥ la carta de invitaci√≥n para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene informaci√≥n de rendimiento acad√©mico antes y despu√©s del curso de verano.\n\n\n\nObjetivos:\nLa idea de este taller es poner en pr√°ctica los primeros pasos para un an√°lisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada. II. Tambi√©n exploraremos los datos, usaremos estad√≠sticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre los pasos anteriores m√∫ltiples veces. Tambi√©n invlocura tomar desiciones por parte de los investigadores, por lo cual la documentaci√≥n de esta fase es especialmente importante.\nEn nuestro caso, ser√° bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser as√≠.\n\n\nPasos que debe realizar:\n\n1. Preparaci√≥n inicial de los datos\nTenemos 3 bases de datos con informaci√≥n diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta informaci√≥n, especialmente si estimaremos modelos en base a √©sta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido deber√≠a tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de informaci√≥n del estudiante si recibi√≥ o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de uni√≥n person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que ac√° unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id √∫nica tiene un solo colegio, pero sirve de ejemplo en como usar m√°s de una columna mediante.\nUsamos la funci√≥n summary() para obtener una estad√≠stica descriptiva de las variables en el dataset unido.\n\n\n\n2. Limpieza de los datos\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera ‚Äútidy‚Äù (Seg√∫n Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observaci√≥n.\n\n(puedes profundizar y ver m√°s ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observaci√≥n.\nSi inspeccionamos el n√∫mero de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del a√±o 2 al 10. Este tipo de datos son de panel.\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y an√°lisi preferimos que est√© en formato long. Especialmente cuando modelamos incluyendo efectos fijos Tambi√©n es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos pivot_longer.\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selecci√≥n de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podr√≠amos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y n√∫mero de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn √∫ltimo paso que haremos antes de hacer estad√≠stica decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviaci√≥n est√°ndar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto espec√≠fico de la medida y es m√°s facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estad√≠stica descriptiva.\n\n\n3. Estad√≠stica descriptiva\nHasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estad√≠sticas descriptivas y tambi√©n algunos graficos descriptivos.\n3.1 Tablas de ets√°distocas descriptivas Incluya la media, la desviaci√≥n estandar, la mediana, max y min, al menos.\n3.2 Gr√°ficos de estad√≠stica descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  },
  {
    "objectID": "sesion2_slides.html#la-preparaci√≥n-de-los-datos",
    "href": "sesion2_slides.html#la-preparaci√≥n-de-los-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "La preparaci√≥n de los datos",
    "text": "La preparaci√≥n de los datos\n\nLa preparaci√≥n de datos es una fase esencial en el proceso de an√°lisis de datos que involucra una serie de actividades destinadas a garantizar que los datos est√©n en condiciones √≥ptimas para su posterior an√°lisis.\n\nExtraccion de los datos\nLimpieza\nTransformaci√≥n\nOrganizaci√≥n"
  },
  {
    "objectID": "sesion2_slides.html#tipos-de-datos",
    "href": "sesion2_slides.html#tipos-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tipos de datos",
    "text": "Tipos de datos\n\nAntes de adentrarnos en la preparaci√≥n de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\n\nPodemos clasificar estos datos que recopilamos de muchas maneras:\n\nEstructura\nTamanÃÉo\nOperacionalizacioÃÅn\nTemporalidad y unidad de anaÃÅlisis."
  },
  {
    "objectID": "sesion2_slides.html#estructura-de-los-datos",
    "href": "sesion2_slides.html#estructura-de-los-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Estructura de los datos",
    "text": "Estructura de los datos\n\nLa estructura de un dato se refiere a su formato y codificaci√≥n."
  },
  {
    "objectID": "sesion2_slides.html#datos-estructurados",
    "href": "sesion2_slides.html#datos-estructurados",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos estructurados",
    "text": "Datos estructurados\n\nConjuntos de tablas forman bases de datos estructuradas.\nTablas son formas organizadas y ordenadas de datos.\nLas entradas en tablas pueden ser texto o n√∫meros.\nLos datos estructurados son recopilados y organizados intencionalmente."
  },
  {
    "objectID": "sesion2_slides.html#datos-no-estructurados",
    "href": "sesion2_slides.html#datos-no-estructurados",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos No estructurados",
    "text": "Datos No estructurados\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, im√°genes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido."
  },
  {
    "objectID": "sesion2_slides.html#algunos-ejemplos-de-datos-no-estructurados-son",
    "href": "sesion2_slides.html#algunos-ejemplos-de-datos-no-estructurados-son",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Algunos ejemplos de datos no estructurados son:",
    "text": "Algunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteoroloÃÅgicos\nColecciones de documentos digitalizados: Facturas, registros, correos electroÃÅnicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos."
  },
  {
    "objectID": "sesion2_slides.html#estructurados-vs-no-estructuraods",
    "href": "sesion2_slides.html#estructurados-vs-no-estructuraods",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Estructurados vs no estructuraods",
    "text": "Estructurados vs no estructuraods\n\nDatos estructurados est√°n en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la informaci√≥n, pero los no estructurados son m√°s abundantes."
  },
  {
    "objectID": "sesion2_slides.html#datos-seg√∫n-tamano",
    "href": "sesion2_slides.html#datos-seg√∫n-tamano",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos seg√∫n tamanÃÉo",
    "text": "Datos seg√∫n tamanÃÉo\n\nBig Data:\n\nSe refiere a conjuntos de datos extremadamente grandes y complejos que son dif√≠ciles de gestionar, procesar y analizar con herramientas y m√©todos tradicionales.\nBig Data involucra terabytes o incluso petabytes de informaci√≥n y generalmente requiere tecnolog√≠as y enfoques especializados para extraer conocimientos significativos."
  },
  {
    "objectID": "sesion2_slides.html#datos-seg√∫n-tamano-1",
    "href": "sesion2_slides.html#datos-seg√∫n-tamano-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos seg√∫n tamanÃÉo",
    "text": "Datos seg√∫n tamanÃÉo\n\nSmall Data:\n\nSe refiere a conjuntos de datos m√°s peque√±os y manejables en comparaci√≥n con Big Data.\nEstos conjuntos de datos son m√°s accesibles y pueden ser procesados y analizados utilizando herramientas y m√©todos convencionales.\nA menudo, Small Data se centra en obtener informaci√≥n valiosa de fuentes limitadas y espec√≠ficas."
  },
  {
    "objectID": "sesion2_slides.html#datos-seg√∫n-tipo-y-su-operacionalizacion",
    "href": "sesion2_slides.html#datos-seg√∫n-tipo-y-su-operacionalizacion",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos seg√∫n tipo y su operacionalizacioÃÅn",
    "text": "Datos seg√∫n tipo y su operacionalizacioÃÅn\n\nLas variables en an√°lisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan n√∫meros medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos n√∫meros o factores que representen categor√≠as, facilitando su an√°lisis cuantitativo.\n\n\nContinuas y discretas"
  },
  {
    "objectID": "sesion2_slides.html#datos-seg√∫n-temporalidad-y-unidad-de-analisis.",
    "href": "sesion2_slides.html#datos-seg√∫n-temporalidad-y-unidad-de-analisis.",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos seg√∫n temporalidad y unidad de anaÃÅlisis.",
    "text": "Datos seg√∫n temporalidad y unidad de anaÃÅlisis.\nOtra manera de clasificar los datos con relaci√≥n a la temporalidad en la cual son tomados y cu√°l es la unidad de an√°lisis.\n\nCorte transversal\nSerie de tiempo\nPanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_slides.html#corte-transversal",
    "href": "sesion2_slides.html#corte-transversal",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Corte transversal",
    "text": "Corte transversal"
  },
  {
    "objectID": "sesion2_slides.html#serie-temporal",
    "href": "sesion2_slides.html#serie-temporal",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Serie temporal",
    "text": "Serie temporal"
  },
  {
    "objectID": "sesion2_slides.html#panel",
    "href": "sesion2_slides.html#panel",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Panel",
    "text": "Panel"
  },
  {
    "objectID": "sesion2_slides.html#leyendo-datos-en-pandas",
    "href": "sesion2_slides.html#leyendo-datos-en-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a trav√©s de la familia de funciones read_tipo"
  },
  {
    "objectID": "sesion2_slides.html#leyendo-datos-en-pandas-1",
    "href": "sesion2_slides.html#leyendo-datos-en-pandas-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a trav√©s de la familia de funciones read_tipo\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read:\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: autom√°tico o definido por el usuario\nDatetime parsing: puede combinar informaci√≥n de m√∫ltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con n√∫meros con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2_slides.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2_slides.html#formato-csv-valores-separados-por-comas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de informaci√≥n: no sabemos qu√© son las columnas (n√∫meros, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qu√© tipo para transformar cada columna basado en lo que parece ser\n\n¬øY qu√© pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2_slides.html#csv-en-pandas",
    "href": "sesion2_slides.html#csv-en-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nB√°sica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nB√°sica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representaci√≥n de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2_slides.html#json-java-script-object-notation",
    "href": "sesion2_slides.html#json-java-script-object-notation",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, n√∫meros, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores num√©ricos"
  },
  {
    "objectID": "sesion2_slides.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2_slides.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2_slides.html#orientaciones-posibles-de-json",
    "href": "sesion2_slides.html#orientaciones-posibles-de-json",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2_slides.html#extensible-markup-language-xml",
    "href": "sesion2_slides.html#extensible-markup-language-xml",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato m√°s antiguo y auto descriptivo, con estructura jer√°rquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un m√©todo incorporado en Python.\nSe puede usar la librer√≠a lxml (tambi√©n ElementTree)"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-xml",
    "href": "sesion2_slides.html#ejemplo-xml",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ejemplo XML",
    "text": "Ejemplo XML"
  },
  {
    "objectID": "sesion2_slides.html#formatos-binarios",
    "href": "sesion2_slides.html#formatos-binarios",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¬øQu√© es un formato binario?\nPickle: Python‚Äôs built-in serialization\n\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\n\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}"
  },
  {
    "objectID": "sesion2_slides.html#formatos-binarios-1",
    "href": "sesion2_slides.html#formatos-binarios-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nHDF5: Librer√≠a para almacenar gran cantidad de datos cient√≠ficos\n\nFormato jer√°rquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresi√≥n\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de c√°lculo tiene m√∫ltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2_slides.html#bases-de-datos-relacionales",
    "href": "sesion2_slides.html#bases-de-datos-relacionales",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales"
  },
  {
    "objectID": "sesion2_slides.html#bases-de-datos-relacionales-1",
    "href": "sesion2_slides.html#bases-de-datos-relacionales-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\nLas bases de datos relacionales son similares a los marcos de datos m√∫ltiples pero tienen muchas m√°s caracter√≠sticas\n\nv√≠nculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos¬†\n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayor√≠a de los sistemas de bases de datos a trav√©s de un API com√∫n\nSintaxis similar (¬°pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.)¬†\nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n\n\n\nimport sqlalchemy as sqla\ndb = sqla.create_engine('sqlite:///mydata.sqlite')\npd.read_sql('select \\* from test', db)"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-estad√≠stico",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-estad√≠stico",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty data: punto de vista estad√≠stico",
    "text": "Dirty data: punto de vista estad√≠stico\n\nLos datos son generados desde alg√∫n proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsi√≥n: algunas muestras se corrompieron por un proceso\nSesgo de selecci√≥n: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: hay valores que no observamos\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisi√≥n y simplicidad"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores est√°n missing, corrompidos, err√≥neos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-del-experto-en-un-√°rea",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-del-experto-en-un-√°rea",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un √°rea",
    "text": "Dirty Data: Punto de vista del experto en un √°rea\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¬øQu√© ocurri√≥?\n\nLos expertos en un √°rea llevan un modelo impl√≠cito de los datos que est√°n testeando\n\nNo siempre necesitas ser un experto en un √°rea para hacer esto\n\n¬øPuede una persona correr 500 km por hora?\n¬øPuede una monta√±a en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido com√∫n"
  },
  {
    "objectID": "sesion2_slides.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2_slides.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinaci√≥n de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregaci√≥n es menos susceptible a los errores num√©ricos\nSer cuidadoso, puede que los datos est√©n bien‚Ä¶"
  },
  {
    "objectID": "sesion2_slides.html#d√≥nde-se-origina-el-dirty-data",
    "href": "sesion2_slides.html#d√≥nde-se-origina-el-dirty-data",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "¬øD√≥nde se origina el dirty data?",
    "text": "¬øD√≥nde se origina el dirty data?\n\nLa fuente est√° mal, por ejemplo, una persona la ingres√≥ de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integraci√≥n de diferentes sets de datos causa problemas\nPropagaci√≥n del error: se magnifica un error"
  },
  {
    "objectID": "sesion2_slides.html#problemas-dirty-comunes",
    "href": "sesion2_slides.html#problemas-dirty-comunes",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs.¬†New York\nP√©rdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs.¬†dos\nDatos truncados: ‚ÄúJanice Keihanaikukauakahihuliheekahaunaele‚Äù se vuelve ‚ÄúJanice - Keihanaikukauakahihuliheek‚Äù en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposici√≥n\nProblemas de formato: 2017-11-07 vs.¬†07/11/2017 vs.¬†11/07/2017"
  },
  {
    "objectID": "sesion2_slides.html#preparando-los-datos",
    "href": "sesion2_slides.html#preparando-los-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Preparando los datos",
    "text": "Preparando los datos\nMuchas veces los datos con los que queremos trabajar no est√°n en el formato adecuado para los an√°lisis que querenos realizar."
  },
  {
    "objectID": "sesion2_slides.html#data-wrangling",
    "href": "sesion2_slides.html#data-wrangling",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato m√°s significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representaci√≥n a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2_slides.html#tidy-data",
    "href": "sesion2_slides.html#tidy-data",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para an√°lisis de corte transversal nuestro ideal es trabajar con Tidy Data\n‚ÄúTidy Data is a standar way of mapping the meaning of a dataset to its structure¬®\nUn tipo de estructura de datos √∫til para poder realizar modelos y an√°lisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un est√°ndar deseable para analizar datos."
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-1",
    "href": "sesion2_slides.html#tidy-data-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observacioÃÅn (registro) forma una fila.\nCada dato (valor) estaÃÅ en una celda de la tabla."
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-2",
    "href": "sesion2_slides.html#tidy-data-2",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data"
  },
  {
    "objectID": "sesion2_slides.html#tidy-data-3",
    "href": "sesion2_slides.html#tidy-data-3",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tidy data",
    "text": "Tidy data\nCon Tidy data podemos tener una estandarizaci√≥n en el tratamiento de los datos:"
  },
  {
    "objectID": "sesion2_slides.html#tidy-workflow",
    "href": "sesion2_slides.html#tidy-workflow",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tidy & workflow",
    "text": "Tidy & workflow\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados."
  },
  {
    "objectID": "sesion2_slides.html#porqu√©-datos-tidy",
    "href": "sesion2_slides.html#porqu√©-datos-tidy",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "¬øPorqu√© datos Tidy?",
    "text": "¬øPorqu√© datos Tidy?\n\nEstandarizanci√≥n\n\n\n\n\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#facilitar-la-colaboraci√≥n",
    "href": "sesion2_slides.html#facilitar-la-colaboraci√≥n",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Facilitar la colaboraci√≥n",
    "text": "Facilitar la colaboraci√≥n\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#simplifica-la-reproducibilidad",
    "href": "sesion2_slides.html#simplifica-la-reproducibilidad",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Simplifica la reproducibilidad",
    "text": "Simplifica la reproducibilidad\n\nFuente: https://allisonhorst.com/other-r-fun"
  },
  {
    "objectID": "sesion2_slides.html#siempre-tidy",
    "href": "sesion2_slides.html#siempre-tidy",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "¬øSiempre Tidy?",
    "text": "¬øSiempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean √∫tiles o ‚Äúdesordenadas‚Äù.\nEs importante tener en cuenta que siempre existen m√∫ltiples formas de representar la misma informaci√≥n."
  },
  {
    "objectID": "sesion2_slides.html#otras-estructuras",
    "href": "sesion2_slides.html#otras-estructuras",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Otras estructuras:",
    "text": "Otras estructuras:\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempe√±o computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2_slides.html#algunas-tareas-comunes",
    "href": "sesion2_slides.html#algunas-tareas-comunes",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Algunas tareas comunes",
    "text": "Algunas tareas comunes\n\nDescartar e imputar missing data\nRemover duplicados.\nModificar datos\n\nmapear strings, expresiones aritm√©ticas. Ejemplos:\n\nConvertir strings de may√∫sculas a min√∫sculas (upper/lower case)\nConvertir T en Fahrenheit a Celsius\nCrear una nueva columna basada en la columna anterior."
  },
  {
    "objectID": "sesion2_slides.html#algunas-tareas-comunes-1",
    "href": "sesion2_slides.html#algunas-tareas-comunes-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Algunas tareas comunes",
    "text": "Algunas tareas comunes\n\nReemplazar valores\n\n(e.g.¬†-999 ‚Üí NaN). Usar m√©todo df[‚Äòcolumn‚Äô].replace()\n\nRestringir valores:\n\nvalores por encima o por debajo de los umbrales especificados se establecen en un valor m√°ximo/m√≠nimo."
  },
  {
    "objectID": "sesion2_slides.html#datos-perdidos-o-missing-na-nan",
    "href": "sesion2_slides.html#datos-perdidos-o-missing-na-nan",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos perdidos (o missing, NA, NAN)",
    "text": "Datos perdidos (o missing, NA, NAN)\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\nse pueden escoger filas o columnas\n\nLlenar/ reemplazar :\n\ncon un valor por default\ncon un valor interpolados, otros"
  },
  {
    "objectID": "sesion2_slides.html#datos-perdidos",
    "href": "sesion2_slides.html#datos-perdidos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-missing",
    "href": "sesion2_slides.html#ejemplo-missing",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ejemplo missing",
    "text": "Ejemplo missing\n\nimport pandas as pd\nimport numpy as np\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', np.nan, 'Carlos'],\n        'Edad': [25, 30, np.nan, 30, 28]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar filas con valores faltantes\ndf = df.dropna()\n\n# Llenar valores faltantes con un valor espec√≠fico\ndf['Edad'].fillna(0, inplace=True)\n\nprint(df)\n\n\n\n   Nombre  Edad\n0    Juan  25.0\n1     Ana  30.0\n4  Carlos  28.0"
  },
  {
    "objectID": "sesion2_slides.html#fitrando-y-limpiando",
    "href": "sesion2_slides.html#fitrando-y-limpiando",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cu√°l de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas espec√≠fics para chequear por duplicados, e.g.¬†chequear solo la columna key."
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-fitrando-y-limpiando",
    "href": "sesion2_slides.html#ejemplo-fitrando-y-limpiando",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ejemplo Fitrando y limpiando",
    "text": "Ejemplo Fitrando y limpiando\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-datos-de-educaci√≥n",
    "href": "sesion2_slides.html#ejemplo-datos-de-educaci√≥n",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ejemplo: Datos de educaci√≥n",
    "text": "Ejemplo: Datos de educaci√≥n\n\n\n\n\n\n\nTaller de aplicaci√≥n 1: Pregunta 4"
  },
  {
    "objectID": "sesion2_slides.html#errores-de-registro-y-textos",
    "href": "sesion2_slides.html#errores-de-registro-y-textos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\nUno de los errores de registro m√°s t√≠pico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) ‚Äúquiebra‚Äù o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14‚Äù"
  },
  {
    "objectID": "sesion2_slides.html#errores-de-registro-y-textos-1",
    "href": "sesion2_slides.html#errores-de-registro-y-textos-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Errores de registro y textos",
    "text": "Errores de registro y textos\n\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14‚Äù\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD‚Äù \"AbCd\".lower() # \"abcd\""
  },
  {
    "objectID": "sesion2_slides.html#errores-de-formato",
    "href": "sesion2_slides.html#errores-de-formato",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Errores de formato",
    "text": "Errores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a n√∫meros eliminando el s√≠mbolo de d√≥lar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0"
  },
  {
    "objectID": "sesion2_slides.html#transformando-strings",
    "href": "sesion2_slides.html#transformando-strings",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Transformando strings",
    "text": "Transformando strings\nPodemos encontrar donde estan los problemas facilmente:\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n\n\n2\n\n\n\n\n\n#s.index(':') # ValueError raised"
  },
  {
    "objectID": "sesion2_slides.html#encontrar-problemas",
    "href": "sesion2_slides.html#encontrar-problemas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Encontrar problemas",
    "text": "Encontrar problemas\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado . . .\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n\n\n2\n\n\n\n\ns.find(':') # -1\n\n\n\n-1"
  },
  {
    "objectID": "sesion2_slides.html#encontrar-problemas-1",
    "href": "sesion2_slides.html#encontrar-problemas-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Encontrar problemas",
    "text": "Encontrar problemas\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string . . .\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\n\n\nFalse"
  },
  {
    "objectID": "sesion2_slides.html#m√©todos-para-strings",
    "href": "sesion2_slides.html#m√©todos-para-strings",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "M√©todos para strings",
    "text": "M√©todos para strings"
  },
  {
    "objectID": "sesion2_slides.html#m√©todos-para-strings-1",
    "href": "sesion2_slides.html#m√©todos-para-strings-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "M√©todos para strings",
    "text": "M√©todos para strings\n\nCualquier columna o serie puede tener m√©todos string (e.g.¬†replace, split) aplicado a la serie completa\nEst√° vectorizado para columnas completas o incluso el dataframe (lo cual lo hace r√°pido)\nSe debe usar .str. (es importante el .str)."
  },
  {
    "objectID": "sesion2_slides.html#ejemplo",
    "href": "sesion2_slides.html#ejemplo",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ejemplo:",
    "text": "Ejemplo:\n\ngmailsplit3 char\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\n\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\n\n\n\n    \nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\n\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\n\n\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2_slides.html#expresiones-regulares-para-strings",
    "href": "sesion2_slides.html#expresiones-regulares-para-strings",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Expresiones regulares para strings",
    "text": "Expresiones regulares para strings"
  },
  {
    "objectID": "sesion2_slides.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2_slides.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets"
  },
  {
    "objectID": "sesion2_slides.html#eliminar-elegir-columnas",
    "href": "sesion2_slides.html#eliminar-elegir-columnas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Eliminar/ elegir columnas",
    "text": "Eliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro an√°lisis. . . .\n\nCodetabla originaltabla sin peso\n\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\ndf.head(6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\nEdad\nPeso (kg)\n\n\n\n\n0\nJuan\n25\n70\n\n\n1\nAna\n30\n65\n\n\n2\nLuis\n35\n80\n\n\n3\nCarlos\n28\n75\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\nEdad\n\n\n\n\n0\nJuan\n25\n\n\n1\nAna\n30\n\n\n2\nLuis\n35\n\n\n3\nCarlos\n28"
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato",
    "href": "sesion2_slides.html#reshapesre-formato",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato\n\nEl ‚Äúreshape‚Äù o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposici√≥n de filas y columnas para adaptarse a un formato espec√≠fico.\nEsto puede implicar la transformaci√≥n de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el an√°lisis, la visualizaci√≥n o la aplicaci√≥n de modelos estad√≠sticos.\nEl ‚Äúreshape‚Äù es com√∫nmente realizado en la manipulaci√≥n de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de c√°lculo."
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato-1",
    "href": "sesion2_slides.html#reshapesre-formato-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato"
  },
  {
    "objectID": "sesion2_slides.html#reshapesre-formato-2",
    "href": "sesion2_slides.html#reshapesre-formato-2",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Reshapes/re formato",
    "text": "Reshapes/re formato\n\nCodeDF anchoDF largo\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCiudad\nA\nB\n\n\nFecha\n\n\n\n\n\n\n2023-08-01\n25\n28\n\n\n2023-08-02\n26\n29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFecha\nCiudad\nTemperatura\n\n\n\n\n0\n2023-08-01\nA\n25\n\n\n1\n2023-08-01\nB\n28\n\n\n2\n2023-08-02\nA\n26\n\n\n3\n2023-08-02\nB\n29"
  },
  {
    "objectID": "sesion2_slides.html#crear-variables-dummies-dicot√≥micas",
    "href": "sesion2_slides.html#crear-variables-dummies-dicot√≥micas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Crear variables dummies / dicot√≥micas",
    "text": "Crear variables dummies / dicot√≥micas\n\nMuy √∫til para representar informaci√≥n cualitativa.\nMuy usando en an√°lisis de regresi√≥n y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o m√°s indicadores que toman valor 0 √≥ 1.\nSe pueden generar mediante pd.get_dummies(df[‚Äòkey‚Äô])"
  },
  {
    "objectID": "sesion2_slides.html#one-hot-encoding-razas-de-perros",
    "href": "sesion2_slides.html#one-hot-encoding-razas-de-perros",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "One hot encoding: Razas de perros",
    "text": "One hot encoding: Razas de perros"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-razas-de-perros",
    "href": "sesion2_slides.html#ejemplo-razas-de-perros",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ejemplo : Razas de perros",
    "text": "Ejemplo : Razas de perros\n\nCodetabla\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicot√≥micas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicot√≥micas para las razas de inter√©s\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabrador\nPoodle\nBulldog\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n\n\n3\n0\n0\n0\n\n\n4\n0\n0\n0\n\n\n5\n0\n0\n0"
  },
  {
    "objectID": "sesion2_slides.html#unir-datasets",
    "href": "sesion2_slides.html#unir-datasets",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos m√©todos principales:\n\nApliar/concatenar/append: Consiste en agregar un dataset a continuaci√≥n de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join: Las uniones combinan DataFrames utilizando columnas en com√∫n como clave. Puedes realizar diferentes tipos de uniones, como ‚Äúinner‚Äù, ‚Äúouter‚Äù, ‚Äúleft‚Äù y ‚Äúright‚Äù"
  },
  {
    "objectID": "sesion2_slides.html#apilar-dataframes",
    "href": "sesion2_slides.html#apilar-dataframes",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "1. Apilar dataframes",
    "text": "1. Apilar dataframes\nCreemos los dataframes: . . .\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\n\n\n   P  Q\n0  2  3\n1  4  5\n\n\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n\n\n   P  Q\n0  6  7\n1  8  9"
  },
  {
    "objectID": "sesion2_slides.html#apilar",
    "href": "sesion2_slides.html#apilar",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Apilar",
    "text": "Apilar\nHagamos el apilar. Atenci√≥n a los index\n\n\ndf.append(df2)\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9"
  },
  {
    "objectID": "sesion2_slides.html#apilar-1",
    "href": "sesion2_slides.html#apilar-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Apilar",
    "text": "Apilar"
  },
  {
    "objectID": "sesion2_slides.html#ojo-al-apilar-con-los-index",
    "href": "sesion2_slides.html#ojo-al-apilar-con-los-index",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ojo al apilar con los index",
    "text": "Ojo al apilar con los index\nUsemos ignorar index . . .\n\ndf.append(df2, ignore_index=True)\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9"
  },
  {
    "objectID": "sesion2_slides.html#ojo-al-apilar-con-los-index-1",
    "href": "sesion2_slides.html#ojo-al-apilar-con-los-index-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ojo al apilar con los index",
    "text": "Ojo al apilar con los index"
  },
  {
    "objectID": "sesion2_slides.html#unir-dataframes",
    "href": "sesion2_slides.html#unir-dataframes",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "2. Unir dataframes",
    "text": "2. Unir dataframes"
  },
  {
    "objectID": "sesion2_slides.html#unir-datasets-1",
    "href": "sesion2_slides.html#unir-datasets-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nUtilizando la funci√≥n pd.merge()\n\n\ncodeinner joinouter join\n\n\n\n# Crear dos DataFrames con columnas en com√∫n\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Uni√≥n interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Uni√≥n externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Uni√≥n interna:\")\nprint(inner_join)\n\nprint(\"\\nUni√≥n externa:\")\nprint(outer_join)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nValue_x\nValue_y\n\n\n\n\n0\nB\n2\n4\n\n\n1\nC\n3\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nValue_x\nValue_y\n\n\n\n\n0\nA\n1.0\nNaN\n\n\n1\nB\n2.0\n4.0\n\n\n2\nC\n3.0\n5.0\n\n\n3\nD\nNaN\n6.0"
  },
  {
    "objectID": "sesion2_slides.html#transformaciones-estad√≠sticas",
    "href": "sesion2_slides.html#transformaciones-estad√≠sticas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Transformaciones estad√≠sticas:",
    "text": "Transformaciones estad√≠sticas:\n1. Outliers\n\nLa identificaci√≥n de valores at√≠picos es importante por varias razones:\n\n\nCalidad de los datos\nImpacto en estad√≠sticas y modelos\nAnomal√≠as y problemas reales\nToma de decisiones informada"
  },
  {
    "objectID": "sesion2_slides.html#ejemplo-outliers",
    "href": "sesion2_slides.html#ejemplo-outliers",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Ejemplo outliers",
    "text": "Ejemplo outliers\nDetecci√≥n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores at√≠picos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuart√≠lico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los l√≠mites para identificar valores at√≠picos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores at√≠picos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores at√≠picos:\")\nprint(outliers)\n\n\n\nValores at√≠picos:\n   Valor\n6    200"
  },
  {
    "objectID": "sesion2_slides.html#estandarizaci√≥n-de-datos",
    "href": "sesion2_slides.html#estandarizaci√≥n-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "2. Estandarizaci√≥n de Datos",
    "text": "2. Estandarizaci√≥n de Datos\n\nLa estandarizaci√≥n es un proceso importante en el an√°lisis de datos y el aprendizaje autom√°tico.\nConsiste en transformar los valores de una variable de manera que tengan una media de cero y una desviaci√≥n est√°ndar de uno.\nEsta transformaci√≥n se logra mediante la f√≥rmula:\n\n\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]"
  },
  {
    "objectID": "sesion2_slides.html#estandarizaci√≥n",
    "href": "sesion2_slides.html#estandarizaci√≥n",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Estandarizaci√≥n",
    "text": "Estandarizaci√≥n\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviaci√≥n est√°ndar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarizaci√≥n de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviaci√≥n est√°ndar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarizaci√≥n de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\n\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]"
  },
  {
    "objectID": "sesion2_slides.html#normalizaci√≥n-de-datos",
    "href": "sesion2_slides.html#normalizaci√≥n-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Normalizaci√≥n de Datos",
    "text": "Normalizaci√≥n de Datos\n\nLa normalizaci√≥n es un proceso esencial en el an√°lisis de datos y el aprendizaje autom√°tico.\nConsiste en ajustar los valores de una variable para que se encuentren dentro de un rango espec√≠fico, generalmente entre 0 y 1.\nLa f√≥rmula matem√°tica utilizada para la normalizaci√≥n es:\n\n\n\\[x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\]"
  },
  {
    "objectID": "sesion2_slides.html#normalizaci√≥n-de-datos-1",
    "href": "sesion2_slides.html#normalizaci√≥n-de-datos-1",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Normalizaci√≥n de Datos",
    "text": "Normalizaci√≥n de Datos\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores m√≠nimo y m√°ximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalizaci√≥n min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores m√≠nimo y m√°ximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalizaci√≥n min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\n\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]\n\n\n\n\n\n\n\n\nCurso An√°lisis de Datos - Sesi√≥n 2"
  },
  {
    "objectID": "sesion1_slides.html#contenidos",
    "href": "sesion1_slides.html#contenidos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Contenidos:",
    "text": "Contenidos:\n\n 1. El proceso de an√°lisis de datos \n \n     Visi√≥n general\n      \n        Las metodolog√≠as de an√°lisis que veremos en el curso\n        Adquisici√≥n y almacenamiento de los datos\n        Preparaci√≥n de los datos\n      \n    \n     Preguntando a los datos\n      \n        Abstrayendo la realidad.\n        Planteamiento de preguntas.\n        El rol de las hip√≥tesis.\n      \n    \n  \n\n\n\n   2. Respondiendo desde los datos: Pruebas de hip√≥tesis  \n  \n    Conceptos B√°sicos de Pruebas de Hip√≥tesis:\n      \n        Definici√≥n de hip√≥tesis nula y alternativa.\n        Intervalos de confianza.\n        Niveles de significancia y p-values.\n        Errores tipo I y tipo II.\n      \n    \n    Tipos de Pruebas de Hip√≥tesis:\n      \n        Pruebas t para comparaci√≥n de medias.\n        Pruebas chi-cuadrado para variables categ√≥ricas.\n        Pruebas ANOVA para comparaci√≥n de m√∫ltiples grupos.\n      \n    \n    Interpretaci√≥n de Resultados:\n  \n\n\n\n\n   3. Buenas pr√°cticas en an√°lisis de datos \n  \n     Desaf√≠os y Consideraciones:\n      \n        Privacidad y seguridad de los datos.\n        Limpieza y transformaci√≥n durante la preparaci√≥n de datos.\n      \n    \n     Reproducibilidad y Control de Versiones (GIT):\n      \n        Importancia de mantener un registro de los cambios en los datos.\n        Uso de sistemas de control de versiones como GIT para rastrear cambios.\n        Aplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos."
  },
  {
    "objectID": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-1",
    "href": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "El proceso de la ciencia de datos",
    "text": "El proceso de la ciencia de datos"
  },
  {
    "objectID": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-2",
    "href": "sesion1_slides.html#el-proceso-de-la-ciencia-de-datos-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "El proceso de la ciencia de datos",
    "text": "El proceso de la ciencia de datos\nEn este curso nos enfocaremos en:\n\n\n\nLa preparaci√≥n de los datos\nAn√°lisis mediante modelos de regresi√≥n"
  },
  {
    "objectID": "sesion1_slides.html#el-objetivo-dar-valor",
    "href": "sesion1_slides.html#el-objetivo-dar-valor",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "El objetivo: dar valor",
    "text": "El objetivo: dar valor\n\nEsto con el objetivo de responder preguntas desde los datos, que provean informaci√≥n valiosa."
  },
  {
    "objectID": "sesion1_slides.html#adquisici√≥n-de-datos",
    "href": "sesion1_slides.html#adquisici√≥n-de-datos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Adquisici√≥n de datos:",
    "text": "Adquisici√≥n de datos:\n\nEl primer paso en el proceso de an√°lisis de datos implica la adquisici√≥n y el almacenamiento de los datos.\nEsto se refiere a la recolecci√≥n de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilaci√≥n de datos de fuentes diversas, como bases de datos, archivos CSV, p√°ginas web o incluso sensores en tiempo real."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-i",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-i",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes I",
    "text": "Fuentes de datos comunes I\nExisten tantas fuentes de datos, como podr√≠amos imaginar‚Ä¶\n\nEncuestas y Cuestionarios:\n\nDise√±o y administraci√≥n de encuestas para recopilar datos directamente de los participantes.\nPermite obtener informaci√≥n espec√≠fica y detallada seg√∫n las preguntas planteadas.\n\nExperimentos Controlados:\n\nDise√±o de experimentos para recopilar datos bajo condiciones controladas.\n√ötil para establecer relaciones causales y evaluar efectos de cambios controlados."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-ii",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-ii",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes II",
    "text": "Fuentes de datos comunes II\n\nObservaci√≥n y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar informaci√≥n.\nUtilizaci√≥n de sensores en dispositivos m√≥viles y wearables para recopilar datos de ubicaci√≥n, salud y actividad.\n\nRecopilaci√≥n de Datos Existentes:\n\nUtilizaci√≥n de datos ya recopilados y disponibles en bases de datos o fuentes p√∫blicas.\nReduce el tiempo y costo de recopilaci√≥n, pero puede tener limitaciones en t√©rminos de calidad y relevancia."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-iii",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-iii",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes III",
    "text": "Fuentes de datos comunes III\n\nWeb Scraping (Web Scrapping):\n\nExtracci√≥n de datos de sitios web utilizando herramientas y t√©cnicas automatizadas.\nPermite recopilar informaci√≥n no estructurada de manera eficiente, pero requiere atenci√≥n a la √©tica y t√©rminos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracci√≥n program√°tica con sistemas y servicios para obtener datos en tiempo real.\nCom√∫n en la obtenci√≥n de datos de redes sociales, informaci√≥n clim√°tica, finanzas, entre otros."
  },
  {
    "objectID": "sesion1_slides.html#fuentes-de-datos-comunes-iv",
    "href": "sesion1_slides.html#fuentes-de-datos-comunes-iv",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Fuentes de datos comunes IV",
    "text": "Fuentes de datos comunes IV\n\nColaboraci√≥n y Participaci√≥n Comunitaria:\n\nColaboraci√≥n con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser √∫til para proyectos de mapeo colaborativo, ciencia ciudadana y recopilaci√≥n de informaci√≥n local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes vol√∫menes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilaci√≥n y posterior an√°lisis de datos heterog√©neos.\nUsualmente se accede a trav√©s de querys SQL"
  },
  {
    "objectID": "sesion1_slides.html#proyecto",
    "href": "sesion1_slides.html#proyecto",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Proyecto",
    "text": "Proyecto\n\n\n\n\n\n\nDatos disponibles\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos p√∫blicos sobre educaci√≥n chilena\nDatos p√∫blicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad econ√≥mica\n\nVeamos como acceder algunos de estos datos."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-encuesta-casen",
    "href": "sesion1_slides.html#ejemplo-encuesta-casen",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Encuesta Casen",
    "text": "Ejemplo: Encuesta Casen\nDatos p√∫blicos\nLa Encuesta de Caracterizaci√≥n Socioecon√≥mica Nacional (CASEN), se realuza en chile:\n\nObjetivo: recopilar informaci√≥n detallada sobre la situaci√≥n socioecon√≥mica de los hogares y las personas en el pa√≠s.\n\nEsta encuesta se lleva a cabo de manera peri√≥dica y abarca una amplia variedad de temas, como ingresos, educaci√≥n, empleo, salud, vivienda y otros aspectos.\nSe utiliza para informar pol√≠ticas p√∫blicas, tomar decisiones informadas y analizar la evoluci√≥n de indicadores sociales a lo largo del tiempo.\n\nSitio Web oficial"
  },
  {
    "objectID": "sesion1_slides.html#encuesta-casen",
    "href": "sesion1_slides.html#encuesta-casen",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Encuesta Casen",
    "text": "Encuesta Casen\nEjemplo\n\n\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. . . .\n\n\ncodeOutput\n\n\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(4) # veamos las 4 primeros registros\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educaci√≥n formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nB√°sica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nT√©cnico nivel superior incompleta\nS√≠\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n\n\n4 rows √ó 22 columns"
  },
  {
    "objectID": "sesion1_slides.html#encuesta-casen-1",
    "href": "sesion1_slides.html#encuesta-casen-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Encuesta Casen",
    "text": "Encuesta Casen\nEjemplo\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploraci√≥n, para ser preparados para analizarlos.\nDe esto se tratar√° la siguiente sesi√≥n del curso."
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\nOtra opci√≥n es que los datos est√©n en una API:\n\nCodeOutput\n\n\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb \n# para instalar: conda install pandas-datareader  \n# o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. \n# En este caso revisare de PIB (GDP en ing√©s), \n# pero se pueden explorar muchas m√°s opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-1",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\n\ncoderesults\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-2",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\n\ncoderesults\n\n\n\n#sabemos que queremos Chile, asi que busquemos su info\n\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n\n\n\n\nObservar que este es un data frame con dos √≠ndices: pais y a√±o.\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-3",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-3",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del banco mundial",
    "text": "Datos desde la API del banco mundial\nEjemplo\nData frame con los datos de Chile, entre 1980 y 2020.\n\ncoderesults\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n2015\n13569.948127\n\n\n2014\n13421.538342\n\n\n2013\n13318.595215"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-4",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-4",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del Banco Mundial",
    "text": "Datos desde la API del Banco Mundial\nEjemplo\nSi quisieramos, por simplicidad quedarnos solo con el indice del a√±o y reordenar el dataframe:\n\ncoderesults\n\n\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el an√°lisis es para un solo pa√≠s\nreversed_df.head(5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068"
  },
  {
    "objectID": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-5",
    "href": "sesion1_slides.html#datos-desde-la-api-del-banco-mundial-5",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Datos desde la API del banco mundial",
    "text": "Datos desde la API del banco mundial\nEjemplo\nAhora, realicemos un grafico r√°pido con nuestros datos:\n\ncodeplot\n\n\n\n# Graficamos\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'A√±o')\n\n\n\n\n\n\n\n\nText(0.5, 0, 'A√±o')"
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicaci√≥n-1",
    "href": "sesion1_slides.html#taller-de-aplicaci√≥n-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicaci√≥n 1",
    "text": "Taller de aplicaci√≥n 1\n\n\n\n\n\n\nPregunta 1 - Bajando y formateando datos del Banco Mundial\n\n\nReplique el ejemplo pr√°ctico de importar datos desde la API del Banco Mundial y empezar la base para su an√°lisis de series de tiempo.\nImporte la serie de GDP total Y Percapita para otro pa√≠s serie desde la API del Banco mundial, muestre sus principales caracter√≠sticas y realice un grafico.\n¬øpareciera haber tendencias?"
  },
  {
    "objectID": "sesion1_slides.html#preguntando-a-los-datos",
    "href": "sesion1_slides.html#preguntando-a-los-datos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntando a los datos",
    "text": "Preguntando a los datos\n¬øC√≥mo plantear preguntas y formular hip√≥tesis en el contexto del an√°lisis de datos?\n\nLa formulaci√≥n de preguntas relevantes que se puedan responder mediante la exploraci√≥n y el examen de los datos disponibles.\nPueden surgir de la necesidad de resolver un problema, entender un fen√≥meno o explorar patrones en los datos.\nUn buen planteamiento de preguntas es crucial, ya que guiar√° todo el proceso de an√°lisis."
  },
  {
    "objectID": "sesion1_slides.html#abstrayendo-la-realidad",
    "href": "sesion1_slides.html#abstrayendo-la-realidad",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Abstrayendo la realidad",
    "text": "Abstrayendo la realidad\n\nEl proceso de abstraer la realidad"
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hip√≥tesis",
    "href": "sesion1_slides.html#preguntas-e-hip√≥tesis",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hip√≥tesis:",
    "text": "Preguntas e hip√≥tesis:\n\nUna hip√≥tesis es una afirmaci√≥n, verificable con evidencia.\nEn este sentido, para toda pregunta podemos responderla mediante hip√≥tesis.\nPara responder a las preguntas en el contexto de datos, es com√∫n formular hip√≥tesis nulas y alternativas."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hip√≥tesis-1",
    "href": "sesion1_slides.html#preguntas-e-hip√≥tesis-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hip√≥tesis:",
    "text": "Preguntas e hip√≥tesis:\n\nLa hip√≥tesis nula es aquella que propone que algun par√°metro toma cierto valor.\nEste generlamente es un punto de verdad.\nSi bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto.\nEn general, planteamos el problema de tal manera que podamos rechazar la hip√≥tesis nula, en favor de otra que llamamos alternatiba."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hip√≥tesis-2",
    "href": "sesion1_slides.html#preguntas-e-hip√≥tesis-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hip√≥tesis:",
    "text": "Preguntas e hip√≥tesis:\nPrueba de significancia\n\nQuizas, la hipotesis nula m√°s famosa es la prueba de ‚Äúsignificancia‚Äù.\nEn esta se propone que un par√°metro (muchas veces un efecto, o correlaci√≥n) es 0,\n\nes decir, plantea que no hay efecto o relaci√≥n entre las variables\nmientras que la hip√≥tesis alternativa sugiere que s√≠ existe una relaci√≥n o efecto significativo."
  },
  {
    "objectID": "sesion1_slides.html#preguntas-e-hip√≥tesis-3",
    "href": "sesion1_slides.html#preguntas-e-hip√≥tesis-3",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Preguntas e hip√≥tesis:",
    "text": "Preguntas e hip√≥tesis:\nUna gu√≠a clave para el an√°lisis\n\nSon fundamentales para establecer una base objetiva para el an√°lisis y para evaluar las evidencias encontradas en los datos.\nEl proceso de plantear preguntas y formular hip√≥tesis es el primer paso en el an√°lisis de datos, ya que establece una gu√≠a clara para el enfoque y la direcci√≥n del trabajo.\nAl identificar preguntas y establecer hip√≥tesis, se crea un marco s√≥lido que orientar√° la exploraci√≥n y el an√°lisis de los datos disponibles."
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicaci√≥n-1-1",
    "href": "sesion1_slides.html#taller-de-aplicaci√≥n-1-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicaci√≥n 1",
    "text": "Taller de aplicaci√≥n 1\n\n\n\n\n\n\nPregunta 2 - Investigando sobre pa√≠ses:\n\n\n\nConsidere que tenemos los datos del banco mundial, del pa√≠s que selecciono anteriormente, y desea aprender sobre alguna caracter√≠stica de dicho pa√≠s en el periodo.\nEscriba una pregunta de investigaci√≥n que se pueda responder con los datos disponibles.\n\n¬øC√≥mo definiria la variable aleatoria relevante?\n¬øQu√© hip√≥tesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estad√≠stica\nInferencia se refiere al proceso de hacer generalizaciones de una poblaci√≥n a partir de una muestra de esa poblaci√≥n.\n\nPoblaci√≥n y Muestra"
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos-1",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estad√≠stica\n\nSi tenemos un sub-conjunto de datos representativos de una poblaci√≥n\n\npodemos utilizar m√©todos estad√≠sticos\npara sacar conclusiones sobre las caracter√≠sticas\nY propiedades de esa poblaci√≥n en su totalidad."
  },
  {
    "objectID": "sesion1_slides.html#respondiendo-desde-los-datos-2",
    "href": "sesion1_slides.html#respondiendo-desde-los-datos-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\nInferencia estad√≠stica\n\nEl proceso de inferencia estad√≠stica se basa en el principio de que una muestra bien s eleccionada puede proporcionar informaci√≥n valiosa sobre la poblaci√≥n en general.\nEl uso de la inferencia estad√≠stica es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una poblaci√≥n en particular."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos",
    "href": "sesion1_slides.html#estad√≠grafos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos",
    "text": "Estad√≠grafos\nFunciones que aproximan par√°metros\n\nEstadigrafos"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-1",
    "href": "sesion1_slides.html#estad√≠grafos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos",
    "text": "Estad√≠grafos\nson variables aleatorias\n\n\n\nDado que por cada muestra que tenemos, vamos a calcular un estad√≠grafo este es en si mismo una variable aleatoria.\nTiene su propia distribuci√≥n, media y varianza!"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-2",
    "href": "sesion1_slides.html#estad√≠grafos-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos",
    "text": "Estad√≠grafos\nlos m√°s comunes\n\nEstadigrafos m√°s comunes"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nConectados por el Teorema del L√≠mite central\n\nLa distribuci√≥n de las medias muestrales de una poblaci√≥n se aproxima a una distribuci√≥n normal\nIndependientemente de la forma de la distribuci√≥n original de la poblaci√≥n.\nEste teorema es esencial en inferencia estad√≠stica y tiene amplias aplicaciones en an√°lisis de datos y toma de decisiones."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-1",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nConectados por el Teorema del L√≠mite central\n\nLa media muestral se distribuye normal, sin importar la distribuci√≥n de la variable subyacente"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-2",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nConectados por el Teorema del L√≠mite central\n\n\nFormalmente: \\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\nSea x con media Œº y desviaci√≥n est√°ndar œÉ finitas.\n\n\n\nSi tomamos muestras aleatorias de tama√±o n de esta poblaci√≥n y calculamos la media muestral de cada muestra\nLas medias muestrales se aproximar√° a una distribuci√≥n normal con media Œº y desviaci√≥n est√°ndar œÉ/‚àön."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-3",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-3",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nConectados por el Teorema del L√≠mite central\n\n\nFormalmente: \\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hip√≥tesis\np-valor"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-4",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-4",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nIntervalos de confianza"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-5",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-5",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nIntervalos de confianza"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-6",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-6",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nIntervalos de confianza\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un l√≠mite inferior y un l√≠mite superior, con cierta probabilidad.\nEste intervalo es aleatorio, porque \\(\\bar{y}\\) es diferente en cada muestra."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-7",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-7",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nIntervalos de confianza\nMatem√°ticamente, para cada muestra podemos construir un intervalo.\n\nCon varianza conocidaCon varianza desconocida\n\n\n\\[ P\\left( \\bar{y}-\\frac{1.96\\sigma }{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{1.96\\sigma }{\\sqrt{n}}  \\right) = 0.95 \\]\n\n\n\\[ \\left(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} \\right) \\]"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-8",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-8",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nIntervalos de confianza\n\nCon 20 muestras, tenemos 20 intervalos."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-9",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-9",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nIntervalos de confianza - Interpretaci√≥n\nPensemos en un 95% de confianza (un valor usual):\n\nEsto quiere decir, que si se repitiera este ejercicio muchas veces y construy√©ramos un intervalo de esta forma‚Ä¶\nel 95% de ellos contendr√≠a el verdadero par√°metro poblacional.\nNo significa que con 95% de certeza el par√°metro est√° exactamente en estos valores."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-10",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-10",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nIntervalos de confianza - Interpretaci√≥n\n\nAl 95% de confianza con 20 intervalos 19 contendr√°n el par√°metro."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-11",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-11",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nPruebas de hip√≥tesis\nUna forma de verificar hipotesis sobre los par√°metros es mediante el contraste de hip√≥tesis."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-12",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-12",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nPruebas de hip√≥tesis\nEmpezamos suponiendo que hay una distribuci√≥n conocida para el estad√≠grafo, centrada en un valor espec√≠fico."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-13",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-13",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nPruebas de hip√≥tesis\nY nos preguntamos, si esto fuea verdad ¬øqu√© tan probable es la muestra que tengo?"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-14",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-14",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nPruebas de hip√≥tesis\nLlamamos la hip√≥tesis a probar Ho, y su alternativa H1."
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-15",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-15",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-16",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-16",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nErrores y P-valor\n\nSe elige nivel de significancia de contraste (Œ±) = probabilidad de cometer error Tipo I. T√≠picamente Œ± = 0,01, 0,05, 0,10.\nDefinimos la prueba de hip√≥tesis de significancia como aquella que indica si un estimador \\(\\hat{T}\\) es 0.\n\n\\[ H_0: T =0\\text{ vs }H_1: T \\neq 0 \\]"
  },
  {
    "objectID": "sesion1_slides.html#estad√≠grafos-y-par√°metros-17",
    "href": "sesion1_slides.html#estad√≠grafos-y-par√°metros-17",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Estad√≠grafos y par√°metros",
    "text": "Estad√≠grafos y par√°metros\nErrores y P-valor\nEl Valor de probabilidad (√≥ p-valor) es el nivel probabilidad m√°s alto para el cual no podemos rechazar la hip√≥tesis nula de la prueba de significancia."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Ping√ºinos Palmer",
    "text": "Ejemplo: Peso de los Ping√ºinos Palmer\n\nLos datos ‚ÄúPalmer Penguins‚Äù son un conjunto que detalla medidas morfol√≥gicas y caracter√≠sticas de tres especies de ping√ºinos: Adelie, Gentoo y Chinstrap.\nRecopilados por el Dr.¬†Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-1",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Ping√ºinos Palmer",
    "text": "Ejemplo: Peso de los Ping√ºinos Palmer\n\n\n\n\n\ncodetabla\n\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head(6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-2",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Ping√ºinos Palmer",
    "text": "Ejemplo: Peso de los Ping√ºinos Palmer\n\nEn el contexto de los ping√ºinos y el peso de su poblaci√≥n:\n\npodr√≠amos tomar una muestra de ping√ºinos\ny calcular un intervalo de confianza para el peso promedio.\n\nEsto nos dar√≠a una estimaci√≥n del peso promedio de la poblaci√≥n total, junto con la confianza en que este valor estimado es preciso."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-3",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-3",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Ping√ºinos Palmer",
    "text": "Ejemplo: Peso de los Ping√ºinos Palmer\n\nLa elecci√≥n de la muestra, la interpretaci√≥n de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hip√≥tesis, sobre el peso de los ping√ºinos.\nPor ahora, pensemos que nuestra informaci√≥n es la poblaci√≥n completa"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-4",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-4",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Ping√ºinos Palmer",
    "text": "Ejemplo: Peso de los Ping√ºinos Palmer\nCalcularemos el promedio muestral y lo veremos en el contexto de los datos observados: . . .\n\ncodeoutput\n\n\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los ping√ºinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribuci√≥n del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribuci√≥n de Peso de Ping√ºinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-5",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-5",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Ping√ºinos Palmer",
    "text": "Ejemplo: Peso de los Ping√ºinos Palmer\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral.\nDe que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus caracter√≠sticas.\nPor ejemplo, consideremos que de esta poblaci√≥n de ping√ºinos obtenemos 1000 muestras de 40 individuos cada una.\nSi graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tama√±o de muestra, m√°s nos alejamos de la distribuci√≥n normal.\nSi reducimos el n√∫mero de repeticiones tambie√©."
  },
  {
    "objectID": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-6",
    "href": "sesion1_slides.html#ejemplo-peso-de-los-ping√ºinos-palmer-6",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Ejemplo: Peso de los Ping√ºinos Palmer",
    "text": "Ejemplo: Peso de los Ping√ºinos Palmer\n\ncodePlot\n\n\n\nimport numpy as np\n\n# Definir el tama√±o de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y c√°lculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gr√°fico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribuci√≥n de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#intervalo-de-confianza",
    "href": "sesion1_slides.html#intervalo-de-confianza",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Intervalo de confianza",
    "text": "Intervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n#  Obtener una muestra simple de 40 ping√ºinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error est√°ndar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviaci√≥n est√°ndar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)"
  },
  {
    "objectID": "sesion1_slides.html#intervalo-de-confianza-1",
    "href": "sesion1_slides.html#intervalo-de-confianza-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Intervalo de confianza",
    "text": "Intervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nEl resultado ser√° un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los ping√ºinos en la poblaci√≥n, con un nivel de confianza del 95%.\n¬øComo nos fue? ¬øContiene al verdadero valor?\n\n\n\nprint(\"Media poblacional:\", penguins['body_mass_g'].mean())\n\nprint(\"Intervalo de Confianza para el Peso:\", confidence_interval)\n\n\n\nMedia poblacional: 4201.754385964912\nIntervalo de Confianza para el Peso: (3788.9457238155824, 4341.054276184418)"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos",
    "href": "sesion1_slides.html#comparaciones-de-grupos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribuci√≥n de tama√±o por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n\n\ncodeTabla\n\n\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\nprint(tabla_doble_entrada)\n\n\n\n\n\n\n\n\n     species     sex     Promedio       Varianza\n0     Adelie  Female  3368.835616   72565.639269\n1     Adelie    Male  4043.493151  120278.253425\n2  Chinstrap  Female  3527.205882   81415.441176\n3  Chinstrap    Male  3938.970588  131143.605169\n4     Gentoo  Female  4679.741379   79286.335451\n5     Gentoo    Male  5484.836066   98068.306011"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-1",
    "href": "sesion1_slides.html#comparaciones-de-grupos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hip√≥tesis: ¬øExiste una diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù?"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-2",
    "href": "sesion1_slides.html#comparaciones-de-grupos-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nHip√≥tesis Nula (H0):\nNo hay diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù.\nHip√≥tesis Alternativa (H1):\nExiste una diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù."
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-3",
    "href": "sesion1_slides.html#comparaciones-de-grupos-3",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nPara probar esta hip√≥tesis, podr√≠as utilizar una prueba de hip√≥tesis para comparar las medias de las muestras de peso de los ping√ºinos machos y hembras en la especie ‚ÄúAdelie‚Äù.\n\n\n\ncodePlot\n\n\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los ping√ºinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribuci√≥n de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribuci√≥n de Peso por Sexo para Ping√ºinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-4",
    "href": "sesion1_slides.html#comparaciones-de-grupos-4",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nA simple vista podriamos pensar ambos grupos son diferentes.\nEs m√°s claro si dibujamos el promedio muestral observado.\n\n\n\ncodeplot\n\n\n\n# Crear un gr√°fico de densidad con l√≠neas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Ping√ºinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-5",
    "href": "sesion1_slides.html#comparaciones-de-grupos-5",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\ncodeTest t diferenciaplot\n\n\n\n# Filtrar los ping√ºinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estad√≠stica t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gr√°fico de comparaci√≥n de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparaci√≥n de Peso entre Machos y Hembras de Ping√ºinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\n\n\n\n\n\n\n\nEstad√≠stica t: 13.126285923485874\nValor p: 6.402319748031793e-26"
  },
  {
    "objectID": "sesion1_slides.html#comparaciones-de-grupos-6",
    "href": "sesion1_slides.html#comparaciones-de-grupos-6",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Comparaciones de grupos",
    "text": "Comparaciones de grupos\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas.\nPara esto podriamos usar una prueba ANOVA.\n\n\n\ncodeANOVA ResultsPlot\n\n\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estad√≠stica F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\n\n\n\n\n\n\n\nEstad√≠stica F: 72.96098633250911\nValor p: 4.897246751596325e-16"
  },
  {
    "objectID": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab",
    "href": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Experimentos Aleatorios y pruebas A/B",
    "text": "Experimentos Aleatorios y pruebas A/B\n\nUn experimento estad√≠stico busca establecer relaciones causales entre variables y obtener conclusiones sobre su impacto.\nSe dise√±an para manipular variables independientes y observar sus efectos en una variable dependiente.\nLos experimentos controlan y manipulan variables para hacer afirmaciones s√≥lidas sobre relaciones causales.\nLas pruebas A/B son comunes en √°reas como marketing y dise√±o de productos.\nEn una prueba A/B, se comparan dos grupos de muestra (A y B) para evaluar si la variante B produce cambios significativos en una m√©trica de inter√©s."
  },
  {
    "objectID": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab-1",
    "href": "sesion1_slides.html#experimentos-aleatorios-y-pruebas-ab-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Experimentos Aleatorios y pruebas A/B",
    "text": "Experimentos Aleatorios y pruebas A/B\nCuidados:\n\nPruebas A/B ofrecen evidencia de asociaci√≥n causal, pero no aseguran causalidad total debido a factores no controlados.\nExperimentos controlados y m√©todos de dise√±o s√≥lidos son esenciales para una comprensi√≥n completa de la causalidad.\nPruebas A/B son herramientas poderosas para analizar efectos y comparar opciones en condiciones controladas."
  },
  {
    "objectID": "sesion1_slides.html#enunciado",
    "href": "sesion1_slides.html#enunciado",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Enunciado",
    "text": "Enunciado\n\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electr√≥nicos y queremos aumentar las ventas en una l√≠nea de productos espec√≠fica, como tel√©fonos m√≥viles.\nPara ello, decidimos utilizar una promoci√≥n de ventas basada en una ruleta l√∫dica que ofrecer√° descuentos a los clientes que la utilicen.\nPara implementar la promoci√≥n, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electr√≥nico"
  },
  {
    "objectID": "sesion1_slides.html#enunciado-1",
    "href": "sesion1_slides.html#enunciado-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Enunciado",
    "text": "Enunciado\n\nLos clientes son asignados a uno de los siguientes grupos:\n\nControl: no les da una promoci√≥n (mala suerte, intentalo otra vez)\nTratamiento 1: 20% de descuento en el producto\n-Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento."
  },
  {
    "objectID": "sesion1_slides.html#creaci√≥n-de-los-datos",
    "href": "sesion1_slides.html#creaci√≥n-de-los-datos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Creaci√≥n de los datos",
    "text": "Creaci√≥n de los datos\n\ncodeDataframe\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n#| output: false\n\n# Define una semilla para la generaci√≥n de n√∫meros aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de n√∫mero de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows √ó 3 columns\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n#| echo: false\n\n# Define una semilla para la generaci√≥n de n√∫meros aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de n√∫mero de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows √ó 3 columns"
  },
  {
    "objectID": "sesion1_slides.html#taller-de-aplicaci√≥n-1-2",
    "href": "sesion1_slides.html#taller-de-aplicaci√≥n-1-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Taller de aplicaci√≥n 1:",
    "text": "Taller de aplicaci√≥n 1:\n\n\n\n\n\n\nPregunta 3 -Ejemplo AB test en Marketing:\n\n\nEstudiemos si la promoci√≥n fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¬øFue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¬øCual de las promociones fue m√°s efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "sesion1_slides.html#desaf√≠os-y-consideraciones",
    "href": "sesion1_slides.html#desaf√≠os-y-consideraciones",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desaf√≠os y Consideraciones:",
    "text": "Desaf√≠os y Consideraciones:\nImportancia de la Adquisici√≥n y Almacenamiento de Datos\n\nLa adquisici√≥n y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de an√°lisis.\nLa calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes.\nGarant√≠a de Calidad y Fiabilidad en la Obtenci√≥n de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean s√≥lidas."
  },
  {
    "objectID": "sesion1_slides.html#desaf√≠os-y-consideraciones-1",
    "href": "sesion1_slides.html#desaf√≠os-y-consideraciones-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desaf√≠os y Consideraciones:",
    "text": "Desaf√≠os y Consideraciones:\nImportancia de la Adquisici√≥n y Almacenamiento de Datos\n\nExploraci√≥n de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros.\nCada fuente tiene sus propias caracter√≠sticas y potenciales sesgos.\nComprender las diferencias entre estas fuentes y c√≥mo pueden influir en los resultados es crucial para tomar decisiones informadas."
  },
  {
    "objectID": "sesion1_slides.html#desaf√≠os-y-consideraciones-2",
    "href": "sesion1_slides.html#desaf√≠os-y-consideraciones-2",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desaf√≠os y Consideraciones:",
    "text": "Desaf√≠os y Consideraciones:\nPrivacidad y Seguridad de los Datos:\n\nUno de los aspectos m√°s cr√≠ticos en el an√°lisis de datos es la privacidad y seguridad de la informaci√≥n.\nLos datos pueden contener informaci√≥n sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas."
  },
  {
    "objectID": "sesion1_slides.html#desaf√≠os-y-consideraciones-3",
    "href": "sesion1_slides.html#desaf√≠os-y-consideraciones-3",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desaf√≠os y Consideraciones:",
    "text": "Desaf√≠os y Consideraciones:\nPrivacidad y Seguridad de los Datos:\n\nExploraremos pr√°cticas y regulaciones para garantizar que los datos se manejen de manera √©tica y legal.\nDiscutiremos c√≥mo anonimizar los datos, utilizar t√©cnicas de enmascaramiento y seguir las mejores pr√°cticas para resguardar la privacidad de los individuos."
  },
  {
    "objectID": "sesion1_slides.html#desaf√≠os-y-consideraciones-4",
    "href": "sesion1_slides.html#desaf√≠os-y-consideraciones-4",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desaf√≠os y Consideraciones:",
    "text": "Desaf√≠os y Consideraciones:\nLimpieza y Transformaci√≥n durante la Preparaci√≥n de Datos:\n\nLa etapa de preparaci√≥n de datos es crucial para asegurarse de que los datos sean aptos para el an√°lisis.\nSin embargo, este proceso no est√° exento de desaf√≠os.\nLos datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada."
  },
  {
    "objectID": "sesion1_slides.html#desaf√≠os-y-consideraciones-5",
    "href": "sesion1_slides.html#desaf√≠os-y-consideraciones-5",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Desaf√≠os y Consideraciones:",
    "text": "Desaf√≠os y Consideraciones:\nLimpieza y Transformaci√≥n durante la Preparaci√≥n de Datos:\n\nExploraremos t√©cnicas para identificar y manejar valores at√≠picos y faltantes, errores de digitaci√≥n, etc.\nLos invetsigadores toman muchas decisiones en este proceso, que deben ser transparentes.\nAbordar estos desaf√≠os de manera adecuada es esencial para garantizar que nuestras conclusiones sean s√≥lidas, confiables y √©ticas."
  },
  {
    "objectID": "sesion1_slides.html#reproducibilidad-y-control-de-versiones-git",
    "href": "sesion1_slides.html#reproducibilidad-y-control-de-versiones-git",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Reproducibilidad y Control de Versiones (GIT):",
    "text": "Reproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos."
  },
  {
    "objectID": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios",
    "href": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:",
    "text": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\n\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que tambi√©n es una herramienta poderosa en el an√°lisis de datos.\nPermite rastrear cada modificaci√≥n realizada en el c√≥digo y en los documentos, incluidos los notebooks.\nCada cambio es registrado como un ‚Äúcommit‚Äù, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\nLa aplicaci√≥n de GIT en proyectos de preparaci√≥n de datos agrega un nivel adicional de transparencia y colaboraci√≥"
  },
  {
    "objectID": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios-1",
    "href": "sesion1_slides.html#uso-de-sistemas-de-control-de-versiones-como-git-para-rastrear-cambios-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:",
    "text": "Uso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\n\nUn esquema de git por Allison Horst @allison_horst"
  },
  {
    "objectID": "sesion1_slides.html#actividad-de-proyecto",
    "href": "sesion1_slides.html#actividad-de-proyecto",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos.",
    "section": "Actividad de proyecto",
    "text": "Actividad de proyecto\n\n\n\n\n\n\nInicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del an√°lisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustar√≠a trabajar.\nPropongan una o dos preguntas de investigaci√≥n y las hipotesis que las responder√≠an.\n\nLa siguiente sesi√≥n, vamos a explorar los datos y empezar los primeros pasos en su an√°lisis.\n\n\n\n\n\n\n\nCurso An√°lisis de Datos - Sesi√≥n 1"
  },
  {
    "objectID": "taller1_aplicacion_educ.html",
    "href": "taller1_aplicacion_educ.html",
    "title": "Caso aplicaci√≥n: Cursos de Verano",
    "section": "",
    "text": "Vamos a usar una situaci√≥n ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de identificaci√≥n revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy r√°pidamente y en otras, vamos a tener mayor √©nfasis.\n\n\nNuestro objetivo es responder la siguiente pregunta ficticia de investigaci√≥n:\n\nAsistir a cursos de verano mejora los resultados acad√©micos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\n\nLa pregunta de investigaci√≥n se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioecon√≥mico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opci√≥n de asistir a un curso de verano intensivo durante el verano entre 5 y 6to b√°sico.\nEl curso de verano se enfoca en mejorar las habilidades acad√©micas de preparar la prueba de admisi√≥n a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participaci√≥n en el curso en los resultados acad√©micos de los estudiantes.\n\n\n\n\nLos datos estan disponibles en https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene informaci√≥n sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si particip√≥ en el curso de verano, sexo, ingreso del hogar (en logaritmo), educaci√≥n de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el a√±o 5 como para el a√±o 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene informaci√≥n de cada individuo (con identificador id).\nEste dataset tiene la informaci√≥n si el individuo recibi√≥ la carta de invitaci√≥n para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene informaci√≥n de rendimiento acad√©mico antes y despu√©s del curso de verano.\n\n\n\n\n\nLa idea de este taller es poner en pr√°ctica los primeros pasos para un an√°lisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n\nTambi√©n exploraremos los datos, usaremos estad√≠sticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\n\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre lso pasos anteriores m√∫ltiples veces. Tambi√©n invlocura tomar desiciones por parte de los investigadores, por lo cual la documentaci√≥n de esta fase es especialmente importante.\nEn nuestro caso, ser√° bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser as√≠.\nPasos que debe realizar:\n\nPreparaci√≥n de los datos\n\n1.1. Cargue las tres bases de datos\n1.2. Unir los datasets\nTenemos 3 bases de datos con informaci√≥n diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta informaci√≥n, especialmente si estimaremos modelos en base a √©sta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido deber√≠a tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de informaci√≥n del estudiante si recibi√≥ o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de uni√≥n person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que ac√° unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id √∫nica tiene un solo colegio, pero sirve de ejemplo en como usar m√°s de una columna mediante.\nUsamos la funci√≥n summary() para obtener una estad√≠stica descriptiva de las variables en el dataset unido.\nPreparar los datos\n\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera ‚Äútidy‚Äù (Seg√∫n Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observaci√≥n.\n\n(puedes profundizar y ver m√°s ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observaci√≥n.\nSi inspeccionamos el n√∫mero de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del a√±o 2 al 10. Este tipo de datos son de panel\n\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y an√°lisi preferimos que est√© en formato long. Especialmente cuando modelamos incluyendo efectos fijos Tambi√©n es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos melt()\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selecci√≥n de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podr√≠amos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y n√∫mero de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn √∫ltimo paso que haremos antes de hacer estad√≠stica decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviaci√≥n est√°ndar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto espec√≠fico de la medida y es m√°s facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estad√≠stica descriptiva.\n\n\n\nHasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estad√≠sticas descriptivas y tambi√©n algunos graficos descriptivos.\n3.1 Tablas de ets√°distocas descriptivas Incluya la media, la desviaci√≥n estandar, la mediana, max y min, al menos.\n3.2 Gr√°ficos de estad√≠stica descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#taller-1-pregunta-4---aplicaci√≥n-datos-de-educaci√≥n",
    "href": "taller1_aplicacion_educ.html#taller-1-pregunta-4---aplicaci√≥n-datos-de-educaci√≥n",
    "title": "Caso aplicaci√≥n: Cursos de Verano",
    "section": "",
    "text": "Vamos a usar una situaci√≥n ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de identificaci√≥n revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy r√°pidamente y en otras, vamos a tener mayor √©nfasis.\n\n\nNuestro objetivo es responder la siguiente pregunta ficticia de investigaci√≥n:\n\nAsistir a cursos de verano mejora los resultados acad√©micos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\n\nLa pregunta de investigaci√≥n se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioecon√≥mico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opci√≥n de asistir a un curso de verano intensivo durante el verano entre 5 y 6to b√°sico.\nEl curso de verano se enfoca en mejorar las habilidades acad√©micas de preparar la prueba de admisi√≥n a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participaci√≥n en el curso en los resultados acad√©micos de los estudiantes.\n\n\n\n\nLos datos estan disponibles en https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene informaci√≥n sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si particip√≥ en el curso de verano, sexo, ingreso del hogar (en logaritmo), educaci√≥n de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el a√±o 5 como para el a√±o 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene informaci√≥n de cada individuo (con identificador id).\nEste dataset tiene la informaci√≥n si el individuo recibi√≥ la carta de invitaci√≥n para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene informaci√≥n de rendimiento acad√©mico antes y despu√©s del curso de verano."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#objetivos",
    "href": "taller1_aplicacion_educ.html#objetivos",
    "title": "Caso aplicaci√≥n: Cursos de Verano",
    "section": "",
    "text": "La idea de este taller es poner en pr√°ctica los primeros pasos para un an√°lisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n\nTambi√©n exploraremos los datos, usaremos estad√≠sticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\n\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre lso pasos anteriores m√∫ltiples veces. Tambi√©n invlocura tomar desiciones por parte de los investigadores, por lo cual la documentaci√≥n de esta fase es especialmente importante.\nEn nuestro caso, ser√° bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser as√≠.\nPasos que debe realizar:\n\nPreparaci√≥n de los datos\n\n1.1. Cargue las tres bases de datos\n1.2. Unir los datasets\nTenemos 3 bases de datos con informaci√≥n diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta informaci√≥n, especialmente si estimaremos modelos en base a √©sta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido deber√≠a tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de informaci√≥n del estudiante si recibi√≥ o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de uni√≥n person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que ac√° unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id √∫nica tiene un solo colegio, pero sirve de ejemplo en como usar m√°s de una columna mediante.\nUsamos la funci√≥n summary() para obtener una estad√≠stica descriptiva de las variables en el dataset unido.\nPreparar los datos\n\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera ‚Äútidy‚Äù (Seg√∫n Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observaci√≥n.\n\n(puedes profundizar y ver m√°s ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observaci√≥n.\nSi inspeccionamos el n√∫mero de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del a√±o 2 al 10. Este tipo de datos son de panel\n\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y an√°lisi preferimos que est√© en formato long. Especialmente cuando modelamos incluyendo efectos fijos Tambi√©n es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos melt()\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selecci√≥n de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podr√≠amos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y n√∫mero de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn √∫ltimo paso que haremos antes de hacer estad√≠stica decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviaci√≥n est√°ndar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto espec√≠fico de la medida y es m√°s facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estad√≠stica descriptiva."
  },
  {
    "objectID": "taller1_aplicacion_educ.html#estad√≠stica-descriptiva",
    "href": "taller1_aplicacion_educ.html#estad√≠stica-descriptiva",
    "title": "Caso aplicaci√≥n: Cursos de Verano",
    "section": "",
    "text": "Hasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estad√≠sticas descriptivas y tambi√©n algunos graficos descriptivos.\n3.1 Tablas de ets√°distocas descriptivas Incluya la media, la desviaci√≥n estandar, la mediana, max y min, al menos.\n3.2 Gr√°ficos de estad√≠stica descriptiva Realice al menos scatter plot, histograma, box plot y correalograma."
  },
  {
    "objectID": "sesion0_notas.html",
    "href": "sesion0_notas.html",
    "title": "Presentaci√≥n del curso",
    "section": "",
    "text": "Hola! Soy Melanie y ser√© la docente de este curso, en el que aprender√°s los fundamentos del an√°lisis de datos, tanto desde una perspectiva te√≥rica como pr√°ctica (en Python).\nMe pueden contactar al mail melanie.oyarzun@udd.cl\n\n\n\n\n\n\n\nRevisaremos los objetivos del curso, metodolog√≠a, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hip√≥tesis."
  },
  {
    "objectID": "sesion0_notas.html#en-la-sesi√≥n-de-hoy",
    "href": "sesion0_notas.html#en-la-sesi√≥n-de-hoy",
    "title": "Presentaci√≥n del curso",
    "section": "",
    "text": "Revisaremos los objetivos del curso, metodolog√≠a, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hip√≥tesis."
  },
  {
    "objectID": "sesion0_notas.html#contexto-en-el-programa-de-magister",
    "href": "sesion0_notas.html#contexto-en-el-programa-de-magister",
    "title": "Presentaci√≥n del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la l√≠nea de desarrollo de data science.\nEsta asignatura tributa, a trav√©s de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Mag√≠ster en Data Science:\n\nAplicar teor√≠as, algoritmos, m√©todos, t√©cnicas y herramientas b√°sicas y avanzadas de Data Science para analizar, resolver y hacer una evaluaci√≥n cr√≠tica de desaf√≠os complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones.\nComunica efectivamente y argumenta sobre los resultados de su trabajo a p√∫blicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento √©tico, cumpliendo los protocolos y normas que gu√≠an su desempe√±o, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicaci√≥n de estrategias para utilizar nuevo conocimiento en data science en su √°mbito de desempe√±o."
  },
  {
    "objectID": "sesion0_notas.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "href": "sesion0_notas.html#objetivos-de-la-asignatura-resultados-de-aprendizaje",
    "title": "Presentaci√≥n del curso",
    "section": "Objetivos de la asignatura (resultados de aprendizaje)",
    "text": "Objetivos de la asignatura (resultados de aprendizaje)\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el an√°lisis de datos, utilizando lenguaje t√©cnico af√≠n.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso.\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer informaci√≥n y generar conclusiones basadas en evidencia.\nIdentificar las buenas pr√°cticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0_notas.html#resumen",
    "href": "sesion0_notas.html#resumen",
    "title": "Presentaci√≥n del curso",
    "section": "Resumen:",
    "text": "Resumen:"
  },
  {
    "objectID": "sesion0_notas.html#detallado",
    "href": "sesion0_notas.html#detallado",
    "title": "Presentaci√≥n del curso",
    "section": "Detallado",
    "text": "Detallado\n\n\n\n\n\n\nSesi√≥n 1: Respondiendo Preguntas con datos\n\n\n\n\n\nFecha: 19 agosto\nObjetivos:\n\nAprender a formular preguntas y plantear hip√≥tesis que puedan ser abordadas mediante el an√°lisis de datos.\nDesarrollar la habilidad de realizar pruebas de hip√≥tesis y comprender la interpretaci√≥n de sus resultados.\nComprender el papel del proceso de adquisici√≥n y almacenamiento en un proyecto de an√°lisis de datos.\n\nContenidos:\n\nEl proceso de an√°lisis de datos\n\nPlanteamiento de preguntas\nAdquision y almacenmiento de los datos\nPreparaci√≥n de los datos\nUna visi√≥n general a las metodolog√≠as de an√°lisis que veremos en el curso\n\nFormulaci√≥n de Preguntas y Hip√≥tesis:\n\nImportancia de definir preguntas claras y espec√≠ficas.\nDiferenciaci√≥n entre preguntas exploratorias y confirmatorias.\nCreaci√≥n de hip√≥tesis nulas y alternativas.\n\nHip√≥tesis y Variables:\n\nIdentificaci√≥n de variables independientes y dependientes.\nRelaci√≥n entre hip√≥tesis y variables a analizar.\n\nConceptos B√°sicos de Pruebas de Hip√≥tesis:\n\nDefinici√≥n de hip√≥tesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hip√≥tesis:\n\nPruebas t para comparaci√≥n de medias.\nPruebas chi-cuadrado para variables categ√≥ricas.\nPruebas ANOVA para comparaci√≥n de m√∫ltiples grupos.\n\nInterpretaci√≥n de Resultados:\n\nEvaluaci√≥n de p-values y toma de decisiones.\nSignificaci√≥n estad√≠stica vs.¬†significaci√≥n pr√°ctica.\nComunicaci√≥n de los resultados de las pruebas de hip√≥tesis.\n\nImportancia de la Adquisici√≥n y Almacenamiento de Datos:\n\nGarant√≠a de calidad y fiabilidad en la obtenci√≥n de datos.\nExploraci√≥n de diferentes fuentes de datos y su impacto en los resultados.\nMetodologias de levantamiento y adquision\n\nDesaf√≠os y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformaci√≥n durante la preparaci√≥n de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos.\n\n\nBibliografia recomendada:\nActividades:\n\nTaller 1 (incluidas en slides 1)\nProyecto clase 1: Conformaci√≥n de grupos, definici√≥n de temas, primeras hip√≥tesis y datos.\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 2:Preparando los datos\n\n\n\n\n\nFecha: 26 agosto\nObjetivos:\n\nComprender la importancia del proceso de preparaci√≥n de datos para el an√°lisis, reconociendo principios y enfoques clave junto con sus ventajas y desventajas.\nDesarrollar habilidades pr√°cticas en la preparaci√≥n de datos, identificando y abordando problemas comunes como valores faltantes, valores at√≠picos y formatos inconsistentes, as√≠ como enfoques de trabajo eficientes.\n\nContenidos:\n\nEl proceso de preparaci√≥n de los datos\n\nSignificado y relevancia de la preparaci√≥n de datos.\nEjemplos reales de c√≥mo la falta de preparaci√≥n puede afectar los resultados.\n\nPrincipios y enfoques\n\nExtract, Transform, Load (ETL): Proceso fundamental en la preparaci√≥n de datos.\nData Wrangling: T√©cnicas para dar forma y estructura a los datos.\nDatos Tidy: Organizaci√≥n y reestructuraci√≥n para un an√°lisis eficaz.\n\nBuenas Pr√°cticas en la Preparaci√≥n de Datos\n\nDocumentaci√≥n y Consistencia\n\nImportancia de la documentaci√≥n detallada.\nMantener nomenclatura y convenciones consistentes.\n\nValidaci√≥n y Verificaci√≥n\n\nValidaci√≥n cruzada y verificaci√≥n de integridad.\nCumplir con reglas y restricciones esperadas.\n\nReproducibilidad y Versionado\n\nEntorno de trabajo reproducible (Jupyter Notebooks, R Markdown).\nUtilizaci√≥n de sistemas de control de versiones (GIT).\n\nComunicaci√≥n y Validaci√≥n Colaborativa\n\nComunicaci√≥n clara de pasos y resultados.\nValidaci√≥n intermedia con colaboradores para feedback.\n\nSeguridad y privacidad de los datos\n\nProblemas comunes¬†presentes en datos\n\nValores faltantes:¬†\n\nEstrategias para manejar valores faltantes.\nDecidir entre imputaci√≥n, eliminaci√≥n o conservaci√≥n.\n\nValores at√≠picos\nNormalizaci√≥n y estandarizaci√≥n\nErrores de registro Bibliografia recomendada:\n\n\n\n‚ÄúPractical Statistics for Data Scientists‚Äù (Cap√≠tulo 2).\n‚ÄúDoing Data Science‚Äù (Cap√≠tulo 1).\n\nActividades de aplicaci√≥n pr√°ctica:\n\nTaller 1: Limpieza y an√°lisis descriptivo de datos en la practica con datos de educaci√≥n (repasa elementos del curso anterior) (sesi√≥n 1)\nProyecto:\n\nInicie el proyecto, cree un documento notebook en el cual van a alojarsu proyecto\nExplorar los datos\nDiagnosticar problemas.\nLa hip√≥tesis que pensamos, ¬øtienen variables que pueda concretizarlas? ¬øQu√© variables usar?\n\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 3: Introduccion al an√°lisis de regresi√≥n\n\n\n\n\n\nFecha: 2 septiembre\nObjetivos:\n\nComprender los conceptos fundamentales del an√°lisis de regresi√≥n lineal y su aplicaci√≥n en la resoluci√≥n de problemas.\nDesarrollar la habilidad de plantear modelos e interpretar los resultados obtenidos del an√°lisis de regresi√≥n, para aplicarlos en la toma de decisiones.\n\nContenidos\n\nIntroducci√≥n al An√°lisis de Regresi√≥n:\n\nDefinici√≥n y concepto de regresi√≥n.\nUso y aplicabilidad en la toma de decisiones.\n\nRegresi√≥n Lineal M√∫ltiple:\n\nExtensi√≥n del modelo de regresi√≥n a m√∫ltiples variables predictoras.\nEcuaci√≥n de regresi√≥n lineal m√∫ltiple.\n\nInterpretaci√≥n de Coeficientes:\n\nSignificado e interpretaci√≥n de los coeficientes de regresi√≥n.\nInfluencia de las variables predictoras en la variable de respuesta.\n\nEvaluaci√≥n de Modelos de Regresi√≥n:\n\nUso de medidas como el coeficiente de determinaci√≥n (R¬≤) y el error est√°ndar de estimaci√≥n.\nInterpretaci√≥n de los resultados de evaluaci√≥n.\n\nIncorporaci√≥n de Variables Categ√≥ricas:\n\nTransformaci√≥n de variables categ√≥ricas en variables num√©ricas.\nInterpretaci√≥n de coeficientes para variables categ√≥ricas.\n\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 2:\nProyecto:\nPlantear modelos de regresi√≥n que implementen las hip√≥tesis del proyecto\nEstimar e interpretar modelos\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 4: Profundizando en An√°lisis de Regresi√≥n, Supuestos y Limitaciones\n\n\n\n\n\nFecha: 9 septiembre\nObjetivos:\n\nExplorar los supuestos y limitaciones asociados al an√°lisis de regresi√≥n y desarrollar estrategias para manejar problemas comunes.\nAplicar estrategias pr√°cticas para identificar y abordar problemas en el an√°lisis de regresi√≥n.\n\nContenidos\n\nSupuestos en el An√°lisis de Regresi√≥n:\n\nIdentificaci√≥n de supuestos clave: linealidad, independencia, homoscedasticidad y normalidad.\nSignificado de cada supuesto y su importancia en la interpretaci√≥n de resultados.\n\nIdentificaci√≥n de Problemas en la Regresi√≥n:\n\nIdentificaci√≥n y manejo de outliers en los datos.\nReconocimiento de la heterocedasticidad y sus implicaciones.\nDetecci√≥n de la no-normalidad de los residuos.\n\nEstrategias para Manejar Problemas:\n\nTransformaci√≥n de variables para abordar problemas de linealidad.\nM√©todos para reducir la influencia de outliers.\nUso de transformaciones para tratar la heterocedasticidad.\nPruebas y t√©cnicas para verificar y mejorar la normalidad.\n\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 2:\nProyecto:\nRevisar supuestos de modelo de regresi√≥n\nDiscutir problemas de identificaci√≥n y limitaciones\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 5: Introduccion al an√°lisis de series de tiempo\n\n\n\n\n\nFecha: 30 septiembre\nObjetivos:\n\nIdentificar las caracter√≠sticas y particularidades de los datos de series de tiempo, comprendiendo sus aplicaciones profesionales.\nRealizar un an√°lisis exploratorio de una serie de tiempo, identificando caracter√≠sticas clave para su modelamiento.\n\nContenidos\n\nConceptos B√°sicos de Series de Tiempo:\n\nDefinici√≥n y caracter√≠sticas de una serie de tiempo.\nEjemplos de aplicaciones en distintos campos profesionales.\n\nParticularidades de los Datos Temporales:\n\nDependencia temporal y autocorrelaci√≥n.\nTendencias, estacionalidad y ciclos.\n\nAplicaciones Profesionales:\n\nCasos de estudio en finanzas, econom√≠a, medicina y otros campos.\nC√≥mo el an√°lisis de series de tiempo puede brindar insights valiosos.\n\nB√∫squeda y Reorganizaci√≥n de Datos Temporales:\n\nFuentes de datos para series de tiempo (bases de datos, APIs, archivos).\nImportancia de la temporalidad y el orden en los datos.\n\nVisualizaci√≥n y Exploraci√≥n Inicial:\n\nGr√°ficos de l√≠nea y dispersi√≥n para identificar tendencias y patrones.\nEstudio de estacionalidad y ciclos mediante gr√°ficos.\n\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 6: Modelando series temporales\n\n\n\n\n\nFecha: 7 occtubre\nObjetivos:\n\nComprender los conceptos y aplicaciones de los modelos ARIMA y VAR en el an√°lisis de series temporales.\nEvaluar las ventajas y desventajas de los m√©todos estad√≠sticos para el an√°lisis de series de tiempo y seleccionar la t√©cnica m√°s adecuada.\n\nContenidos\n\nModelos ARIMA:\n\nDefinici√≥n y componentes de los modelos ARIMA.\nIdentificaci√≥n, Estimaci√≥n y Validaci√≥n de un modelo ARIMA.\nUso de correlogramas y gr√°ficos ACF/PACF para la identificaci√≥n.\n\nModelos VAR (Vector Autoregressive):\n\nIntroducci√≥n a los modelos VAR y su aplicaci√≥n.\nUso de matrices de coeficientes para representar relaciones entre variables.\n\nVentajas y Desventajas de los M√©todos Estad√≠sticos:\n\nUso de modelos estad√≠sticos en comparaci√≥n con otros enfoques.\nLimitaciones y supuestos asociados a los modelos ARIMA y VAR.\n\nSelecci√≥n del M√©todo Adecuado:\n\nCriterios para elegir entre modelos ARIMA y VAR.\nConsideraciones al evaluar las alternativas disponibles.\n\nOtros modelos\n\n\nGARCH\nSARIMA y SARIMAX\nAlisado exponencial\nCambio estructural\n\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nTaller 3:\nProyecto:\n\n\n\n\n\n\n\n\n\n\nSesi√≥n 7: Principales lecciones para el an√°lisis de datos y presentaciones de proyectos\n\n\n\n\n\nFecha: TBA\nObjetivos:\n\nCerrar el curso, poniendo en contexto las principales herramientas de an√°lisis de datos.\nPresentar un proyecto de an√°lisis de datos\nRecibir feedback y propuestas de mejoras, tanto del trabajo propio como el de sus compa√±eros.\n\nContenidos\nBibliograf√≠a recomendada\nActividades de aplicaci√≥n pr√°ctica\n\nProyecto: Presentaciones finales"
  },
  {
    "objectID": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n",
    "href": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El an√°lisis de regresi√≥n",
    "text": "El an√°lisis de regresi√≥n\n\nEn las aplicaciones de la ciencia de datos, es muy com√∫n estar interesado en la relaci√≥n entre dos o m√°s variables.\nEl an√°lisis de regresi√≥n es una t√©cnica en la cual buscamos encontrar una funci√≥n que pueda describir la relaci√≥n observada en los datos entre dos o mas variables.\nPor ejemplo, podr√≠amos querer relacionar los pesos de los individuos con sus alturas‚Ä¶\n\n¬øson los m√°s altos, m√°s pesados?\ny‚Ä¶ ¬øcu√°nto m√°s pesados?"
  },
  {
    "objectID": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-regresi√≥n-simple",
    "href": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-regresi√≥n-simple",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El an√°lisis de regresi√≥n: Regresi√≥n Simple",
    "text": "El an√°lisis de regresi√≥n: Regresi√≥n Simple\n\nCaso m√°s sencillo: univariada o regresi√≥n lineal simple.\n\nUna variable que deseamos explicar o predecir (Y) como funci√≥n de otra (X).\nBuscamos la pendiente e intercepto de una funci√≥nla recta de la forma:\n\n\n\n\\[Y = \\alpha + \\beta X\\]\ndonde:\n\nY es la variale dependiente o que deseamos entender\nX es la variable independiente\n\\(\\beta\\) es la pendiente de la recta\n\\(\\alpha\\) es la constante o intersecci√≥n (el valor de y cuando x=0)"
  },
  {
    "objectID": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-1",
    "href": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El an√°lisis de regresi√≥n",
    "text": "El an√°lisis de regresi√≥n\nBuscamos los coeficientes de la funci√≥n entre Y y X: constante y pendiente"
  },
  {
    "objectID": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-2",
    "href": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El an√°lisis de regresi√≥n",
    "text": "El an√°lisis de regresi√≥n\nPara esto, pensamos que la variable que deseamos entender (Y, variable dependiente) se puede descomponer en dos partes:\n\nuna que es sistem√°tica o que se puede explicar directamente con una o m√°s variables independientes (Xs o regresores)\ny otra que es no sistem√°tica o error (\\(\\mu\\) o \\(epsilon\\)) , que es aquella parte que no se puede explicar y representa a la aleatoriedad del fen√≥meno."
  },
  {
    "objectID": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-3",
    "href": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-3",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El an√°lisis de regresi√≥n",
    "text": "El an√°lisis de regresi√≥n\n\n\n\nLa parte sistem√°tica entonces la describimos con una forma funcional, que depende de otras variables o regresores.\n\n\nEsta forma funcional puede:\n\nser lineal univariada,\nlineal m√∫ltiple o\nno lineal.\n\n. . . El tipo de forma funcional, definir√° el tipo de regresi√≥n de la que estemos hablando."
  },
  {
    "objectID": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-4",
    "href": "sesion3_slides.html#el-an√°lisis-de-regresi√≥n-4",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El an√°lisis de regresi√≥n",
    "text": "El an√°lisis de regresi√≥n\nVentajas del an√°lisis de regersi√≥n: es facil decsribir cuantitaivamente una rlaci√≥n.\nEsquem√°ticamente, los elementos son:"
  },
  {
    "objectID": "sesion3_slides.html#para-qu√©-hacer-regresiones",
    "href": "sesion3_slides.html#para-qu√©-hacer-regresiones",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "¬øPara qu√© hacer regresiones?",
    "text": "¬øPara qu√© hacer regresiones?\nPodemos pensar en tres uso, al menos, del an√°lisis d eregresi√≥n:\n\nDescribir cuantitativamente una relaci√≥n emp√≠rica\nProbar hip√≥tesis sobre ciertas teor√≠as\nRealizar predicciones"
  },
  {
    "objectID": "sesion3_slides.html#regresi√≥n-simple-y-scatterplot",
    "href": "sesion3_slides.html#regresi√≥n-simple-y-scatterplot",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Regresi√≥n simple y scatterplot",
    "text": "Regresi√≥n simple y scatterplot\n\nPor ejemplo, pensemos en la relaci√≥n entre los a√±os de educaci√≥n y el ingreso de las personas. Este ha sido un tema constante de estudio en diversas disciplinas, especialmente econom√≠a.\nUsemos un subconjunto de datos de la encuesta CASEN 2022.\n\n\n\nimport pandas as pd\n# cargamos los datos, es un subconjunto de pregungas, solo mayores de 18 a√±os\n\ncasen_2022 = pd.read_stata(\"data/small_casen2022.dta\")\n# casen_2022 = pd.read_stata(\"https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/blob/main/data/small_casen2022.dta)\n\ncasen_2022.head()\n\n\n\n\n\n\n\n\n\n\nid_vivienda\nfolio\nid_persona\nregion\narea\nnse\nexpr\ntot_per_h\nedad\nsexo\npco1_a\ne3\no6\no8\ny1\nytrabajocor\nesc\ndesercion\neduc\ncontrato\n\n\n\n\n0\n1000901\n100090101\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n72\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n1.0\nNaN\nB√°sica incompleta\nNaN\n\n\n1\n1000901\n100090101\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n43\n3\n67\n1. Hombre\nS√≠\n2. No\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nB√°sica incompleta\nNaN\n\n\n2\n1000901\n100090101\n3\nRegi√≥n de √ëuble\nRural\nBajo-medio\n44\n3\n40\n2. Mujer\nNo\n2. No\nNaN\nNaN\nNo sabe\n411242.0\n15.0\nNaN\nT√©cnico nivel superior completo\nNo sabe\n\n\n3\n1000902\n100090201\n1\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n56\n1. Hombre\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\nNaN\nNaN\nNo sabe\nNaN\n\n\n4\n1000902\n100090201\n2\nRegi√≥n de √ëuble\nRural\nBajo-medio\n51\n4\n25\n2. Mujer\nNo\n2. No\n2. No\nNaN\nNaN\nNaN\n12.0\nDeserci√≥n\nMedia humanista completa\nNaN"
  },
  {
    "objectID": "sesion3_slides.html#regresi√≥n-simple-y-scatterplot-1",
    "href": "sesion3_slides.html#regresi√≥n-simple-y-scatterplot-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Regresi√≥n simple y scatterplot",
    "text": "Regresi√≥n simple y scatterplot\nY lo agruparemos por regi√≥n, para facilitar el ejemplo:\n\n\nimport pandas as pd\n\n# Supongamos que tienes un DataFrame llamado 'data' con las columnas 'region', 'ytrabajocor', 'esc' y 'desercion'\n\n# Agrupar por 'region' y aplicar funciones de agregaci√≥n\ncasen_2022_region = casen_2022.groupby('region').agg({'ytrabajocor': 'mean', 'esc': 'mean'}).reset_index()\n\n# Ahora contiene los resultados agregados por regi√≥n\n\ncasen_2022_region.head()\n\n\n\n\n\n\n\n\n\n\nregion\nytrabajocor\nesc\n\n\n\n\n0\nRegi√≥n de Tarapac√°\n658026.6250\n11.679582\n\n\n1\nRegi√≥n de Antofagasta\n791351.8125\n11.833934\n\n\n2\nRegi√≥n de Atacama\n666128.3125\n11.126735\n\n\n3\nRegi√≥n de Coquimbo\n656137.8750\n10.973584\n\n\n4\nRegi√≥n de Valpara√≠so\n611298.1250\n11.559877"
  },
  {
    "objectID": "sesion3_slides.html#regresi√≥n-simple-y-scatterplot-2",
    "href": "sesion3_slides.html#regresi√≥n-simple-y-scatterplot-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Regresi√≥n simple y scatterplot",
    "text": "Regresi√≥n simple y scatterplot\nRealicemos un scatter sencillo:\n\nmatplotlibseabornseaborn + linea de regresionCon codigos de region\n\n\n\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nplt.scatter( casen_2022_region['esc'], casen_2022_region['ytrabajocor'],)\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc (por regi√≥n)')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.scatterplot(data=casen_2022_region, y='ytrabajocor', x='esc')\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot entre ytrabajocor y esc')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022 es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Intervalo de Confianza')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'})  # El argumento ci controla el intervalo de confianza\n\n# Procesar y agregar etiquetas de regi√≥n a los puntos\nfor i, label in enumerate(casen_2022_region['region']):\n    last_word = label.split()[-1]  # Obtener la √∫ltima palabra de la etiqueta\n    plt.text(casen_2022_region['esc'][i], casen_2022_region['ytrabajocor'][i], last_word, fontsize=8, ha='left', va='bottom')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Scatter Plot con L√≠nea de Regresi√≥n y Etiquetas de Regi√≥n (√öltima Palabra)')\nplt.show()\n\n\n\n\n\n\n\n\n\nPodemos ver que se aprecia una relaci√≥n positiva: a mayor escolaridad promedio, mayor salario promedio por regi√≥n."
  },
  {
    "objectID": "sesion3_slides.html#especificaci√≥n",
    "href": "sesion3_slides.html#especificaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Especificaci√≥n",
    "text": "Especificaci√≥n\nLlamamos especifiaci√≥n al precisar la relaci√≥n entre las variables que deseamos estimar. . . .\nEn nuestro caso, la funci√≥n base que queremos entender es entre salario y educaci√≥n: . . .\n\\[ \\text{Salario} = f(Educacion))\\]\n\nEste es una relaci√≥n teorica entre variables aleatorias, porque no hemos especificado tres elementos cruciales:\n\nagregar el error aleatorio\nespecificar una forma funcional\ndefinir una forma de medir las variables en los datos\n\n\n\nEn nuestro caso, entonces el modelo especificado ser√≠a:\n. . . \\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\\]"
  },
  {
    "objectID": "sesion3_slides.html#interpretaci√≥n",
    "href": "sesion3_slides.html#interpretaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Interpretaci√≥n",
    "text": "Interpretaci√≥n\nCon nuestro modelo especificado:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\\]\nPodemos interpretar \\(\\beta\\) y \\(alpha\\):\n\n\\(\\beta = \\frac{\\partial ingr}{\\partial educ}\\): un a√±o adici√≥nal de educaci√≥n, en cuanto incrementa el salario (si nada m√°s cambia)\n\\(\\alpha\\) valor esperado de y, si x=0‚Ä¶"
  },
  {
    "objectID": "sesion3_slides.html#modelo-poblaci√≥nal-y-estimaci√≥n",
    "href": "sesion3_slides.html#modelo-poblaci√≥nal-y-estimaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Modelo poblaci√≥nal y estimaci√≥n",
    "text": "Modelo poblaci√≥nal y estimaci√≥n\nEste modelo especificado esta definido en la poblaci√≥n:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta \\text{a√±os educaci√≥n}_i + \\mu_i\\]\npero necesitamos calcularlo con la muestra‚Ä¶. por lo cual tenemos estimadores para los coeficientes poblacionales!\n\\[\\hat{ \\text{ingreso del trabajo}}_i = \\hat{\\alpha} + \\hat{\\beta} \\text{a√±os educaci√≥n}_i \\]"
  },
  {
    "objectID": "sesion3_slides.html#modelo-poblaci√≥nal-y-estimaci√≥n-1",
    "href": "sesion3_slides.html#modelo-poblaci√≥nal-y-estimaci√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Modelo poblaci√≥nal y estimaci√≥n",
    "text": "Modelo poblaci√≥nal y estimaci√≥n\nEl m√©todo m√°s comun de estimaci√≥n es el de los m√≠nimos cuadrados ordinarios. Veremos detalles sobre la estimaci√≥n, supuestos, propiedades estad√≠sticas la proxima sesi√≥n.\nPor ahora, pensaremos que es el m√©todo que busca la l√≠nea que produce menores residuos, es decir, menor diferencia entre evalor predicho (linea de regresi√≥n).\n\\[ \\hat{\\mu}_i= y_i-\\hat{y}_i\\]\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\n# Suponiendo que casen_2022_region es tu DataFrame\nsns.set(style='whitegrid')  # Configuraci√≥n del estilo del gr√°fico\n\n# Agregar una columna de constante al DataFrame\ncasen_2022_region['constante'] = 1\n\n# Crear el gr√°fico de dispersi√≥n con la l√≠nea de regresi√≥n\nsns.regplot(data=casen_2022_region, y='ytrabajocor', x='esc', ci=95, line_kws={'color': 'magenta'}, scatter_kws={'color': 'blue'})  # El argumento ci controla el intervalo de confianza\n\n# Ajustar el modelo de regresi√≥n lineal\ny = casen_2022_region['ytrabajocor']\nX = casen_2022_region[['esc', 'constante']]  # 'constante' es la columna que agregamos para el t√©rmino constante\nmodelo = sm.OLS(y, X).fit()\n\n# Calcular las predicciones ('ytrabajocor_pred') a partir del modelo\ncasen_2022_region['ytrabajocor_pred'] = modelo.predict(X)\n\n# Agregar l√≠neas que conecten cada punto a la l√≠nea de regresi√≥n\nfor i in range(len(casen_2022_region)):\n    x_point = casen_2022_region['esc'][i]\n    y_point = casen_2022_region['ytrabajocor'][i]\n    y_pred = casen_2022_region['ytrabajocor_pred'][i]  # Usamos las predicciones del modelo\n    \n    # L√≠nea que conecta el punto a la l√≠nea de regresi√≥n\n    plt.plot([x_point, x_point], [y_point, y_pred], linestyle='--', color='gray')\n\nplt.ylabel('ytrabajocor')\nplt.xlabel('esc')\nplt.title('Lineas de regresi√≥n y residuos')\nplt.show()\n\n\n\n\n\n\nEs decir, minimiza \\[\\sum_{i}^{n} \\hat{\\mu}_i \\]"
  },
  {
    "objectID": "sesion3_slides.html#modelo-estimado",
    "href": "sesion3_slides.html#modelo-estimado",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Modelo estimado",
    "text": "Modelo estimado\nPor ahora, solo estimaremos el modelo directamente usando statsmodels\n\nAgrupados por regi√≥nTodos los datos\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022_region.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.532\nMethod:                 Least Squares   F-statistic:                     18.07\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):           0.000808\nTime:                        11:26:49   Log-Likelihood:                -201.85\nNo. Observations:                  16   AIC:                             407.7\nDf Residuals:                      14   BIC:                             409.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -7.467e+05   3.29e+05     -2.272      0.039   -1.45e+06   -4.17e+04\nesc         1.258e+05   2.96e+04      4.250      0.001    6.23e+04    1.89e+05\n==============================================================================\nOmnibus:                        0.030   Durbin-Watson:                   1.473\nProb(Omnibus):                  0.985   Jarque-Bera (JB):                0.151\nSkew:                          -0.072   Prob(JB):                        0.927\nKurtosis:                       2.547   Cond. No.                         189.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.135\nModel:                            OLS   Adj. R-squared:                  0.135\nMethod:                 Least Squares   F-statistic:                 1.371e+04\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3153e+06\nNo. Observations:               87910   AIC:                         2.631e+06\nDf Residuals:                   87908   BIC:                         2.631e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante  -2.701e+05   8430.418    -32.035      0.000   -2.87e+05   -2.54e+05\nesc         7.707e+04    658.196    117.100      0.000    7.58e+04    7.84e+04\n==============================================================================\nOmnibus:                   165004.626   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1073370403.828\nSkew:                          13.789   Prob(JB):                         0.00\nKurtosis:                     543.626   Cond. No.                         42.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nPodemos ver que un a√±o adicional de educaci√≥n ase asocia con 126.000/77.000 app pesos mensuales, el resto constante.\n¬øy la constante, como la podemos interpretar?"
  },
  {
    "objectID": "sesion3_slides.html#modelos-simples-y-m√∫ltiples",
    "href": "sesion3_slides.html#modelos-simples-y-m√∫ltiples",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Modelos simples y m√∫ltiples",
    "text": "Modelos simples y m√∫ltiples\nMuchas veces una sola variable no es suficiente para describir bien un fen√≥meno. Necesitamos incluir m√°s variables.\nEsto puede ser:\n\nUna nueva variable\nUna forma funcional no lineal de la variable ya incluida\n\nNuestra interpretaci√≥n del modelo no cambia, solo que ahora efectivamente estamos controlando por otros factores."
  },
  {
    "objectID": "sesion3_slides.html#modelos-simples-y-m√∫ltiples-1",
    "href": "sesion3_slides.html#modelos-simples-y-m√∫ltiples-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Modelos simples y m√∫ltiples",
    "text": "Modelos simples y m√∫ltiples\nProbemos, agregar edad al modelo:\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta_1 \\text{a√±os educaci√≥n}_i + \\beta_2 \\text{edad}_i + \\mu_i\\]\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente) y 'esc' (variable independiente),\n# y que puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad']]  # Usar 'constante' como t√©rmino constante\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.150\nModel:                            OLS   Adj. R-squared:                  0.150\nMethod:                 Least Squares   F-statistic:                     7754.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3145e+06\nNo. Observations:               87910   AIC:                         2.629e+06\nDf Residuals:                   87907   BIC:                         2.629e+06\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconstante   -7.37e+05   1.45e+04    -50.839      0.000   -7.65e+05   -7.09e+05\nesc         8.792e+04    708.136    124.163      0.000    8.65e+04    8.93e+04\nedad        7591.3242    192.583     39.419      0.000    7213.864    7968.784\n==============================================================================\nOmnibus:                   165576.617   Durbin-Watson:                   1.666\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1120061445.830\nSkew:                          13.885   Prob(JB):                         0.00\nKurtosis:                     555.280   Cond. No.                         272.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "sesion3_slides.html#modelos-simples-y-m√∫ltiples-2",
    "href": "sesion3_slides.html#modelos-simples-y-m√∫ltiples-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Modelos simples y m√∫ltiples",
    "text": "Modelos simples y m√∫ltiples\nEs muy usual, agregar edad al cuadrado‚Ä¶. para representar que los salarios crecen con la edad hasta cierto punto, y luego empieza a decaer‚Ä¶\n\\[ \\text{ingreso del trabajo}_i = \\alpha + \\beta_1 \\text{a√±os educaci√≥n}_i + \\beta_2 \\text{edad}_i  + \\beta_3 \\text{edad}^2_i + \\mu_i\\]\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Supongamos que 'casen_2022' contiene las columnas 'ytrabajocor' (variable dependiente), 'esc' (variable independiente),\n# 'edad' (variable independiente) y puedes tener valores NaN en tus datos.\n\n# Eliminar filas con valores NaN\ncasen_2022_clean = casen_2022.dropna(subset=['ytrabajocor', 'esc', 'edad'])\n\n# Agregar una columna de constantes para el t√©rmino constante en el modelo\ncasen_2022_clean['constante'] = 1\n\n# Agregar una columna con 'edad' al cuadrado\ncasen_2022_clean['edad_cuadrado'] = casen_2022_clean['edad'] ** 2\n\n# Definir las variables dependiente e independiente\ny = casen_2022_clean['ytrabajocor']\nX = casen_2022_clean[['constante', 'esc', 'edad', 'edad_cuadrado']]  # Incluye 'edad_cuadrado'\n\n# Ajustar el modelo de regresi√≥n lineal\nmodelo = sm.OLS(y, X).fit()\n\n# Imprimir un resumen del modelo\nprint(modelo.summary())\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            ytrabajocor   R-squared:                       0.157\nModel:                            OLS   Adj. R-squared:                  0.157\nMethod:                 Least Squares   F-statistic:                     5454.\nDate:                Sat, 09 Sep 2023   Prob (F-statistic):               0.00\nTime:                        11:26:49   Log-Likelihood:            -1.3142e+06\nNo. Observations:               87910   AIC:                         2.628e+06\nDf Residuals:                   87906   BIC:                         2.628e+06\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconstante     -1.273e+06   2.46e+04    -51.789      0.000   -1.32e+06   -1.23e+06\nesc            8.514e+04    712.733    119.463      0.000    8.37e+04    8.65e+04\nedad           3.529e+04   1045.445     33.754      0.000    3.32e+04    3.73e+04\nedad_cuadrado  -302.7647     11.234    -26.950      0.000    -324.784    -280.746\n==============================================================================\nOmnibus:                   166354.248   Durbin-Watson:                   1.665\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1155260651.345\nSkew:                          14.028   Prob(JB):                         0.00\nKurtosis:                     563.898   Cond. No.                     2.46e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.46e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "sesion3_slides.html#ya-no-es-lineal-el-modelo",
    "href": "sesion3_slides.html#ya-no-es-lineal-el-modelo",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Ya no es lineal el modelo?",
    "text": "Ya no es lineal el modelo?\nOjo! La linealidad es en los par√°metros, no en las variables.\nLa siguiente ecuaci√≥n muestra un modelo lineal en el que el predictor ùë•1 no es lineal respecto a y:\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2log(x_1) + \\epsilon\\)\n\nEn contraposici√≥n, el siguiente no es un modelo lineal:\n\\(y = \\beta_0 + \\beta_1x_1^{\\beta_2} + \\epsilon\\)"
  },
  {
    "objectID": "sesion3_slides.html#ya-no-es-lineal-el-modelo-1",
    "href": "sesion3_slides.html#ya-no-es-lineal-el-modelo-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Ya no es lineal el modelo?",
    "text": "Ya no es lineal el modelo?\nEn ocasiones, algunas relaciones no-lineales pueden transformarse de forma que se pueden expresar de manera lineal:\n\nModelo no-lineal a estimar: \\(y = \\beta_0x_1^{\\beta_1}\\epsilon\\)\nSolucion: pasamos todo a logaritmos:\n\n\\[log(y)=log(\\beta_0) + \\beta_1log(x_1) + log(\\epsilon)\\]\n\\[y^{'}=\\beta_0^{'}+\\beta_1x_1^{'} + \\epsilon^{'}\\]\n\nEstimar el modelo y extraer los coeficientes.\nVolvera a la forma funcional incial exponenciando los logaritmos.\n\n\\(\\beta_1\\) es explicito.\n\\(\\beta_0^{'}=log(\\beta_0)=&gt; exp(log(\\beta_0))\\)"
  },
  {
    "objectID": "sesion3_slides.html#un-poco-m√°s-sobre-interpretaci√≥n",
    "href": "sesion3_slides.html#un-poco-m√°s-sobre-interpretaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Un poco m√°s sobre interpretaci√≥n",
    "text": "Un poco m√°s sobre interpretaci√≥n\nElementos clave en la interpretaci√≥n de un modelo de regresi√≥n lineal:\n\n\\(\\beta_0\\): Ordenada en el origen, valor esperado de \\(y\\) cuando todos los predictores son cero.\n\\(\\beta_j\\): Coeficientes de regresi√≥n parcial de cada predictor, representan el cambio promedio esperado en \\(y\\) al aumentar en una unidad \\(x_j\\), manteniendo otros predictores constantes (‚Äúceteris paribus‚Äù)."
  },
  {
    "objectID": "sesion3_slides.html#un-poco-m√°s-sobre-interpretaci√≥n-magnitud",
    "href": "sesion3_slides.html#un-poco-m√°s-sobre-interpretaci√≥n-magnitud",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Un poco m√°s sobre interpretaci√≥n: Magnitud",
    "text": "Un poco m√°s sobre interpretaci√≥n: Magnitud\nLos coeficientes est√°n medidos en las unidades que se est√° trabajando.\n\nImportancia de coeficientes parciales estandarizados: - Se obtienen al estandarizar las variables predictoras antes del ajuste del modelo. - \\(\\beta_0\\) refleja el valor esperado de \\(y\\) cuando los predictores est√°n en su promedio. - \\(\\beta_j\\) indica el cambio promedio esperado en \\(y\\) al aumentar en una desviaci√≥n est√°ndar \\(x_j\\), manteniendo otros predictores constantes."
  },
  {
    "objectID": "sesion3_slides.html#causalidad-regresi√≥n-y-correlaci√≥n",
    "href": "sesion3_slides.html#causalidad-regresi√≥n-y-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Causalidad, regresi√≥n y correlaci√≥n",
    "text": "Causalidad, regresi√≥n y correlaci√≥n\nImportante tener en cuenta:\n\nAntes de intentar ajustar un modelo lineal a los datos observados, la persona debe determinar primero si existe o no una relaci√≥n entre las variables de inter√©s.\nEsto no implica necesariamente que una variable cause la otra (por ejemplo, puntajes m√°s altos en la PSU no causan calificaciones superiores en la universidad), pero existe alguna asociaci√≥n significativa entre las dos variables.\nUn diagrama de dispersi√≥n puede ser una herramienta √∫til para determinar la fuerza de la relaci√≥n entre dos variables."
  },
  {
    "objectID": "sesion3_slides.html#causalidad-regresi√≥n-y-correlaci√≥n-1",
    "href": "sesion3_slides.html#causalidad-regresi√≥n-y-correlaci√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Causalidad, regresi√≥n y correlaci√≥n",
    "text": "Causalidad, regresi√≥n y correlaci√≥n\nImportante tener en cuenta:\n\nSi parece no haber asociaci√≥n entre las variables explicativas y dependiente propuestas (es decir, el diagrama de dispersi√≥n no indica ninguna tendencia creciente o decreciente),\n\nentonces ajustar un modelo de regresi√≥n lineal a los datos probablemente no proporcionar√° un modelo √∫til.\nUna valiosa medida num√©rica de asociaci√≥n entre dos variables es el coeficiente de correlaci√≥n, que es un valor entre -1 y 1 que indica la fuerza de la asociaci√≥n de los datos observados para las dos variables."
  },
  {
    "objectID": "sesion3_slides.html#una-perspectiva-hist√≥rica",
    "href": "sesion3_slides.html#una-perspectiva-hist√≥rica",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Una perspectiva hist√≥rica:",
    "text": "Una perspectiva hist√≥rica:\n\nEl origen de la t√©cnica, podemos remontarlo a la gen√©tica.\nFrancis Galton estudi√≥ la variaci√≥n y la herencia de los rasgos humanos. Entre muchos otros rasgos, Galton recolect√≥ y estudi√≥ datos de altura de familias para tratar de entender la herencia. Mientras hac√≠a esto, desarroll√≥ los conceptos de correlaci√≥n y regresi√≥n.\nPregunta: ¬øqu√© tan bien podemos predecir la estatura de un ni√±o basado en la estatura de los padres?\nLa t√©cnica que desarroll√≥ para responder a esta pregunta, la regresi√≥n, tambi√©n puede aplicarse en muchas otras circunstancias."
  },
  {
    "objectID": "sesion3_slides.html#una-perspectiva-hist√≥rica-1",
    "href": "sesion3_slides.html#una-perspectiva-hist√≥rica-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Una perspectiva hist√≥rica:",
    "text": "Una perspectiva hist√≥rica:\n\n\n\n\nNota hist√≥rica:\n\nGalton hizo importantes contribuciones a la estad√≠stica y la gen√©tica‚Ä¶\npero tambi√©n fue uno de los primeros defensores de la eugenesia‚Ä¶\nun movimiento filos√≥fico cient√≠ficamente defectuoso favorecido por muchos bi√≥logos de la √©poca de Galton, pero con terribles consecuencias hist√≥ricas."
  },
  {
    "objectID": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura",
    "href": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Estudio de caso: ¬øes hereditaria la altura?",
    "text": "Estudio de caso: ¬øes hereditaria la altura?\n\nTenemos acceso a los datos de altura de familias recolectado por Galton, a trav√©s del paquete HistData.\nEstos datos contienen las alturas de varias docenas de familias: madres, padres, hijas e hijos.\n\n\n\nCargar datosTabla de datos\n\n\n\n# Cargamos los paquetes que vamos a usar\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# Si no tiene stats models, instalar: pip install statsmodels\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Mostrar las primeras filas del DataFrame\nprint(galton_data.head(4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfamily\nfather\nmother\nmidparentHeight\nchildren\nchildNum\ngender\nchildHeight\n\n\n\n\n0\n001\n78.5\n67.0\n75.43\n4\n1\nmale\n73.2\n\n\n1\n001\n78.5\n67.0\n75.43\n4\n2\nfemale\n69.2\n\n\n2\n001\n78.5\n67.0\n75.43\n4\n3\nfemale\n69.0\n\n\n3\n001\n78.5\n67.0\n75.43\n4\n4\nfemale\n69.0"
  },
  {
    "objectID": "sesion3_slides.html#an√°lisis-de-caso-es-hereditaria-la-altura",
    "href": "sesion3_slides.html#an√°lisis-de-caso-es-hereditaria-la-altura",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "An√°lisis de caso: ¬øes hereditaria la altura?",
    "text": "An√°lisis de caso: ¬øes hereditaria la altura?\nPara imitar el an√°lisis de Galton, crearemos un conjunto de datos con las alturas de los padres y un hijo de cada familia seleccionado al azar:\n\n\nCargar datosTabla de datos\n\n\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\ngalton_heights.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfather\nson\n\n\n\n\n0\n78.5\n73.2\n\n\n1\n75.5\n73.5\n\n\n2\n75.0\n71.0\n\n\n3\n75.0\n68.5\n\n\n4\n75.0\n68.0"
  },
  {
    "objectID": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-1",
    "href": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Estudio de caso: ¬øes hereditaria la altura?",
    "text": "Estudio de caso: ¬øes hereditaria la altura?\n\nSupongamos que se nos pidiera que resumi√©ramos (describieramos) los datos de padres e hijos.\nComo ambas distribuciones est√°n aproximadas por la distribuci√≥n normal, podr√≠amos usar los dos promedios y dos desviaciones est√°ndar como res√∫menes:\n\n\n\npromedio_padre = galton_heights['father'].mean()\nsd_padre = galton_heights['father'].std()\npromedio_hijo = galton_heights['son'].mean()\nsd_hijo = galton_heights['son'].std()\n\nresumen_estadistico = pd.DataFrame({\n    'promedio_padre': [promedio_padre],\n    'sd_padre': [sd_padre],\n    'promedio_hijo': [promedio_hijo],\n    'sd_hijo': [sd_hijo]\n})\n\nprint(resumen_estadistico)\n\n\n\n   promedio_padre  sd_padre  promedio_hijo   sd_hijo\n0       69.098883  2.546555      69.248045  2.680733\n\n\nSin embargo, este resumen no describe una caracter√≠stica importante de los datos:\nla tendencia de que cuanto m√°s alto es el padre, m√°s alto es el hijo."
  },
  {
    "objectID": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-2",
    "href": "sesion3_slides.html#estudio-de-caso-es-hereditaria-la-altura-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Estudio de caso: ¬øes hereditaria la altura?",
    "text": "Estudio de caso: ¬øes hereditaria la altura?\n\nCodePlot\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configurar el tama√±o de la figura\nplt.figure(figsize=(10, 6))\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Crear el gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n\nsns.set(style=\"whitegrid\")\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5, size=3)\nsns.regplot(data=galton_heights, x='father', y='son', scatter=False)\n\nplt.xlabel(\"Altura del Padre\")\nplt.ylabel(\"Altura del Hijo\")\nplt.title(\"Relaci√≥n entre Altura del Padre y Altura del Hijo\")\n\n# Mostrar el gr√°fico\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#regresi√≥n-y-la-correlaci√≥n",
    "href": "sesion3_slides.html#regresi√≥n-y-la-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "¬øRegresi√≥n? ¬øY la correlaci√≥n?",
    "text": "¬øRegresi√≥n? ¬øY la correlaci√≥n?\n\n\n\nAmbos est√°n muy relacionados.\nAprenderemos que el coeficiente de correlaci√≥n es un resumen informativo de c√≥mo dos variables se mueven juntas‚Ä¶\ny luego veremos c√≥mo esto puede ser usado para predecir una variable usando la otra y modelado en una regresi√≥n"
  },
  {
    "objectID": "sesion3_slides.html#taller-de-aplicaci√≥n-2",
    "href": "sesion3_slides.html#taller-de-aplicaci√≥n-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Taller de aplicaci√≥n 2:",
    "text": "Taller de aplicaci√≥n 2:\nCaso aplicaci√≥n: Cursos de Verano\n\n\n\n\n\n\nTaller de aplicaci√≥n 2: Pregunta 1\n\n\n\nConsidere los datos trabajados en el taller 1, sobre los cursos de verano. Recordemos la pregunta que quer√≠amos responder:\nAsistir a cursos de verano mejora los resultados acad√©micos?\n\n\nPlantee un modelo de regresi√≥n con los datos disponibles que deseamos estimar.\nGrafique la dispersi√≥n y la recta de regresi√≥n estimada."
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n",
    "href": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El coeficiente de correlaci√≥n",
    "text": "El coeficiente de correlaci√≥n\nEl coeficiente de correlaci√≥n se define para una lista de pares \\((x_1,y_1),...(x_n,y_n)\\) como la media de los productos de los valores normalizados:\n\\[\n\\rho = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\frac{x_i-\\mu_x }{\\sigma_x}\\big)\\big(\\frac{y_i-\\mu_y}{\\sigma_y}\\big)\n\\]\nD√≥nde \\(\\mu\\) son promedios y \\(\\sigma\\) son desviaciones est√°ndar. La letra griega para r, \\(\\rho\\) se utiliza com√∫nmente en los libros de estad√≠stica para denotar la correlaci√≥n, porque es la primera letra de regresi√≥n. Pronto aprenderemos sobre la conexi√≥n entre correlaci√≥n y regresi√≥n.\nPodemos representar la f√≥rmula anterior con el c√≥digo R usando:\nrho &lt;- mean(scale(x) * scale(y))\nLa correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\):"
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n-1",
    "href": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El coeficiente de correlaci√≥n",
    "text": "El coeficiente de correlaci√≥n\nPodemos representar la f√≥rmula anterior con el siguiente c√≥digo usando:\n\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])  # Tu arreglo x aqu√≠\ny = np.array([6, 7, 8, 9, 10])  # Tu arreglo y aqu√≠\n\nrho = np.mean((x - np.mean(x)) * (y - np.mean(y))) / (np.std(x) * np.std(y))\n\nprint(rho)\n\n\n\n0.9999999999999998"
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n-2",
    "href": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El coeficiente de correlaci√≥n",
    "text": "El coeficiente de correlaci√≥n\nLa correlaci√≥n entre las alturas del padre y del hijo es de aproximadamente \\(0,4\\).\n\ncorrelation_coefficient = galton_heights[['father', 'son']].corr().iloc[0, 1]\nprint(\"Coeficiente de Correlaci√≥n:\", correlation_coefficient)\n\n\n\nCoeficiente de Correlaci√≥n: 0.4501189204142688\n\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar el conjunto de datos GaltonFamilies\ngalton_data = sm.datasets.get_rdataset(\"GaltonFamilies\", package=\"HistData\").data\n\n# Filtrar por g√©nero masculino y seleccionar una muestra de una altura de hijo por familia\ngalton_heights = galton_data[galton_data['gender'] == 'male']\\\n    .groupby('family')\\\n    .apply(lambda group: group.sample(n=1))\\\n    .reset_index(drop=True)\\\n    .loc[:, ['father', 'childHeight']]\\\n    .rename(columns={'childHeight': 'son'})\n\n# Calcular la media y la desviaci√≥n est√°ndar de la altura del padre\nmean_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).mean()\nsd_scaled_father = StandardScaler().fit_transform(galton_heights[['father']]).std()\n\nmean_father = galton_heights['father'].mean()\nsd_father = galton_heights['father'].std()\n\nprint(\"Media de la Altura del Padre (Estandarizada):\", mean_scaled_father)\nprint(\"Desviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada):\", sd_scaled_father)\nprint(\"Media de la Altura del Padre:\", mean_father)\nprint(\"Desviaci√≥n Est√°ndar de la Altura del Padre:\", sd_father)\n\n\n\nMedia de la Altura del Padre (Estandarizada): 4.6046344887246715e-15\nDesviaci√≥n Est√°ndar de la Altura del Padre (Estandarizada): 1.0\nMedia de la Altura del Padre: 69.09888268156423\nDesviaci√≥n Est√°ndar de la Altura del Padre: 2.546555038637639"
  },
  {
    "objectID": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n-3",
    "href": "sesion3_slides.html#el-coeficiente-de-correlaci√≥n-3",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "El coeficiente de correlaci√≥n",
    "text": "El coeficiente de correlaci√≥n\n\n# Calcular la correlaci√≥n entre father y son usando una muestra de galton_heights\nR = galton_heights.sample(n=75, replace=True).corr().loc[\"father\", \"son\"]\nprint(R)\n\n\n\n0.5104684885147245\n\n\nPara ver c√≥mo se ven los datos para los diferentes valores de \\(\\rho\\) aqu√≠ hay seis ejemplos de pares con correlaciones que van de -0,9 a 0,99:\n\nimage"
  },
  {
    "objectID": "sesion3_slides.html#la-correlaci√≥n-es-variable-aleatoria",
    "href": "sesion3_slides.html#la-correlaci√≥n-es-variable-aleatoria",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La correlaci√≥n es variable aleatoria",
    "text": "La correlaci√≥n es variable aleatoria\nAntes de continuar conectando la correlaci√≥n con la regresi√≥n, recordemos la variabilidad aleatoria.\nEn la mayor√≠a de las aplicaciones de la ciencia de datos, observamos datos que incluyen variaci√≥n aleatoria.\nA modo de ejemplo, supongamos que las 179 parejas de padres e hijos son toda nuestra poblaci√≥n. Un genetista menos afortunado s√≥lo puede costear las mediciones de una muestra aleatoria de 25 pares. La correlaci√≥n de la muestra se puede calcular con:\n\nimport pandas as pd\n\n# Seleccionar una muestra aleatoria de tama√±o 75 con reemplazo\nR = galton_heights.sample(n=75, replace=True)\n\n# Calcular el coeficiente de correlaci√≥n entre las columnas \"father\" y \"son\"\ncorrelation_coefficient = R[['father', 'son']].corr().iloc[0, 1]\n\nprint(\"Coeficiente de Correlaci√≥n en la Muestra:\", correlation_coefficient)\n\n\n\nCoeficiente de Correlaci√≥n en la Muestra: 0.5072834085504967"
  },
  {
    "objectID": "sesion3_slides.html#section",
    "href": "sesion3_slides.html#section",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "",
    "text": "R es una variable aleatoria. Podemos ejecutar una simulaci√≥n de Monte Carlo para ver su distribuci√≥n:\n\nNota: el objetivo principal de la simulaci√≥n de Montecarlo es intentar imitar el comportamiento de variables reales para, en la medida de lo posible, analizar o predecir c√≥mo van a evolucionar.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nB = 1000\nN = 100\nR = np.zeros(B)\n\nfor i in range(B):\n    sample = galton_heights.sample(n=N, replace=False)\n    correlation_coefficient = sample[['father', 'son']].corr().iloc[0, 1]\n    R[i] = correlation_coefficient\n\n# Crear un histograma de los coeficientes de correlaci√≥n\nplt.hist(R, bins=np.arange(-1, 1.1, 0.05), color='black')\nplt.xlabel(\"Coeficiente de Correlaci√≥n\")\nplt.ylabel(\"Frecuencia\")\nplt.title(\"Histograma de Coeficientes de Correlaci√≥n\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#section-1",
    "href": "sesion3_slides.html#section-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "",
    "text": "Vemos que el valor esperado de R es la correlaci√≥n de la poblaci√≥n:\n\nmean_R = np.mean(R)\nprint(\"Media de Coeficientes de Correlaci√≥n:\", mean_R)\n\n\n\nMedia de Coeficientes de Correlaci√≥n: 0.45842882440885\n\n\ny que tiene un error est√°ndar relativamente alto en relaci√≥n con el rango de valores que puede tomar R:\n\nsd_R = np.std(R)\nprint(\"Desviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n:\", sd_R)\n\n\n\nDesviaci√≥n Est√°ndar de Coeficientes de Correlaci√≥n: 0.047460810230179874"
  },
  {
    "objectID": "sesion3_slides.html#section-2",
    "href": "sesion3_slides.html#section-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "",
    "text": "Por lo tanto, al interpretar las correlaciones, recuerde que las correlaciones derivadas de las muestras son estimaciones que contienen incertidumbre.\nAdem√°s, tenga en cuenta que debido a que la correlaci√≥n de la muestra es un promedio de extracciones independientes, el teorema del l√≠mite central realmente funciona.\nPor lo tanto, para \\(N\\) lo suficientemente grande la distribuci√≥n de \\(R\\) es aproximadamente normal con el valor esperado \\(\\rho\\).\nLa desviaci√≥n est√°ndar, que es algo compleja de derivar, es: \\(\\sqrt{\\frac{1-r^2}{N-2}}\\)."
  },
  {
    "objectID": "sesion3_slides.html#section-3",
    "href": "sesion3_slides.html#section-3",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "",
    "text": "En nuestro ejemplo, \\(N=25\\) no parece ser lo suficientemente grande para que la aproximaci√≥n sea buena\n\n-Si N aumenta ver√°s que la distribuci√≥n converge a una normal.\n\nNota: El gr√°fico Q-Q, o gr√°fico cuantitativo, es una herramienta gr√°fica que nos ayuda a evaluar si un conjunto de datos proviene plausiblemente de alguna distribuci√≥n te√≥rica como una Normal o exponencial.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Crear un DataFrame con los coeficientes de correlaci√≥n\ndf_R = pd.DataFrame({'R': R})\n\n# Calcular la media y el tama√±o de la muestra\nmean_R = np.mean(R)\nN = len(R)\n\n# Crear el gr√°fico QQ-plot\nplt.figure(figsize=(6, 6))\nstats.probplot(df_R['R'], dist='norm', plot=plt)\nplt.xlabel(\"Cuantiles Te√≥ricos\")\nplt.ylabel(\"Cuantiles de R\")\nplt.title(\"Gr√°fico QQ-plot para los Coeficientes de Correlaci√≥n\")\nplt.plot([np.min(R), np.max(R)], [np.min(R), np.max(R)], color='red')  # L√≠nea de referencia\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#la-correlaci√≥n-no-siempre-es-un-resumen-√∫til",
    "href": "sesion3_slides.html#la-correlaci√≥n-no-siempre-es-un-resumen-√∫til",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La correlaci√≥n no siempre es un resumen √∫til",
    "text": "La correlaci√≥n no siempre es un resumen √∫til\nLa correlaci√≥n no siempre es un buen resumen de la relaci√≥n entre dos variables. Los siguientes cuatro conjuntos de datos artificiales, conocidos como el cuarteto de Anscombe, ilustran este punto. Todos estos pares tienen una correlaci√≥n de 0,82:\n\nimageLa correlaci√≥n s√≥lo tiene sentido en un contexto particular. Para ayudarnos a entender cu√°ndo es que la correlaci√≥n es significativa como estad√≠stica de resumen, volveremos al ejemplo de predecir la estatura del hijo usando la estatura del padre. Esto ayudar√° a motivar y definir la regresi√≥n lineal. Comenzamos demostrando c√≥mo la correlaci√≥n puede ser √∫til para la predicci√≥n."
  },
  {
    "objectID": "sesion3_slides.html#correlaci√≥n-espuria",
    "href": "sesion3_slides.html#correlaci√≥n-espuria",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Correlaci√≥n espuria",
    "text": "Correlaci√≥n espuria\nVemos una fuerte correlaci√≥n entre las tasas de divorcio y el consumo de margarina.\n\nimage(Ac√° pueden encontrar m√°s http://tylervigen.com/old-version.html)\n\n¬øSignifica esto que la margarina causa divorcios?\n\n¬øO los divorcios hacen que la gente coma m√°s margarina?"
  },
  {
    "objectID": "sesion3_slides.html#la-paradoja-de-simpson",
    "href": "sesion3_slides.html#la-paradoja-de-simpson",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La paradoja de Simpson",
    "text": "La paradoja de Simpson\n\nSe llama paradoja porque vemos el signo de la correlaci√≥n cambiar cuando comparamos toda la data y estratos espec√≠ficos.\nComo ejemplo ilustrativo, supongamos que tiene tres variables aleatorias \\(X\\), \\(Y\\) y \\(Z\\) y que observamos realizaciones de estas.\nAqu√≠ est√° el gr√°fico de observaciones simuladas para \\(X\\) y \\(Y\\) a lo largo de la correlaci√≥n de la muestra:"
  },
  {
    "objectID": "sesion3_slides.html#la-paradoja-de-simpson-1",
    "href": "sesion3_slides.html#la-paradoja-de-simpson-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La paradoja de Simpson",
    "text": "La paradoja de Simpson\n\nPuedes ver que \\(X\\) e \\(Y\\) est√°n negativamente correlacionados.\nSin embargo, una vez que estratificamos por \\(Z\\) (mostrado en diferentes colores abajo) emerge otro patr√≥n:"
  },
  {
    "objectID": "sesion3_slides.html#la-paradoja-de-simpson-2",
    "href": "sesion3_slides.html#la-paradoja-de-simpson-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La paradoja de Simpson",
    "text": "La paradoja de Simpson\n\n\nEs realmente \\(Z\\) que est√° negativamente correlacionado con \\(X\\).\nSi estratificamos por \\(Z\\) las variables \\(X\\) e \\(Y\\) est√°n en realidad correlacionados positivamente como se ha visto en el gr√°fico anterior."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales",
    "href": "sesion3_slides.html#expectativas-condicionales",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nSupongamos que nos piden que adivinemos la altura de un hijo seleccionado al azar y no sabemos la altura de su padre.\nDebido a que la distribuci√≥n de las alturas de los hijos es aproximadamente normal, sabemos que la altura media, \\(69.2\\), es el valor con la mayor proporci√≥n y ser√≠a la predicci√≥n con mayores posibilidades de minimizar el error.\nPero, ¬øy si nos dicen que el padre es m√°s alto que el promedio, digamos que mide 72 pulgadas de alto, todav√≠a esperar√≠amos que la altura m√°s probable del hijo sea 69.2 pulgadas?"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-1",
    "href": "sesion3_slides.html#expectativas-condicionales-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "expectativas condicionales",
    "text": "expectativas condicionales\n\nResulta que si pudi√©ramos recolectar datos de un gran n√∫mero de padres que miden 72 pulgadas‚Ä¶\n\nla distribuci√≥n de las alturas de sus hijos ser√≠a normalmente distribuida.\nEsto implica que el promedio de la distribuci√≥n calculada en este subconjunto ser√≠a nuestra mejor predicci√≥n."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-2",
    "href": "sesion3_slides.html#expectativas-condicionales-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "expectativas condicionales",
    "text": "expectativas condicionales\n\nEn general, llamamos a este enfoque condicional.\nLa idea general es que estratificamos una poblaci√≥n en grupos y calculamos res√∫menes en cada grupo.\nPor lo tanto, el condicionamiento est√° relacionado con el concepto de estratificaci√≥n descrito.\nPorque la expectativa condicional \\(E(Y|X=x)\\) es el mejor predictor para la variable aleatoria \\(Y\\) para un individuo en los estratos definidos por \\(X=x\\) muchos de los desaf√≠os de la ciencia de datos se reducen a la estimaci√≥n de esta cantidad."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-3",
    "href": "sesion3_slides.html#expectativas-condicionales-3",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nEn el ejemplo que hemos estado considerando, estamos interesados en calcular la altura promedio del hijo condicionada a que el padre tenga 72 pulgadas de altura.\nQueremos estimar \\(E(Y|X=72)\\) usando la muestra recolectada por Galton.\n¬øCuantos padres miden 72?\n\n\n\ncount_72 = (galton_heights['father'] == 72).sum()\nprint(\"Cantidad de registros con valor 72 en la columna 'father':\", count_72)\n\n\n\nCantidad de registros con valor 72 en la columna 'father': 8\n\n\n\nSi cambiamos el n√∫mero a 72.5, obtenemos a√∫n menos puntos de datos:\n\n\n\n\ncount_725 = (galton_heights['father'] == 72.5).sum()\nprint(\"Cantidad de registros con valor 72.5 en la columna 'father':\", count_725)\n\n\n\nCantidad de registros con valor 72.5 en la columna 'father': 1"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-4",
    "href": "sesion3_slides.html#expectativas-condicionales-4",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nUna forma pr√°ctica de mejorar estas estimaciones de las expectativas condicionales, es definir estratos con valores similares de \\(x\\).\nEn nuestro ejemplo, podemos redondear las alturas paternas a la pulgada m√°s cercana y asumir que todas son de 72 pulgadas.\nSi hacemos esto, terminamos con la siguiente predicci√≥n para el hijo de un padre que mide 72 pulgadas de alto:\n\n\nconditional_avg = galton_heights[galton_heights['father'].round() == 72]['son'].mean()\nprint(\"Promedio condicional para father == 72:\", conditional_avg)\n\n\n\nPromedio condicional para father == 72: 70.44285714285715"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-5",
    "href": "sesion3_slides.html#expectativas-condicionales-5",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nNote que un padre de 72 pulgadas es m√°s alto que el promedio ‚Äì espec√≠ficamente, 72 - 69.1/2.5 = 1.1 desviaciones est√°ndar m√°s alto que el padre promedio.\nNuestra predicci√≥n, \\(70.5\\), es tambi√©n m√°s alta que el promedio, pero s√≥lo \\(0.49\\) desviaciones est√°ndar m√°s grandes que el hijo promedio.\nLos hijos de padres de 72 pulgadas han regresado algunos a la estatura promedio.\nObservamos que la reducci√≥n en el n√∫mero de SD m√°s altas es de alrededor de \\(0.5\\), lo que resulta ser la correlaci√≥n.\nComo veremos en una secci√≥n posterior, esto no es una coincidencia."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-6",
    "href": "sesion3_slides.html#expectativas-condicionales-6",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nSi queremos hacer una predicci√≥n de cualquier altura, no s√≥lo de 72, podr√≠amos aplicar el mismo enfoque a cada estrato.\nLa estratificaci√≥n seguida de los boxplots nos permite ver la distribuci√≥n de cada grupo:\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Crear una nueva columna 'father_strata' con los valores redondeados de 'father'\ngalton_heights['father_strata'] = galton_heights['father'].round().astype(int)\n\n# Crear el gr√°fico de boxplots\nplt.figure(figsize=(10, 6))  # Tama√±o del gr√°fico\nsns.boxplot(data=galton_heights, x='father_strata', y='son')\n\n# Agregar puntos para mostrar las medias condicionadas\nsns.swarmplot(data=galton_heights, x='father_strata', y='son', color='black', size=4)\n\nplt.xlabel('father_strata')\nplt.ylabel('son')\nplt.title('Boxplots de son condicionado por father_strata con Medias Condicionadas')\nplt.xticks(rotation=45)  # Rotar etiquetas del eje x si es necesario\n\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-7",
    "href": "sesion3_slides.html#expectativas-condicionales-7",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\nNo es de extra√±ar que los centros de los grupos aumenten con la altura.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n# Crear un gr√°fico de puntos para mostrar el promedio condicional por \"father\"\nplt.figure(figsize=(10, 6))\nplt.scatter(conditional_avg_by_father['father'], conditional_avg_by_father['son'], color='blue')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres\")\nplt.show()\n\n\n\n\n\n\n\nAdem√°s, estos centros parecen seguir una relaci√≥n lineal."
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-8",
    "href": "sesion3_slides.html#expectativas-condicionales-8",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nA continuaci√≥n se presentan los promedios de cada grupo.\nSi tenemos en cuenta que estos promedios son variables aleatorias con errores est√°ndar, los datos son consistentes con estos puntos siguiendo una l√≠nea recta:\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Redondear los valores de la columna \"father\"\ngalton_heights['father'] = galton_heights['father'].round()\n\n# Calcular el promedio condicional de \"son\" para cada valor de \"father\"\nconditional_avg_by_father = galton_heights.groupby('father')['son'].mean().reset_index()\n\n\nconditional_avg_by_father.head()\n\n\n# Crear un gr√°fico de puntos con ajuste de regresi√≥n lineal\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='father', y='son', data=conditional_avg_by_father, color='blue')\nsns.regplot(x='father', y='son', data=conditional_avg_by_father, scatter=False, color='orange')\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Conditional Son Height Average\")\nplt.title(\"Promedio Condicional de Alturas de Hijos por Altura de Padres con Regresi√≥n Lineal\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#expectativas-condicionales-9",
    "href": "sesion3_slides.html#expectativas-condicionales-9",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Expectativas condicionales",
    "text": "Expectativas condicionales\n\nEl hecho de que estos promedios condicionales sigan una l√≠nea no es una coincidencia.\nEn la siguiente secci√≥n, explicamos que la l√≠nea que siguen estos promedios es lo que llamamos la l√≠nea de regresi√≥n, que mejora la precisi√≥n de nuestras estimaciones.\nSin embargo, no siempre es apropiado estimar las expectativas condicionales con la l√≠nea de regresi√≥n, por lo que tambi√©n describimos la justificaci√≥n te√≥rica de Galton para usar la l√≠nea de regresi√≥n."
  },
  {
    "objectID": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n",
    "href": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La l√≠nea/recta de regresi√≥n",
    "text": "La l√≠nea/recta de regresi√≥n\n\nSi estamos prediciendo una variable aleatoria \\(Y\\) conociendo el valor de otra variable \\(X=x\\) usando una l√≠nea de regresi√≥n, entonces predecimos que para cada desviaci√≥n est√°ndar, \\(\\sigma_x\\) que \\(x\\) aumenta por encima de la media \\(\\mu_x\\), \\(Y\\) incrementa \\(\\rho\\) veces la desviaci√≥n est√°ndar \\(\\sigma_Y\\) sobre el promedio \\(\\mu_Y\\), con \\(\\rho\\) la correlaci√≥n entre \\(X\\) e \\(Y\\). Por lo tanto, la formula de la regresi√≥n es:\n\n\\[\n\\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right)=\\rho \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\n\\]\nLo que podemos reescribir como:\n\\[\nY=\\mu_Y + \\rho \\big(\\frac{x-\\mu_X}{\\sigma_X}\\big) \\sigma_Y\n\\]"
  },
  {
    "objectID": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n-1",
    "href": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La l√≠nea/recta de regresi√≥n",
    "text": "La l√≠nea/recta de regresi√≥n\n\nSi existe una correlaci√≥n perfecta, la l√≠nea de regresi√≥n predice un aumento que corresponde al mismo n√∫mero de desviacones est√°ndar.\nSi hay correlaci√≥n 0, entonces no usamos \\(x\\) en absoluto en la predicci√≥n y simplemente predecimos el promedio \\(\\mu_Y\\).\nPara valores entre 0 y 1, la predicci√≥n se encuentra en un punto intermedio.\nSi la correlaci√≥n es negativa, predecimos una reducci√≥n en lugar de un aumento."
  },
  {
    "objectID": "sesion3_slides.html#regresi√≥n-a-la-media",
    "href": "sesion3_slides.html#regresi√≥n-a-la-media",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Regresi√≥n a la media",
    "text": "Regresi√≥n a la media\n\nN√≥tese que si la correlaci√≥n es positiva e inferior a 1, nuestra predicci√≥n est√° m√°s cerca (en unidades est√°ndar) de la altura media que de lo que el valor utilizado para predecir, \\(x\\), est√° del promedio de los \\(x\\).\nPor eso lo llamamos regresi√≥n: el hijo regresa a la estatura media.\nDe hecho, el t√≠tulo del art√≠culo de Galton era: Regresi√≥n a la mediocridad en la estatura hereditaria (Regression toward mediocrity in hereditary stature.)."
  },
  {
    "objectID": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n-2",
    "href": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La l√≠nea/recta de regresi√≥n",
    "text": "La l√≠nea/recta de regresi√≥n\n\nPara a√±adir l√≠neas de regresi√≥n a los gr√°ficos, necesitaremos la f√≥rmula anterior en la forma: \\(y=b+mx\\), con pendiente \\(m=\\rho \\sigma_y / \\sigma_x\\) e intercepto \\(b=\\mu_y - m \\mu_x\\)\nAqu√≠ agregamos la l√≠nea de regresi√≥n a la data original.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# C√°lculo de las medias y desviaciones est√°ndar\nmu_x = galton_heights['father'].mean()\nmu_y = galton_heights['son'].mean()\ns_x = galton_heights['father'].std()\ns_y = galton_heights['son'].std()\n\n# C√°lculo del coeficiente de correlaci√≥n\nr = galton_heights['father'].corr(galton_heights['son'])\n\n# C√°lculo de la pendiente y el intercepto para la l√≠nea de regresi√≥n\nm = r * s_y / s_x\nb = mu_y - m * mu_x\n\n# Configuraci√≥n del tama√±o de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n\nsns.scatterplot(x='father', y='son', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father', y='son', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height\")\nplt.ylabel(\"Son Height\")\nplt.title(\"Relaci√≥n entre Altura de Padres e Hijos con L√≠nea de Regresi√≥n\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n-3",
    "href": "sesion3_slides.html#la-l√≠nearecta-de-regresi√≥n-3",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "La l√≠nea/recta de regresi√≥n",
    "text": "La l√≠nea/recta de regresi√≥n\n\nLa f√≥rmula de regresi√≥n implica que si primero estandarizamos las variables, es decir, restamos el promedio y dividimos por la desviaci√≥n est√°ndar, entonces la l√≠nea de regresi√≥n tiene intercepto 0 y pendiente igual a la correlaci√≥n \\(\\rho\\).\nAqu√≠ est√° la misma gr√°fica, pero usando unidades est√°ndar:\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Supongamos que tienes un DataFrame llamado 'galton_heights' con las columnas 'father' y 'son'.\n\n# Estandarizar las variables 'father' y 'son'\ngalton_heights['father_standardized'] = (galton_heights['father'] - galton_heights['father'].mean()) / galton_heights['father'].std()\ngalton_heights['son_standardized'] = (galton_heights['son'] - galton_heights['son'].mean()) / galton_heights['son'].std()\n\n# Calcular la correlaci√≥n de las variables estandarizadas\nr = galton_heights['father_standardized'].corr(galton_heights['son_standardized'])\n\n# Configuraci√≥n del tama√±o de la figura\nplt.figure(figsize=(10, 6))\n\n# Crear un gr√°fico de dispersi√≥n con l√≠nea de regresi√≥n\nsns.scatterplot(x='father_standardized', y='son_standardized', data=galton_heights, alpha=0.5, size=3)\nsns.regplot(x='father_standardized', y='son_standardized', data=galton_heights, scatter=False, color='red', line_kws={'color': 'blue'})\nplt.xlabel(\"Father Height (Standardized)\")\nplt.ylabel(\"Son Height (Standardized)\")\nplt.title(\"Relaci√≥n Estandarizada entre Altura de Padres e Hijos con L√≠nea de Regresi√≥n (Intercepto = 0, Pendiente = Correlaci√≥n)\")\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#interpretaci√≥n-1",
    "href": "sesion3_slides.html#interpretaci√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Interpretaci√≥n:",
    "text": "Interpretaci√≥n:\nEn ambos casos, la interpretaci√≥n de los elementos del modelo es la misma:\n\n\\(\\beta_0\\): es la ordenada en el origen, se corresponde con el valor promedio de la variable respuesta \\(y\\) cuando todos los predictores son cero.\n\\(\\beta_j\\): es el efecto promedio que tiene sobre la variable respuesta el incremento en una unidad de la variable predictora \\(x_j\\), manteni√©ndose constantes el resto de variables. Se conocen como coeficientes de regresi√≥n.\n\\(\\epsilon\\): es el residuo o error, la diferencia entre el valor observado y el estimado por el modelo. Recoge el efecto de todas aquellas variables que influyen en \\(y\\) pero que no se incluyen en el modelo como predictores."
  },
  {
    "objectID": "sesion3_slides.html#interpretaci√≥n-2",
    "href": "sesion3_slides.html#interpretaci√≥n-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Interpretaci√≥n:",
    "text": "Interpretaci√≥n:\n\nEn la gran mayor√≠a de casos, los valores \\(\\beta_0\\) y \\(\\beta_j\\) poblacionales se desconocen, por lo que, a partir de una muestra, se obtienen sus estimaciones \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_j}\\).\nAjustar el modelo consiste en estimar, a partir de los datos disponibles, los valores de los coeficientes de regresi√≥n que maximizan la verosimilitud (likelihood), es decir, los que dan lugar al modelo que con mayor probabilidad puede haber generado los datos observados.\nEl m√©todo empleado con m√°s frecuencia es el ajuste por m√≠nimos cuadrados ordinarios (OLS)\n\nque identifica como mejor modelo la recta (o plano si es regresi√≥n m√∫ltiple)\nque minimiza la suma de las desviaciones verticales entre cada dato de entrenamiento y la recta, elevadas al cuadrado."
  },
  {
    "objectID": "sesion3_slides.html#magnitud-y-significancia",
    "href": "sesion3_slides.html#magnitud-y-significancia",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Magnitud y significancia",
    "text": "Magnitud y significancia\n\nLa magnitud de cada coeficiente parcial de regresi√≥n depende de las unidades en las que se mida la variable predictora a la que corresponde, por lo que su magnitud no est√° asociada con la importancia de cada predictor.\nUna buena practica es estandarizar"
  },
  {
    "objectID": "sesion3_slides.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n",
    "href": "sesion3_slides.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Cuidado: hay dos l√≠neas de regresi√≥n",
    "text": "Cuidado: hay dos l√≠neas de regresi√≥n\nCalculamos una l√≠nea de regresi√≥n para predecir la altura del hijo desde la altura del padre.\nUsamos estos c√°lculos:\n\nimport numpy as np\n\n# Calcular la media de las alturas del padre\nmu_x = galton_heights['father'].mean()\n\n# Calcular la media de las alturas del hijo\nmu_y = galton_heights['son'].mean()\n\n# Calcular la desviaci√≥n est√°ndar de las alturas del padre\ns_x = galton_heights['father'].std()\n\n# Calcular la desviaci√≥n est√°ndar de las alturas del hijo\ns_y = galton_heights['son'].std()\n\n# Calcular el coeficiente de correlaci√≥n entre las alturas del padre y el hijo\nr = galton_heights['father'].corr(galton_heights['son'])\n\n\n\nprint(r)\nprint(s_x)\nprint(s_y)\nprint(mu_x)\nprint(mu_y)\n\n\n\n0.4658308243792417\n2.56441709039222\n2.6076529186746025\n69.08938547486034\n69.263687150838\n\n\n\n# Calcular la pendiente de la primera l√≠nea de regresi√≥n\nm_1 = r * s_y / s_x\n\n# Calcular el intercepto de la primera l√≠nea de regresi√≥n\nb_1 = mu_y - m_1 * mu_x\n\nprint(\"pendiente\", m_1)\nprint(\"constante\", b_1)\n\n\n\npendiente 0.47368468778038647\nconstante 36.53710316324001"
  },
  {
    "objectID": "sesion3_slides.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n-1",
    "href": "sesion3_slides.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n-1",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Cuidado: hay dos l√≠neas de regresi√≥n",
    "text": "Cuidado: hay dos l√≠neas de regresi√≥n\n\n¬øY si queremos predecir la estatura del padre bas√°ndonos en la del hijo?\nEs importante saber que esto no se determina calculando la funci√≥n inversa!.\nNecesitamos computar \\(E(X‚à£Y=y)\\). Dado que los datos son aproximadamente normales bivariados, la teor√≠a descrita anteriormente nos dice que esta expectativa condicional seguir√° una l√≠nea con pendiente e intercepto:\n\n\nm_2 = r * s_x / s_y\nb_2 = mu_x - m_2 * mu_y\n\nprint(\"pendiente\", m_2)\nprint(\"constante\", b_2)\n\n\n\npendiente 0.4581071808731349\nconstante 37.35919301731116"
  },
  {
    "objectID": "sesion3_slides.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n-2",
    "href": "sesion3_slides.html#cuidado-hay-dos-l√≠neas-de-regresi√≥n-2",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Cuidado: hay dos l√≠neas de regresi√≥n",
    "text": "Cuidado: hay dos l√≠neas de regresi√≥n\n\nAqu√≠ hay un gr√°fico que muestra las dos l√≠neas de regresi√≥n:\ncon azul para la predicci√≥n de las alturas del hijo con las alturas del padre y rojo para la predicci√≥n de las alturas del padre con las alturas del hijo.\n\n\nCodigoplot\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Crear el gr√°fico utilizando Matplotlib y Seaborn\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=galton_heights, x='father', y='son', alpha=0.5)\nplt.plot(galton_heights['father'], b_1 + m_1 * galton_heights['father'], color='blue', label='y = b_1 + m_1 * x')\nplt.plot(galton_heights['father'], -b_2/m_2 + 1/m_2 * galton_heights['father'], color='red', label='y = -b_2/m_2 + 1/m_2 * x')\nplt.legend()\nplt.xlabel('Father Height')\nplt.ylabel('Son Height')\nplt.title('Scatter Plot with Regression Lines')\nplt.show()"
  },
  {
    "objectID": "sesion3_slides.html#taller-aplicaci√≥n-2-altura-de-madres-padres-hijos-e-hijas",
    "href": "sesion3_slides.html#taller-aplicaci√≥n-2-altura-de-madres-padres-hijos-e-hijas",
    "title": "Sesi√≥n 3: Introducci√≥n al an√°lisis de regresi√≥n",
    "section": "Taller aplicaci√≥n 2: ALtura de madres, padres, hijos e hijas",
    "text": "Taller aplicaci√≥n 2: ALtura de madres, padres, hijos e hijas\n\n\n\n\n\n\nTaller aplicacci√≥n 2: Altura de madres, padres, hijos e hijas\n\n\n\nCargue los datos de GaltonFamilies desde el HistData. Los ni√±os de cada familia est√°n ordenados por sexo y luego por estatura. Cree un conjunto de datos llamado galton_heights seleccionando ni√±os y ni√±as al azar. (HINT: use sample).\nHaga una gr√°fica para las alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nCalcular la correlaci√≥n para alturas entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos.\nPlotear las correalciones sobre cada grafica defindia en 2 (linea de regresion).\nObtener el modelo de regresi√≥n e interpretar los coeficientes.\n\n\n\n\n\n\n\nCurso An√°lisis de Datos - Sesi√≥n 3"
  },
  {
    "objectID": "sesion1_notas.html",
    "href": "sesion1_notas.html",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "",
    "text": "Notas detalladas de la sesi√≥n 1, curso an√°lisis de datos, mag√≠ster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versi√≥n 1"
  },
  {
    "objectID": "sesion1_notas.html#detalles",
    "href": "sesion1_notas.html#detalles",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "",
    "text": "Notas detalladas de la sesi√≥n 1, curso an√°lisis de datos, mag√≠ster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versi√≥n 1"
  },
  {
    "objectID": "sesion1_notas.html#objetivos-de-aprendizaje-de-la-sesi√≥n",
    "href": "sesion1_notas.html#objetivos-de-aprendizaje-de-la-sesi√≥n",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Objetivos de aprendizaje de la sesi√≥n",
    "text": "Objetivos de aprendizaje de la sesi√≥n\n\nComprender el papel del proceso de adquisici√≥n y almacenamiento en un proyecto de an√°lisis de datos, junto a buenas pr√°cticas que promuevan la transparencia y replicabilidad.\nAprender a formular preguntas y plantear hip√≥tesis que puedan ser abordadas mediante el an√°lisis de datos.\nDesarrollar la habilidad de realizar pruebas de hip√≥tesis y comprender la interpretaci√≥n de sus resultados."
  },
  {
    "objectID": "sesion1_notas.html#contenidos",
    "href": "sesion1_notas.html#contenidos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Contenidos:",
    "text": "Contenidos:\n\nEl proceso de an√°lisis de datos\n\nEl proceso de an√°lisis de datos\n\nUna visi√≥n general a las metodolog√≠as de an√°lisis que veremos en el curso\nAdquision y almacenmiento de los datos\nPreparaci√≥n de los datos\n\nPreguntando a los datos\n\nAsbtrayendo la realidad, variables aleatorias y probabilidades.\nPlanteamiento de preguntas.\nPreguntas y respuestas: el rol de las hip√≥tesis.\n\n\n\n\nRespondiendo desde los datos: Pruebas de hip√≥tesis\n\nConceptos B√°sicos de Pruebas de Hip√≥tesis:\n\nDefinici√≥n de hip√≥tesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hip√≥tesis:\n\nPruebas t para comparaci√≥n de medias.\nPruebas chi-cuadrado para variables categ√≥ricas.\nPruebas ANOVA para comparaci√≥n de m√∫ltiples grupos.\n\nInterpretaci√≥n de Resultados:\n\nEvaluaci√≥n de p-values y toma de decisiones.\nSignificaci√≥n estad√≠stica vs.¬†significaci√≥n pr√°ctica.\nComunicaci√≥n de los resultados de las pruebas de hip√≥tesis.\n\n\n\n\nBuenas pr√°cticas en an√°lisis de datos\n\nDesaf√≠os y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformaci√≥n durante la preparaci√≥n de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos."
  },
  {
    "objectID": "sesion1_notas.html#el-proceso-de-an√°lisis-de-datos-1",
    "href": "sesion1_notas.html#el-proceso-de-an√°lisis-de-datos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "El proceso de an√°lisis de datos",
    "text": "El proceso de an√°lisis de datos\nEn el mundo actual, la generaci√≥n y recopilaci√≥n de datos se ha vuelto m√°s accesible y significativa que nunca antes. Esta abundancia de informaci√≥n ofrece la oportunidad de extraer conocimientos valiosos que pueden influir en la toma de decisiones y el desarrollo de soluciones eficientes.\nSin embargo, el proceso de transformar estos datos crudos en informaci√≥n √∫til y significativa requiere una serie de pasos fundamentales que forman parte integral de la disciplina conocida como Ciencia de Datos.\n\nEn esta primera parte, daremos un vistazo general a las metodolog√≠as y enfoques clave que exploraremos a lo largo del curso, con √©nfasis en la importancia de la preparaci√≥n de los datos.\nEl proceso de an√°lisis de datos se puede dividir en varias etapas interconectadas, cada una con su propio conjunto de desaf√≠os y consideraciones.\n\nBajo esta mirada, tenemos varias fases clave que est√°n interconectadas. En este curso nos enfocaremos en la preparaci√≥n de los datos y en su an√°lisis mediante modelos de regresi√≥n. Esto con el objetivo de responder preguntas desde los datos, que provean informaci√≥n valiosa.\n\nAdquisici√≥n de datos:\nEl primer paso en el proceso de an√°lisis de datos implica la adquisici√≥n y el almacenamiento de los datos. Esto se refiere a la recolecci√≥n de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilaci√≥n de datos de fuentes diversas, como bases de datos, archivos CSV, p√°ginas web o incluso sensores en tiempo real.\nEs crucial comprender c√≥mo recopilar y almacenar estos datos de manera adecuada, garantizando su calidad, integridad y seguridad.\nExisten tantas fuentes de datos, como podr√≠amos imaginar. ALgunas de las m√°s comunes son las siguientes:\n\nEncuestas y Cuestionarios:\n\nDise√±o y administraci√≥n de encuestas para recopilar datos directamente de los participantes.\nPermite obtener informaci√≥n espec√≠fica y detallada seg√∫n las preguntas planteadas.\n\nExperimentos Controlados:\n\nDise√±o de experimentos para recopilar datos bajo condiciones controladas.\n√ötil para establecer relaciones causales y evaluar efectos de cambios controlados.\n\nObservaci√≥n y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar informaci√≥n ambiental.\nUtilizaci√≥n de sensores en dispositivos m√≥viles y wearables para recopilar datos de ubicaci√≥n, salud y actividad.\n\nRecopilaci√≥n de Datos Existentes:\n\nUtilizaci√≥n de datos ya recopilados y disponibles en bases de datos o fuentes p√∫blicas.\nReduce el tiempo y costo de recopilaci√≥n, pero puede tener limitaciones en t√©rminos de calidad y relevancia.\n\nWeb Scraping (Web Scrapping):\n\nExtracci√≥n de datos de sitios web utilizando herramientas y t√©cnicas automatizadas.\nPermite recopilar informaci√≥n no estructurada de manera eficiente, pero requiere atenci√≥n a la √©tica y t√©rminos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracci√≥n program√°tica con sistemas y servicios para obtener datos en tiempo real.\nCom√∫n en la obtenci√≥n de datos de redes sociales, informaci√≥n clim√°tica, finanzas, entre otros.\n\nColaboraci√≥n y Participaci√≥n Comunitaria:\n\nColaboraci√≥n con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser √∫til para proyectos de mapeo colaborativo, ciencia ciudadana y recopilaci√≥n de informaci√≥n local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes vol√∫menes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilaci√≥n y posterior an√°lisis de datos heterog√©neos.\nUsualmente se accede a trav√©s de querys SQL\n\n\n\n\n\n\n\n\nDatos disponibles para el proyecto\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos p√∫blicos sobre educaci√≥n chilena\nDatos p√∫blicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad econ√≥mica\n\nVeamos como acceder algunos de estos datos.\n\n\nEjemplo: Datos p√∫blicos sobre individuos en comunas chilenas (encuesta Casen)\nLa Encuesta de Caracterizaci√≥n Socioecon√≥mica Nacional (CASEN) es una investigaci√≥n realizada en Chile que tiene como objetivo principal recopilar informaci√≥n detallada sobre la situaci√≥n socioecon√≥mica de los hogares y las personas en el pa√≠s. Esta encuesta se lleva a cabo de manera peri√≥dica y abarca una amplia variedad de temas, como ingresos, educaci√≥n, empleo, salud, vivienda y otros aspectos relevantes para comprender la realidad socioecon√≥mica de la poblaci√≥n chilena. La informaci√≥n recopilada en la Encuesta CASEN se utiliza para informar pol√≠ticas p√∫blicas, tomar decisiones informadas y analizar la evoluci√≥n de indicadores sociales a lo largo del tiempo.\nSitio Web oficial\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. El formato mas comun es .csv, pero a veces estan en formatos extra√±os. Por ejemplo, .dta de STATA.\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(5)\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educaci√≥n formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nB√°sica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nT√©cnico nivel superior incompleta\nS√≠\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n4\n1.101100e+11\n3\n30\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n19\nMujer\n3\n...\nNaN\nNo sabe\nNo\n27083.0\n941583\n27083.0\n941583\n2083.0\n891583.0\n439170.0\n\n\n\n\n5 rows √ó 22 columns\n\n\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploraci√≥n, para ser preparados para analizarlos. De esto se tratar√° la siguiente sesi√≥n del curso.\nEjemplo: Datos desde la API del banco mundial Primero siga este ejemplo practico de importar datos, luego ser√° facil responder la pregunta anterior.\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb # para instalar: conda install pandas-datareader  o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. En este caso revisare de PIB (GDP en ing√©s), pero se pueden explorar muchas m√°s opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n1503\nBG.GSR.NFSV.GD.ZS\nTrade in services (% of GDP)\n\nWorld Development Indicators\nTrade in services is the sum of service export...\nb'International Monetary Fund, Balance of Paym...\nEconomy & Growth ; Private Sector ; Trade\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16704\nUIS.XUNIT.GDPCAP.23.FSGOV\nInitial government funding per secondary stude...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16705\nUIS.XUNIT.GDPCAP.23.FSHH\nInitial household funding per secondary studen...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n16706\nUIS.XUNIT.GDPCAP.3.FSGOV\nInitial government funding per upper secondary...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16707\nUIS.XUNIT.GDPCAP.5T8.FSGOV\nInitial government funding per tertiary studen...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16708\nUIS.XUNIT.GDPCAP.5T8.FSHH\nInitial household funding per tertiary student...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n\n\n540 rows √ó 7 columns\n\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n\n\n\n\n\nObservar que este es un data frame con dos √≠ndices: pais y a√±o. Para mayor referencia coo tratar este tipo de datos ver en https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\nObtengamos un data frame con los datos de Chile, entre 1980 y 2020.\n\n#sabemos que queremos Chile, asi que busquemos su info\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n\n\n\n\n\nSi quisieramos, por simplicidad quedarnos solo con el indice del a√±o y reordenar el dataframe:\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el an√°lisis es para un solo pa√≠s\nreversed_df.head(5)\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068\n\n\n\n\n\n\n\nAhora, realicemos un grafico r√°pido con nuestros datos:\n\n# Graficamos\n\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'A√±o')\n\nText(0.5, 0, 'A√±o')\n\n\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 1 - Bajando y formateando datos del Banco Mundial**\n\n\n\nReplique el ejemplo pr√°ctico de importar datos desde la API del Banco Mundial y empezar la base para su an√°lisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro pa√≠s serie desde la API del Banco mundial, muestre sus principales caracter√≠sticas y realice un grafico.\n¬øpareciera haber tendencias?\n\n\n\n\nPreguntando a los datos\n¬øC√≥mo plantear preguntas y formular hip√≥tesis en el contexto del an√°lisis de datos? El proceso de an√°lisis comienza con la curiosidad y/o necesidad.cLa formulaci√≥n de preguntas relevantes que se puedan responder mediante la exploraci√≥n y el examen de los datos disponibles.\nInicia con la identificaci√≥n de √°reas de inter√©s y la formulaci√≥n de preguntas espec√≠ficas relacionadas con esos temas. Estas preguntas pueden surgir de la necesidad de resolver un problema, entender un fen√≥meno o explorar patrones en los datos. Un buen planteamiento de preguntas es crucial, ya que guiar√° todo el proceso de an√°lisis.\n\n\n\nEl proceso de abstraer la realidad\n\n\nPreguntas e hip√≥tesis:\nUna hip√≥tesis es una afirmaci√≥n, verificable con evidencia. En este sentido, para toda pregunta podemos responderla mediante hip√≥tesis.\nEn particular, para responder a las preguntas en el contexto de datos, es com√∫n formular hip√≥tesis nulas y alternativas.\nLa hip√≥tesis nula es aquella que propone que algun par√°metro toma cierto valor. Este generlamente es un punto de verdad. Si bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto. En general, planteamos el problema de tal manera que podamos rechazar la hip√≥tesis nula, en favor de otra que llamamos alternatiba.\nQuizas, la hipotesis nula m√°s famosa es la prueba de ‚Äúsignificancia‚Äù. En esta se propone que un par√°metro (muchas veces un efecto, o correlaci√≥n) es 0, es decir, plantea que no hay efecto o relaci√≥n entre las variables, mientras que la hip√≥tesis alternativa sugiere que s√≠ existe una relaci√≥n o efecto significativo.\nEstas hip√≥tesis son fundamentales para establecer una base objetiva para el an√°lisis y para evaluar las evidencias encontradas en los datos. El proceso de plantear preguntas y formular hip√≥tesis es el primer paso en el an√°lisis de datos, ya que establece una gu√≠a clara para el enfoque y la direcci√≥n del trabajo. Al identificar preguntas y establecer hip√≥tesis, se crea un marco s√≥lido que orientar√° la exploraci√≥n y el an√°lisis de los datos disponibles.\n\n\n\n\n\n\nTaller 1: Pregunta 2 - Investigando sobre pa√≠ses:\n\n\n\nConsidere que tenemos los datos del banco mundial, del pa√≠s que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigaci√≥n que se pueda responder con los datos disponibles. ¬øC√≥mo definiria la variable aleatoria relevante? ¬øQu√© hip√≥tesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1_notas.html#respondiendo-desde-los-datos",
    "href": "sesion1_notas.html#respondiendo-desde-los-datos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\n\nInferencia estad√≠stica\nInferencia se refiere al proceso de hacer generalizaciones de una poblaci√≥n a partir de una muestra de esa poblaci√≥n. En particular, la idea es que si tenemos un conjunto de datos (muestra) obtenido de una poblaci√≥n m√°s grande, el cual es representativo de esta, podemos utilizar m√©todos estad√≠sticos para sacar conclusiones sobre las caracter√≠sticas y propiedades de esa poblaci√≥n en su totalidad.\n\n\n\nPoblaci√≥n y Muestra\n\n\nEl proceso de inferencia estad√≠stica se basa en el principio de que una muestra bien seleccionada puede proporcionar informaci√≥n valiosa sobre la poblaci√≥n en general. Mediante el an√°lisis de la muestra, podemos estimar par√°metros poblacionales, como la media, la proporci√≥n o la desviaci√≥n est√°ndar, y tambi√©n podemos construir intervalos de confianza para estimar el rango dentro del cual se espera que se encuentren estos par√°metros.\nEl uso de la inferencia estad√≠stica es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una poblaci√≥n en particular. Por ejemplo, en lugar de encuestar a todos los ciudadanos de un pa√≠s, es mucho m√°s factible encuestar una muestra representativa y utilizar esa informaci√≥n para hacer suposiciones sobre la opini√≥n de la poblaci√≥n en general.\n\nEstad√≠grafos y el Teorema del L√≠mite central\nEntonces, en cada muestra que tenemos podemos calcular aproximaciones a los par√°metros poblacionales de inter√©s. Estos son los llamados estad√≠sgrafos \nDado que por cada muestra que tenemos, vamos a calcular un estad√≠grafo este es en si mismo una variable aleatoria. Tiene su propia distribuci√≥n, media y varianza!\n\nEl estad√≠grafo m√°s conocido es el promedio o media muestral.\n\n\n\nEstadigrafos m√°s comunes\n\n\nCada estimador es una funci√≥n de la muestra, por ende para cada muestra que tengamos obtendremos un valor num√©rico espec√≠fico para el estimador. Por este motivo, cuando estamos trabajando con una √∫nica muestra espec√≠fica, tenemos un √∫nico valor del estimador, o estimador puntual.\nNunca (o casi nunca) podemos conocer el valor verdadero de los par√°metros en la poblaci√≥n, por lo cual un primer camino tentador es usar el estimador puntual para tomar una decisi√≥n. Como nunca podemos conocer el verdadero par√°metro, tampoco podemos saber a ciencia cierta si el estimador puntual es cercano a este.\n¬øC√≥mo conectamos estadpigrafos y par√°metros?\nEl teorema del l√≠mite central, nos dice que, bajo ciertas condiciones, la distribuci√≥n de las medias muestrales de una poblaci√≥n se aproxima a una distribuci√≥n normal a medida que el tama√±o de la muestra aumenta, independientemente de la forma de la distribuci√≥n original de la poblaci√≥n. Este teorema es esencial en inferencia estad√≠stica y tiene amplias aplicaciones en an√°lisis de datos y toma de decisiones.\n\n\n\nLa media muestral se distribuye normal, sin importar la distribuci√≥n de la variable subyacente\n\n\nFormalmente, el Teorema del L√≠mite Central establece lo siguiente:\nx‚Äæ‚àºaN(Œº,œÉn)\\bar{x} \\sim_a N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) \nSupongamos que tenemos una poblaci√≥n con media Œº y desviaci√≥n est√°ndar œÉ finitas. Si tomamos muestras aleatorias de tama√±o n de esta poblaci√≥n y calculamos la media muestral de cada muestra, entonces, a medida que n tiende a infinito, la distribuci√≥n de estas medias muestrales se aproximar√° a una distribuci√≥n normal con media Œº y desviaci√≥n est√°ndar œÉ/‚àön.\nEn otras palabras, sin importar la distribuci√≥n original de la poblaci√≥n, cuando el tama√±o de la muestra es suficientemente grande, la distribuci√≥n de las medias muestrales seguir√° una forma de campana similar a la distribuci√≥n normal. Este resultado es fundamental para realizar inferencias sobre la poblaci√≥n a partir de muestras, ya que nos permite aplicar m√©todos basados en la distribuci√≥n normal incluso cuando la poblaci√≥n original no sigue una distribuci√≥n normal.\n\n\nError est√°ndar\n\nCorresponde a un estimador de la desviaci√≥n est√°ndar del estimador.\nIdentifica que tan lejos estamos del verdadero valor poblacional.\nPara la media muestral:\n\nSE=Syn SE = \\frac{S_y}{\\sqrt{n}}\nSe utiliza para evaluar a los estimadores, mediante pruebas de hipotesis y construir intervalos de confianza\n\nSi se conoce un estimador y su desviaci√≥n est√°ndar, podemos saber qu√© tan precisa es la estimaci√≥n (mucha o poca varianza), pero no podemos saber si el estimador est√° cercano o no a su valor verdadero en la poblaci√≥n (el cual no conocemos).\nNunca (o casi nunca) podemos conocer el valor verdadero de los par√°metros en la poblaci√≥n.\nS√≠ se puede construir un conjunto de valores que contienen el par√°metro poblacional con alguna probabilidad (llamada el nivel de confianza).\nUn intervalo de confianza contiene los posibles valores del estimador, entre un l√≠mite inferior y un l√≠mite superior, con cierta probabilidad.\n\n\n\nInferencia sobre Estad√≠grafos y par√°metros - Conectados por el Teorema del L√≠mite central\nx‚Äæ‚àºaN(Œº,œÉn) \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hip√≥tesis\np-valor\n\n\n\n\n\nIntervalos de confianza\nUna primera manera de aproximarnos a los par√°metros poblacionales (particularmente a la esperanza) es mediante la construcci√≥n de intervalos de confianza.\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un l√≠mite inferior y un l√≠mite superior, con cierta probabilidad.\n\n¬øDe d√≥nde saco los valores cr√≠ticos?\nLos valores cr√≠ticos de una distribuci√≥n los obtenemos de una tabla de distribuci√≥n o para calcular podemos usar excel, R o en python:\nscipy.stats.t.isf(alpha, n-p)\nSi estamos trabajando con dos colas, usar alpha/2 porque la probabilidad de error la estamos repartiendo a ambas colas.\n\n\n\n\n\n\n[Matem√°ticamente] Caso 1: Varianza conocida\n\n\n\nSumpongamos que tenemos una muestra aleatoria: y1,y2,‚Ä¶,yny_1, y_2, \\dots, y_n de una poblaci√≥n Y‚àºN(Œº,œÉ2)Y\\sim N(\\mu, \\sigma^2)\n\nLa media muestral y‚Äæ=1n‚àëi=1nyi\\bar{y}= \\frac{1}{n}\\sum_{i=1}{n}y_i\n\nSu esperanza es: E(y‚Äæ)=ŒºE(\\bar{y}) =\\mu\nSu varianza es: var(y‚Äæ)=œÉ2nvar(\\bar{y}) = \\frac{\\sigma^2}{n}\nse distribuye normal, tal que podemos estandarrizar: $ N(0,1) $\n\n\nEntonces, podemos describir que:\n$ P( -1.96 &lt; &lt; 1.96 ) = 0.95 $$\n$ P( {y}- &lt; &lt; {y} + ) = 0.95 $\n\neste intervalo es aleatorio, porque y‚Äæ\\bar{y} es diferente en cada muestra.\npara el 95% de las muestras elatorias, el intervalo construido de esta manera contendr√° a Œº\\mu\n\n\n\n\n\n\n\n\n\n[Matem√°ticamente] Caso 2: Varianza desconocida\n\n\n\n\nSupongamos que tenemos una muestra aleatoria y1,y2,‚Ä¶,yny_1, y_2, \\dots, y_n de una poblaci√≥n y‚àºN(Œº,œÉ2)y\\sim N(\\mu, \\sigma^2 )\nUsamos la estimaci√≥n de la desviaci√≥n est√°ndar muestral: Sy=1n‚àí1‚àëi=1n(yi‚àíy‚Äæ)2 S_y = \\sqrt{\\frac{1}{n-1} \\sum{i=1}{n}(y_i - \\bar{y})^2} \nY si estandarizamos y‚Äæ\\bar{y}: Y‚Äæ‚àíŒºySn‚àºtn‚àí1 \\frac{\\bar{Y} - \\mu_y }{ \\frac{S}{\\sqrt{n}}} \\sim t_{n-1} \nPodeos constrir un intervalo de la porbabilidad de estar al 95 con el valor critico c adecuado a los grados de libertad n-1:\n\nP(‚àíc&lt;Y‚Äæ‚àíŒºySn&lt;c)=0.95 P( -c &lt;   \\frac{\\bar{Y} - \\mu_y }{ \\frac{ S}{\\sqrt{n}}}  &lt; c  ) =0.95  \nP(y‚Äæ‚àíc√óSn&lt;Œº&lt;y‚Äæ+c√óSn)0.95 P(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) 0.95\n\nSi llamamos al error estandar SE: SE(y‚Äæ)=SnSE(\\bar{y})=\\frac{S}{\\sqrt{n}}\nEl IC es: (y‚Äæ‚àíc√óSn,y‚Äæ+c√óSn) (\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) \n\n\n\n\n\nEl intervalo es una muestra aleatoria\nEsto quiere decir que para cada muestra podemos construir un intervalo.\nAs√≠ como el estimador es una variable aleatoria, esto tambi√©n es cierto para los intervalos de confianza. Por eso tambi√©n se les llama intervalos aleatorios, ya que con diferentes muestras obtendremos un diferente estimador e intervalo.\nPor ende, supongamos que contamos con 20 muestras, entonces construiremos 20 intervalos de confanza diferentes para los 20 estimadores puntuales.\n\nInterpretaci√≥n de intervalo de confianza\nPensemos en un 95% de confianza (un valor usual). Esto quiere decir, que si se repitiera este ejercicio muchas veces y construy√©ramos un intervalo de esta forma el 95% de ellos contendr√≠a el verdadero par√°metro poblacional.\nUn elemento importante a considerar es que esto no significa que con 95% de certeza el par√°metro est√° exactamente en estos valores. Por ejemplo, al 95% de confianza con 20 intervalos 19 contendr√°n el par√°metro.\n\n\n\nPruebas de hip√≥tesis\n\nUna forma de verificar hipotesis sobre los par√°metros es mediante el contraste de hip√≥tesis.\n\nEmpezamos suponiendo que hay una distribuci√≥n conocida para el estad√≠grafo, centrada en un valor espec√≠fico. Y nos preguntamos, si esto fuea verdad ¬øqu√© tan probable es la muestra que tengo?\n\nLlamamos la hip√≥tesis a probar Ho, y su alternativa H1.\n\n\n\n\n\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:\n\n\nTipo I: Rechazar Ho cuando es cierta\nTipo II: No rechazar Ho cuando es falsa.\n\n\n\n\nSe elige nivel de significancia de contraste (Œ±) = probabilidad de cometer error Tipo I. T√≠picamente Œ± = 0,01, 0,05, 0,10.\nPara contrastar una hip√≥tesis con su alternativa, debemos elegir:\n\nUn estad√≠stico de contraste\nUna regla de rechazo, la cual depende de un valor cr√≠tico.\n\n\n\n\n\nPrueba de significancia\nDefinimos la prueba de hip√≥tesis de significancia como aquella que indica si un estimador TÃÇ\\hat{T} es 0.\nH0:T=0 vs H1:T‚â†0 H_0: T =0\\text{ vs }H_1: T \\neq 0 \nEl Valor de probabilidad (√≥ p-valor) es el nivel probabilidad m√°s alto para el cual no podemos rechazar la hip√≥tesis nula de la prueba de significancia.\nEjemplo: $H_0: $ y en la muestra especifica t= 1.52:\nP‚àívalor=P(T&gt;1.52|h0)=1‚àíœï(1.52)=0.0065 P-valor = P(T&gt;1.52 \\vert h_0)= 1- \\phi(1.52)=0.0065\n\nel mayor nivel de significancia estadistica al cual no rechazamos H0H_0 es 6.5%\nla probabilidad de observar un velor T‚â•1.52T\\geq 1.52 cuando H0H_0 es cierta es en un 6.5 de las muestras.\nP-valores bajos dan evidencia en contra de H0H_0, ya que la probabilidad de observarlo si H0H_0 es cierta es bajo.\n\n\n\nEjemplo de aplicaci√≥n: Peso de los Ping√ºinos Palmer\nConsideremos los datos de los ping√ºinos Palmer. Los datos ‚ÄúPalmer Penguins‚Äù son un conjunto que detalla medidas morfol√≥gicas y caracter√≠sticas de tres especies de ping√ºinos: Adelie, Gentoo y Chinstrap. Recopilados por el Dr.¬†Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nEn el contexto de los ping√ºinos y el peso de su poblaci√≥n, podr√≠amos tomar una muestra de ping√ºinos y calcular un intervalo de confianza para el peso promedio. Esto nos dar√≠a una estimaci√≥n del peso promedio de la poblaci√≥n total, junto con la confianza en que este valor estimado es preciso.\nEs importante tener en cuenta que el proceso de inferencia estad√≠stica se basa en suposiciones y en el uso adecuado de t√©cnicas estad√≠sticas.\nLa elecci√≥n de la muestra, la interpretaci√≥n de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hip√≥tesis, sobre el peso de los ping√ºinos.\nPrimero calcularemos el promedio muestral y lo veremos en el contexto de los datos observados:\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los ping√ºinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribuci√≥n del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribuci√≥n de Peso de Ping√ºinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral. De que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus caracter√≠sticas.\nPor ejemplo, consideremos que de esta poblaci√≥n de ping√ºinos obtenemos 1000 muestras de 50 individuos cada una. Si graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tama√±o de muestra, m√°s nos alejamos de la distribuci√≥n normal.\nSi reducimos el n√∫mero de repeticiones tambie√©.\n\n\nimport numpy as np\n\n# Definir el tama√±o de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y c√°lculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gr√°fico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribuci√≥n de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\n\nIntervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n# Obtener una muestra simple de 40 ping√ºinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error est√°ndar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviaci√≥n est√°ndar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)\n\nIntervalo de Confianza para el Peso:\n(4091.059218311082, 4515.190781688918)\n\n\nEl resultado ser√° un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los ping√ºinos en la poblaci√≥n, con un nivel de confianza del 95%.\n¬øComo nos fue? ¬øContiene al verdadero valor?\n\n\n\nComparaciones de grupos\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribuci√≥n de tama√±o por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\ntabla_doble_entrada\n\n\n\n\n\n\n\n\nspecies\nsex\nPromedio\nVarianza\n\n\n\n\n0\nAdelie\nFemale\n3368.835616\n72565.639269\n\n\n1\nAdelie\nMale\n4043.493151\n120278.253425\n\n\n2\nChinstrap\nFemale\n3527.205882\n81415.441176\n\n\n3\nChinstrap\nMale\n3938.970588\n131143.605169\n\n\n4\nGentoo\nFemale\n4679.741379\n79286.335451\n\n\n5\nGentoo\nMale\n5484.836066\n98068.306011\n\n\n\n\n\n\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hip√≥tesis:\n¬øExiste una diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù?\n\nHip√≥tesis Nula (H0):\n\nNo hay diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù.\n\nHip√≥tesis Alternativa (H1):\n\nExiste una diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù.\nPara probar esta hip√≥tesis, podr√≠as utilizar una prueba de hip√≥tesis para comparar las medias de las muestras de peso de los ping√ºinos machos y hembras en la especie ‚ÄúAdelie‚Äù. Esto te permitir√≠a determinar si la diferencia observada en el peso promedio es lo suficientemente grande como para considerarse estad√≠sticamente significativa.\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los ping√ºinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribuci√≥n de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribuci√≥n de Peso por Sexo para Ping√ºinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nA simple vista podriamos pensar ambos grupos son diferentes. Es m√°s claro si dibujamos el promedio muestral observado.\n\n# Crear un gr√°fico de densidad con l√≠neas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Ping√ºinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()\n\n\n\n\nSi construimos una prueba t de diferencia de medias:\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los ping√ºinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estad√≠stica t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gr√°fico de comparaci√≥n de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparaci√≥n de Peso entre Machos y Hembras de Ping√ºinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\nEstad√≠stica t: 13.126285923485874\nValor p: 6.402319748031793e-26\n\n\n\n\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas. Para esto podriamos usar una prueba ANOVA.\nEn este c√≥digo, primero cargamos el conjunto de datos ‚ÄúPenguins‚Äù y luego creamos dos subconjuntos separados para machos y hembras. Despu√©s, utilizamos la funci√≥n stats.f_oneway() para realizar una prueba ANOVA para comparar los pesos entre hembras y machos. El resultado incluye la estad√≠stica F y el valor p.\nEl valor p nos indica si hay una diferencia significativa entre los grupos. Si el valor p es menor que un umbral de significancia (por ejemplo, 0.05), podr√≠amos rechazar la hip√≥tesis nula y concluir que hay una diferencia significativa en el peso entre hembras y machos de diferentes islas.\nRecuerda que, antes de realizar una prueba ANOVA, es importante verificar las suposiciones necesarias, como la normalidad y la homogeneidad de varianzas en los grupos. Si estas suposiciones no se cumplen, podr√≠a ser necesario considerar otras pruebas estad√≠sticas o transformaciones de los datos.\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estad√≠stica F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\nEstad√≠stica F: 72.96098633250911\nValor p: 4.897246751596325e-16\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Calcular las medias de peso por g√©nero e isla\nmedias_peso = penguins.groupby(['species', 'island', 'sex'])['body_mass_g'].mean().reset_index()\n\n# Crear un gr√°fico de barras con puntos y intervalos de confianza\nplt.figure(figsize=(10, 6))\nsns.barplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', errorbar='sd', palette=['pink', 'blue'])\n#sns.boxplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', palette=['blue', 'pink'])\nplt.title('Comparaci√≥n de Peso entre Hembras y Machos por Isla')\nplt.xlabel('Especie e Isla')\nplt.ylabel('Media de Peso (g)')\nplt.legend(title='Sexo')\nplt.show()\n\n\n\n\n\n\nExperimentos Aleatorios y pruebas A/B\nUn experimento estad√≠stico es un enfoque cient√≠fico que busca establecer relaciones de causalidad y obtener conclusiones sobre c√≥mo ciertas variables afectan a otras. Los experimentos estad√≠sticos se dise√±an para manipular deliberadamente una o m√°s variables independientes y observar los efectos que tienen sobre una variable dependiente. Al controlar y manipular las variables de inter√©s, los experimentos permiten a los investigadores hacer afirmaciones m√°s s√≥lidas sobre las relaciones causales.\nUna prueba A/B, tambi√©n conocida como prueba de divisi√≥n, es una t√©cnica utilizada en la investigaci√≥n y el an√°lisis para comparar dos variantes o grupos con el fin de determinar cu√°l de ellos produce un mejor resultado en t√©rminos de rendimiento, efectividad o preferencia. En una prueba A/B, se selecciona un grupo de muestra y se divide en dos grupos, uno que experimenta la variante ‚ÄúA‚Äù (por ejemplo, una versi√≥n actual) y otro que experimenta la variante ‚ÄúB‚Äù (por ejemplo, una versi√≥n modificada). Luego, se recopilan datos y se comparan los resultados de ambos grupos para determinar cu√°l variante es m√°s efectiva. Las pruebas A/B son comunes en marketing, dise√±o de productos y desarrollo web para tomar decisiones informadas sobre mejoras y optimizaciones.\nLas pruebas A/B es son ampliamente utilizado en diversas √°reas, como el marketing, la investigaci√≥n de usuarios y el dise√±o de productos. En una prueba A/B, se seleccionan dos grupos de muestra: uno experimenta la versi√≥n original (A) y el otro experimenta una variante modificada (B). La idea detr√°s de una prueba A/B es evaluar si la variante B produce un efecto significativamente diferente en una m√©trica de inter√©s en comparaci√≥n con la variante A.\nMediante la asignaci√≥n aleatoria de los participantes a los grupos A y B, y al controlar las condiciones en las que se les presenta cada variante, se reduce la posibilidad de sesgos y se permite un an√°lisis causal m√°s confiable. Al comparar las diferencias observadas en los resultados entre los grupos A y B, es posible inferir si la variante B tiene un impacto significativo en la variable de inter√©s.\nSin embargo, es importante tener en cuenta que aunque las pruebas A/B proporcionan evidencia de asociaci√≥n causal, no garantizan que la causalidad sea absoluta. Otros factores no controlados pueden influir en los resultados. Para obtener una comprensi√≥n m√°s completa de la causalidad, los experimentos controlados aleatorizados y el uso de m√©todos de dise√±o experimental s√≥lidos son esenciales. Las pruebas A/B son una herramienta poderosa para explorar causas y efectos en condiciones controladas y analizar el rendimiento relativo de diferentes opciones.\nVeamos un ejemplo en la pr√°ctica. Este es parte del ejercicio de aplicaci√≥n."
  },
  {
    "objectID": "sesion1_notas.html#caso-aplicaci√≥n-de-ab-testing-para-promoci√≥n-de-marketing",
    "href": "sesion1_notas.html#caso-aplicaci√≥n-de-ab-testing-para-promoci√≥n-de-marketing",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Caso: Aplicaci√≥n de A/B testing para promoci√≥n de Marketing",
    "text": "Caso: Aplicaci√≥n de A/B testing para promoci√≥n de Marketing\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electr√≥nicos y queremos aumentar las ventas en una l√≠nea de productos espec√≠fica, como tel√©fonos m√≥viles.\nPara ello, decidimos utilizar una promoci√≥n de ventas basada en una ruleta l√∫dica que ofrecer√° descuentos a los clientes que la utilicen.\nPara implementar la promoci√≥n, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electr√≥nico con un enlace a la ruleta l√∫dica. Al hacer clic en el enlace, los clientes son redirigidos a una p√°gina en la que pueden girar la ruleta y ganar un descuento en su pr√≥xima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoci√≥n (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreaci√≥n de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste c√≥digo crear√° un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generaci√≥n de n√∫meros aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de n√∫mero de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows √ó 3 columns\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 3 -Ejemplo AB test en Marketing:\n\n\n\nConsidere que tenemos los datos del banco mundial, del pa√≠s que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigaci√≥n que se pueda responder con los datos disponibles. ¬øC√≥mo definiria la variable aleatoria relevante? ¬øQu√© hip√≥tesis podria responder su pregunta?\nEstudiemos si la promoci√≥n fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¬øFue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¬øCual de las promociones fue m√°s efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "sesion1_notas.html#buenas-pr√°cticas-en-an√°lisis-de-datos-1",
    "href": "sesion1_notas.html#buenas-pr√°cticas-en-an√°lisis-de-datos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Buenas pr√°cticas en an√°lisis de datos",
    "text": "Buenas pr√°cticas en an√°lisis de datos\n\nImportancia de la Adquisici√≥n y Almacenamiento de Datos\nLa adquisici√≥n y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de an√°lisis. La calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes. En esta secci√≥n, exploraremos la importancia de esta etapa y c√≥mo afecta todo el flujo de trabajo de la ciencia de datos.\nGarant√≠a de Calidad y Fiabilidad en la Obtenci√≥n de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean s√≥lidas. La calidad de los datos est√° relacionada con la precisi√≥n, integridad y consistencia de la informaci√≥n que recopilamos. Asegurarnos de que los datos sean precisos desde el principio minimiza la posibilidad de errores en an√°lisis posteriores. Exploraremos t√©cnicas y pr√°cticas para verificar la calidad de los datos y c√≥mo mitigar posibles fuentes de error.\nExploraci√≥n de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros. Cada fuente tiene sus propias caracter√≠sticas y potenciales sesgos. Comprender las diferencias entre estas fuentes y c√≥mo pueden influir en los resultados es crucial para tomar decisiones informadas. Analizaremos ejemplos de c√≥mo la elecci√≥n de la fuente de datos puede afectar las conclusiones y c√≥mo evaluar la confiabilidad de las fuentes.\n\n\nMetodolog√≠as de Levantamiento y Adquisici√≥n de Datos:\nEl proceso de obtenci√≥n de datos implica una planificaci√≥n cuidadosa. Exploraremos diversas metodolog√≠as utilizadas para recopilar datos, desde encuestas y experimentos hasta scraping de datos en l√≠nea. Cada metodolog√≠a tiene sus propias ventajas y desventajas, y es importante seleccionar la m√°s adecuada para los objetivos del an√°lisis. Discutiremos c√≥mo dise√±ar encuestas efectivas, c√≥mo considerar la √©tica en la recopilaci√≥n de datos y c√≥mo aprovechar las fuentes de datos existentes.\nEsta secci√≥n nos proporcionar√° una base s√≥lida para comprender c√≥mo adquirir y almacenar datos de manera efectiva y confiable. Una vez que comprendamos c√≥mo obtener datos de calidad, podremos avanzar con confianza en las etapas posteriores del proceso de an√°lisis, sabiendo que estamos trabajando con una base s√≥lida y confiable.\n\n\nDesaf√≠os y Consideraciones:\nA medida que ingresamos al emocionante mundo del an√°lisis de datos, nos encontramos con una serie de desaf√≠os y consideraciones que debemos abordar de manera efectiva para garantizar el √©xito de nuestro proyecto. Estos desaf√≠os abarcan desde la protecci√≥n de la privacidad de los datos hasta las complejidades de la limpieza y transformaci√≥n durante la etapa de preparaci√≥n.\n\n\nPrivacidad y Seguridad de los Datos:\nUno de los aspectos m√°s cr√≠ticos en el an√°lisis de datos es la privacidad y seguridad de la informaci√≥n. Los datos pueden contener informaci√≥n sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas. Exploraremos pr√°cticas y regulaciones para garantizar que los datos se manejen de manera √©tica y legal. Discutiremos c√≥mo anonimizar los datos, utilizar t√©cnicas de enmascaramiento y seguir las mejores pr√°cticas para resguardar la privacidad de los individuos.\n\n\nLimpieza y Transformaci√≥n durante la Preparaci√≥n de Datos:\nLa etapa de preparaci√≥n de datos es crucial para asegurarse de que los datos sean aptos para el an√°lisis. Sin embargo, este proceso no est√° exento de desaf√≠os. Los datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada. Exploraremos t√©cnicas para identificar y manejar valores at√≠picos y faltantes, as√≠ como la importancia de la normalizaci√≥n y estandarizaci√≥n de los datos. Aprenderemos c√≥mo transformar los datos en un formato adecuado para el an√°lisis, incluida la reorganizaci√≥n de variables y la creaci√≥n de nuevas caracter√≠sticas. En resumen, enfrentamos una serie de desaf√≠os y consideraciones clave en nuestro viaje hacia el an√°lisis de datos significativo. Desde la protecci√≥n de la privacidad hasta la preparaci√≥n efectiva de los datos, abordar estos desaf√≠os de manera adecuada es esencial para garantizar que nuestras conclusiones sean s√≥lidas, confiables y √©ticas.\n\n\nReproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos.\n\nLa reproducibilidad y el control de versiones son componentes fundamentales para garantizar la integridad y la transparencia en el an√°lisis de datos. Adem√°s de mantener un registro detallado de las decisiones tomadas durante el proceso, el uso de sistemas de control de versiones como GIT se vuelve esencial para mantener la trazabilidad y la colaboraci√≥n efectiva en proyectos de preparaci√≥n y an√°lisis de datos.\nDocumentaci√≥n Detallada del An√°lisis y Uso de Notebooks: Una documentaci√≥n exhaustiva del an√°lisis es esencial para comprender el flujo de trabajo, las decisiones tomadas y las transformaciones aplicadas a los datos. Los notebooks, como Jupyter Notebooks, ofrecen una herramienta excepcional para lograr esto. En cada celda de un notebook, es posible combinar explicaciones en lenguaje natural con c√≥digo ejecutable y visualizaciones. Esto permite registrar no solo el qu√© y el c√≥mo, sino tambi√©n el porqu√© detr√°s de cada paso.\nImportancia de Mantener un Registro de los Cambios en los Datos: Cada decisi√≥n tomada durante la preparaci√≥n y el an√°lisis de datos puede tener un impacto significativo en los resultados finales. Mantener un registro detallado de estas decisiones, desde la limpieza de datos hasta la creaci√≥n de variables derivadas, es crucial para comprender c√≥mo se obtuvieron ciertos resultados. Una documentaci√≥n precisa y detallada permite a otros analistas validar y replicar el an√°lisis en el futuro.\nUso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que tambi√©n es una herramienta poderosa en el an√°lisis de datos. Permite rastrear cada modificaci√≥n realizada en el c√≥digo y en los documentos, incluidos los notebooks. Cada cambio es registrado como un ‚Äúcommit‚Äù, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\n\n\n\nUn esquema de git por Allison Horst @allison_horst\n\n\nAplicaci√≥n de Control de Versiones en Proyectos de Preparaci√≥n de Datos:* La aplicaci√≥n de GIT en proyectos de preparaci√≥n de datos agrega un nivel adicional de transparencia y colaboraci√≥n. Los repositorios de GIT almacenan no solo los datos originales, sino tambi√©n los notebooks y scripts utilizados en el proceso. Esto permite a los analistas colaborar en un entorno controlado y mantener un historial de cambios. En caso de que surjan problemas o se necesite retroceder en el tiempo, GIT ofrece la capacidad de volver a versiones anteriores de manera segura.\nLa combinaci√≥n de documentaci√≥n detallada a trav√©s de notebooks y el uso de sistemas de control de versiones como GIT proporciona una base s√≥lida para el an√°lisis de datos reproducible y transparente. Esto no solo facilita la comprensi√≥n y validaci√≥n de los resultados, sino que tambi√©n fomenta la colaboraci√≥n y la mejora continua en proyectos de preparaci√≥n y an√°lisis de datos.\n\n\n\n\n\n\nActividad de proyecto - Inicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del an√°lisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustar√≠a trabajar.\nPropongan una o dos preguntas de investigaci√≥n y las hipotesis que las responder√≠an.\n\nLa siguiente sesi√≥n, vamos a explorar los datos y empezar los primeros pasos en su an√°lisis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An√°lisis de datos",
    "section": "",
    "text": "Sitio web complementario con material curso An√°lisis de Datos\n\n\n\nSesi√≥n\nFecha\nTema\nSlides\nNotas\n\n\n\n\n0\n17 ago\nIntroducci√≥n al Curso\nslides sesi√≥n introductoria\nNotas sesi√≥n introductoria\n\n\n1\n17 ago\nRespondiendo Preguntas con datos\nslides sesi√≥n 1\nNotas sesi√≥n 1\n\n\n2\n25 ago\nPreparando los datos\nslides sesi√≥n 2\nNotas sesi√≥n 2\n\n\n3\n2 sep\nIntroducci√≥n al an√°lisis de regresi√≥n\nslides sesi√≥n 3\nNotas sesi√≥n 3\n\n\n4\n9 sep\nProfundizando en an√°lisis de regresi√≥n, supuestos y limitaciones\n\n\n\n\n5\n30 sep\nIntroduccion al an√°lisis de series de tiempo\n\n\n\n\n6\n7 oct\nModelando series temporales"
  },
  {
    "objectID": "index.html#calendario-clases",
    "href": "index.html#calendario-clases",
    "title": "An√°lisis de datos",
    "section": "",
    "text": "Sitio web complementario con material curso An√°lisis de Datos\n\n\n\nSesi√≥n\nFecha\nTema\nSlides\nNotas\n\n\n\n\n0\n17 ago\nIntroducci√≥n al Curso\nslides sesi√≥n introductoria\nNotas sesi√≥n introductoria\n\n\n1\n17 ago\nRespondiendo Preguntas con datos\nslides sesi√≥n 1\nNotas sesi√≥n 1\n\n\n2\n25 ago\nPreparando los datos\nslides sesi√≥n 2\nNotas sesi√≥n 2\n\n\n3\n2 sep\nIntroducci√≥n al an√°lisis de regresi√≥n\nslides sesi√≥n 3\nNotas sesi√≥n 3\n\n\n4\n9 sep\nProfundizando en an√°lisis de regresi√≥n, supuestos y limitaciones\n\n\n\n\n5\n30 sep\nIntroduccion al an√°lisis de series de tiempo\n\n\n\n\n6\n7 oct\nModelando series temporales"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n2"
  },
  {
    "objectID": "sesion2_notas.html",
    "href": "sesion2_notas.html",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "",
    "text": "La preparaci√≥n de datos es una fase esencial en el proceso de an√°lisis de datos que involucra una serie de actividades destinadas a garantizar que los datos est√©n en condiciones √≥ptimas para su posterior an√°lisis.\n\nExtraccion de los datos\nLimpieza\nTransformaci√≥n\nOrganizaci√≥n\n\n\nEsta fase implica la extracci√≥n de los datos, su limpieza, transformaci√≥n y organizaci√≥n de manera que sean coherentes, completos y adecuados para el an√°lisis que se va a realizar.\nTrash in , trash out\n\n\nLa preparaci√≥n de datos es crucial porque afecta directamente la calidad y confiabilidad de los resultados obtenidos en cualquier an√°lisis posterior.\n\n\n\n\n\n\nAntes de adentrarnos en la preparaci√≥n de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje com√∫n.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relaci√≥n a:\n\nEstructura\nTamanÃÉo\nOperacionalizacioÃÅn\nTemporalidad y unidad de anaÃÅlisis.\n\n\n\n\nLa estructura de un dato se refiere a su formato y codificaci√≥n.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\n\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o n√∫meros.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\n\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, im√°genes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteoroloÃÅgicos\nColecciones de documentos digitalizados: Facturas, registros, correos electroÃÅnicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\n\n\nDatos estructurados est√°n en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la informaci√≥n, pero los no estructurados son m√°s abundantes.\n\n\n\n\n\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son dif√≠ciles de gestionar, procesar y analizar con herramientas y m√©todos tradicionales. Big Data involucra terabytes o incluso petabytes de informaci√≥n y generalmente requiere tecnolog√≠as y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos m√°s peque√±os y manejables en comparaci√≥n con Big Data. Estos conjuntos de datos son m√°s accesibles y pueden ser procesados y analizados utilizando herramientas y m√©todos convencionales. A menudo, Small Data se centra en obtener informaci√≥n valiosa de fuentes limitadas y espec√≠ficas.\n\n\n\n\n\nLas variables en an√°lisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan n√∫meros medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos n√∫meros o factores que representen categor√≠as, facilitando su an√°lisis cuantitativo.\n\n\n\n\n\n\nOtra manera de clasificar los datos con relaci√≥n a la temporalidad en la cual son tomados y cu√°l es la unidad de an√°lisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_notas.html#tipos-de-datos",
    "href": "sesion2_notas.html#tipos-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "",
    "text": "Antes de adentrarnos en la preparaci√≥n de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje com√∫n.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relaci√≥n a:\n\nEstructura\nTamanÃÉo\nOperacionalizacioÃÅn\nTemporalidad y unidad de anaÃÅlisis.\n\n\n\n\nLa estructura de un dato se refiere a su formato y codificaci√≥n.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\n\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o n√∫meros.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\n\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, im√°genes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteoroloÃÅgicos\nColecciones de documentos digitalizados: Facturas, registros, correos electroÃÅnicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\n\n\nDatos estructurados est√°n en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la informaci√≥n, pero los no estructurados son m√°s abundantes.\n\n\n\n\n\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son dif√≠ciles de gestionar, procesar y analizar con herramientas y m√©todos tradicionales. Big Data involucra terabytes o incluso petabytes de informaci√≥n y generalmente requiere tecnolog√≠as y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos m√°s peque√±os y manejables en comparaci√≥n con Big Data. Estos conjuntos de datos son m√°s accesibles y pueden ser procesados y analizados utilizando herramientas y m√©todos convencionales. A menudo, Small Data se centra en obtener informaci√≥n valiosa de fuentes limitadas y espec√≠ficas.\n\n\n\n\n\nLas variables en an√°lisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan n√∫meros medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos n√∫meros o factores que representen categor√≠as, facilitando su an√°lisis cuantitativo.\n\n\n\n\n\n\nOtra manera de clasificar los datos con relaci√≥n a la temporalidad en la cual son tomados y cu√°l es la unidad de an√°lisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales"
  },
  {
    "objectID": "sesion2_notas.html#leyendo-datos-en-pandas",
    "href": "sesion2_notas.html#leyendo-datos-en-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a trav√©s de la familia de funciones read_tipo\n\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read_*\n\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: autom√°tico o definido por el usuario\nDatetime parsing: puede combinar informaci√≥n de m√∫ltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con n√∫meros con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2_notas.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2_notas.html#formato-csv-valores-separados-por-comas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de informaci√≥n: no sabemos qu√© son las columnas (n√∫meros, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qu√© tipo para transformar cada columna basado en lo que parece ser\n\n¬øY qu√© pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2_notas.html#csv-en-pandas",
    "href": "sesion2_notas.html#csv-en-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nB√°sica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nB√°sica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representaci√≥n de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2_notas.html#json-java-script-object-notation",
    "href": "sesion2_notas.html#json-java-script-object-notation",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, n√∫meros, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores num√©ricos"
  },
  {
    "objectID": "sesion2_notas.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2_notas.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2_notas.html#orientaciones-posibles-de-json",
    "href": "sesion2_notas.html#orientaciones-posibles-de-json",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2_notas.html#extensible-markup-language-xml",
    "href": "sesion2_notas.html#extensible-markup-language-xml",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato m√°s antiguo y auto descriptivo, con estructura jer√°rquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un m√©todo incorporado en Python.\nSe puede usar la librer√≠a lxml (tambi√©n ElementTree)\n\n\nEjemplo XML"
  },
  {
    "objectID": "sesion2_notas.html#formatos-binarios",
    "href": "sesion2_notas.html#formatos-binarios",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¬øQu√© es un formato binario?\nPickle: Python‚Äôs built-in serialization\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n\n\nHDF5: Librer√≠a para almacenar gran cantidad de datos cient√≠ficos\n\nFormato jer√°rquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresi√≥n\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de c√°lculo tiene m√∫ltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2_notas.html#bases-de-datos-relacionales",
    "href": "sesion2_notas.html#bases-de-datos-relacionales",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\n\nLas bases de datos relacionales son similares a los marcos de datos m√∫ltiples pero tienen muchas m√°s caracter√≠sticas\n\nv√≠nculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos¬†\n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayor√≠a de los sistemas de bases de datos a trav√©s de un API com√∫n\nSintaxis similar (¬°pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.)¬†\nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n`import sqlalchemy as sqla\ndb = sqla.create_engine(‚Äòsqlite:///mydata.sqlite‚Äô)\npd.read_sql(‚Äòselect * from test‚Äô, db)`"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-estad√≠stico",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-estad√≠stico",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty data: punto de vista estad√≠stico",
    "text": "Dirty data: punto de vista estad√≠stico\n\nLos datos son generados desde alg√∫n proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsi√≥n: algunas muestras se corrompieron por un proceso\nSesgo de selecci√≥n: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: Left and right censorship: los usuarios van y vienen del escrutinio\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisi√≥n y simplicidad"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores est√°n missing, corrompidos, err√≥neos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-del-experto-en-un-√°rea",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-del-experto-en-un-√°rea",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un √°rea",
    "text": "Dirty Data: Punto de vista del experto en un √°rea\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¬øQu√© ocurri√≥?\nLos expertos en un √°rea llevan un modelo impl√≠cito de los datos que est√°n testeando\nNo siempre necesitas ser un experto en un √°rea para hacer esto\n\n¬øPuede una persona correr 500 km por hora?\n¬øPuede una monta√±a en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido com√∫n"
  },
  {
    "objectID": "sesion2_notas.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2_notas.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinaci√≥n de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregaci√≥n es menos susceptible a los errores num√©ricos\nSer cuidadoso, puede que los datos est√©n bien‚Ä¶"
  },
  {
    "objectID": "sesion2_notas.html#d√≥nde-se-origina-el-dirty-data",
    "href": "sesion2_notas.html#d√≥nde-se-origina-el-dirty-data",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "¬øD√≥nde se origina el dirty data?",
    "text": "¬øD√≥nde se origina el dirty data?\n\nLa fuente est√° mal, por ejemplo, una persona la ingres√≥ de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integraci√≥n de diferentes sets de datos causa problemas\nPropagaci√≥n del error: se magnifica un error"
  },
  {
    "objectID": "sesion2_notas.html#problemas-dirty-comunes",
    "href": "sesion2_notas.html#problemas-dirty-comunes",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs.¬†New York\nP√©rdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs.¬†dos\nDatos truncados: ‚ÄúJanice Keihanaikukauakahihuliheekahaunaele‚Äù se vuelve ‚ÄúJanice - Keihanaikukauakahihuliheek‚Äù en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposici√≥n\nProblemas de formato: 2017-11-07 vs.¬†07/11/2017 vs.¬†11/07/2017"
  },
  {
    "objectID": "sesion2_notas.html#data-wrangling",
    "href": "sesion2_notas.html#data-wrangling",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato m√°s significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representaci√≥n a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2_notas.html#tidy-data",
    "href": "sesion2_notas.html#tidy-data",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para an√°lisis de corte transversal nuestro ideal es trabajar con Tidy Data\n‚ÄúTidy Data is a standar way of mapping the meaning of a dataset to its structure¬®\nUn tipo de estructura de datos √∫til para poder realizar modelos y an√°lisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un est√°ndar deseable para analizar datos.\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observacioÃÅn (registro) forma una fila.\nCada dato (valor) estaÃÅ en una celda de la tabla.\n\n\nCon Tidy data podemos tener una estandarizaci√≥n en el tratamiento de los datos:\n\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n\n¬øPorqu√© datos Tidy?\n\nEstandarizanci√≥n\n\nLos datos organizados te permiten ser m√°s eficiente al utilizar herramientas existentes dise√±adas espec√≠ficamente para realizar las tareas que necesitas hacer, desde la selecci√≥n de porciones de tus datos hasta la creaci√≥n de mapas de tu √°rea de estudio.\nUtilizar herramientas existentes te ahorra tener que construir todo desde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual puede ser consume tiempo y desmotivante).\nAfortunadamente, existen muchas herramientas dise√±adas espec√≠ficamente para transformar datos desorganizados en datos organizados (por ejemplo, en el paquete tidyr). Al estar mejor preparado para transformar tus datos en un formato organizado, podr√°s llegar m√°s r√°pido a tus an√°lisis y comenzar a responder las preguntas que est√°s planteando.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nFacilitar la colaboraci√≥n\n\nDado que nuestros colegas pueden emplear las mismas herramientas de manera familiar. Tanto si consideramos a los colaboradores como compa√±eros actuales, su futuro propio o futuros colegas, la organizaci√≥n y compartici√≥n de datos de manera coherente y predecible implica menos ajustes, tiempo y esfuerzo para todos.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nSimplifica la reproducibilidad\n\nLos datos organizados tambi√©n facilitan la reproducci√≥n de an√°lisis, ya que son m√°s f√°ciles de comprender, actualizar y reutilizar. Al utilizar herramientas que esperan datos organizados como entrada, puedes construir e iterar flujos de trabajo realmente potentes. Y cuando tienes entradas de datos adicionales, ¬°no hay problema en volver a ejecutar tu c√≥digo!\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\n\n¬øSiempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean √∫tiles o ‚Äúdesordenadas‚Äù.\nEs importante tener en cuenta que siempre existen m√∫ltiples formas de representar la misma informaci√≥n.\n\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempe√±o computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2_notas.html#datos-perdidos",
    "href": "sesion2_notas.html#datos-perdidos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\n\nse pueden escoger filas o columnas\n\n\nLlenar/ reemplazar :\n\n\ncon un valor por default\ncon un valor interpolados, otros\n\n\n\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2_notas.html#fitrando-y-limpiando",
    "href": "sesion2_notas.html#fitrando-y-limpiando",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cu√°l de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas espec√≠fics para chequear por duplicados, e.g. chequear solo la columna key.\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2_notas.html#problemas-de-registro-relacionados-con-textos",
    "href": "sesion2_notas.html#problemas-de-registro-relacionados-con-textos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Problemas de registro relacionados con textos",
    "text": "Problemas de registro relacionados con textos\nTextos o strings, es muy com√∫n tener problemas de codificaci√≥n, errores de tipeo, problemas de formato, etc.\nUno de los errores de registro m√°s t√≠pico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) ‚Äúquiebra‚Äù o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14‚Äù\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14‚Äù\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD‚Äù \"AbCd\".lower() # \"abcd\"\n\n\nErrores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a n√∫meros eliminando el s√≠mbolo de d√≥lar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_18558/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\n\n\n\n\nTransformando strings\n\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n2\n\n\n\n#s.index(':') # ValueError raised\n\n\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado\n\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n2\n\n\n\ns.find(':') # -1\n\n-1\n\n\n\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string\n\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\nFalse\n\n\n\n\nM√©todos para strings\n\n\nCualquier columna o serie puede tener m√©todos string (e.g.¬†replace, split) aplicado a la serie completa\nEst√° vectorizado para columnas completas o incluso el dataframe (lo cual lo hace r√°pido) Se debe usar .str. (es importante el .str).\nEjemplo:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2_notas.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2_notas.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets\n\n\nEliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro an√°lisis.\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n\n\n\n\nReshapes/re formato\n\nEl ‚Äúreshape‚Äù o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposici√≥n de filas y columnas para adaptarse a un formato espec√≠fico.\nEsto puede implicar la transformaci√≥n de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el an√°lisis, la visualizaci√≥n o la aplicaci√≥n de modelos estad√≠sticos.\nEl ‚Äúreshape‚Äù es com√∫nmente realizado en la manipulaci√≥n de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de c√°lculo.\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n\n\n\n\nCrear variables dummies / dicot√≥micas\n\nMuy √∫til para representar informaci√≥n cualitativa.\nMuy usando en an√°lisis de regresi√≥n y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o m√°s indicadores que toman valor 0 √≥ 1.\nSe pueden generar mediante pd.get_dummies(df[‚Äòkey‚Äô])\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicot√≥micas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicot√≥micas para las razas de inter√©s\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0"
  },
  {
    "objectID": "sesion2_notas.html#unir-datasets",
    "href": "sesion2_notas.html#unir-datasets",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos m√©todos principales:\n\nApliar/concatenar/append Consiste en agregar un dataset a continuaci√≥n de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join Las uniones combinan DataFrames utilizando columnas en com√∫n como clave. Puedes realizar diferentes tipos de uniones, como ‚Äúinner‚Äù, ‚Äúouter‚Äù, ‚Äúleft‚Äù y ‚Äúright‚Äù\n\n\n\n1. Apilar dataframes\nCreemos los dataframes:\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n\n\nHagamos el apilar. Atenci√≥n a los index\n\ndf.append(df2)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_18558/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9\n\n\n\n\n\n\n\n\nUsemos ignorar index\n\ndf.append(df2, ignore_index=True)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_18558/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9\n\n\n\n\n\n\n\n\n\n\n2. Unir dataframes\nUtilizando la funci√≥n pd.merge().\n\n# Crear dos DataFrames con columnas en com√∫n\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Uni√≥n interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Uni√≥n externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Uni√≥n interna:\")\nprint(inner_join)\n\nprint(\"\\nUni√≥n externa:\")\nprint(outer_join)\n\nUni√≥n interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUni√≥n externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0"
  },
  {
    "objectID": "sesion2_notas.html#transformaciones-estad√≠sticas",
    "href": "sesion2_notas.html#transformaciones-estad√≠sticas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Transformaciones estad√≠sticas:",
    "text": "Transformaciones estad√≠sticas:\n\n1. Outliers\nUn valor at√≠pico, tambi√©n conocido como valor outlier en ingl√©s, es un punto de datos que difiere significativamente del patr√≥n general de los dem√°s datos en un conjunto. Estos valores son inusuales en relaci√≥n con el resto de la distribuci√≥n de los datos y pueden ser considerablemente m√°s altos o m√°s bajos que los valores t√≠picos del conjunto.\nLa identificaci√≥n de valores at√≠picos es importante por varias razones: 1. Calidad de los datos 2. Impacto en estad√≠sticas y modelos 3. Anomal√≠as y problemas reales 4. Toma de decisiones informada\nVeamos un ejemplo:\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores at√≠picos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuart√≠lico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los l√≠mites para identificar valores at√≠picos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores at√≠picos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores at√≠picos:\")\nprint(outliers)\n\nValores at√≠picos:\n   Valor\n6    200\n\n\n\n\n2. Estandarizaci√≥n de Datos\nLa estandarizaci√≥n es un proceso importante en el an√°lisis de datos y el aprendizaje autom√°tico. Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviaci√≥n est√°ndar de uno. Esta transformaci√≥n se logra mediante la f√≥rmula:\nz=x‚àíŒºœÉ z = \\frac{x - \\mu}{\\sigma} \nDonde $ x $ es el valor original, $ $ es la media de los valores y $ $ es la desviaci√≥n est√°ndar.\nLa estandarizaci√≥n es especialmente √∫til cuando se tienen variables con diferentes escalas y distribuciones, ya que permite compararlas de manera equitativa y mejorar la eficacia de los algoritmos de aprendizaje autom√°tico al mitigar el impacto de las diferencias en las magnitudes de las caracter√≠sticas.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviaci√≥n est√°ndar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarizaci√≥n de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviaci√≥n est√°ndar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarizaci√≥n de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]\n\n\n\n\n\n\n\n3. Normalizaci√≥n de Datos\nLa normalizaci√≥n es un proceso esencial en el an√°lisis de datos y el aprendizaje autom√°tico. Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango espec√≠fico, generalmente entre 0 y 1. La f√≥rmula matem√°tica utilizada para la normalizaci√≥n es:\nxnorm=x‚àímin(x)max(x)‚àímin(x) x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \nDonde $ x $ es el valor original y $x_{} $ es el valor normalizado.\nLa normalizaci√≥n es beneficiosa para igualar la escala de las variables y mejorar el rendimiento de los modelos que son sensibles a la magnitud de las caracter√≠sticas.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores m√≠nimo y m√°ximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalizaci√≥n min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores m√≠nimo y m√°ximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalizaci√≥n min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]"
  },
  {
    "objectID": "sesion0_slides.html#un-poco-sobre-mi",
    "href": "sesion0_slides.html#un-poco-sobre-mi",
    "title": "Presentaci√≥n del curso",
    "section": "Un poco sobre mi:",
    "text": "Un poco sobre mi:"
  },
  {
    "objectID": "sesion0_slides.html#en-la-sesi√≥n-de-hoy",
    "href": "sesion0_slides.html#en-la-sesi√≥n-de-hoy",
    "title": "Presentaci√≥n del curso",
    "section": "En la sesi√≥n de hoy:",
    "text": "En la sesi√≥n de hoy:\n\nRevisaremos los objetivos del curso, metodolog√≠a, calendario y evaluaciones.\nRevisaremos algunos principios de como plantear y responder preguntas con datos\nEn ese contexto, realizaremos un breve repaso a pruebas de hip√≥tesis."
  },
  {
    "objectID": "sesion0_slides.html#descripci√≥n-del-curso",
    "href": "sesion0_slides.html#descripci√≥n-del-curso",
    "title": "Presentaci√≥n del curso",
    "section": "Descripci√≥n del curso",
    "text": "Descripci√≥n del curso\n\nEsta asignatura presentar√° los conceptos b√°sicos de pre-procesamiento y an√°lisis descriptivo de datos.\nEl objetivo principal es poder determinar cu√°les datos son susceptibles de ser convertidos en informaci√≥n para apoyar la toma de decisiones, y separar el ruido de la se√±al."
  },
  {
    "objectID": "sesion0_slides.html#descripci√≥n-del-curso-1",
    "href": "sesion0_slides.html#descripci√≥n-del-curso-1",
    "title": "Presentaci√≥n del curso",
    "section": "Descripci√≥n del curso",
    "text": "Descripci√≥n del curso\n\nEs el primer paso en un proyecto de ciencia de datos.\nLos estudiantes aprender√°n a identificar\n\nlas problem√°ticas que presentan los datos desde el momento de su registro (por ej., error muestral, outliers),\nas√≠ como usar las herramientas necesarias para describirlos (por ej., distribuciones e histogramas),"
  },
  {
    "objectID": "sesion0_slides.html#descripci√≥n-del-curso-2",
    "href": "sesion0_slides.html#descripci√≥n-del-curso-2",
    "title": "Presentaci√≥n del curso",
    "section": "Descripci√≥n del curso",
    "text": "Descripci√≥n del curso\n\nexplorarlos (por ej., agrupar o filtrar bajo un criterio espec√≠fico),\ny cruzarlos (por ej., utilizando otras fuentes).\nAsimismo, los estudiantes comprender√°n que las etapas de este proceso no son lineales, sino que se benefician del dise√±o iterativo."
  },
  {
    "objectID": "sesion0_slides.html#contexto-en-el-programa-de-magister",
    "href": "sesion0_slides.html#contexto-en-el-programa-de-magister",
    "title": "Presentaci√≥n del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nEsta asignatura se enmarca en la l√≠nea de desarrollo de data science.\nEsta asignatura tributa, a trav√©s de sus resultados de aprendizaje, a las siguientes competencias del perfil de egreso del Mag√≠ster en Data Science:\n\nAplicar teor√≠as, algoritmos, m√©todos, t√©cnicas y herramientas b√°sicas y avanzadas de Data Science para analizar, resolver y hacer una evaluaci√≥n cr√≠tica de desaf√≠os complejos e interdisciplinarios, utilizando datos internos y externos de las organizaciones."
  },
  {
    "objectID": "sesion0_slides.html#contexto-en-el-programa-de-magister-1",
    "href": "sesion0_slides.html#contexto-en-el-programa-de-magister-1",
    "title": "Presentaci√≥n del curso",
    "section": "Contexto en el programa de magister",
    "text": "Contexto en el programa de magister\n\nComunica efectivamente y argumenta sobre los resultados de su trabajo a p√∫blicos especializados y no especializados, de forma oral, escrita y visual, utilizando distintos medios y soportes.\nDemuestra responsabilidad y comportamiento √©tico, cumpliendo los protocolos y normas que gu√≠an su desempe√±o, en las iniciativas de Data science.\nDemuestra capacidad de aprendizaje continuo, mediante la aplicaci√≥n de estrategias para utilizar nuevo conocimiento en data science en su √°mbito de desempe√±o."
  },
  {
    "objectID": "sesion0_slides.html#objetivos-de-la-asignatura",
    "href": "sesion0_slides.html#objetivos-de-la-asignatura",
    "title": "Presentaci√≥n del curso",
    "section": "Objetivos de la asignatura",
    "text": "Objetivos de la asignatura\nResultados de aprendizaje\n\nIdentificar las ventajas y desventajas de las herramientas computacionales utilizadas para el an√°lisis de datos, utilizando lenguaje t√©cnico af√≠n.\nRecopilar y limpiar datos, en base a una propuesta de replicabilidad del proceso."
  },
  {
    "objectID": "sesion0_slides.html#objetivos-de-la-asignatura-1",
    "href": "sesion0_slides.html#objetivos-de-la-asignatura-1",
    "title": "Presentaci√≥n del curso",
    "section": "Objetivos de la asignatura",
    "text": "Objetivos de la asignatura\nResultados de aprendizaje\n\nTransformar y analizar datos, realizando preguntas clave para resolver problemas a partir del contexto en que se desarrollan.\nModelar datos para extraer informaci√≥n y generar conclusiones basadas en evidencia.\nIdentificar las buenas pr√°cticas en el modelamiento de datos."
  },
  {
    "objectID": "sesion0_slides.html#contenidos",
    "href": "sesion0_slides.html#contenidos",
    "title": "Presentaci√≥n del curso",
    "section": "Contenidos",
    "text": "Contenidos\n\nLimpieza y estructura de datos.\nRegresi√≥n y predicci√≥n.\nSeries de tiempo"
  },
  {
    "objectID": "sesion0_slides.html#contenidos-1",
    "href": "sesion0_slides.html#contenidos-1",
    "title": "Presentaci√≥n del curso",
    "section": "Contenidos",
    "text": "Contenidos\n1. Limpieza y estructura de datos.\na. Formateo de datos\nb. Transformaci√≥n de datos\nc. ETL"
  },
  {
    "objectID": "sesion0_slides.html#contenidos-2",
    "href": "sesion0_slides.html#contenidos-2",
    "title": "Presentaci√≥n del curso",
    "section": "Contenidos",
    "text": "Contenidos\n2. Regresi√≥n y predicci√≥n.\na. Regresi√≥n lineal m√∫ltiple.\nb. Predicci√≥n usando regresi√≥n y los peligros de la extrapolaci√≥n.\nc. Factores y variables categ√≥ricas en una regresi√≥n.\nd. Multicolinealidad, variables de confusi√≥n e interacciones.\ne. Diagn√≥stico de una regresi√≥n y supuestos (outliers, heterocedasticidad, no-normalidad, errores correlacionados y no-linealidad)\nf. Sesgos en los an√°lisis: Paradoja de Simpson, Paradoja de Berkson y Collider Bias."
  },
  {
    "objectID": "sesion0_slides.html#contenidos-3",
    "href": "sesion0_slides.html#contenidos-3",
    "title": "Presentaci√≥n del curso",
    "section": "Contenidos",
    "text": "Contenidos\n3. Series de tiempo\na. B√∫squeda y reorganizaci√≥n de datos de series de tiempo\nb. An√°lisis de datos exploratorios para series temporales\n    i. Histogramas, gr√°fico de dispersi√≥n y m√©todos exploratorios para series de tiempo\n    ii. Estacionariedad y ra√≠z unitaria\n    iii. Autocorrelaci√≥n y correlaciones espurias en series de tiempo\nc. Modelos estad√≠sticos para series de tiempo\n    i. ¬øPor qu√© no utilizar una regresi√≥n lineal?\n    ii. Modelos autorregresivos (AR), ARIMA y Autorregresi√≥n vectorial (VAR)\n    iii. Ventajas y desventajas de los m√©todos estad√≠sticos para series de tiempo"
  },
  {
    "objectID": "sesion0_slides.html#evaluaci√≥n",
    "href": "sesion0_slides.html#evaluaci√≥n",
    "title": "Presentaci√≥n del curso",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\n\nEl curso tendr√° dos evaluaciones basadas en el trabajo en clase y refuerzo de los contenidos fuera del horario lectivo.\n\nTalleres de aplicaci√≥n (30%)\nProyecto final (70%)\n\nTodas las evaluaciones se realizar√°n mediante un set de rubricas, publicadas en Canva."
  },
  {
    "objectID": "sesion0_slides.html#evaluaci√≥n-1",
    "href": "sesion0_slides.html#evaluaci√≥n-1",
    "title": "Presentaci√≥n del curso",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\nTalleres de aplicacion (30%)\n\nDurante las clases se desarrollar√° un taller que aplique los contenidos desarrollados en cada una de las tres principales unidades. Se pueden trabajar de manera individual, o en grupo de hasta 3 personas.\n\nTaller 1: Limpieza, an√°lisis descriptivo de datosy pruebas de hip√≥tesis (repasa elementos del curso anterior) (sesi√≥n 1 - 2)\nTaller 2: An√°lisis de regresi√≥n. (Sesiones¬† 3-4)\nTaller 3: An√°lisis de serie de tiempo (sesiones 5-6)"
  },
  {
    "objectID": "sesion0_slides.html#evaluaci√≥n-2",
    "href": "sesion0_slides.html#evaluaci√≥n-2",
    "title": "Presentaci√≥n del curso",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\nTalleres de aplicacion (30%) (cont.)\n\nLa mayor√≠a del taller se espera lo puedan responder durante la clase, sin embargo tendr√°n una semana de margen para su entrega. Estos se deben entregar en la plataforma canvas, en PDF (para su evaluaci√≥n) y en .ipynb/.qmd, que ser√° corroborado que se pueda ejecutar y sea consistente con el pdf.¬†\nstos se evaluar√°n de acuerdo a la rubrica talleres de aplicacion\nUna vez sean entregados los talleres, se har√° publica una pauta de desarrollo de cada taller."
  },
  {
    "objectID": "sesion0_slides.html#evaluaci√≥n-3",
    "href": "sesion0_slides.html#evaluaci√≥n-3",
    "title": "Presentaci√≥n del curso",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\nProyecto de an√°lisis de datos (70%)\n\nVamos a desarrollar durante el curso un proyecto.\nEn este deben elegir un conjunto de datos, a proponer una pregunta e hip√≥tesis a testear, desarrollar an√°lisis (de regresi√≥n o serie de tiempo) y concluir en base a sus resultados obtenidos, mencionando las limitaciones de su an√°lisis.\nEl proyecto se debe realizar en grupos entre 3 a 5 personas."
  },
  {
    "objectID": "sesion0_slides.html#evaluaci√≥n-4",
    "href": "sesion0_slides.html#evaluaci√≥n-4",
    "title": "Presentaci√≥n del curso",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\nProyecto de an√°lisis de datos (70%)\nEste proyecto se evaluar√° entonces en base a tres elementos:\n\nAvance durante la clase (20%)\nReporte de an√°lisis y resultados (20%)\nPresentaci√≥n oral final (30%)"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-an√°lisis-de-datos",
    "href": "sesion0_slides.html#proyecto-de-an√°lisis-de-datos",
    "title": "Presentaci√≥n del curso",
    "section": "Proyecto de an√°lisis de datos",
    "text": "Proyecto de an√°lisis de datos\nAvance en clase ( 20% )\n\nAlgunos elementos del proyecto ser√°n desarrollados durante tiempo de clase, pero se espera que la profundizaci√≥n sea llevada en el tiempo lectivo dedicado al curso.\nNO ES SUFICIENTE PARA TERMINAR EL TRABAJO COMPLETO.\nRubrica de trabajo en clase (se eval√∫a al final de la clase)\n\nAsistencia y participaci√≥n\nPlanteamiento de problemas¬†\nDesarrollo\nResultados, interpretaci√≥n y conclusiones"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-an√°lisis-de-datos-1",
    "href": "sesion0_slides.html#proyecto-de-an√°lisis-de-datos-1",
    "title": "Presentaci√≥n del curso",
    "section": "Proyecto de an√°lisis de datos",
    "text": "Proyecto de an√°lisis de datos\nReporte de an√°lisis y resultados (20%)\n\nDeben documentar su an√°lisis de datos mediante un notebook. Este se revisar√° en si mismo, para fomentar las buenas pr√°cticas y reproducibilidad de su an√°lisis.\nRubrica de notebook reporte de an√°lisis\n\nEntrega a tiempo\nUso correcto del lenguaje y redacci√≥n a nivel profesional\nOrden\nCalidad de c√≥digo\nConsistencia con presentaci√≥n"
  },
  {
    "objectID": "sesion0_slides.html#proyecto-de-an√°lisis-de-datos-2",
    "href": "sesion0_slides.html#proyecto-de-an√°lisis-de-datos-2",
    "title": "Presentaci√≥n del curso",
    "section": "Proyecto de an√°lisis de datos",
    "text": "Proyecto de an√°lisis de datos\nPresentaci√≥n oral final (30%)\n\nEn la √∫ltima sesi√≥n del curso, cada grupo debe presentar su an√°lisis y resultados.\nEsta presentaci√≥n ser√° de 10 minutos por grupo y 5 minutos para preguntas, las cuales ser√°n dirigidas a cada estudiante del grupo."
  },
  {
    "objectID": "sesion0_slides.html#calendario-por-sesi√≥n",
    "href": "sesion0_slides.html#calendario-por-sesi√≥n",
    "title": "Presentaci√≥n del curso",
    "section": "Calendario por sesi√≥n:",
    "text": "Calendario por sesi√≥n:\n\n\n\n\nCurso An√°lisis de Datos - Sesi√≥n 1"
  },
  {
    "objectID": "sesion2.html",
    "href": "sesion2.html",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "",
    "text": "Slides Sesi√≥n 1"
  },
  {
    "objectID": "sesion2.html#tipos-de-datos",
    "href": "sesion2.html#tipos-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tipos de datos",
    "text": "Tipos de datos\nAntes de adentrarnos en la preparaci√≥n de los datos propiamente tal, es importante que revisemos los diferentes tipos de datos con los cuales nos podemos encontrar.\nEn especial, para que trabajemos desde un lenguaje com√∫n.\nLos tipos de datos y sus estructuras\nPodemos clasificar estos datos que recopilamos de muchas maneras, unas posibles agrupaciones son en relaci√≥n a:\n\nEstructura\nTamanÃÉo\nOperacionalizacioÃÅn\nTemporalidad y unidad de anaÃÅlisis.\n\n\nSeg√∫n estructura de los datos\n\nLa estructura de un dato se refiere a su formato y codificaci√≥n.\n\n\n\n\nfuente: https://www.igneous.io/blog/structured-data-vs-unstructured-data\n\n\n\nDatos estructurados\n\nTablas son formas organizadas y ordenadas de datos.\nConjuntos de tablas forman bases de datos estructuradas.\nLas entradas en tablas pueden ser texto o n√∫meros.\nLos datos estructurados son recopilados y organizados intencionalmente.\n\n\n\nDatos No estructurados\n\nLos datos no estructurados son comunes en la sociedad moderna.\nPueden ser multimedia, im√°genes, audio, datos de sensores, texto, etc.\nNo siguen un formato de base de datos definido.\n\nAlgunos ejemplos de datos no estructurados son:\n\nMedios enriquecidos: Datos de medios y entretenimiento, datos de vigilancia, datos geoespaciales, audio, datos meteoroloÃÅgicos\nColecciones de documentos digitalizados: Facturas, registros, correos electroÃÅnicos, aplicaciones de productividad\nInternet de las cosas: Datos de sensores, datos de teletipos.\n\n\n\nEstructurados vs no estructuraods\n\nDatos estructurados est√°n en sistemas transaccionales y bases de datos.\nNo hay preferencia entre estructurados y no estructurados.\nAmbos tienen herramientas para acceder a la informaci√≥n, pero los no estructurados son m√°s abundantes.\n\n\n\n\nDatos seg√∫n tamanÃÉo\n\nBig Data: Se refiere a conjuntos de datos extremadamente grandes y complejos que son dif√≠ciles de gestionar, procesar y analizar con herramientas y m√©todos tradicionales. Big Data involucra terabytes o incluso petabytes de informaci√≥n y generalmente requiere tecnolog√≠as y enfoques especializados para extraer conocimientos significativos.\nSmall Data: Se refiere a conjuntos de datos m√°s peque√±os y manejables en comparaci√≥n con Big Data. Estos conjuntos de datos son m√°s accesibles y pueden ser procesados y analizados utilizando herramientas y m√©todos convencionales. A menudo, Small Data se centra en obtener informaci√≥n valiosa de fuentes limitadas y espec√≠ficas.\n\n\n\nDatos seg√∫n tipo y su operacionalizacioÃÅn\n\nLas variables en an√°lisis de datos pueden ser cuantitativas o cualitativas.\n\nLas cuantitativas representan n√∫meros medibles, como medidas o cantidades.\nLas cualitativas representan cualidades y se subdividen en nominales (sin orden) y ordinales (con orden).\n\nPara operacionalizar variables cualitativas, asignamos n√∫meros o factores que representen categor√≠as, facilitando su an√°lisis cuantitativo.\n\n\n\n\n\nDatos seg√∫n temporalidad y unidad de anaÃÅlisis.\nOtra manera de clasificar los datos con relaci√≥n a la temporalidad en la cual son tomados y cu√°l es la unidad de an√°lisis.\n\nCorte transversal\nserie de tiempo\npanel - datos longitudinales\n\n\nCorte transversal\n\n\n\nSerie temproal\n\n\n\nPanel"
  },
  {
    "objectID": "sesion2.html#leyendo-datos-en-pandas",
    "href": "sesion2.html#leyendo-datos-en-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Leyendo datos en Pandas",
    "text": "Leyendo datos en Pandas\nEn Pandas podemos cargar una gran variedad de datos, a trav√©s de la familia de funciones read_tipo\n\n\n\n\n\n\n\nAlgunos tipos de argumentos para funciones read_*\n\n\n\n\nIndexing: escoger una columna para indexar los datos, obtener nombres de columnas del archivo o usuario\nType inference y data conversion: autom√°tico o definido por el usuario\nDatetime parsing: puede combinar informaci√≥n de m√∫ltiples columnas\nIterating: trabajar con archivos muy grandes\nUnclean Data: saltarse filas (por ejemplo, comentarios) o trabajar con n√∫meros con formato (por ejemplo, 1,000,345)\nMemory: low_memory indica que usemos pocos recursos (o no) del sistema."
  },
  {
    "objectID": "sesion2.html#formato-csv-valores-separados-por-comas",
    "href": "sesion2.html#formato-csv-valores-separados-por-comas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Formato CSV (valores separados por comas)",
    "text": "Formato CSV (valores separados por comas)\n\nLa coma es un separador de campos, newline denota registros\n\na,b,c,d,message\n1,2,3,4,hello\n5,6,7,8,world\n9,10,11,12,foo\n\nPuede tener un encabezado (a,b,c,d,message), pero no es requisito\nSin tipo de informaci√≥n: no sabemos qu√© son las columnas (n√∫meros, strings, punto flotante, etc.)\n\nDefault: simplemente mantener todo como string\ninferencia del tipo: descubrir qu√© tipo para transformar cada columna basado en lo que parece ser\n\n¬øY qu√© pasa con las comas en un valor?: doble comillas\nSe puede utilizar otros delimitadores (|, , )"
  },
  {
    "objectID": "sesion2.html#csv-en-pandas",
    "href": "sesion2.html#csv-en-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "CSV en pandas",
    "text": "CSV en pandas\n\nLectura:\n\nB√°sica: df = pd.read_csv(fname)\nUtilizar diferente delimitador: df = pd.read_csv(fname, sep='\\t\\')\nSaltarse las primeras columnas: df = pd.read_csv(fname, skiprows=3)\n\nEscritura:\n\nB√°sica df.to_csv()\nCambiar el delimitador con sep kwarg: df.to_csv('example.dsv', sep='\\|')\nCambiar la representaci√≥n de missing value df.to_csv('example.dsv', na_rep='NULL')"
  },
  {
    "objectID": "sesion2.html#json-java-script-object-notation",
    "href": "sesion2.html#json-java-script-object-notation",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "JSON (Java Script Object Notation)",
    "text": "JSON (Java Script Object Notation)\n\nUn formato para datos web\nAspecto muy similar a a diccionarios y listas python\nEjemplo: {   \"name\": \"Wes\",    \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],    \"pet\": null,    \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},    {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]    }\nNo hay variables sino elementos independientes, pero permite nulos Valores: strings, arreglos, diccionarios, n√∫meros, booleanos, o nulos\n\nKeys de diccionario deben ser strings\nSignos de pregunta ayudan a diferenciar string o valores num√©ricos"
  },
  {
    "objectID": "sesion2.html#lectura-y-escritura-de-json-con-pandas",
    "href": "sesion2.html#lectura-y-escritura-de-json-con-pandas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Lectura y escritura de JSON con pandas",
    "text": "Lectura y escritura de JSON con pandas\n\npd.read_json(&lt;filename&gt;, orient=&lt;orientation&gt;)\ndf.to_json(&lt;filename&gt;, orient=&lt;orientation&gt;)"
  },
  {
    "objectID": "sesion2.html#orientaciones-posibles-de-json",
    "href": "sesion2.html#orientaciones-posibles-de-json",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Orientaciones posibles de JSON",
    "text": "Orientaciones posibles de JSON\n\nsplit: tipo diccionario {index -&gt; [index],  columns -&gt; [columns],data -&gt; [values]}\nrecords: tipo lista\n[{column -&gt; value}, ... , {column -&gt; value}]\nindex: tipo diccionario {index -&gt; {column -&gt; value}}\ncolumns: tipo diccionario {column -&gt; {index -&gt; value}}\nvalues: solo los valores del arreglo"
  },
  {
    "objectID": "sesion2.html#extensible-markup-language-xml",
    "href": "sesion2.html#extensible-markup-language-xml",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "eXtensible Markup Language (XML)",
    "text": "eXtensible Markup Language (XML)\n\nFormato m√°s antiguo y auto descriptivo, con estructura jer√°rquica anidada.\nCada campo tiene tags\nTiene un elemento inicial llamado root\nNo tiene un m√©todo incorporado en Python.\nSe puede usar la librer√≠a lxml (tambi√©n ElementTree)\n\n\nEjemplo XML"
  },
  {
    "objectID": "sesion2.html#formatos-binarios",
    "href": "sesion2.html#formatos-binarios",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Formatos binarios",
    "text": "Formatos binarios\n\nCSV, JSON y XML son todos formatos de texto\n¬øQu√© es un formato binario?\nPickle: Python‚Äôs built-in serialization\n\n\nimport pickle\n\n# Objeto que queremos guardar\ndata = {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n# Guardar el objeto en un archivo utilizando pickle\nwith open('datos.pickle', 'wb') as archivo:\n    pickle.dump(data, archivo)\n\n# Cargar el objeto desde el archivo utilizando pickle\nwith open('datos.pickle', 'rb') as archivo:\n    datos_cargados = pickle.load(archivo)\n\nprint(\"Objeto cargado:\", datos_cargados)\n\nObjeto cargado: {'nombre': 'Juan', 'edad': 25, 'profesion': 'ingeniero'}\n\n\n\nHDF5: Librer√≠a para almacenar gran cantidad de datos cient√≠ficos\n\nFormato jer√°rquico de datos\nInterfaces in C, Java, MATLAB, etc.\nAguanta compresi√≥n\nUsar pd.HDFStore para acceder\nComandos: read_hdf/to_hdf, se necesita especificar objeto\nExcel: se necesita especificar la hoja cuando la hoja de c√°lculo tiene m√∫ltiples hojas\npd.ExcelFile or pd.read_excel"
  },
  {
    "objectID": "sesion2.html#bases-de-datos-relacionales",
    "href": "sesion2.html#bases-de-datos-relacionales",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Bases de datos relacionales",
    "text": "Bases de datos relacionales\n\n\nLas bases de datos relacionales son similares a los marcos de datos m√∫ltiples pero tienen muchas m√°s caracter√≠sticas\n\nv√≠nculos entre tablas via foreign keys\nSQL para crear, almacenar y query datos¬†\n\nsqlite3 es una base de datos simple con built-in support in python\nPython tiene una database API que te permite acceder a la mayor√≠a de los sistemas de bases de datos a trav√©s de un API com√∫n\nSintaxis similar (¬°pero no igual!) de otros sistemas de bases de datos (MySQL, Microsoft SQL Server, Oracle, etc.)¬†\nSQLAlchemy: Paquete de python que se abstrae de las diferencias entre distintos sistemas de bases de datos\nSQLAlchemy apoya la lectura de queries a data frame:\n`import sqlalchemy as sqla\ndb = sqla.create_engine(‚Äòsqlite:///mydata.sqlite‚Äô)\npd.read_sql(‚Äòselect * from test‚Äô, db)`"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-estad√≠stico",
    "href": "sesion2.html#dirty-data-punto-de-vista-estad√≠stico",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty data: punto de vista estad√≠stico",
    "text": "Dirty data: punto de vista estad√≠stico\n\nLos datos son generados desde alg√∫n proceso\nSe quiere modelar el proceso, pero se tienen muestras no ideales:\n\nDistorsi√≥n: algunas muestras se corrompieron por un proceso\nSesgo de selecci√≥n: probabilidad de que una muestra dependa de su valor\nCensura de izquierda y derecha: Left and right censorship: los usuarios van y vienen del escrutinio\nDependiencia: las muestras no son independientes (por ejemplo, redes sociales)\n\nPuedes agregar/aumentar modelos para diferentes problemas, pero no modelar todo\nTrade-off entre precisi√≥n y simplicidad"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "href": "sesion2.html#dirty-data-punto-de-vista-experto-en-base-de-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty data: punto de vista experto en base de datos",
    "text": "Dirty data: punto de vista experto en base de datos\n\nSe obtuvo un set de datos\nAlgunos valores est√°n missing, corrompidos, err√≥neos, duplicados\nLos resultados son absolutos (modelo relacional)\nMejores respuestas provienen de mejorar la calidad de los valores en el set de datos"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-del-experto-en-un-√°rea",
    "href": "sesion2.html#dirty-data-punto-de-vista-del-experto-en-un-√°rea",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del experto en un √°rea",
    "text": "Dirty Data: Punto de vista del experto en un √°rea\n\nAlgo se ve mal en los datos\nAlgo se ve mal en la respuesta\n¬øQu√© ocurri√≥?\nLos expertos en un √°rea llevan un modelo impl√≠cito de los datos que est√°n testeando\nNo siempre necesitas ser un experto en un √°rea para hacer esto\n\n¬øPuede una persona correr 500 km por hora?\n¬øPuede una monta√±a en la Tierra estar a 50.000 metros sobre el nivel del mar?\nUtilizar sentido com√∫n"
  },
  {
    "objectID": "sesion2.html#dirty-data-punto-de-vista-del-data-scientist",
    "href": "sesion2.html#dirty-data-punto-de-vista-del-data-scientist",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Dirty Data: Punto de vista del Data Scientist",
    "text": "Dirty Data: Punto de vista del Data Scientist\n\nCombinaci√≥n de los tres puntos de vista anteriores\nTodos los puntos de vista presentan problemas con los datos La meta puede determinar las soluciones:\n\nValor de la mediana: no preocuparse mucho de los outliers muy improbables\nGeneralmente, agregaci√≥n es menos susceptible a los errores num√©ricos\nSer cuidadoso, puede que los datos est√©n bien‚Ä¶"
  },
  {
    "objectID": "sesion2.html#d√≥nde-se-origina-el-dirty-data",
    "href": "sesion2.html#d√≥nde-se-origina-el-dirty-data",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "¬øD√≥nde se origina el dirty data?",
    "text": "¬øD√≥nde se origina el dirty data?\n\nLa fuente est√° mal, por ejemplo, una persona la ingres√≥ de forma incorrecta\nLas transformaciones corrompen los datos, por ejemplo, ciertos valores fueron procesados de forma incorrecta debido a un software bug\nLa integraci√≥n de diferentes sets de datos causa problemas\nPropagaci√≥n del error: se magnifica un error"
  },
  {
    "objectID": "sesion2.html#problemas-dirty-comunes",
    "href": "sesion2.html#problemas-dirty-comunes",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Problemas Dirty comunes:",
    "text": "Problemas Dirty comunes:\n\nProblemas de separadores: por ejemplo, CSV sin respetar comillas dobles\nConvenciones de nombres o denominaciones: NYC vs.¬†New York\nP√©rdida de campos requeridos, por ejemplo, key\nRepresentaciones diferentes: 2 vs.¬†dos\nDatos truncados: ‚ÄúJanice Keihanaikukauakahihuliheekahaunaele‚Äù se vuelve ‚ÄúJanice - Keihanaikukauakahihuliheek‚Äù en la licencia de Hawaii\nRegistros redundantes: pueden ser exactamente el mismo o tener alguna superposici√≥n\nProblemas de formato: 2017-11-07 vs.¬†07/11/2017 vs.¬†11/07/2017"
  },
  {
    "objectID": "sesion2.html#data-wrangling",
    "href": "sesion2.html#data-wrangling",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling: transformar datos en bruto a un formato m√°s significativo que pueda ser analizado mejor\nData cleaning: deshacerse de datos imprecisos\nData transformations: cambiar los datos de una representaci√≥n a otra\nData reshaping: reorganizar los datos\nData merging: combinar dos sets de datos"
  },
  {
    "objectID": "sesion2.html#tidy-data",
    "href": "sesion2.html#tidy-data",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Tidy Data",
    "text": "Tidy Data\nEn este sentido, generalmente para an√°lisis de corte transversal nuestro ideal es trabajar con Tidy Data\n‚ÄúTidy Data is a standar way of mapping the meaning of a dataset to its structure¬®\nUn tipo de estructura de datos √∫til para poder realizar modelos y an√°lisis, son los llamados datos ordenados o Tidy Data Propuesto por Hadley Wickham (2014), se ha vuelto un est√°ndar deseable para analizar datos.\nSe caracteriza por que los datos son representados en tablas rectangulares, de tal manera que:\n\nCada variable (feature) forma una columna.\nCada observacioÃÅn (registro) forma una fila.\nCada dato (valor) estaÃÅ en una celda de la tabla.\n\n\nCon Tidy data podemos tener una estandarizaci√≥n en el tratamiento de los datos:\n\nMuchas veces un flujo de trabajo workflow empieza limpiando y ordenando los datos con el objetivo de que sean Tidy, para luego ser analizados y finalmente, comunicados sus resultados.\n\n\n¬øPorqu√© datos Tidy?\n\nEstandarizanci√≥n\n\nLos datos organizados te permiten ser m√°s eficiente al utilizar herramientas existentes dise√±adas espec√≠ficamente para realizar las tareas que necesitas hacer, desde la selecci√≥n de porciones de tus datos hasta la creaci√≥n de mapas de tu √°rea de estudio.\nUtilizar herramientas existentes te ahorra tener que construir todo desde cero cada vez que trabajas con un nuevo conjunto de datos (lo cual puede ser consume tiempo y desmotivante).\nAfortunadamente, existen muchas herramientas dise√±adas espec√≠ficamente para transformar datos desorganizados en datos organizados (por ejemplo, en el paquete tidyr). Al estar mejor preparado para transformar tus datos en un formato organizado, podr√°s llegar m√°s r√°pido a tus an√°lisis y comenzar a responder las preguntas que est√°s planteando.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nFacilitar la colaboraci√≥n\n\nDado que nuestros colegas pueden emplear las mismas herramientas de manera familiar. Tanto si consideramos a los colaboradores como compa√±eros actuales, su futuro propio o futuros colegas, la organizaci√≥n y compartici√≥n de datos de manera coherente y predecible implica menos ajustes, tiempo y esfuerzo para todos.\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\nSimplifica la reproducibilidad\n\nLos datos organizados tambi√©n facilitan la reproducci√≥n de an√°lisis, ya que son m√°s f√°ciles de comprender, actualizar y reutilizar. Al utilizar herramientas que esperan datos organizados como entrada, puedes construir e iterar flujos de trabajo realmente potentes. Y cuando tienes entradas de datos adicionales, ¬°no hay problema en volver a ejecutar tu c√≥digo!\n\n\n\nFuente: https://allisonhorst.com/other-r-fun\n\n\n\n\n¬øSiempre Tidy?\n\nNO!\nExisten muchos otros tipos de estructuras de datos que no son tidy y que eso no hace que no sean √∫tiles o ‚Äúdesordenadas‚Äù.\nEs importante tener en cuenta que siempre existen m√∫ltiples formas de representar la misma informaci√≥n.\n\nExisten dos principales motivos para utilizar otras estructuras de datos:\n\nRepresentaciones alternativas que tengan mucho mayor desempe√±o computacional o ventajas en uso de memoria. Especialmente importante al tratar con grandes datos.\nAlgunos campos de estudio especializados tienen sus propias convenciones, que pueden ser diferentes a estas, por ejemplo las series temporales."
  },
  {
    "objectID": "sesion2.html#datos-perdidos",
    "href": "sesion2.html#datos-perdidos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Datos perdidos",
    "text": "Datos perdidos\n2 enfoques para lidiar con ellos:\n\nFiltrar\n\n\nse pueden escoger filas o columnas\n\n\nLlenar/ reemplazar :\n\n\ncon un valor por default\ncon un valor interpolados, otros\n\n\n\n\nMissing en Pandas"
  },
  {
    "objectID": "sesion2.html#fitrando-y-limpiando",
    "href": "sesion2.html#fitrando-y-limpiando",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Fitrando y limpiando",
    "text": "Fitrando y limpiando\n\nEncontrar duplicados\n\nduplicated : retorna una Series booleana , indicando si la fila es un duplicado o no la primera instancia no es marcada como un duplicado.\n\nRemover duplicados:\n\ndrop_duplicates: saca todas las filas donde duplicated es True\nkeep: Cu√°l de los valores es el que quiero mantener (first or last)\n\nPuede recibir columnas espec√≠fics para chequear por duplicados, e.g. chequear solo la columna key.\n\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Ana', 'Carlos'],\n        'Edad': [25, 30, 35, 30, 28]}\n\ndf = pd.DataFrame(data)\ndf = df.drop_duplicates()\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n4  Carlos    28"
  },
  {
    "objectID": "sesion2.html#problemas-de-registro-relacionados-con-textos",
    "href": "sesion2.html#problemas-de-registro-relacionados-con-textos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Problemas de registro relacionados con textos",
    "text": "Problemas de registro relacionados con textos\nTextos o strings, es muy com√∫n tener problemas de codificaci√≥n, errores de tipeo, problemas de formato, etc.\nUno de los errores de registro m√°s t√≠pico, tiene que ver con que los textos/strings estan ingresados con errores.\n\nUna de las razones de la popularidad de Python es por su capacidad de procesar texto/strings.\nsplit(&lt;delimiter&gt;) ‚Äúquiebra‚Äù o separa un string en partes: s = \"12,13,14\" slist = s.split(',') # [\"12\", \"13\", \" 14\"]\n&lt;delimiter&gt;.join([&lt;str&gt;]): Une varios strings por un delimitador \":\".join(slist) # \"12:13: 14‚Äù\nstrip(): remueve los espacios en blanco iniciales y finales [p.strip() for p in slist] # [\"12\", \"13\", \"14\"]\nreplace(&lt;from&gt;,&lt;to&gt;): Cambia un substring por otro s.replace(',', ':') # \"12:13: 14‚Äù\nupper()/lower(): casing \"AbCd\".upper () # \"ABCD‚Äù \"AbCd\".lower() # \"abcd\"\n\n\nErrores de formato\nA veces, los datos se almacenan en el formato incorrecto y necesitas corregirlos.\n\nimport pandas as pd\n\ndata = {'Fecha': ['2023-08-26', '2023-08-27', '2023-08-28'],\n        'Ventas': ['$1000', '$1500', '$1200']}\n\ndf = pd.DataFrame(data)\n\n# Convertir la columna 'Ventas' a n√∫meros eliminando el s√≠mbolo de d√≥lar\ndf['Ventas'] = df['Ventas'].str.replace('$', '').astype(float)\n\nprint(df)\n\n        Fecha  Ventas\n0  2023-08-26  1000.0\n1  2023-08-27  1500.0\n2  2023-08-28  1200.0\n\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46417/2310739606.py:9: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\n\n\n\n\nTransformando strings\n\nindex()encuentra donde ocurre primero un substring(error si no lo encuentra):\n\n\ns = \"12,13, 14\"\ns.index(',') # 2\n\n2\n\n\n\n#s.index(':') # ValueError raised\n\n\nfind(&lt;str&gt;): Lo mismo que index pero -1 si no es encontrado\n\n\ns = \"12,13, 14\"\ns.find(',') # 2\n\n2\n\n\n\ns.find(':') # -1\n\n-1\n\n\n\nstartswith() or endswith(): chequeo booleano para la ocurrencia de un string\n\n\n    s.startswith(\"1\") # True\n    s.endswith(\"5\") # False`\n\nFalse\n\n\n\n\nM√©todos para strings\n\n\nCualquier columna o serie puede tener m√©todos string (e.g.¬†replace, split) aplicado a la serie completa\nEst√° vectorizado para columnas completas o incluso el dataframe (lo cual lo hace r√°pido) Se debe usar .str. (es importante el .str).\nEjemplo:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series({'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan})\n\ncontains_gmail = data.str.contains('gmail')\nsplit_at = data.str.split('@').str[1]\nlast_three_chars = data.str[-3:]\n\nprint(\"Contains 'gmail':\")\nprint(contains_gmail)\n\nprint(\"\\nSplit at '@', second part:\")\nprint(split_at)\n\nprint(\"\\nLast three characters:\")\nprint(last_three_chars)\n\nContains 'gmail':\nDave     False\nSteve     True\nRob       True\nWes        NaN\ndtype: object\n\nSplit at '@', second part:\nDave     google.com\nSteve     gmail.com\nRob       gmail.com\nWes             NaN\ndtype: object\n\nLast three characters:\nDave     com\nSteve    com\nRob      com\nWes      NaN\ndtype: object"
  },
  {
    "objectID": "sesion2.html#transformaciones-del-formato-de-los-datos",
    "href": "sesion2.html#transformaciones-del-formato-de-los-datos",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Transformaciones del formato de los datos",
    "text": "Transformaciones del formato de los datos\n\nReshapes\nEliminar/elegir columnas\nRe codificar: variables dummies\nUnir datasets\n\n\nEliminar/ elegir columnas\nA veces, queremos deshacernos de columnas que no son relevantes para nuestro an√°lisis.\n\nimport pandas as pd\n\ndata = {'Nombre': ['Juan', 'Ana', 'Luis', 'Carlos'],\n        'Edad': [25, 30, 35, 28],\n        'Peso (kg)': [70, 65, 80, 75]}\n\ndf = pd.DataFrame(data)\n\n# Eliminar la columna 'Peso (kg)'\ndf = df.drop(columns=['Peso (kg)'])\n\nprint(df)\n\n   Nombre  Edad\n0    Juan    25\n1     Ana    30\n2    Luis    35\n3  Carlos    28\n\n\n\n\nReshapes/re formato\n\nEl ‚Äúreshape‚Äù o remodelado de datos se refiere al proceso de reorganizar la estructura de un conjunto de datos, cambiando su disposici√≥n de filas y columnas para adaptarse a un formato espec√≠fico.\nEsto puede implicar la transformaci√≥n de datos de un formato largo (tall) a uno ancho (wide) o viceversa, con el fin de facilitar el an√°lisis, la visualizaci√≥n o la aplicaci√≥n de modelos estad√≠sticos.\nEl ‚Äúreshape‚Äù es com√∫nmente realizado en la manipulaci√≥n de datos utilizando bibliotecas como Pandas en Python o funciones de pivote en hojas de c√°lculo.\n\n\n\nimport pandas as pd\n\n# Crear DataFrame de ejemplo en formato largo\ndata = {'Fecha': ['2023-08-01', '2023-08-01', '2023-08-02', '2023-08-02'],\n        'Ciudad': ['A', 'B', 'A', 'B'],\n        'Temperatura': [25, 28, 26, 29]}\n\ndf_largo = pd.DataFrame(data)\n\n# Realizar el reshape de largo a ancho usando pivot()\ndf_ancho = df_largo.pivot(index='Fecha', columns='Ciudad', values='Temperatura')\n\nprint(\"DataFrame en formato largo:\")\nprint(df_largo)\n\nprint(\"\\nDataFrame en formato ancho:\")\nprint(df_ancho)\n\nDataFrame en formato largo:\n        Fecha Ciudad  Temperatura\n0  2023-08-01      A           25\n1  2023-08-01      B           28\n2  2023-08-02      A           26\n3  2023-08-02      B           29\n\nDataFrame en formato ancho:\nCiudad       A   B\nFecha             \n2023-08-01  25  28\n2023-08-02  26  29\n\n\n\n\nCrear variables dummies / dicot√≥micas\n\nMuy √∫til para representar informaci√≥n cualitativa.\nMuy usando en an√°lisis de regresi√≥n y machine learning.\nQueremos tomar valores posibles y mapearlos a uno o m√°s indicadores que toman valor 0 √≥ 1.\nSe pueden generar mediante pd.get_dummies(df[‚Äòkey‚Äô])\n\n\n\nimport pandas as pd\n\n# Lista de razas de perros\nrazas = ['Labrador', 'Poodle', 'Bulldog', 'Golden Retriever', 'Chihuahua', 'Husky']\n\n# Crear un DataFrame con la lista de razas\ndf = pd.DataFrame({'Raza': razas})\n\n# Razas que queremos crear como variables dicot√≥micas\nrazas_interes = ['Labrador', 'Poodle', 'Bulldog']\n\n# Crear variables dicot√≥micas para las razas de inter√©s\nfor raza in razas_interes:\n    df[raza] = df['Raza'].apply(lambda x: 1 if x == raza else 0)\n\n# Eliminar la columna original de 'Raza'\ndf.drop(columns=['Raza'], inplace=True)\n\nprint(df)\n\n   Labrador  Poodle  Bulldog\n0         1       0        0\n1         0       1        0\n2         0       0        1\n3         0       0        0\n4         0       0        0\n5         0       0        0"
  },
  {
    "objectID": "sesion2.html#unir-datasets",
    "href": "sesion2.html#unir-datasets",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Unir datasets",
    "text": "Unir datasets\nMuchas veces tenemos varios dataset que queremos unir. Existen dos m√©todos principales:\n\nApliar/concatenar/append Consiste en agregar un dataset a continuaci√≥n de otro, para esto tienen que tener la misma forma.\nUniones Merge / Join Las uniones combinan DataFrames utilizando columnas en com√∫n como clave. Puedes realizar diferentes tipos de uniones, como ‚Äúinner‚Äù, ‚Äúouter‚Äù, ‚Äúleft‚Äù y ‚Äúright‚Äù\n\n\n\n1. Apilar dataframes\nCreemos los dataframes:\n\ndf = pd.DataFrame([[2, 3], [4, 5]], columns=list('PQ'))\nprint(df)\n\ndf2 = pd.DataFrame([[6, 7], [8, 9]], columns=list('PQ'))\nprint(df2)\n\n   P  Q\n0  2  3\n1  4  5\n   P  Q\n0  6  7\n1  8  9\n\n\nHagamos el apilar. Atenci√≥n a los index\n\ndf.append(df2)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46417/2094904254.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n0\n6\n7\n\n\n1\n8\n9\n\n\n\n\n\n\n\n\nUsemos ignorar index\n\ndf.append(df2, ignore_index=True)\n\n/var/folders/ry/0yrdh5kn1m92t_pmx56nnqkc0000gn/T/ipykernel_46417/26416246.py:1: FutureWarning:\n\nThe frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\n\n\n\nP\nQ\n\n\n\n\n0\n2\n3\n\n\n1\n4\n5\n\n\n2\n6\n7\n\n\n3\n8\n9\n\n\n\n\n\n\n\n\n\n\n2. Unir dataframes\nUtilizando la funci√≥n pd.merge().\n\n# Crear dos DataFrames con columnas en com√∫n\ndf3 = pd.DataFrame({'Key': ['A', 'B', 'C'], 'Value': [1, 2, 3]})\ndf4 = pd.DataFrame({'Key': ['B', 'C', 'D'], 'Value': [4, 5, 6]})\n\n# Uni√≥n interna basada en la columna 'Key'\ninner_join = pd.merge(df3, df4, on='Key', how='inner')\n\n# Uni√≥n externa basada en la columna 'Key'\nouter_join = pd.merge(df3, df4, on='Key', how='outer')\n\nprint(\"Uni√≥n interna:\")\nprint(inner_join)\n\nprint(\"\\nUni√≥n externa:\")\nprint(outer_join)\n\nUni√≥n interna:\n  Key  Value_x  Value_y\n0   B        2        4\n1   C        3        5\n\nUni√≥n externa:\n  Key  Value_x  Value_y\n0   A      1.0      NaN\n1   B      2.0      4.0\n2   C      3.0      5.0\n3   D      NaN      6.0"
  },
  {
    "objectID": "sesion2.html#transformaciones-estad√≠sticas",
    "href": "sesion2.html#transformaciones-estad√≠sticas",
    "title": "Sesi√≥n 2: Preparando los datos",
    "section": "Transformaciones estad√≠sticas:",
    "text": "Transformaciones estad√≠sticas:\n\n1. Outliers\nUn valor at√≠pico, tambi√©n conocido como valor outlier en ingl√©s, es un punto de datos que difiere significativamente del patr√≥n general de los dem√°s datos en un conjunto. Estos valores son inusuales en relaci√≥n con el resto de la distribuci√≥n de los datos y pueden ser considerablemente m√°s altos o m√°s bajos que los valores t√≠picos del conjunto.\nLa identificaci√≥n de valores at√≠picos es importante por varias razones: 1. Calidad de los datos 2. Impacto en estad√≠sticas y modelos 3. Anomal√≠as y problemas reales 4. Toma de decisiones informada\nVeamos un ejemplo:\n\nimport pandas as pd\nimport numpy as np\n\n# Crear un conjunto de datos con valores at√≠picos\ndata = [10, 15, 20, 25, 30, 35, 200, 40, 45, 50]\n\n# Crear un DataFrame\ndf = pd.DataFrame({'Valor': data})\n\n# Calcular el rango intercuart√≠lico (IQR)\nQ1 = df['Valor'].quantile(0.25)\nQ3 = df['Valor'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Definir los l√≠mites para identificar valores at√≠picos\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Identificar valores at√≠picos\noutliers = df[(df['Valor'] &lt; lower_limit) | (df['Valor'] &gt; upper_limit)]\n\nprint(\"Valores at√≠picos:\")\nprint(outliers)\n\nValores at√≠picos:\n   Valor\n6    200\n\n\n\n\n2. Estandarizaci√≥n de Datos\nLa estandarizaci√≥n es un proceso importante en el an√°lisis de datos y el aprendizaje autom√°tico. Consiste en transformar los valores de una variable de manera que tengan una media de cero y una desviaci√≥n est√°ndar de uno. Esta transformaci√≥n se logra mediante la f√≥rmula:\n\\[ z = \\frac{x - \\mu}{\\sigma} \\]\nDonde $ x $ es el valor original, $ $ es la media de los valores y $ $ es la desviaci√≥n est√°ndar.\nLa estandarizaci√≥n es especialmente √∫til cuando se tienen variables con diferentes escalas y distribuciones, ya que permite compararlas de manera equitativa y mejorar la eficacia de los algoritmos de aprendizaje autom√°tico al mitigar el impacto de las diferencias en las magnitudes de las caracter√≠sticas.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular la media y la desviaci√≥n est√°ndar\nmean = df['Valor'].mean()\nstd_dev = df['Valor'].std()\n\n# Estandarizaci√≥n de los datos utilizando Pandas\ndf['Valor_estandarizado'] = (df['Valor'] - mean) / std_dev\n\nprint(df)\n\n   Valor  Valor_estandarizado\n0     10            -1.460593\n1     15            -1.095445\n2     20            -0.730297\n3     25            -0.365148\n4     30             0.000000\n5     35             0.365148\n6     40             0.730297\n7     45             1.095445\n8     50             1.460593\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Calcular la media y la desviaci√≥n est√°ndar\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Estandarizaci√≥n de los datos\nstandardized_data = [(x - mean) / std_dev for x in data]\n\nprint(\"Datos originales:\", data)\nprint(\"Datos estandarizados:\", standardized_data)\n\nDatos originales: [10, 15, 20, 25, 30, 35, 40, 45, 50]\nDatos estandarizados: [-1.5491933384829668, -1.161895003862225, -0.7745966692414834, -0.3872983346207417, 0.0, 0.3872983346207417, 0.7745966692414834, 1.161895003862225, 1.5491933384829668]\n\n\n\n\n\n\n\n3. Normalizaci√≥n de Datos\nLa normalizaci√≥n es un proceso esencial en el an√°lisis de datos y el aprendizaje autom√°tico. Consiste en ajustar los valores de una variable para que se encuentren dentro de un rango espec√≠fico, generalmente entre 0 y 1. La f√≥rmula matem√°tica utilizada para la normalizaci√≥n es:\n\\[ x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\]\nDonde $ x $ es el valor original y $x_{} $ es el valor normalizado.\nLa normalizaci√≥n es beneficiosa para igualar la escala de las variables y mejorar el rendimiento de los modelos que son sensibles a la magnitud de las caracter√≠sticas.\n\nPandasNumpy\n\n\n\nimport pandas as pd\n\n# Crear un DataFrame con datos\ndata = {'Valor': [10, 15, 20, 25, 30, 35, 40, 45, 50]}\ndf = pd.DataFrame(data)\n\n# Calcular los valores m√≠nimo y m√°ximo\nmin_value = df['Valor'].min()\nmax_value = df['Valor'].max()\n\n# Aplicar la normalizaci√≥n min-max\ndf['Valor_normalizado'] = (df['Valor'] - min_value) / (max_value - min_value)\n\nprint(df)\n\n   Valor  Valor_normalizado\n0     10              0.000\n1     15              0.125\n2     20              0.250\n3     25              0.375\n4     30              0.500\n5     35              0.625\n6     40              0.750\n7     45              0.875\n8     50              1.000\n\n\n\n\n\nimport numpy as np\n\n# Datos originales\ndata = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Calcular los valores m√≠nimo y m√°ximo\nmin_value = np.min(data)\nmax_value = np.max(data)\n\n# Aplicar la normalizaci√≥n min-max utilizando NumPy\nnormalized_data = (data - min_value) / (max_value - min_value)\n\nprint(\"Datos originales:\", data)\nprint(\"Datos normalizados:\", normalized_data)\n\nDatos originales: [10 15 20 25 30 35 40 45 50]\nDatos normalizados: [0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]"
  },
  {
    "objectID": "sesion1.html",
    "href": "sesion1.html",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "",
    "text": "Slides Sesi√≥n 1"
  },
  {
    "objectID": "sesion1.html#detalles",
    "href": "sesion1.html#detalles",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Detalles",
    "text": "Detalles\nNotas detalladas de la sesi√≥n 1, curso an√°lisis de datos, mag√≠ster en Data Science Universidad del Desarrollo.\nFecha: 19 agosto 2023. Versi√≥n 1"
  },
  {
    "objectID": "sesion1.html#objetivos-de-aprendizaje-de-la-sesi√≥n",
    "href": "sesion1.html#objetivos-de-aprendizaje-de-la-sesi√≥n",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Objetivos de aprendizaje de la sesi√≥n",
    "text": "Objetivos de aprendizaje de la sesi√≥n\n\nComprender el papel del proceso de adquisici√≥n y almacenamiento en un proyecto de an√°lisis de datos, junto a buenas pr√°cticas que promuevan la transparencia y replicabilidad.\nAprender a formular preguntas y plantear hip√≥tesis que puedan ser abordadas mediante el an√°lisis de datos.\nDesarrollar la habilidad de realizar pruebas de hip√≥tesis y comprender la interpretaci√≥n de sus resultados."
  },
  {
    "objectID": "sesion1.html#contenidos",
    "href": "sesion1.html#contenidos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Contenidos:",
    "text": "Contenidos:\n\nEl proceso de an√°lisis de datos\n\nEl proceso de an√°lisis de datos\n\nUna visi√≥n general a las metodolog√≠as de an√°lisis que veremos en el curso\nAdquision y almacenmiento de los datos\nPreparaci√≥n de los datos\n\nPreguntando a los datos\n\nAsbtrayendo la realidad, variables aleatorias y probabilidades.\nPlanteamiento de preguntas.\nPreguntas y respuestas: el rol de las hip√≥tesis.\n\n\n\n\nRespondiendo desde los datos: Pruebas de hip√≥tesis\n\nConceptos B√°sicos de Pruebas de Hip√≥tesis:\n\nDefinici√≥n de hip√≥tesis nula y alternativa.\nNiveles de significancia y p-values.\nErrores tipo I y tipo II.\n\nTipos de Pruebas de Hip√≥tesis:\n\nPruebas t para comparaci√≥n de medias.\nPruebas chi-cuadrado para variables categ√≥ricas.\nPruebas ANOVA para comparaci√≥n de m√∫ltiples grupos.\n\nInterpretaci√≥n de Resultados:\n\nEvaluaci√≥n de p-values y toma de decisiones.\nSignificaci√≥n estad√≠stica vs.¬†significaci√≥n pr√°ctica.\nComunicaci√≥n de los resultados de las pruebas de hip√≥tesis.\n\n\n\n\nBuenas pr√°cticas en an√°lisis de datos\n\nDesaf√≠os y Consideraciones:\n\nPrivacidad y seguridad de los datos.\nLimpieza y transformaci√≥n durante la preparaci√≥n de datos.\n\nReproducibilidad y Control de Versiones (GIT):\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos."
  },
  {
    "objectID": "sesion1.html#el-proceso-de-an√°lisis-de-datos-1",
    "href": "sesion1.html#el-proceso-de-an√°lisis-de-datos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "El proceso de an√°lisis de datos",
    "text": "El proceso de an√°lisis de datos\nEn el mundo actual, la generaci√≥n y recopilaci√≥n de datos se ha vuelto m√°s accesible y significativa que nunca antes. Esta abundancia de informaci√≥n ofrece la oportunidad de extraer conocimientos valiosos que pueden influir en la toma de decisiones y el desarrollo de soluciones eficientes.\nSin embargo, el proceso de transformar estos datos crudos en informaci√≥n √∫til y significativa requiere una serie de pasos fundamentales que forman parte integral de la disciplina conocida como Ciencia de Datos.\n\nEn esta primera parte, daremos un vistazo general a las metodolog√≠as y enfoques clave que exploraremos a lo largo del curso, con √©nfasis en la importancia de la preparaci√≥n de los datos.\nEl proceso de an√°lisis de datos se puede dividir en varias etapas interconectadas, cada una con su propio conjunto de desaf√≠os y consideraciones.\n\nBajo esta mirada, tenemos varias fases clave que est√°n interconectadas. En este curso nos enfocaremos en la preparaci√≥n de los datos y en su an√°lisis mediante modelos de regresi√≥n. Esto con el objetivo de responder preguntas desde los datos, que provean informaci√≥n valiosa.\n\nAdquisici√≥n de datos:\nEl primer paso en el proceso de an√°lisis de datos implica la adquisici√≥n y el almacenamiento de los datos. Esto se refiere a la recolecci√≥n de los datos necesarios para abordar una pregunta o problema en particular.\nPuede implicar la recopilaci√≥n de datos de fuentes diversas, como bases de datos, archivos CSV, p√°ginas web o incluso sensores en tiempo real.\nEs crucial comprender c√≥mo recopilar y almacenar estos datos de manera adecuada, garantizando su calidad, integridad y seguridad.\nExisten tantas fuentes de datos, como podr√≠amos imaginar. ALgunas de las m√°s comunes son las siguientes:\n\nEncuestas y Cuestionarios:\n\nDise√±o y administraci√≥n de encuestas para recopilar datos directamente de los participantes.\nPermite obtener informaci√≥n espec√≠fica y detallada seg√∫n las preguntas planteadas.\n\nExperimentos Controlados:\n\nDise√±o de experimentos para recopilar datos bajo condiciones controladas.\n√ötil para establecer relaciones causales y evaluar efectos de cambios controlados.\n\nObservaci√≥n y Sensores:\n\nUso de sensores y dispositivos para capturar datos en tiempo real.\nAmpliamente utilizado en aplicaciones IoT (Internet of Things) para monitorizar y recopilar informaci√≥n ambiental.\nUtilizaci√≥n de sensores en dispositivos m√≥viles y wearables para recopilar datos de ubicaci√≥n, salud y actividad.\n\nRecopilaci√≥n de Datos Existentes:\n\nUtilizaci√≥n de datos ya recopilados y disponibles en bases de datos o fuentes p√∫blicas.\nReduce el tiempo y costo de recopilaci√≥n, pero puede tener limitaciones en t√©rminos de calidad y relevancia.\n\nWeb Scraping (Web Scrapping):\n\nExtracci√≥n de datos de sitios web utilizando herramientas y t√©cnicas automatizadas.\nPermite recopilar informaci√≥n no estructurada de manera eficiente, pero requiere atenci√≥n a la √©tica y t√©rminos de uso.\n\nAcceso a APIs (Application Programming Interfaces):\n\nInteracci√≥n program√°tica con sistemas y servicios para obtener datos en tiempo real.\nCom√∫n en la obtenci√≥n de datos de redes sociales, informaci√≥n clim√°tica, finanzas, entre otros.\n\nColaboraci√≥n y Participaci√≥n Comunitaria:\n\nColaboraci√≥n con comunidades y grupos para recopilar datos de manera colectiva.\nPuede ser √∫til para proyectos de mapeo colaborativo, ciencia ciudadana y recopilaci√≥n de informaci√≥n local.\n\nData Lakes y Almacenamiento en la Nube:\n\nAlmacenamiento de grandes vol√∫menes de datos sin estructura definida en sistemas de almacenamiento en la nube.\nFacilita la recopilaci√≥n y posterior an√°lisis de datos heterog√©neos.\nUsualmente se accede a trav√©s de querys SQL\n\n\n\n\n\n\n\n\nDatos disponibles para el proyecto\n\n\n\nEn nuestro proyecto vamos a usar datos de tres posibles fuentes:\n\nDatos p√∫blicos sobre educaci√≥n chilena\nDatos p√∫blicos sobre adjudicaciones municipales\nDatos publicos sobre individuos en comunas chilenas (encuesta Casen)\nDatos sobre crecimiento de paises y complejidad econ√≥mica\n\nVeamos como acceder algunos de estos datos.\n\n\nEjemplo: Datos p√∫blicos sobre individuos en comunas chilenas (encuesta Casen)\nLa Encuesta de Caracterizaci√≥n Socioecon√≥mica Nacional (CASEN) es una investigaci√≥n realizada en Chile que tiene como objetivo principal recopilar informaci√≥n detallada sobre la situaci√≥n socioecon√≥mica de los hogares y las personas en el pa√≠s. Esta encuesta se lleva a cabo de manera peri√≥dica y abarca una amplia variedad de temas, como ingresos, educaci√≥n, empleo, salud, vivienda y otros aspectos relevantes para comprender la realidad socioecon√≥mica de la poblaci√≥n chilena. La informaci√≥n recopilada en la Encuesta CASEN se utiliza para informar pol√≠ticas p√∫blicas, tomar decisiones informadas y analizar la evoluci√≥n de indicadores sociales a lo largo del tiempo.\nSitio Web oficial\nSi tenemos los datos alojados en una dependencia, simplemente los cargamos. El formato mas comun es .csv, pero a veces estan en formatos extra√±os. Por ejemplo, .dta de STATA.\n\nimport pandas as pd\n\ndf_casen2020= pd.read_stata(\"https://github.com/melanieoyarzun/taller_seriestiempo_IDS/blob/b0be4e78a8c7a738e41b284a65d350179abbda96/Data/casen_2020_ingresos.dta?raw=true\")\n\ndf_casen2020.head(5)\n\n\n\n\n\n\n\n\nfolio\no\nid_persona\nregion\ncomuna\nzona\nexpr\nedad\nsexo\ntot_per\n...\nesc2\neduc\no1\nyaut\nyauth\nyautcor\nyautcorh\nytrabajocor\nytrabajocorh\nyae\n\n\n\n\n0\n1.101100e+11\n1\n5\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n34\nMujer\n2\n...\n12.0\nMedia humanista completa\nNo\n220000.0\n300000\n220000.0\n300000\n150000.0\n150000.0\n240586.0\n\n\n1\n1.101100e+11\n2\n6\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n4\nMujer\n2\n...\nNaN\nSin educaci√≥n formal\nNaN\n80000.0\n300000\n80000.0\n300000\nNaN\n150000.0\n240586.0\n\n\n2\n1.101100e+11\n2\n31\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n5\nMujer\n3\n...\nNaN\nB√°sica incompleta\nNaN\n25000.0\n941583\n25000.0\n941583\nNaN\n891583.0\n439170.0\n\n\n3\n1.101100e+11\n1\n32\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n45\nHombre\n3\n...\n15.0\nT√©cnico nivel superior incompleta\nS√≠\n889500.0\n941583\n889500.0\n941583\n889500.0\n891583.0\n439170.0\n\n\n4\n1.101100e+11\n3\n30\nRegi√≥n de Tarapac√°\nIquique\nUrbano\n67\n19\nMujer\n3\n...\nNaN\nNo sabe\nNo\n27083.0\n941583\n27083.0\n941583\n2083.0\n891583.0\n439170.0\n\n\n\n\n5 rows √ó 22 columns\n\n\n\nUna vez cargados los datos, debemos proceder a su limpieza y exploraci√≥n, para ser preparados para analizarlos. De esto se tratar√° la siguiente sesi√≥n del curso.\nEjemplo: Datos desde la API del banco mundial Primero siga este ejemplo practico de importar datos, luego ser√° facil responder la pregunta anterior.\n\n#pandas remote data access support for calls to the World Bank Indicators API\n\nfrom pandas_datareader import data, wb # para instalar: conda install pandas-datareader  o  pip install pandas-datareader\n\n#Revisemos que indicadores hay disponibles. En este caso revisare de PIB (GDP en ing√©s), pero se pueden explorar muchas m√°s opciones.\n\nwb.search('gdp')\n\n\n\n\n\n\n\n\nid\nname\nunit\nsource\nsourceNote\nsourceOrganization\ntopics\n\n\n\n\n688\n6.0.GDP_current\nGDP (current $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n689\n6.0.GDP_growth\nGDP growth (annual %)\n\nLAC Equity Lab\nAnnual percentage growth rate of GDP at market...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n690\n6.0.GDP_usd\nGDP (constant 2005 $)\n\nLAC Equity Lab\nGDP is the sum of gross value added by all res...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n691\n6.0.GDPpc_constant\nGDP per capita, PPP (constant 2011 internation...\n\nLAC Equity Lab\nGDP per capita based on purchasing power parit...\nb'World Development Indicators (World Bank)'\nEconomy & Growth\n\n\n1503\nBG.GSR.NFSV.GD.ZS\nTrade in services (% of GDP)\n\nWorld Development Indicators\nTrade in services is the sum of service export...\nb'International Monetary Fund, Balance of Paym...\nEconomy & Growth ; Private Sector ; Trade\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16704\nUIS.XUNIT.GDPCAP.23.FSGOV\nInitial government funding per secondary stude...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16705\nUIS.XUNIT.GDPCAP.23.FSHH\nInitial household funding per secondary studen...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n16706\nUIS.XUNIT.GDPCAP.3.FSGOV\nInitial government funding per upper secondary...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16707\nUIS.XUNIT.GDPCAP.5T8.FSGOV\nInitial government funding per tertiary studen...\n\nEducation Statistics\nTotal general (local, regional and central, cu...\nb'UNESCO Institute for Statistics'\n\n\n\n16708\nUIS.XUNIT.GDPCAP.5T8.FSHH\nInitial household funding per tertiary student...\n\nEducation Statistics\nTotal payments of households (pupils, students...\nb'UNESCO Institute for Statistics'\n\n\n\n\n\n540 rows √ó 7 columns\n\n\n\n\n# Obtengamos la lista de paises disponibles\ncountries=wb.get_countries()\n\n#Preview primeras filas lista de paises\ncountries[:5]\n\n\n\n\n\n\n\n\niso3c\niso2c\nname\nregion\nadminregion\nincomeLevel\nlendingType\ncapitalCity\nlongitude\nlatitude\n\n\n\n\n0\nABW\nAW\nAruba\nLatin America & Caribbean\n\nHigh income\nNot classified\nOranjestad\n-70.0167\n12.5167\n\n\n1\nAFE\nZH\nAfrica Eastern and Southern\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n2\nAFG\nAF\nAfghanistan\nSouth Asia\nSouth Asia\nLow income\nIDA\nKabul\n69.1761\n34.5228\n\n\n3\nAFR\nA9\nAfrica\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n4\nAFW\nZI\nAfrica Western and Central\nAggregates\n\nAggregates\nAggregates\n\nNaN\nNaN\n\n\n\n\n\n\n\nObservar que este es un data frame con dos √≠ndices: pais y a√±o. Para mayor referencia coo tratar este tipo de datos ver en https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\nObtengamos un data frame con los datos de Chile, entre 1980 y 2020.\n\n#sabemos que queremos Chile, asi que busquemos su info\n\ncountries[ countries['name'] == 'Chile' ]\n\n# Descarguemos la data desde la API del banco mundial a un dataframe\n\ndf_GPDpc_Chile = wb.download(\n                    #Use the indicator attribute to identify which indicator or indicators to download\n                    indicator='NY.GDP.PCAP.KD',\n                    #Use the country attribute to identify the countries you want data for\n                    country=['CL'],\n                    #Identify the first year for which you want the data, as an integer or a string\n                    start='1980',\n                    #Identify the last year for which you want the data, as an integer or a string\n                    end=2020\n                )\n\ndf_GPDpc_Chile.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nMultiIndex: 41 entries, ('Chile', '2020') to ('Chile', '1980')\nData columns (total 1 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   NY.GDP.PCAP.KD  41 non-null     float64\ndtypes: float64(1)\nmemory usage: 2.0+ KB\n\n\n\n#Veamos el data frame\ndf_GPDpc_Chile.head()\n\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\ncountry\nyear\n\n\n\n\n\nChile\n2020\n12741.157507\n\n\n2019\n13761.374474\n\n\n2018\n13906.770558\n\n\n2017\n13615.523858\n\n\n2016\n13644.623261\n\n\n\n\n\n\n\nSi quisieramos, por simplicidad quedarnos solo con el indice del a√±o y reordenar el dataframe:\n\ndf_GPDpc_Chile.droplevel('country')\n\n\nreversed_df = df_GPDpc_Chile.iloc[::-1] #invertimos el dataframe \nreversed_df= reversed_df.droplevel('country') # removemos el level pais, ya que todo el an√°lisis es para un solo pa√≠s\nreversed_df.head(5)\n\n\n\n\n\n\n\n\nNY.GDP.PCAP.KD\n\n\nyear\n\n\n\n\n\n1980\n4694.337113\n\n\n1981\n4928.563103\n\n\n1982\n4322.647868\n\n\n1983\n4047.790234\n\n\n1984\n4154.496068\n\n\n\n\n\n\n\nAhora, realicemos un grafico r√°pido con nuestros datos:\n\n# Graficamos\n\nax = df_GPDpc_Chile['1980':].plot(legend=False) \nax.set_ylabel(r'GDP')\nax.set_xlabel(r'A√±o')\n\nText(0.5, 0, 'A√±o')\n\n\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 1 - Bajando y formateando datos del Banco Mundial**\n\n\n\nReplique el ejemplo pr√°ctico de importar datos desde la API del Banco Mundial y empezar la base para su an√°lisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro pa√≠s serie desde la API del Banco mundial, muestre sus principales caracter√≠sticas y realice un grafico.\n¬øpareciera haber tendencias?\n\n\n\n\nPreguntando a los datos\n¬øC√≥mo plantear preguntas y formular hip√≥tesis en el contexto del an√°lisis de datos? El proceso de an√°lisis comienza con la curiosidad y/o necesidad.cLa formulaci√≥n de preguntas relevantes que se puedan responder mediante la exploraci√≥n y el examen de los datos disponibles.\nInicia con la identificaci√≥n de √°reas de inter√©s y la formulaci√≥n de preguntas espec√≠ficas relacionadas con esos temas. Estas preguntas pueden surgir de la necesidad de resolver un problema, entender un fen√≥meno o explorar patrones en los datos. Un buen planteamiento de preguntas es crucial, ya que guiar√° todo el proceso de an√°lisis.\n\n\n\nEl proceso de abstraer la realidad\n\n\nPreguntas e hip√≥tesis:\nUna hip√≥tesis es una afirmaci√≥n, verificable con evidencia. En este sentido, para toda pregunta podemos responderla mediante hip√≥tesis.\nEn particular, para responder a las preguntas en el contexto de datos, es com√∫n formular hip√≥tesis nulas y alternativas.\nLa hip√≥tesis nula es aquella que propone que algun par√°metro toma cierto valor. Este generlamente es un punto de verdad. Si bien, con datos no podemos corroborar que algo es cierto, si podemos dar evidencia de que no es cierto. En general, planteamos el problema de tal manera que podamos rechazar la hip√≥tesis nula, en favor de otra que llamamos alternatiba.\nQuizas, la hipotesis nula m√°s famosa es la prueba de ‚Äúsignificancia‚Äù. En esta se propone que un par√°metro (muchas veces un efecto, o correlaci√≥n) es 0, es decir, plantea que no hay efecto o relaci√≥n entre las variables, mientras que la hip√≥tesis alternativa sugiere que s√≠ existe una relaci√≥n o efecto significativo.\nEstas hip√≥tesis son fundamentales para establecer una base objetiva para el an√°lisis y para evaluar las evidencias encontradas en los datos. El proceso de plantear preguntas y formular hip√≥tesis es el primer paso en el an√°lisis de datos, ya que establece una gu√≠a clara para el enfoque y la direcci√≥n del trabajo. Al identificar preguntas y establecer hip√≥tesis, se crea un marco s√≥lido que orientar√° la exploraci√≥n y el an√°lisis de los datos disponibles.\n\n\n\n\n\n\nTaller 1: Pregunta 2 - Investigando sobre pa√≠ses:\n\n\n\nConsidere que tenemos los datos del banco mundial, del pa√≠s que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigaci√≥n que se pueda responder con los datos disponibles. ¬øC√≥mo definiria la variable aleatoria relevante? ¬øQu√© hip√≥tesis podria responder su pregunta?"
  },
  {
    "objectID": "sesion1.html#respondiendo-desde-los-datos",
    "href": "sesion1.html#respondiendo-desde-los-datos",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Respondiendo desde los datos",
    "text": "Respondiendo desde los datos\n\nInferencia estad√≠stica\nInferencia se refiere al proceso de hacer generalizaciones de una poblaci√≥n a partir de una muestra de esa poblaci√≥n. En particular, la idea es que si tenemos un conjunto de datos (muestra) obtenido de una poblaci√≥n m√°s grande, el cual es representativo de esta, podemos utilizar m√©todos estad√≠sticos para sacar conclusiones sobre las caracter√≠sticas y propiedades de esa poblaci√≥n en su totalidad.\n\n\n\nPoblaci√≥n y Muestra\n\n\nEl proceso de inferencia estad√≠stica se basa en el principio de que una muestra bien seleccionada puede proporcionar informaci√≥n valiosa sobre la poblaci√≥n en general. Mediante el an√°lisis de la muestra, podemos estimar par√°metros poblacionales, como la media, la proporci√≥n o la desviaci√≥n est√°ndar, y tambi√©n podemos construir intervalos de confianza para estimar el rango dentro del cual se espera que se encuentren estos par√°metros.\nEl uso de la inferencia estad√≠stica es fundamental, especialmente si es impracticable o costoso analizar cada elemento de una poblaci√≥n en particular. Por ejemplo, en lugar de encuestar a todos los ciudadanos de un pa√≠s, es mucho m√°s factible encuestar una muestra representativa y utilizar esa informaci√≥n para hacer suposiciones sobre la opini√≥n de la poblaci√≥n en general.\n\nEstad√≠grafos y el Teorema del L√≠mite central\nEntonces, en cada muestra que tenemos podemos calcular aproximaciones a los par√°metros poblacionales de inter√©s. Estos son los llamados estad√≠sgrafos \nDado que por cada muestra que tenemos, vamos a calcular un estad√≠grafo este es en si mismo una variable aleatoria. Tiene su propia distribuci√≥n, media y varianza!\n\nEl estad√≠grafo m√°s conocido es el promedio o media muestral.\n\n\n\nEstadigrafos m√°s comunes\n\n\nCada estimador es una funci√≥n de la muestra, por ende para cada muestra que tengamos obtendremos un valor num√©rico espec√≠fico para el estimador. Por este motivo, cuando estamos trabajando con una √∫nica muestra espec√≠fica, tenemos un √∫nico valor del estimador, o estimador puntual.\nNunca (o casi nunca) podemos conocer el valor verdadero de los par√°metros en la poblaci√≥n, por lo cual un primer camino tentador es usar el estimador puntual para tomar una decisi√≥n. Como nunca podemos conocer el verdadero par√°metro, tampoco podemos saber a ciencia cierta si el estimador puntual es cercano a este.\n¬øC√≥mo conectamos estadpigrafos y par√°metros?\nEl teorema del l√≠mite central, nos dice que, bajo ciertas condiciones, la distribuci√≥n de las medias muestrales de una poblaci√≥n se aproxima a una distribuci√≥n normal a medida que el tama√±o de la muestra aumenta, independientemente de la forma de la distribuci√≥n original de la poblaci√≥n. Este teorema es esencial en inferencia estad√≠stica y tiene amplias aplicaciones en an√°lisis de datos y toma de decisiones.\n\n\n\nLa media muestral se distribuye normal, sin importar la distribuci√≥n de la variable subyacente\n\n\nFormalmente, el Teorema del L√≠mite Central establece lo siguiente:\n\\[\\bar{x} \\sim_a N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) \\]\nSupongamos que tenemos una poblaci√≥n con media Œº y desviaci√≥n est√°ndar œÉ finitas. Si tomamos muestras aleatorias de tama√±o n de esta poblaci√≥n y calculamos la media muestral de cada muestra, entonces, a medida que n tiende a infinito, la distribuci√≥n de estas medias muestrales se aproximar√° a una distribuci√≥n normal con media Œº y desviaci√≥n est√°ndar œÉ/‚àön.\nEn otras palabras, sin importar la distribuci√≥n original de la poblaci√≥n, cuando el tama√±o de la muestra es suficientemente grande, la distribuci√≥n de las medias muestrales seguir√° una forma de campana similar a la distribuci√≥n normal. Este resultado es fundamental para realizar inferencias sobre la poblaci√≥n a partir de muestras, ya que nos permite aplicar m√©todos basados en la distribuci√≥n normal incluso cuando la poblaci√≥n original no sigue una distribuci√≥n normal.\n\n\nError est√°ndar\n\nCorresponde a un estimador de la desviaci√≥n est√°ndar del estimador.\nIdentifica que tan lejos estamos del verdadero valor poblacional.\nPara la media muestral:\n\n\\[ SE = \\frac{S_y}{\\sqrt{n}}\\]\nSe utiliza para evaluar a los estimadores, mediante pruebas de hipotesis y construir intervalos de confianza\n\nSi se conoce un estimador y su desviaci√≥n est√°ndar, podemos saber qu√© tan precisa es la estimaci√≥n (mucha o poca varianza), pero no podemos saber si el estimador est√° cercano o no a su valor verdadero en la poblaci√≥n (el cual no conocemos).\nNunca (o casi nunca) podemos conocer el valor verdadero de los par√°metros en la poblaci√≥n.\nS√≠ se puede construir un conjunto de valores que contienen el par√°metro poblacional con alguna probabilidad (llamada el nivel de confianza).\nUn intervalo de confianza contiene los posibles valores del estimador, entre un l√≠mite inferior y un l√≠mite superior, con cierta probabilidad.\n\n\n\nInferencia sobre Estad√≠grafos y par√°metros - Conectados por el Teorema del L√≠mite central\n\\[ \\bar{x} \\sim_a N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\nCon este teorema, podemos construir inferencia de a partir de {x} indirectamente.\n\nIntervalos de confianza\nPruebas de hip√≥tesis\np-valor\n\n\n\n\n\nIntervalos de confianza\nUna primera manera de aproximarnos a los par√°metros poblacionales (particularmente a la esperanza) es mediante la construcci√≥n de intervalos de confianza.\n\nUn intervalo de confianza contiene los posibles valores del estimador, entre un l√≠mite inferior y un l√≠mite superior, con cierta probabilidad.\n\n¬øDe d√≥nde saco los valores cr√≠ticos?\nLos valores cr√≠ticos de una distribuci√≥n los obtenemos de una tabla de distribuci√≥n o para calcular podemos usar excel, R o en python:\nscipy.stats.t.isf(alpha, n-p)\nSi estamos trabajando con dos colas, usar alpha/2 porque la probabilidad de error la estamos repartiendo a ambas colas.\n\n\n\n\n\n\n[Matem√°ticamente] Caso 1: Varianza conocida\n\n\n\nSumpongamos que tenemos una muestra aleatoria: \\(y_1, y_2, \\dots, y_n\\) de una poblaci√≥n \\(Y\\sim N(\\mu, \\sigma^2)\\)\n\nLa media muestral \\(\\bar{y}= \\frac{1}{n}\\sum_{i=1}{n}y_i\\)\n\nSu esperanza es: \\(E(\\bar{y}) =\\mu\\)\nSu varianza es: \\(var(\\bar{y}) = \\frac{\\sigma^2}{n}\\)\nse distribuye normal, tal que podemos estandarrizar: $ N(0,1) $\n\n\nEntonces, podemos describir que:\n$ P( -1.96 &lt; &lt; 1.96 ) = 0.95 $$\n$ P( {y}- &lt; &lt; {y} + ) = 0.95 $\n\neste intervalo es aleatorio, porque \\(\\bar{y}\\) es diferente en cada muestra.\npara el 95% de las muestras elatorias, el intervalo construido de esta manera contendr√° a \\(\\mu\\)\n\n\n\n\n\n\n\n\n\n[Matem√°ticamente] Caso 2: Varianza desconocida\n\n\n\n\nSupongamos que tenemos una muestra aleatoria \\(y_1, y_2, \\dots, y_n\\) de una poblaci√≥n \\(y\\sim N(\\mu, \\sigma^2 )\\)\nUsamos la estimaci√≥n de la desviaci√≥n est√°ndar muestral: \\[ S_y = \\sqrt{\\frac{1}{n-1} \\sum{i=1}{n}(y_i - \\bar{y})^2} \\]\nY si estandarizamos \\(\\bar{y}\\): \\[ \\frac{\\bar{Y} - \\mu_y }{ \\frac{S}{\\sqrt{n}}} \\sim t_{n-1} \\]\nPodeos constrir un intervalo de la porbabilidad de estar al 95 con el valor critico c adecuado a los grados de libertad n-1:\n\n\\[ P( -c &lt;   \\frac{\\bar{Y} - \\mu_y }{ \\frac{ S}{\\sqrt{n}}}  &lt; c  ) =0.95  \\]\n\\[ P(\\bar{y} - \\frac{c\\times S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) 0.95\\]\n\nSi llamamos al error estandar SE: \\(SE(\\bar{y})=\\frac{S}{\\sqrt{n}}\\)\nEl IC es: \\[ (\\bar{y} - \\frac{c\\times S}{\\sqrt{n}}, \\bar{y} + \\frac{c\\times S}{\\sqrt{n}} ) \\]\n\n\n\n\n\nEl intervalo es una muestra aleatoria\nEsto quiere decir que para cada muestra podemos construir un intervalo.\nAs√≠ como el estimador es una variable aleatoria, esto tambi√©n es cierto para los intervalos de confianza. Por eso tambi√©n se les llama intervalos aleatorios, ya que con diferentes muestras obtendremos un diferente estimador e intervalo.\nPor ende, supongamos que contamos con 20 muestras, entonces construiremos 20 intervalos de confanza diferentes para los 20 estimadores puntuales.\n\nInterpretaci√≥n de intervalo de confianza\nPensemos en un 95% de confianza (un valor usual). Esto quiere decir, que si se repitiera este ejercicio muchas veces y construy√©ramos un intervalo de esta forma el 95% de ellos contendr√≠a el verdadero par√°metro poblacional.\nUn elemento importante a considerar es que esto no significa que con 95% de certeza el par√°metro est√° exactamente en estos valores. Por ejemplo, al 95% de confianza con 20 intervalos 19 contendr√°n el par√°metro.\n\n\n\nPruebas de hip√≥tesis\n\nUna forma de verificar hipotesis sobre los par√°metros es mediante el contraste de hip√≥tesis.\n\nEmpezamos suponiendo que hay una distribuci√≥n conocida para el estad√≠grafo, centrada en un valor espec√≠fico. Y nos preguntamos, si esto fuea verdad ¬øqu√© tan probable es la muestra que tengo?\n\nLlamamos la hip√≥tesis a probar Ho, y su alternativa H1.\n\n\n\n\n\nErrores y P-valor\nAsociada esta prueba, entonces, hay asociados dos tipos de errores:\n\n\nTipo I: Rechazar Ho cuando es cierta\nTipo II: No rechazar Ho cuando es falsa.\n\n\n\n\nSe elige nivel de significancia de contraste (Œ±) = probabilidad de cometer error Tipo I. T√≠picamente Œ± = 0,01, 0,05, 0,10.\nPara contrastar una hip√≥tesis con su alternativa, debemos elegir:\n\nUn estad√≠stico de contraste\nUna regla de rechazo, la cual depende de un valor cr√≠tico.\n\n\n\n\n\nPrueba de significancia\nDefinimos la prueba de hip√≥tesis de significancia como aquella que indica si un estimador \\(\\hat{T}\\) es 0.\n\\[ H_0: T =0\\text{ vs }H_1: T \\neq 0 \\]\nEl Valor de probabilidad (√≥ p-valor) es el nivel probabilidad m√°s alto para el cual no podemos rechazar la hip√≥tesis nula de la prueba de significancia.\nEjemplo: $H_0: $ y en la muestra especifica t= 1.52:\n\\[ P-valor = P(T&gt;1.52 \\vert h_0)= 1- \\phi(1.52)=0.0065\\]\n\nel mayor nivel de significancia estadistica al cual no rechazamos \\(H_0\\) es 6.5%\nla probabilidad de observar un velor \\(T\\geq 1.52\\) cuando \\(H_0\\) es cierta es en un 6.5 de las muestras.\nP-valores bajos dan evidencia en contra de \\(H_0\\), ya que la probabilidad de observarlo si \\(H_0\\) es cierta es bajo.\n\n\n\nEjemplo de aplicaci√≥n: Peso de los Ping√ºinos Palmer\nConsideremos los datos de los ping√ºinos Palmer. Los datos ‚ÄúPalmer Penguins‚Äù son un conjunto que detalla medidas morfol√≥gicas y caracter√≠sticas de tres especies de ping√ºinos: Adelie, Gentoo y Chinstrap. Recopilados por el Dr.¬†Bill Link y su equipo. (Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/index.html)\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nEn el contexto de los ping√ºinos y el peso de su poblaci√≥n, podr√≠amos tomar una muestra de ping√ºinos y calcular un intervalo de confianza para el peso promedio. Esto nos dar√≠a una estimaci√≥n del peso promedio de la poblaci√≥n total, junto con la confianza en que este valor estimado es preciso.\nEs importante tener en cuenta que el proceso de inferencia estad√≠stica se basa en suposiciones y en el uso adecuado de t√©cnicas estad√≠sticas.\nLa elecci√≥n de la muestra, la interpretaci√≥n de los resultados y el nivel de confianza seleccionado son aspectos cruciales para realizar inferencias precisas y significativas.\nRelicemos algunos ejemplos de pruebas de hip√≥tesis, sobre el peso de los ping√ºinos.\nPrimero calcularemos el promedio muestral y lo veremos en el contexto de los datos observados:\n\nimport matplotlib.pyplot as plt\n\n# Calcular el promedio del peso de los ping√ºinos\npromedio_peso = penguins['body_mass_g'].mean()\n\n# Crear un histograma de la distribuci√≥n del peso con el promedio\nplt.figure(figsize=(10, 6))\nsns.histplot(data=penguins, x='body_mass_g', bins=20, kde=True)\nplt.axvline(x=promedio_peso, color='red', linestyle='dashed', label='Promedio')\nplt.title('Distribuci√≥n de Peso de Ping√ºinos')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\nNuestra idea de la inferencia, es aprovechar las propiedades del promedio muestral. De que es el promedio muestral el que se distribuye normal, su media es la media poblacional y conocemos sus caracter√≠sticas.\nPor ejemplo, consideremos que de esta poblaci√≥n de ping√ºinos obtenemos 1000 muestras de 50 individuos cada una. Si graficamos sus medias, podremos ver que estas se distribuyen aproximadamente normal.\n\nSi reducimos el tama√±o de muestra, m√°s nos alejamos de la distribuci√≥n normal.\nSi reducimos el n√∫mero de repeticiones tambie√©.\n\n\nimport numpy as np\n\n# Definir el tama√±o de cada muestra y la cantidad de muestras\ntamano_muestra = 50\ncantidad_muestras = 10000\n\n# Crear una lista para almacenar las medias de cada muestra\nmedias_muestras = []\n\n# Realizar el muestreo y c√°lculo de medias para cada muestra\nfor _ in range(cantidad_muestras):\n    muestra = np.random.choice(penguins['body_mass_g'], size=tamano_muestra, replace=False)\n    media_muestra = np.mean(muestra)\n    medias_muestras.append(media_muestra)\n\n# Calcular el promedio de los promedios de las muestras\npromedio_promedios = np.mean(medias_muestras)\n\n# Crear el gr√°fico de las medias de las muestras\nplt.figure(figsize=(10, 6))\nplt.hist(medias_muestras, bins=20, edgecolor='black', alpha=0.7)\nplt.axvline(x=promedio_promedios, color='red', linestyle='dashed', label='Promedio de Promedios')\nplt.title('Distribuci√≥n de Medias de Muestras')\nplt.xlabel('Media de Muestra de Peso (g)')\nplt.ylabel('Frecuencia')\nplt.legend()\nplt.show()\n\n\n\n\n\nIntervalo de confianza\nObtengamos una muestra y calculemos un intervalo de confianza:\n\nimport seaborn as sns\nimport numpy as np\nimport scipy.stats as stats\n\n# Obtener una muestra simple de 40 ping√ºinos\nsample_size = 40\nsample = np.random.choice(penguins[\"body_mass_g\"], size=sample_size)\n\n# Calcular el error est√°ndar de la media muestral\nsample_std = np.std(sample, ddof=1)  # Usar ddof=1 para calcular la desviaci√≥n est√°ndar muestral\nstandard_error = sample_std / np.sqrt(sample_size)\n\n# Nivel de confianza (por ejemplo, 95%)\nconfidence_level = 0.95\n\n# Calcular el margen de error\nmargin_of_error = stats.t.ppf((1 + confidence_level) / 2, df=sample_size - 1) * standard_error\n\n# Calcular el intervalo de confianza\nsample_mean = np.mean(sample)\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint(\"Intervalo de Confianza para el Peso:\")\nprint(confidence_interval)\n\nIntervalo de Confianza para el Peso:\n(3937.394990022367, 4466.355009977633)\n\n\nEl resultado ser√° un rango de valores dentro del cual es probable que se encuentre el verdadero peso promedio de los ping√ºinos en la poblaci√≥n, con un nivel de confianza del 95%.\n¬øComo nos fue? ¬øContiene al verdadero valor?\n\n\n\nComparaciones de grupos\nAhora consideremos que tenemos grupos que queremos comparar.\nSi hacemos una grafica de distribuci√≥n de tama√±o por especie y sexo, podriamos empezar a analizar diferencias entre los grupos.\n\n# Crear la tabla de doble entrada por tipo y sexo de los pinguinos\ntabla_doble_entrada = penguins.groupby(['species', 'sex'])['body_mass_g'].agg(['mean', 'var']).reset_index()\n\n# Renombrar las columnas para mayor claridad\ntabla_doble_entrada.rename(columns={'mean': 'Promedio', 'var': 'Varianza'}, inplace=True)\n\n# Mostrar la tabla de doble entrada\ntabla_doble_entrada\n\n\n\n\n\n\n\n\nspecies\nsex\nPromedio\nVarianza\n\n\n\n\n0\nAdelie\nFemale\n3368.835616\n72565.639269\n\n\n1\nAdelie\nMale\n4043.493151\n120278.253425\n\n\n2\nChinstrap\nFemale\n3527.205882\n81415.441176\n\n\n3\nChinstrap\nMale\n3938.970588\n131143.605169\n\n\n4\nGentoo\nFemale\n4679.741379\n79286.335451\n\n\n5\nGentoo\nMale\n5484.836066\n98068.306011\n\n\n\n\n\n\n\nPodriamos querer saber si el peso es diferente para los pinguinos de la especie Adelie, para diferentes sexos:\nPregunta de Prueba de Hip√≥tesis:\n¬øExiste una diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù?\n\nHip√≥tesis Nula (H0):\n\nNo hay diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù.\n\nHip√≥tesis Alternativa (H1):\n\nExiste una diferencia significativa en el peso promedio entre los ping√ºinos machos y las ping√ºinas hembras en la especie ‚ÄúAdelie‚Äù.\nPara probar esta hip√≥tesis, podr√≠as utilizar una prueba de hip√≥tesis para comparar las medias de las muestras de peso de los ping√ºinos machos y hembras en la especie ‚ÄúAdelie‚Äù. Esto te permitir√≠a determinar si la diferencia observada en el peso promedio es lo suficientemente grande como para considerarse estad√≠sticamente significativa.\n\nimport matplotlib.pyplot as plt\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los ping√ºinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Crear un histograma para la distribuci√≥n de peso por sexo\nplt.figure(figsize=(10, 6))\nsns.histplot(data=adelie_penguins, x='body_mass_g', hue='sex', bins=20, kde=True)\nplt.title('Distribuci√≥n de Peso por Sexo para Ping√ºinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Frecuencia')\nplt.legend(title='Sexo')\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\nA simple vista podriamos pensar ambos grupos son diferentes. Es m√°s claro si dibujamos el promedio muestral observado.\n\n# Crear un gr√°fico de densidad con l√≠neas de promedio\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=adelie_penguins, x='body_mass_g', hue='sex', fill=True, common_norm=False)\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Female'], color='blue', linestyle='dashed', label='Promedio Femenino')\nplt.axvline(x=adelie_penguins.groupby('sex')['body_mass_g'].mean()['Male'], color='orange', linestyle='dashed', label='Promedio Masculino')\nplt.title('Densidad de Peso por Sexo para Ping√ºinos Adelie')\nplt.xlabel('Masa Corporal (g)')\nplt.ylabel('Densidad')\nplt.legend()\nplt.show()\n\n\n\n\nSi construimos una prueba t de diferencia de medias:\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar los ping√ºinos de la especie \"Adelie\"\nadelie_penguins = penguins[penguins['species'] == 'Adelie']\n\n# Filtrar machos y hembras\nmachos = adelie_penguins[adelie_penguins['sex'] == 'Male']\nhembras = adelie_penguins[adelie_penguins['sex'] == 'Female']\n\n# Realizar la prueba t independiente\nt_statistic, p_value = stats.ttest_ind(machos['body_mass_g'], hembras['body_mass_g'], equal_var=False)\n\n# Imprimir resultados\nprint(\"Estad√≠stica t:\", t_statistic)\nprint(\"Valor p:\", p_value)\n\n# Crear un gr√°fico de comparaci√≥n de peso\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=[machos['body_mass_g'], hembras['body_mass_g']], palette=['blue', 'pink'])\nplt.title('Comparaci√≥n de Peso entre Machos y Hembras de Ping√ºinos Adelie')\nplt.xticks([0, 1], ['Machos', 'Hembras'])\nplt.ylabel('Peso (g)')\nplt.show()\n\nEstad√≠stica t: 13.126285923485874\nValor p: 6.402319748031793e-26\n\n\n\n\n\nFinalmente, podriamos querer comparar hembras y machos de diferentes Islas. Para esto podriamos usar una prueba ANOVA.\nEn este c√≥digo, primero cargamos el conjunto de datos ‚ÄúPenguins‚Äù y luego creamos dos subconjuntos separados para machos y hembras. Despu√©s, utilizamos la funci√≥n stats.f_oneway() para realizar una prueba ANOVA para comparar los pesos entre hembras y machos. El resultado incluye la estad√≠stica F y el valor p.\nEl valor p nos indica si hay una diferencia significativa entre los grupos. Si el valor p es menor que un umbral de significancia (por ejemplo, 0.05), podr√≠amos rechazar la hip√≥tesis nula y concluir que hay una diferencia significativa en el peso entre hembras y machos de diferentes islas.\nRecuerda que, antes de realizar una prueba ANOVA, es importante verificar las suposiciones necesarias, como la normalidad y la homogeneidad de varianzas en los grupos. Si estas suposiciones no se cumplen, podr√≠a ser necesario considerar otras pruebas estad√≠sticas o transformaciones de los datos.\n\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Imprimir resultados\nprint(\"Estad√≠stica F:\", result.statistic)\nprint(\"Valor p:\", result.pvalue)\n\nEstad√≠stica F: 72.96098633250911\nValor p: 4.897246751596325e-16\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\n# Cargar el conjunto de datos \"Penguins\"\npenguins = sns.load_dataset(\"penguins\")\n\n# Filtrar machos y hembras\nmachos = penguins[penguins['sex'] == 'Male']\nhembras = penguins[penguins['sex'] == 'Female']\n\n# Realizar una prueba ANOVA\nresult = stats.f_oneway(machos['body_mass_g'], hembras['body_mass_g'])\n\n# Calcular las medias de peso por g√©nero e isla\nmedias_peso = penguins.groupby(['species', 'island', 'sex'])['body_mass_g'].mean().reset_index()\n\n# Crear un gr√°fico de barras con puntos y intervalos de confianza\nplt.figure(figsize=(10, 6))\nsns.barplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', errorbar='sd', palette=['pink', 'blue'])\n#sns.boxplot(data=medias_peso, x='species', y='body_mass_g', hue='sex', palette=['blue', 'pink'])\nplt.title('Comparaci√≥n de Peso entre Hembras y Machos por Isla')\nplt.xlabel('Especie e Isla')\nplt.ylabel('Media de Peso (g)')\nplt.legend(title='Sexo')\nplt.show()\n\n\n\n\n\n\nExperimentos Aleatorios y pruebas A/B\nUn experimento estad√≠stico es un enfoque cient√≠fico que busca establecer relaciones de causalidad y obtener conclusiones sobre c√≥mo ciertas variables afectan a otras. Los experimentos estad√≠sticos se dise√±an para manipular deliberadamente una o m√°s variables independientes y observar los efectos que tienen sobre una variable dependiente. Al controlar y manipular las variables de inter√©s, los experimentos permiten a los investigadores hacer afirmaciones m√°s s√≥lidas sobre las relaciones causales.\nUna prueba A/B, tambi√©n conocida como prueba de divisi√≥n, es una t√©cnica utilizada en la investigaci√≥n y el an√°lisis para comparar dos variantes o grupos con el fin de determinar cu√°l de ellos produce un mejor resultado en t√©rminos de rendimiento, efectividad o preferencia. En una prueba A/B, se selecciona un grupo de muestra y se divide en dos grupos, uno que experimenta la variante ‚ÄúA‚Äù (por ejemplo, una versi√≥n actual) y otro que experimenta la variante ‚ÄúB‚Äù (por ejemplo, una versi√≥n modificada). Luego, se recopilan datos y se comparan los resultados de ambos grupos para determinar cu√°l variante es m√°s efectiva. Las pruebas A/B son comunes en marketing, dise√±o de productos y desarrollo web para tomar decisiones informadas sobre mejoras y optimizaciones.\nLas pruebas A/B es son ampliamente utilizado en diversas √°reas, como el marketing, la investigaci√≥n de usuarios y el dise√±o de productos. En una prueba A/B, se seleccionan dos grupos de muestra: uno experimenta la versi√≥n original (A) y el otro experimenta una variante modificada (B). La idea detr√°s de una prueba A/B es evaluar si la variante B produce un efecto significativamente diferente en una m√©trica de inter√©s en comparaci√≥n con la variante A.\nMediante la asignaci√≥n aleatoria de los participantes a los grupos A y B, y al controlar las condiciones en las que se les presenta cada variante, se reduce la posibilidad de sesgos y se permite un an√°lisis causal m√°s confiable. Al comparar las diferencias observadas en los resultados entre los grupos A y B, es posible inferir si la variante B tiene un impacto significativo en la variable de inter√©s.\nSin embargo, es importante tener en cuenta que aunque las pruebas A/B proporcionan evidencia de asociaci√≥n causal, no garantizan que la causalidad sea absoluta. Otros factores no controlados pueden influir en los resultados. Para obtener una comprensi√≥n m√°s completa de la causalidad, los experimentos controlados aleatorizados y el uso de m√©todos de dise√±o experimental s√≥lidos son esenciales. Las pruebas A/B son una herramienta poderosa para explorar causas y efectos en condiciones controladas y analizar el rendimiento relativo de diferentes opciones.\nVeamos un ejemplo en la pr√°ctica. Este es parte del ejercicio de aplicaci√≥n."
  },
  {
    "objectID": "sesion1.html#caso-aplicaci√≥n-de-ab-testing-para-promoci√≥n-de-marketing",
    "href": "sesion1.html#caso-aplicaci√≥n-de-ab-testing-para-promoci√≥n-de-marketing",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Caso: Aplicaci√≥n de A/B testing para promoci√≥n de Marketing",
    "text": "Caso: Aplicaci√≥n de A/B testing para promoci√≥n de Marketing\n\nEnunciado\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electr√≥nicos y queremos aumentar las ventas en una l√≠nea de productos espec√≠fica, como tel√©fonos m√≥viles.\nPara ello, decidimos utilizar una promoci√≥n de ventas basada en una ruleta l√∫dica que ofrecer√° descuentos a los clientes que la utilicen.\nPara implementar la promoci√≥n, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electr√≥nico con un enlace a la ruleta l√∫dica. Al hacer clic en el enlace, los clientes son redirigidos a una p√°gina en la que pueden girar la ruleta y ganar un descuento en su pr√≥xima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoci√≥n (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\nCreaci√≥n de los datos\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste c√≥digo crear√° un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generaci√≥n de n√∫meros aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de n√∫mero de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n...\n...\n...\n...\n\n\n395\nTreatment\nTreatment 2\n1\n\n\n396\nTreatment\nTreatment 2\n2\n\n\n397\nTreatment\nTreatment 2\n1\n\n\n398\nTreatment\nTreatment 2\n3\n\n\n399\nTreatment\nTreatment 2\n2\n\n\n\n\n400 rows √ó 3 columns\n\n\n\n\n\n\n\n\n\nTaller 1: Pregunta 3 -Ejemplo AB test en Marketing:\n\n\n\nConsidere que tenemos los datos del banco mundial, del pa√≠s que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigaci√≥n que se pueda responder con los datos disponibles. ¬øC√≥mo definiria la variable aleatoria relevante? ¬øQu√© hip√≥tesis podria responder su pregunta?\nEstudiemos si la promoci√≥n fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¬øFue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¬øCual de las promociones fue m√°s efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "sesion1.html#buenas-pr√°cticas-en-an√°lisis-de-datos-1",
    "href": "sesion1.html#buenas-pr√°cticas-en-an√°lisis-de-datos-1",
    "title": "Sesi√≥n 1: Planteando y respondiendo preguntas con datos. (y un -breve- repaso a pruebas de hip√≥tesis)",
    "section": "Buenas pr√°cticas en an√°lisis de datos",
    "text": "Buenas pr√°cticas en an√°lisis de datos\n\nImportancia de la Adquisici√≥n y Almacenamiento de Datos\nLa adquisici√≥n y el almacenamiento de datos son los cimientos sobre los cuales se construye todo el proceso de an√°lisis. La calidad y la confiabilidad de los datos que obtengamos son fundamentales para asegurarnos de que los resultados y conclusiones que extraigamos sean precisos y relevantes. En esta secci√≥n, exploraremos la importancia de esta etapa y c√≥mo afecta todo el flujo de trabajo de la ciencia de datos.\nGarant√≠a de Calidad y Fiabilidad en la Obtenci√≥n de Datos: Obtener datos confiables es el primer paso para garantizar que nuestras conclusiones sean s√≥lidas. La calidad de los datos est√° relacionada con la precisi√≥n, integridad y consistencia de la informaci√≥n que recopilamos. Asegurarnos de que los datos sean precisos desde el principio minimiza la posibilidad de errores en an√°lisis posteriores. Exploraremos t√©cnicas y pr√°cticas para verificar la calidad de los datos y c√≥mo mitigar posibles fuentes de error.\nExploraci√≥n de Diferentes Fuentes de Datos y su Impacto en los Resultados: En el mundo actual, los datos provienen de diversas fuentes: bases de datos, encuestas, sensores, redes sociales, entre otros. Cada fuente tiene sus propias caracter√≠sticas y potenciales sesgos. Comprender las diferencias entre estas fuentes y c√≥mo pueden influir en los resultados es crucial para tomar decisiones informadas. Analizaremos ejemplos de c√≥mo la elecci√≥n de la fuente de datos puede afectar las conclusiones y c√≥mo evaluar la confiabilidad de las fuentes.\n\nMetodolog√≠as de Levantamiento y Adquisici√≥n de Datos:\nEl proceso de obtenci√≥n de datos implica una planificaci√≥n cuidadosa. Exploraremos diversas metodolog√≠as utilizadas para recopilar datos, desde encuestas y experimentos hasta scraping de datos en l√≠nea. Cada metodolog√≠a tiene sus propias ventajas y desventajas, y es importante seleccionar la m√°s adecuada para los objetivos del an√°lisis. Discutiremos c√≥mo dise√±ar encuestas efectivas, c√≥mo considerar la √©tica en la recopilaci√≥n de datos y c√≥mo aprovechar las fuentes de datos existentes.\nEsta secci√≥n nos proporcionar√° una base s√≥lida para comprender c√≥mo adquirir y almacenar datos de manera efectiva y confiable. Una vez que comprendamos c√≥mo obtener datos de calidad, podremos avanzar con confianza en las etapas posteriores del proceso de an√°lisis, sabiendo que estamos trabajando con una base s√≥lida y confiable.\n\n\nDesaf√≠os y Consideraciones:\nA medida que ingresamos al emocionante mundo del an√°lisis de datos, nos encontramos con una serie de desaf√≠os y consideraciones que debemos abordar de manera efectiva para garantizar el √©xito de nuestro proyecto. Estos desaf√≠os abarcan desde la protecci√≥n de la privacidad de los datos hasta las complejidades de la limpieza y transformaci√≥n durante la etapa de preparaci√≥n.\n\n\n\nPrivacidad y Seguridad de los Datos:\nUno de los aspectos m√°s cr√≠ticos en el an√°lisis de datos es la privacidad y seguridad de la informaci√≥n. Los datos pueden contener informaci√≥n sensible y personal, y es esencial proteger la confidencialidad de las personas y organizaciones involucradas. Exploraremos pr√°cticas y regulaciones para garantizar que los datos se manejen de manera √©tica y legal. Discutiremos c√≥mo anonimizar los datos, utilizar t√©cnicas de enmascaramiento y seguir las mejores pr√°cticas para resguardar la privacidad de los individuos.\n\n\nLimpieza y Transformaci√≥n durante la Preparaci√≥n de Datos:\nLa etapa de preparaci√≥n de datos es crucial para asegurarse de que los datos sean aptos para el an√°lisis. Sin embargo, este proceso no est√° exento de desaf√≠os. Los datos pueden contener valores faltantes, duplicados y errores que deben abordarse de manera adecuada. Exploraremos t√©cnicas para identificar y manejar valores at√≠picos y faltantes, as√≠ como la importancia de la normalizaci√≥n y estandarizaci√≥n de los datos. Aprenderemos c√≥mo transformar los datos en un formato adecuado para el an√°lisis, incluida la reorganizaci√≥n de variables y la creaci√≥n de nuevas caracter√≠sticas. En resumen, enfrentamos una serie de desaf√≠os y consideraciones clave en nuestro viaje hacia el an√°lisis de datos significativo. Desde la protecci√≥n de la privacidad hasta la preparaci√≥n efectiva de los datos, abordar estos desaf√≠os de manera adecuada es esencial para garantizar que nuestras conclusiones sean s√≥lidas, confiables y √©ticas.\n\n\nReproducibilidad y Control de Versiones (GIT):\nKey ideas:\n\nUna documentacion detallada del analisis, de las desiciones tomadas.\n\nNotebooks pueden ser una buena herramienta inicial.\n\nImportancia de mantener un registro de los cambios en los datos.\nUso de sistemas de control de versiones como GIT para rastrear cambios.\nAplicaci√≥n de control de versiones en proyectos de preparaci√≥n de datos.\n\nLa reproducibilidad y el control de versiones son componentes fundamentales para garantizar la integridad y la transparencia en el an√°lisis de datos. Adem√°s de mantener un registro detallado de las decisiones tomadas durante el proceso, el uso de sistemas de control de versiones como GIT se vuelve esencial para mantener la trazabilidad y la colaboraci√≥n efectiva en proyectos de preparaci√≥n y an√°lisis de datos.\nDocumentaci√≥n Detallada del An√°lisis y Uso de Notebooks: Una documentaci√≥n exhaustiva del an√°lisis es esencial para comprender el flujo de trabajo, las decisiones tomadas y las transformaciones aplicadas a los datos. Los notebooks, como Jupyter Notebooks, ofrecen una herramienta excepcional para lograr esto. En cada celda de un notebook, es posible combinar explicaciones en lenguaje natural con c√≥digo ejecutable y visualizaciones. Esto permite registrar no solo el qu√© y el c√≥mo, sino tambi√©n el porqu√© detr√°s de cada paso.\nImportancia de Mantener un Registro de los Cambios en los Datos: Cada decisi√≥n tomada durante la preparaci√≥n y el an√°lisis de datos puede tener un impacto significativo en los resultados finales. Mantener un registro detallado de estas decisiones, desde la limpieza de datos hasta la creaci√≥n de variables derivadas, es crucial para comprender c√≥mo se obtuvieron ciertos resultados. Una documentaci√≥n precisa y detallada permite a otros analistas validar y replicar el an√°lisis en el futuro.\nUso de Sistemas de Control de Versiones como GIT para Rastrear Cambios:\nGIT, un sistema de control de versiones ampliamente utilizado, no solo se aplica al desarrollo de software, sino que tambi√©n es una herramienta poderosa en el an√°lisis de datos. Permite rastrear cada modificaci√≥n realizada en el c√≥digo y en los documentos, incluidos los notebooks. Cada cambio es registrado como un ‚Äúcommit‚Äù, lo que proporciona un historial completo y auditable de las transformaciones realizadas en los datos.\n\n\n\nUn esquema de git por Allison Horst @allison_horst\n\n\nAplicaci√≥n de Control de Versiones en Proyectos de Preparaci√≥n de Datos:* La aplicaci√≥n de GIT en proyectos de preparaci√≥n de datos agrega un nivel adicional de transparencia y colaboraci√≥n. Los repositorios de GIT almacenan no solo los datos originales, sino tambi√©n los notebooks y scripts utilizados en el proceso. Esto permite a los analistas colaborar en un entorno controlado y mantener un historial de cambios. En caso de que surjan problemas o se necesite retroceder en el tiempo, GIT ofrece la capacidad de volver a versiones anteriores de manera segura.\nLa combinaci√≥n de documentaci√≥n detallada a trav√©s de notebooks y el uso de sistemas de control de versiones como GIT proporciona una base s√≥lida para el an√°lisis de datos reproducible y transparente. Esto no solo facilita la comprensi√≥n y validaci√≥n de los resultados, sino que tambi√©n fomenta la colaboraci√≥n y la mejora continua en proyectos de preparaci√≥n y an√°lisis de datos.\n\n\n\n\n\n\nActividad de proyecto - Inicio reproducible\n\n\n\nVamos a empezar el proyecto, dando los primeros pasos considerando que sea reproducible y transparente.\nUno de los productos del proyecto es un notebook de reporte del an√°lisis. Para esto, iremos avanzando desde hoy.\n\nDefina a su grupo e inscribase.\nCree un repositorio de Github en el cual van a trabajar, agregue a todos los integrantes como colaboradores y a la profesora (usuario: melanieoyarzun)\nCree el readme listando a los integrantes del grupo.\nDefinan con que base de datos les gustar√≠a trabajar.\nPropongan una o dos preguntas de investigaci√≥n y las hipotesis que las responder√≠an.\n\nLa siguiente sesi√≥n, vamos a explorar los datos y empezar los primeros pasos en su an√°lisis."
  }
]