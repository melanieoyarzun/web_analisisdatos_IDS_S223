[
  {
    "objectID": "taller1_enunciado.html",
    "href": "taller1_enunciado.html",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "",
    "text": "El siguiete taller es un co\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\n\n\n\n\n\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\n\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df.head(5)\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n\n\n\n\n\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "taller1_enunciado.html#instrucciones",
    "href": "taller1_enunciado.html#instrucciones",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "",
    "text": "El siguiete taller es un co\n\n\nReplique el ejemplo práctico de importar datos desde la API del Banco Mundial y empezar la base para su análisis de series de tiempo.\nImporte usted la serie de GDP total Y Percapita para otro país serie desde la API del Banco mundial, muestre sus principales características y realice un grafico.\n¿pareciera haber tendencias?\n\n\n\nConsidere que tenemos los datos del banco mundial, del país que selecciono anteriormente, y desea aprender sobre alguna caracterpistica de dicho pais en el periodo.\nEscriba una pregunta de investigación que se pueda responder con los datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué hipótesis podria responder su pregunta?\n\n\n\n\n\nImaginemos que trabajamos en una empresa de e-commerce que vende productos electrónicos y queremos aumentar las ventas en una línea de productos específica, como teléfonos móviles.\nPara ello, decidimos utilizar una promoción de ventas basada en una ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.\nPara implementar la promoción, primero seleccionamos aleatoriamente un grupo de clientes y les enviamos un correo electrónico con un enlace a la ruleta lúdica. Al hacer clic en el enlace, los clientes son redirigidos a una página en la que pueden girar la ruleta y ganar un descuento en su próxima compra.\nVamos a pensar que los clientes son asignados a uno de los siguientes grupos: - Control: no les da una promoción (mala suerte, intentalo otra vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2: Un complemento gratuito (carcasa) que tiene un costo para la empresa similar al descuento.\n\n\n\nComo nuestro caso es un ejemplo ficticio, vamos a crear los datos.\nEste código creará un conjunto de datos con 400 observaciones (200 en el grupo de control y 200 en el grupo de tratamiento), donde se simulan lascompras de cada usuario.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Define una semilla para la generación de números aleatorios\nnp.random.seed(123)\nrandom.seed(123)\n\n# Crear un vector de 200 valores aleatorios para el grupo de control\ncontrol = np.random.choice([\"Control\"], size=200, replace=True)\n\n# Crear un vector de 200 valores aleatorios para el grupo de tratamiento\ntratamiento = np.random.choice([\"Treatment 1\", \"Treatment 2\"], size=100, replace=True, p=[0.7, 0.3])\n\n# Crear un vector de número de compras para cada grupo\ncontrol_compras = np.random.binomial(5, 0.2, size=200)\ntratamiento1_compras = np.random.binomial(5, 0.4, size=100)\ntratamiento2_compras = np.random.binomial(5, 0.6, size=100)\n\n# Combinar los vectores en un DataFrame\ndata = {\n    'grupo': np.concatenate((control, np.repeat(\"Treatment\", 200))),\n    'tipo_tratamiento': np.concatenate((np.repeat(\"Control\", 200), np.repeat([\"Treatment 1\", \"Treatment 2\"], [100, 100]))),\n    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))\n}\n\nventas_df = pd.DataFrame(data)\n\n# Verificar el DataFrame\nventas_df.head(5)\n\n\n\n\n\n\n\n\n\ngrupo\ntipo_tratamiento\nventas\n\n\n\n\n0\nControl\nControl\n1\n\n\n1\nControl\nControl\n1\n\n\n2\nControl\nControl\n0\n\n\n3\nControl\nControl\n0\n\n\n4\nControl\nControl\n0\n\n\n\n\n\n\n\nEstudiemos si la promoción fue efectiva en estos datos. Para esto:\n\nDescriba los resultados de la promocion para los diferentes grupos, en terminos de estadisticas descriptivas.\nCompare visualmente los resultados de los diferentes grupos.\n¿Fue la promocion efectiva? Use una prueba de hipotesis para analizar el grupo tratado y de control.\n¿Cual de las promociones fue más efectiva? Use una prueba ANOVA."
  },
  {
    "objectID": "taller1_enunciado.html#taller-1-pregunta-4---aplicación-datos-de-educación",
    "href": "taller1_enunciado.html#taller-1-pregunta-4---aplicación-datos-de-educación",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "Taller 1: Pregunta 4 - Aplicación datos de educación",
    "text": "Taller 1: Pregunta 4 - Aplicación datos de educación\nVamos a usar una situación ficticia, con datos simulados para poder aplicar de manera consisa los diferentes tipos de estrategias de identificación revisadas en clase. Algunas (que se supone ya manejan) las revisaremos muy rápidamente y en otras, vamos a tener mayor énfasis.\n\nPregunta de investigación\nNuestro objetivo es responder la siguiente pregunta ficticia de investigación:\n\nAsistir a cursos de verano mejora los resultados académicos?\n\nPara responder esta pregunta, usaremos unos datos ficticios y simulados\n\n\nContexto\nLa pregunta de investigación se inspira en trabajos como el de Matsudaira (2007) e intervenciones en estudiantes de bajo nivel socioeconómico por Dietrichson et al ( 2017).\nEl escenario ficticio es el siguiente:\n\nPara un conjunto de colegios en una comuna, existe la opción de asistir a un curso de verano intensivo durante el verano entre 5 y 6to básico.\nEl curso de verano se enfoca en mejorar las habilidades académicas de preparar la prueba de admisión a la universidad vigente (PSU en ese momento)\nEl curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.\nEstamos interesados en testear el impacto de la participación en el curso en los resultados académicos de los estudiantes.\n\n\n\nDatos ficticios dispobibles\nLos datos estan disponibles en\n\nschool_data_1.csv\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato csv.\nEste dataset tiene información sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si participó en el curso de verano, sexo, ingreso del hogar (en logaritmo), educación de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el año 5 como para el año 6.\n\n\nschool_data_2.dta\n\n\nUsamos esta data para ejemplificar como cargar data guardada en formato STATA.\nEste dataset tiene información de cada individuo (con identificador id).\nEste dataset tiene la información si el individuo recibió la carta de invitación para participar del curso de verano.\n\n\nschool_data_3.xlsx\n\n\nUsamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.\nEste dataset incluye datos sobre cada individuo (con identificador id)\nEste dataset tiene información de rendimiento académico antes y después del curso de verano."
  },
  {
    "objectID": "taller1_enunciado.html#objetivos",
    "href": "taller1_enunciado.html#objetivos",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "Objetivos:",
    "text": "Objetivos:\nLa idea de este taller es poner en práctica los primeros pasos para un análisis:\nI. Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada.\n\nTambién exploraremos los datos, usaremos estadísticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.\n\nEn la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre lso pasos anteriores múltiples veces. También invlocura tomar desiciones por parte de los investigadores, por lo cual la documentación de esta fase es especialmente importante.\nEn nuestro caso, será bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser así.\nPasos que debe realizar:\n\nPreparación de los datos\n\n1.1. Cargue las tres bases de datos\n1.2. Unir los datasets\nTenemos 3 bases de datos con información diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta información, especialmente si estimaremos modelos en base a ésta.\nLa base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.\n\nNotemos que el dataset unido debería tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de información del estudiante si recibió o no la carta (school_data_2)\n\nEntonces, siga el siguiente flujo de trabajo:\n\nUnimos school_data_1 y school_data_2 usando como variable de unión person_id y guardamos el dataset unido como school_data.\nUnimos school_data_3 con school_data y sobre-escribimos school_data.\nNotar que acá unimos por las columnas person_id y school_id. Esto no es realmente necesario porque cada estudiante con id única tiene un solo colegio, pero sirve de ejemplo en como usar más de una columna mediante.\nUsamos la función summary() para obtener una estadística descriptiva de las variables en el dataset unido.\nPreparar los datos\n\n2.1 Tidyng los datos\nAhora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de Tidy Data.\nUn data frame se considera “tidy” (Según Hadley) si se cumplen las siguientes condiciones:\n\nCada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.\nCada fila contiene medidas de la misma unidad de observación.\n\n(puedes profundizar y ver más ejemplos aplicados en https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html )\nUno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observación.\nSi inspeccionamos el número de columnas: Son 17, pero tenemos 9 variables.\nEs porque tenemos puntajes de las pruebas del año 2 al 10. Este tipo de datos son de panel\n\nGeneralmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y análisi preferimos que esté en formato long. Especialmente cuando modelamos incluyendo efectos fijos También es este el que adhiere a los principios tidy de mejor manera.\nPara cambiar a long, usamos melt()\nAhora tenemos nuestros datos listos para que los inspeccionemos.\n2.2 Selección de muestra\nYa que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada.\nEn este trabajo, los unicos problemas que podríamos enfrentar son relacionados con valores faltantes o missing.\nIdentifique cuantas filas y comunas son, los tipos de varables y número de missing values.\nEn estos casos, para parental_schooling tenemos 45 missing y para test_score 11.\nAsumamos que estos valores missing son random y deseamos remover estas filas.\n2.2 Modificar los datos\nUn último paso que haremos antes de hacer estadística decsriptiva es modificar los datos.\n\nCambio de nombre columna Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school.\nEstandarizar puntajes En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviación estándar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto específico de la medida y es más facil de comunicar y comparar.\n\nYa estamos bien, ahora pasamos a conocer mejor nuestros datos con estadística descriptiva."
  },
  {
    "objectID": "taller1_enunciado.html#estadística-descriptiva",
    "href": "taller1_enunciado.html#estadística-descriptiva",
    "title": "Taller 1: Introducción al Análisis de Regresión",
    "section": "3. Estadística descriptiva",
    "text": "3. Estadística descriptiva\nHasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estadísticas descriptivas y también algunos graficos descriptivos.\n3.1 Tablas de etsádistocas descriptivas Incluya la media, la desviación estandar, la mediana, max y min, al menos.\n3.2 Gráficos de estadística descriptiva Realice al menos scatter plot, histograma, box plot y correalograma. r1"
  }
]