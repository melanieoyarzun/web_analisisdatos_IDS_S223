---
institute: Magíster en Data Science - Universidad del Desarrollo
subtitle: 'Curso: Análisis de datos'
title: 'Taller 1: Introducción al Análisis de Regresión'
author: 'Phd (c) Melanie Oyarzún - [melanie.oyarzun@udd.cl](mailto:melanie.oyarzun@udd.cl)'
format:
  html:
    toc: true
    html-math-method: mathml
  ipynb: default
echo: true
code-fold: true
jupyter: python3
---

## Instrucciones:

Este taller es un conjunto de actividades para poner en práctica y consolidar los contenidos revisados en la primera unidad del curso, en las sesiones 1 y 2.

- Puedes realizarlo individualmente o hasta en grupos de 3 personas.
- La entrega es hasta el 15 de septiembre (para todo el crédito)
- Debes entregar un notebook en el que desarrolle las preguntas y no olvide concluir sus respuestas.
- Será evaluado mediante la rúbrica de evaluación disponible.

## Pregunta 1 - Bajando y formateando datos del Banco Mundial

Replique el ejemplo práctico de importar datos desde la API del Banco
Mundial y empezar la base para su análisis de series de tiempo.

Importe usted la serie de GDP total Y Percapita para otro país serie
desde la API del Banco mundial, muestre sus principales características
y realice un grafico.

¿Pareciera haber tendencias? Explique.


## Pregunta 2 - Investigando sobre países:

Considere que tenemos los datos del banco mundial, del país que
selecciono anteriormente, y desea aprender sobre alguna caracterpistica
de dicho pais en el periodo.

Escriba una pregunta de investigación que se pueda responder con los
datos disponibles. ¿Cómo definiria la variable aleatoria relevante? ¿Qué
hipótesis podria responder su pregunta?


## Pregunta 3 - Caso aplicación: Ejemplo AB test en Marketing:


### Enunciado

Imaginemos que trabajamos en una empresa de e-commerce que vende
productos electrónicos y queremos aumentar las ventas en una línea de
productos específica, como teléfonos móviles.

Para ello, decidimos utilizar una promoción de ventas basada en una
ruleta lúdica que ofrecerá descuentos a los clientes que la utilicen.

Para implementar la promoción, primero seleccionamos aleatoriamente un
grupo de clientes y les enviamos un correo electrónico con un enlace a
la ruleta lúdica. Al hacer clic en el enlace, los clientes son
redirigidos a una página en la que pueden girar la ruleta y ganar un
descuento en su próxima compra.

Vamos a pensar que los clientes son asignados a uno de los siguientes
grupos: - Control: no les da una promoción (mala suerte, intentalo otra
vez) - Tratamiento 1: 20% de descuento en el producto - Tratamiento 2:
Un complemento gratuito (carcasa) que tiene un costo para la empresa
similar al descuento.

### Creación de los datos

Como nuestro caso es un ejemplo ficticio, vamos a crear los datos.

Este código creará un conjunto de datos con 400 observaciones (200 en el
grupo de control y 200 en el grupo de tratamiento), donde se simulan
lascompras de cada usuario.

```{python}
import numpy as np
import pandas as pd
import random

# Define una semilla para la generación de números aleatorios
np.random.seed(123)
random.seed(123)

# Crear un vector de 200 valores aleatorios para el grupo de control
control = np.random.choice(["Control"], size=200, replace=True)

# Crear un vector de 200 valores aleatorios para el grupo de tratamiento
tratamiento = np.random.choice(["Treatment 1", "Treatment 2"], size=100, replace=True, p=[0.7, 0.3])

# Crear un vector de número de compras para cada grupo
control_compras = np.random.binomial(5, 0.2, size=200)
tratamiento1_compras = np.random.binomial(5, 0.4, size=100)
tratamiento2_compras = np.random.binomial(5, 0.6, size=100)

# Combinar los vectores en un DataFrame
data = {
    'grupo': np.concatenate((control, np.repeat("Treatment", 200))),
    'tipo_tratamiento': np.concatenate((np.repeat("Control", 200), np.repeat(["Treatment 1", "Treatment 2"], [100, 100]))),
    'ventas': np.concatenate((control_compras, tratamiento1_compras, tratamiento2_compras))
}

ventas_df = pd.DataFrame(data)

# Verificar el DataFrame
ventas_df.head(5)
```

### Preguntas: 

Estudiemos si la promoción fue efectiva en estos datos. Para esto:

1.  Describa los resultados de la promocion para los diferentes grupos,
    en terminos de estadisticas descriptivas.
2.  Compare visualmente los resultados de los diferentes grupos.
3.  ¿Fue la promocion efectiva? Use una prueba de hipotesis para
    analizar el grupo tratado y de control.
4.  ¿Cual de las promociones fue más efectiva? Use una prueba ANOVA.


## Pregunta 4 - Caso de aplicación datos de educación


### Pregunta de investigación

Nuestro objetivo es responder la siguiente pregunta **ficticia** de investigación:

> Asistir a cursos de verano mejora los resultados académicos?

Para responder esta pregunta, usaremos unos datos **ficticios y simulados**

### Contexto

La pregunta de investigación se inspira en trabajos como el de [Matsudaira (2007)](https://www.sciencedirect.com/science/article/pii/S0304407607001194?casa_token=hnnF764CKPoAAAAA:5b9WhCManNDsdW4SmOHnnzNr0fZIarW8s6EsvpQW7MdUt470eNPmN2T8IFCsNc6Iajew5tEeNA) e intervenciones en estudiantes de bajo nivel socioeconómico por [Dietrichson et al ( 2017)](https://journals.sagepub.com/doi/abs/10.3102/0034654316687036).

El **escenario ficticio** es el siguiente:

-   Para un conjunto de colegios en una comuna, existe la opción de asistir a un curso de verano intensivo durante el verano entre 5 y 6to básico.
-   El curso de verano se enfoca en mejorar las habilidades académicas de preparar la prueba de admisión a la universidad vigente (PSU en ese momento)
-   El curso de verano es gratuito, pero para ser matriculados requiere que los padres se involucren en un proceso.
-   Estamos interesados en testear el impacto de la participación en el curso en los resultados académicos de los estudiantes.

### Datos ficticios dispobibles

Los datos estan disponibles en [https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data](https://github.com/melanieoyarzun/web_analisisdatos_IDS_S223/tree/main/data)

1.  school_data_1.csv

-   Usamos esta data para ejemplificar como cargar data guardada en formato csv.
-   Este dataset tiene información sobre cada individuo (con identificador id), la escuela a la que asiste, un indicador si participó en el curso de verano, sexo, ingreso del hogar (en logaritmo), educación de los padres, resultados en una prueba estandarizada que se realiza a nivel de la comuna tanto para el año 5 como para el año 6.

2.  school_data_2.dta

-   Usamos esta data para ejemplificar como cargar data guardada en formato STATA.
-   Este dataset tiene información de cada individuo (con identificador id).
-   Este dataset tiene la información si el individuo recibió la carta de invitación para participar del curso de verano.

3.  school_data_3.xlsx

-   Usamos este dataset para practicar como cargar datos guardados en formato Microsoft Excel.
-   Este dataset incluye datos sobre cada individuo (con identificador id)
-   Este dataset tiene información de rendimiento académico antes y después del curso de verano.

### Objetivos:

La idea de este taller es poner en práctica los primeros pasos para un análisis:

**I.** Este proceso generalmente incluye cargarlos, inspeccionarlos, limpiar y dar la estructura deseada. 
**II.** También exploraremos los datos, usaremos estadísticas descriptivas que nos den una idea y resolver problemas potenciales (missing, ouliers, etc) antes de estimar cualquier modelo.

En la vida real, este proceso suele ser bastante largo, laborioso e implica volver sobre los pasos anteriores múltiples veces. También invlocura tomar desiciones por parte de los investigadores, por lo cual la documentación de esta fase es especialmente importante.

En nuestro caso, será bastante lineal y directo, ya que son datos ficticios simulados. Pero en la realidad, no suele ser así.

### Pasos que debe realizar:

#### 1. Preparación inicial de los datos


Tenemos 3 bases de datos con información diferente de los mismos individuos. Generalmente es buena idea tener una sola gran tabla con toda esta información, especialmente si estimaremos modelos en base a ésta.

La base de datos 1 y 2 tienen una forma similar: los individuos son filas y las variables columnas y hay una sola fila para cada individuo. Empiece uniendo ambos.

- Notemos que el dataset unido debería tener 3491 filas y 9 columnas. Unimos todas las filas y agregamos al dataset de información del estudiante si recibió o no la carta (school_data_2)


Entonces, siga el siguiente flujo de trabajo:

   1.  Unimos *school_data_1* y *school_data_2* usando como variable de unión *person_id* y guardamos el dataset unido como *school_data*.
   2.  Unimos *school_data_3* con *school_data* y sobre-escribimos *school_data*.

       Notar que acá unimos por las columnas *person_id* y *school_id*. Esto no es realmente necesario porque cada estudiante con id única tiene un solo colegio, pero sirve de ejemplo en como usar más de una columna mediante. 

   3. Usamos la función `summary()` para obtener una estadística descriptiva de las variables en el dataset unido.


#### 2.  Limpieza de los datos

2.1 Tidyng los datos

Ahora que hemos unido las bases de datos, trataremos de que satisfazgan los principios de [Tidy Data](https://vita.had.co.nz/papers/tidy-data.pdf).

Un data frame se considera "tidy" (Según Hadley) si se cumplen las siguientes condiciones:

-   Cada columna (variable) contiene todas las medidas de la misma data/feature/caracteristica de las observaciones.
-   Cada fila contiene medidas de la misma unidad de observación.

(puedes profundizar y ver más ejemplos aplicados en <https://sscc.wisc.edu/sscc/pubs/DWE/book/6-2-tidy-data.html> )

Uno de estos, es que cada columna debe ser una variable y cafa fila una unidad de observación.

Si inspeccionamos el número de columnas: Son 17, pero tenemos 9 variables.

Es porque tenemos puntajes de las pruebas del año 2 al 10. Este tipo de datos son de *panel*.

Generalmente, que hacemos con este tipo de datos depende del tipo de modelos que queramos usar. Si bien el formato wide es facil de entender, generlamente para modelos y análisi preferimos que esté en formato long. Especialmente cuando modelamos incluyendo efectos fijos También es este el que adhiere a los principios tidy de mejor manera.

Para cambiar a long, usamos pivot_longer. 

Ahora tenemos nuestros datos listos para que los inspeccionemos.

2.2 Selección de muestra

Ya que contamos con datos que siguen los principios de tidy data, lo siguiente es seleccionar la muestra apropiada. 

En este trabajo, los unicos problemas que podríamos enfrentar son relacionados con valores faltantes o missing. 

Identifique cuantas filas y comunas son, los tipos de varables y número de missing values. 

En estos casos, para *parental_schooling* tenemos 45 missing y para *test_score* 11. 

Asumamos que estos valores missing son random y deseamos remover estas filas. 


2.2 Modificar los datos

Un último paso que haremos antes de hacer estadística decsriptiva es modificar los datos.

1. Cambio de nombre columna
   Cambiaremos los nombres de algunas columnas para que se vean bien en la tablas. Vambos renombrar la variable summpercap a summer_school. 

2. Estandarizar puntajes
   En un siguiente paso, vamos a transformar los puntajes en la pruebas a una variable que tenga media 0 y desviación estándar 1. Es mejor trabajar con variables estandarizadas, ya que no requieren conocer el conexto específico de la medida y es más facil de comunicar y comparar.

Ya estamos bien, ahora pasamos a conocer mejor nuestros datos con estadística descriptiva.

#### 3. Estadística descriptiva

Hasta ahora, cargamos datos en diversos formatos (csv, dta y xlsx) los unimos, re-estructuramos el dataset, removimos valores missing y generamos algunas transformaciones. El siguiente paso es empezar a conocer nuestros datos. Para esto haremos tablas de estadísticas descriptivas y también algunos graficos descriptivos.

3.1 Tablas de etsádistocas descriptivas
   Incluya la media, la desviación estandar, la mediana, max y min, al menos.

3.2  Gráficos de estadística descriptiva
   Realice al menos scatter plot, histograma, box plot y correalograma.

